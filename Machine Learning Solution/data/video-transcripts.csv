video_id,datetime,title,transcript
qPN_XZcJf_s,2025-05-05T04:01:03.000000,"Reinforcement Learning with Human Feedback (RLHF), Clearly Explained!!!","If you tell me what you like and what you don't like, I'll use that to train my model. Stack Quest. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about reinforcement learning with human feedback and it's going to be clearly explained. This Stack Quest is brought to you by the letters A, B, and C. A always B Curious. Always B curious. Note before we begin, I want to mention that reinforcement learning with human feedback is almost always used to help train large language models like chat GPT and DeepSeek. So in order to provide context for understanding reinforcement learning with human feedback, we'll also talk about how to train a large language model from scratch. If you just want to learn about reinforcement learning with human feedback without the context, you can skip ahead. Either way, you need to already be familiar with decoderonly transformers and back propagation. If not, check out the quests or my book on AI. And you should also be familiar with the essential concepts of how reinforcement learning works with neural networks. If not, check out the quest. Now imagine we wanted to build and train the next chat GBT or DeepSeek from scratch so that we could ask it a really important question. What is Stat Quest? And it would generate a polite and helpful response. Stat Quest is an awesome YouTube channel for learning about AI. That means we need to learn how to train this untrained decoderonly transformer model. By untrained, we mean that all of the weights and biases in the model are just random initialization values. For example, if we gave this untrained model the prompt, what is stat quest? Then because the model is untrained, all it will generate is blah blah blah blah blah. Now, since blah blah blah blah blah is not a particularly polite response, and it certainly isn't helpful, we need to train the model. The first step in training a large language model is to train it to predict the next token in a really large body of text like the entire Wikipedia. For example, we could start with the article on ah and use the tokens for a was a Japanese to train the model to predict the next token girl using normal back propagation. And then we might use was a Japanese girl to train the model to predict the next token group. Likewise, we would just keep on using fragments from Wikipedia articles to train the model until we got to the end where we would use of horizontal points has been to train the model to predict the next token changed. Oh no, it's the dreaded terminology alert. Even though we are training the model to predict the next token in terms of large language models, this is actually called pre-training. Thus, by pre-training our untrained model to predict the next word with a huge body of text, we end up with a pre-trained model. Bam. Pre-training a large language model results in a model that is pretty good at taking bits of Wikipedia articles and predicting the next token. However, this isn't how anyone actually uses chat GPT or DeepSeek or whatever. For example, Squatch has never prompted a large language model with of horizontal points has been and expected to get the response changed. Instead, we usually enter prompts like, ""What is a good YouTube channel for learning about AI?"" and expect a polite and helpful response like, ""Stack Quest is a good channel to learn AI."" Bam. In other words, when we prompt a large language model with a question, we expect a response that is both polite and helpful. So our model which so far has only been pre-trained to predict the next token is said to be unaligned from how we actually want to use it. And that means that we need to align the pre-trained model to the way we actually expect to use it. Aligning a pre-trained model usually takes two steps. Supervised fine-tuning and reinforcement learning with human feedback. Supervised fine-tuning uses a data set with pairs of human created prompts and human created responses. For example, someone might write out a prompt like this. What is a good YouTube channel for learning about AI? And that same person or possibly somebody else might write out a response to that prompt like this. Stack Quest is a good channel to learn AI. Bam. And given a prompt paired with response, we could use normal back propagation to train the model to generate the response. As a result, supervised fine-tuning turns our unaligned pre-trained model into a model that has started to be aligned and can generate polite and helpful responses to specific natural language prompts. Note, because creating these prompts and responses is done by real people that need to be paid for their work and takes a long time, the data set for supervised fine-tuning is relatively small compared to the data set used for pre-training. As a result, supervised fine-tuning can easily train a model to overfit the data. In other words, our model might respond appropriately to this specific prompt, but it probably does not generalize well to new prompts that were not part of the supervised fine-tuning data set to begin with. So, after pre-training followed by supervised fine-tuning, we have a model that is somewhat aligned to the way we plan to use it. It can respond appropriately to prompts it was specifically trained on, but not to new prompts. Hey Josh, how do we train the model to respond to new prompts? Well, the ideal way to deal with overfitting is to just use a super huge fine-tuning data set. Unfortunately, creating a super huge fine-tuning data set would cost a super huge amount of money. So instead of spending a super huge amount of money on building a super huge fine-tuning data set, we can use reinforcement learning with human feedback to train our model to generate appropriate responses to prompts it wasn't initially trained on. Bam. Note, before we dive into the details of how reinforcement learning with human feedback can be done, I want to reiterate that the main idea is to create a much larger data set than we used for supervised fine-tuning while simultaneously minimizing the cost required to obtain human preferences in how the output should be polite and helpful. In other words, if it was super cheap to have people create a huge data set for supervised fine-tuning, then we would probably do that instead. So, with that said, the first step in understanding reinforcement learning with human feedback is to understand that given a specific prompt like this, what is a good YouTube channel for learning about AI? We can configure a model to always select the output with the highest value for each token and as a result generate a specific answer every single time like this. Stack Quest is a good channel to learn AI. Bam. Alternatively, we can use the outputs from the soft max function as the probabilities that each token is selected. And when we use the probabilities to select the output token, the token with the highest value will be selected more frequently than the other tokens, but the other tokens still have a chance at being selected. And as a result, we can get different outputs like this. Stack Quest is a great channel for learning AI, double bam, or this. Stack Quest is an awesome channel for learning AI, triple bam. So given these three generated responses to the original prompt, we can list all possible pairs of responses. And for each pair of responses, ask people to tell us which one they prefer. Asking people to list their preferences is much faster than asking people to write out the response that they think is best. And these preferences are the human feedback bit in reinforcement learning with human feedback. Bam. Now that we have the preference data, we can use it to train a model to give relatively high scores to the preferred options. Hey Josh, I'm not sure I understand what you just said. Can you give me an example? Yes. To train a model to give relatively high scores to preferred options, we first make a copy of our model that has already been through supervised fine-tuning. Then we modify the model by removing the unmbedding layer and replacing it with a single output and the result is a new model that we will train to calculate rewards for reinforcement learning. Now given the human feedback preference data that we collected earlier, we can train this model to generate a positive value or positive reward for the preferred response to the prompt. And we can train the model to generate a negative value or negative reward for the response to the prompt that was not preferred. Note to be clear, when we do this training, we pass the prompt and the response to the new model that calculates the rewards. The context of the prompt is needed because different prompts will need different responses in order to be polite and helpful. Anyway, one cool thing about this process is that we don't have to define the ideal output values or rewards in advance. In other words, the model can figure out the appropriate output values or rewards on its own. For example, this loss function, which was used by Open AI in their 2022 manuscript, training language models to follow instructions with human feedback, lets the model figure out the best rewards without us having to define them in advance. This loss function might look super fancy, but it's really just a few relatively simple things multiplied together. We'll start by focusing on this difference. The first term, reward better, corresponds to the reward the model calculates for the better response. The second term, reward worse, corresponds to the reward calculated for the worse response. Now, ideally, we'd like reward better to be a positive number, and we'd like reward worse to be a negative number. And thus, ideally, we want the difference between reward better and reward worse to give us a relatively large positive number. And that means ideally we want to plug a relatively large positive number into the sigmoid function. For any input value or x-axis coordinate, the sigmoid function gives us a y-axis coordinate between 0 and 1 for this sshaped curve. For example, if we plug in a relatively large positive number like 10, then the output or corresponding y-axis coordinate is very close to one. In other words, if in an ideal situation, reward better is positive and reward worse is negative, then the difference will be a relatively large positive number. And the sigmoid function should give us a value close to one. In contrast, if we plug in a relatively large negative number like -10, then the output or corresponding y-axis coordinate is very close to zero. The output from the sigmoid function values from 0 to 1 are then plugged into the log function. And plugging in xaxis values from 0 to 1 into the log function gives us the corresponding y-axis coordinates for this curved shape. Now remember ideally the difference between reward better and reward worse will be a relatively large positive number and thus the output from the sigmoid function should be close to one. So in an ideal situation the log function will output a relatively high value. However, if we use gradient descent to optimize the parameters in our model, then gradient descent, which tries to find the lowest point on a curve, we'll try to find this lowest point. And that low point is the opposite of what we want. So, since we want to converge on this high point, we flip the curve over by multiplying everything by -1. And now without having to tell the model that we want reward better to be positive and we want reward worse to be negative, we have a loss function that can train this model to calculate the rewards correctly. Double bam. Once we're done training this new model, which is called the reward model, we can use it to train the original model that only had supervised fine-tuning. To train the original model with the reward model, we start with new prompts that were not part of the supervised fine-tuning data set and we don't have humanprovided responses for like this. What's another good YouTube channel for learning about AI and have the original model generate a response like this. YouTube is special. Now we run the prompt and generated response through the reward model to calculate a reward. And because the response is not helpful, it will get a negative reward from the reward model. And we can use the reward model to train the original model. Then after training with reinforcement learning, our prompt, what's another good YouTube channel for learning about AI? results in a polite and helpful response. Serrano Academy is a great YouTube channel for AI. And because this response is both helpful and polite, it gets a relatively large and positive reward. In other words, by training a reward model to score responses like a human would, we can use the reward model to train the original model to generate polite and helpful responses to new prompts without requiring us to create a huge and super expensive supervised fine-tuning data set to avoid overfitting. Bam. Once we have finished training the original model with reinforcement learning with human feedback using our reward model, we end up with the final model that is both trained and aligned to the way people actually want to use it. Now when we give our model the prompt, what is stack quest? Instead of just getting blah blah blah blah blah as output, we get Stat Quest with Josh Starmer is an awesome YouTube channel for learning about statistics, machine learning, and AI. Triple BAM. And now it's time for some shameless self-promotion. If you want to review statistics, machine learning, and AI offline, check out the Stat Quest PDF study guides, and my books, the StatQuest Illustrated Guide to Machine Learning and The StatQuest Illustrated Guide to Neural Networks and AI at stackquest.org. There's something for everyone. Hooray! We've made it to the end of another exciting Stack Quest. If you like this Stack Quest and want to see more, please subscribe. And if you want to support Stack Quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. All right, until next time. Quest on."
DVGmsnxB2UQ,2025-04-14T04:00:27.000000,Reinforcement Learning with Neural Networks: Mathematical Details,if you make a guess and you make the wrong guess then multiply it by a negative number and correct the mistake stack Quest bam hello I'm Josh starmer and welcome to stack Quest today we're going to talk about reinforcement learning with neural networks and this time we're going to focus on the mathematical details this stag Quest is brought to you by the letters a b and c a always b b c curious always be curious note this stack Quest assumes that you are already familiar with gradient descent and the essential concepts of how reinforcement learning can be added to a neural network if not check out the quests in the STA Quest reinforcement learning with neural networks essential Concepts there were two places we could go to get a french fry snack squatch's Fred Shack and Norm's fry Hut and we used a neural network to help us decide where to go based on how hungry we were however since we didn't know in advance if Squatch or Norm was going to give us a large or small order of fries we couldn't train the neural network with standard back propagation instead we trained the neural network with reinforcement learning specifically we showed how the PO policy gradients method can train the neural network now we're going to go through the mathematical details and show every single step required to train this neural network specifically we're going to show all of the details required to train this bias B with policy gradients so the first thing we do is run an input value through the neural network to calculate an output value we'll start with 0.0 as the input which means we're not hungry now we just do the math and the probability that we will go to Norms P Norm is 0.5 and that means that the corresponding probability that we will go to squatches P Squatch is 1 minus P Norm which equals 0.5 in other words we can draw a line that is 0.5 units long to represent the probability of going to squatches and then append another line that is 0.5 units long to represent the probability of going to Norms and we end up with a line that goes from 0 to 1 now to decide which place we will visit for a snack we pick a random number between 0o and one in this example we randomly pick the number 0.2 and because 0.2 is is in the region that represents squatch's fry Shack we'll go there now comes the part where we make a guess if we make a guess that the correct thing to do is to go to squatches then ideally we would want the probability of going to squatches P Squatch to be one and that means we can quantify the difference between the ideal value for p Squatch 1.0 that is based on the guess that we should go to squatches and 0.5 the value for p Squatch based on the output from the neural network now because the values for p Squatch are probabilities we will use cross entropy to quantify their differences note there's a lot to be said about cross entropy and if you're interested in the details you can check out these quests or my book on AI however for this example the only thing you really need to know is that cross entropy is the negative log base e of the probability we go to either Norms or squatches now remember the goal of this example is to optimize the bias so just like we do for normal back propagation the first thing we do is calculate the derivative of the Cross entropy with respect to the bias we want to optimize note before we get started I want to be clear that we are calculating in the cross entropy for going to squatch's fry Shack so to help us remember what's going on we'll let C Squatch refer to the Cross entropy for visiting Squatch now the cross entropy for visiting Squatch C Squatch is the negative log of P Squatch because we went to squatches however because the output from the neural network is the probability of going to Norms P Norm we rewrite the cross entropy equation in terms of P Norm because P Squatch = 1 minus P Norm P Norm in turn is the Y AIS output value from the sigmoid activation function and the output from the sigmoid activation function depends on its input the x-axis coordinate x and x comes from multiplying the input value hunger by the weight and then adding the bias now because the cross entropy is connected to the sigmoid activation function by P norm and the sigmoid activation function is connected to the bias by X we can solve for the derivative of the Cross entropy for visiting Squatch with respect to the bias by using the chain rule the chain rule tells us that the derivative of the Cross entropy for visiting Squatch with respect to the bias is the derivative of the Cross entropy for visiting Squatch with respect to the output from the neural network P Norm times the derivative of P Norm with respect to X the xais input value to the sigmoid activation function times the derivative of x with respect to the bias we'll start by working out the derivative of the cross entropy for visiting Squatch with respect to the output from the neural network P Norm now because the cross entropy for visiting Squatch is equal to the negative log of 1us P Norm the first thing we do is plug in the negative log of 1us P Norm for the cross entropy now to solve for this derivative we have to use the chain rule because p Norm the thing we want to take the derivative with respect to is inside the log function so to apply the chain rule we create a new variable called stuff and let stuff equal the stuff inside the log function oneus P norm and that means we are solving for the derivative of the negative log of stuff with respect to P Norm the chain rule tells us that the derivative of the negative log of Stu with respect to P Norm is the derivative of the negative log of stuff with respect to stuff times the derivative of stuff with respect to P Norm now because the derivative of the log of x with respect to X is 1 / X the derivative of the negative log of stuff with respect to stuff is -1 / stuff the next term the derivative of stuff with respect to P Norm is -1 so we multiply the first term by -1 multiplying -1 by -1 gives us 1 ided by stuff lastly we plug in 1 minus P Norm for stuff and that means the derivative of the Cross entropy for visiting Squatch with respect to the output from the neural network P Norm is equal to 1 / 1 - P Norm so we plug that into the equation bam now we need to figure out the derivative of P Norm with respect to X the xais coordinate that is used as input to the sigmoid activation function so the first thing we do is plug in the equation for the sigmoid activation function then we rewrite the fraction as the denominator to the -1 power now because X the thing we want to take the derivative with respect to is inside parentheses we can use the chain rule to solve for the derivative so just like before we Define stuff to be the stuff inside the parentheses then we rewrite the equation with stuff the chain rule then converts this relatively hard to solve derivative into the product of these two easier to solve derivatives for the first term we use the power rule to get -1 * stuff raised to the -2 power and the derivative of the second term is e raised to thex power now we just multiply both terms together and plug in 1 + e raised to thex power power for stuff now technically this is the derivative of the output of the sigmoid activation function P Norm with respect to its input X however you never see it like that ever instead we split the square in the denominator into two terms multiplied together then we put + one and minus one which add up to zero in the numerator of the second term we then rewrite the second term as the difference of two fractions lastly the first term is the same as the equation for the sigmoid activation function so we swap the fraction with the function and the first part of the second term is equal to one and the second part is the same as the sigmoid activation function and this is the derivative of the sigmoid activation function that you'll see pretty much everywhere so we plug that into the equation bam now we need to figure out the derivative of x the input to the sigmoid activation function with respect to the bias so the first thing we do is plug in the equation for x and the derivative with respect to the bias is one so we plug that into the equation bam lastly because P Norm is the output from the sigmoid activation function we can replace sigmoid X in the derivative with P Norm hooray we finally calculated the derivative of the Cross entropy for visiting Squatch with respect to the bias bam now let's calculate the value of the derivative so we plug in P Norm the output from the neural network 0.5 and when we do the math we get 0.5 and thus the slope of the tangent line for the current bias value is 0.5 note if we plugged this derivative directly into gradient descent it would shift the value of the bias to the left however if you remember this derivative was based on the guess that going to squatches for fries was a good idea and our guess might not be correct and if our guess is not correct then instead of Shifting the value for the bias to the left we actually need to shift the value for the bias to the right in other words depending on whether our guess is correct or not we either need to shift the value for the bias to the left or the right the good news is that all we need to do to determine how to update the bias is order some fries from Squatch in this example when we order some fries we get this relatively small serving however this is okay because our hunger level is 0.0 meaning we're not hungry at all so it was actually a good thing that Squatch ate some of our fries in other words getting a small order of fries when we are not hungry means we made the correct guess and since we made the correct guess we set the reward to 1.0 in contrast if Squatch had given us a large order of fra then we wouldn't be hungry enough to eat all of the fries and that means we made the wrong guess so in that case we would set the reward to -1 note we can use different values for the reward and we talked about that in the stat Quest reinforcement learning with neural networks essential Concepts anyway going back to when we made the correct guess and the reward was 1.0 now we multiply the derivative by the reward to get an updated derivative that will point us in the correct direction in this example we were not hungry and Squatch gave us a small order of fries so we made the correct guess and the reward equals 1.0 and multiplying the derivative 0.5 by the reward 1.0 means the updated derivative is the same as the original derivative and the updated derivative tells us to do the same thing as before and shift the value for the bias to the left in other words when we make the correct guess to begin with then the derivative that is based on that guess points Us in the correct Direction note if we guessed wrong and Squatch gave us a ton of fries even though we were not hungry then multiplying the derivative by a negative reward makes it so that the updated derivative 0.5 points in the opposite direction of the original derivative and instead of Shifting the value for the bias to the left we will shift it to the right in other words when we make an incorrect guess to begin with then the derivative that is based on that guess points Us in the wrong direction and multiplying it by a negative reward corrects that error now going back to the original correct guess we then plug the updated derivative into gradient descent to calculate the step size now in this Example The Learning rate is set to 1.0 so the equation ends up being 1 0 * the updated derivative 0.5 and so the step size is equal to 0.5 then we calculate the new value for the bias by subtracting the step size 0.5 from the old bias value 0.0 to get the new bias value 0.5 now we plug the new bias term into our neural network now that we have updated the neural network we have to get some more fries so we can keep training the bias so we plug in our hunger level 0.0 and when we do the math the probability that we will go to Norms is now 0.4 that means that the probability we go to squatches 1 minus P Norm has increased to 0.6 and the probability that we go to Norms has decreased to 0.4 remember the last time our hunger was 0.0 we went to squatches and we got what we wanted so it makes sense that given the same hunger as before we now have a higher probability of going to squatches now we pick a random number between 0 and 1 and we get 0.9 and because 0.9 is in the region that represents Norm's fry Hut that's where we'll go now we make the guess that going to Norms when we are not hungry is the best thing to do and if going to Norms is the right thing to do then ideally the probability of visiting Norms should be 1.0 and that means we can quantify the difference between the output and our guas with cross entropy thus the first part of updating the bias is to calculate the derivative of the Cross entropy with respect to the bias note before we get started I want to be clear that we are calculating the cross entropy for going to Norms fry Hut so to help us remember what's going on we'll let C Norm refer to the Cross entropy for visiting Norm now the good news is calculating the derivative of the Cross entropy for visiting Norms is almost exactly the same as the derivative we calculated earlier for the cross entropy for visiting Squad watches specifically the only difference is that the cross entropy for visiting Norm C Norm is the negative log of P Norm instead of the negative log of P Squatch and remember P Norm is just the output from the neural network and just like we saw before P Norm is the Y access coordinate that comes from the output of the sigmoid activation function and the out output from the sigmoid activation function depends on its input the x-axis coordinate x and x comes from multiplying the input value hunger by the weight and then adding the bias so just like before because the cross entropy is connected to the sigmoid activation function by P norm and the sigmoid activation function is connected to the bias by X the chain rule tells us that the derivative of the Cross entropy for visiting Norm with respect to the bias is the derivative of the Cross entropy for visiting Norm with respect to the output from the neural network P Norm times the derivative of P Norm with respect to x times the derivative of x with respect to the bias first we'll calculate the derivative of the Cross entropy for going to Norms with respect to P norm and we get -1 / P Norm so we plug that into the equation bam everything else is the exact same as when we calculated the derivative of the Cross entropy for going to squatches and thus this is the derivative of the Cross entropy for visiting Norms with respect to the bias now we just plug in the value for p Norm 0.4 and when we do the math we get 0.6 and thus the slope of the tangent line for the current bias value is 0.6 now we need to determine if we made the correct guess so that we know if we should shift the value for the bias to the left or the right so we order some fries and Norm gives us a huge pile of them normally this would be awesome unfortunately we we aren't hungry so that means we made the wrong guess and the reward equals -1 we then multiply the derivative by the reward to get the updated derivative 0.6 note because we made the wrong guess the original derivative which was based on that guess was pointing in the wrong direction so by multiplying the original derivative by a negative number we flip the direction and correct the mistake this is super important so let me repeat it when we multiply a derivative that is based on an incorrect guess by a negative reward we flip the direction and correct the mistake bam we then plug the updated derivative into gradient descent to calculate the step size and remember in this Example The Learning rate is set to 1.0 so the step size ends up being 0.6 then we calculate the new value for the bias by subtracting the step size 0.6 from the old bias value 0.5 to get the new bias value -1.1 now we plug the new bias term into our neural network now if we plug 0 .0 which means we are not hungry into the neural network the probability that we will go to Norms is even smaller than it was before now it's 0.2 and it makes sense that the probability of going to Norms is getting smaller because so far each time we've plugged 0.0 into the input we should have gone to squatches instead of norms and that means that the reinforcement learning algorithm we're using ordinary policy gradients is working double bam note so far we've only plugged 0.0 for Hunger into the neural network however in order to fully train the model we have to use values from 0 to one as inputs to the neural network and after doing lots of updates just like we did earlier using the same derivative equations and updating them with the rewards but now using all kinds of input values between 0er and one eventually the value for the bias starts to hover around -10 and that means we're done training triple bam note in this stat Quest We simply Define the rewards to be -1 and one however out in the wild you'll probably see much more elaborate ways to determine the reward that that said regardless of how fancy the reward function is its purpose is the same to update and possibly correct the derivative that we calculated based on our guess now it's time for some Shameless self-promotion if you want to review statistics machine learning and AI offline check out the stack Quest PDF study guides and my books the stat Quest Illustrated guide to machine learning and the stat Quest Illustrated guide to neural networks and AI at stack quest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stat Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
9hbQieQh7-o,2025-04-07T04:00:17.000000,Reinforcement Learning with Neural Networks: Essential Concepts,"When you don't know, take a guess and then see what happens and update your guess after that. Stack Quest. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about reinforcement learning with neural networks and we're going to cover the essential concepts. This stack quest is brought to you by the letters A, B, and C. A always B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B C Curious. Always B curious. Note, this stack quest assumes that you are already familiar with the main ideas of how neural networks work as well as the basics of back propagation. If not, check out the quests or my book on AI. Now, imagine it was snack time and we had to choose between going to Squatch's Fry Shack or Norm's Fry Hut for a delicious French fry snack. There are a lot of ways we could pick one of the two places for a snack. For example, we could just flip a coin. If we got heads, we could go to squatches. And if we got tails, we could go to norms. However, just flipping a coin wouldn't take into account how hungry we were. And some days we might be more hungry than others. Also, flipping a coin doesn't take into account how many fries we get at each place. For example, we might usually get a lot of fries at norms. And that would be great if we were really hungry, but maybe less great if we were not so hungry. It's also possible that Norm might give us a skimpy order of fries. And if we were really hungry, that would be a total bummer. In contrast, if we were not very hungry, it might not be so bad. Likewise, squatch might serve us a skimpy order of fries or a large one. And depending on how hungry we were, each order could be good or bad. So, it would be nice to have a way to decide which place to go to for a snack that took our hunger into account, as well as the possibility that we could get a large or skimpy order of fries at each place. To solve this problem, we're going to use a neural network that takes our hunger as input and gives us the probability that we will go to norms porm as the output. The only problem is that we need to train this one bias. Now, usually when we want to train a neural network, we would start with a training data set like this that has possible input values paired with their ideal output values. With this type of training data, we could train our neural network using standard back propagation. However, in this example, we don't know in advance if norm or squatch will serve us a large or small order of fries. And that means we don't know in advance what the output value should be. Josh, if we don't know what the output value should be, how can we train our neural network? When we don't have known output values in a training data set, we can still train our model using reinforcement learning. There are lots of different reinforcement learning algorithms for training neural networks, but in this video, we'll focus on one called policy gradients. BAM note, the goal of this stack quest is to provide you with a solid understanding of the essential concepts of how policy gradients work. That said, if you want to see all of the nitty-gritty mathematical details worked out step by step, check out the follow-up quest. Now, before we dive into reinforcement learning and policy gradients, I want to do a quick review of how traditional back propagation works so we can understand its limitations and see how policy gradients overcome them. So for now, let's just assume that we have this training data and know the desired outputs in advance. Given this data, we could run the input values through the neural network one at a time and quantify the differences between the output from the neural network and the ideal output values. Then if we wanted to, we could calculate these differences for different values for the bias and draw a graph that shows how the differences change with different bias values. Then using a value for the bias, we could calculate the derivative of the differences with respect to the bias. And because the derivative the slope of this green line is negative, we would know that the value for the bias that minimizes the differences and thus results in a better fitting model is larger than the current value. In other words, when the derivative the slope of this green line is negative, then we need to shift the value for the bias to the right on the xaxis. Alternatively, if we use this value for the bias to calculate the derivative, then the derivative, the slope of this green line would be positive, telling us that the optimal value is less than the current value for the bias. In other words, when the derivative, the slope of this green line is positive, then we need to shift the value for the bias to the left on the xaxis. In summary, when the derivative is negative, we need to increase the value for the bias by shifting it to the right. And when the derivative is positive, we need to decrease the value for the bias by shifting it to the left. Lastly, the derivatives can correctly tell us which direction to shift the bias because the training data contains the ideal output values. So this is how back propagation normally works. Bam. However, our problem is that we don't know in advance where we can consistently get a small or large order of fries. And that means our data set can't have ideal output values. And without the ideal output values, we can't calculate the differences between them and the outputs from the neural network. And without those differences, we can't calculate these derivatives. And without the derivatives, we don't know if we should shift the bias value to the right or to the left. Josh, what are we going to do? Well, it turns out we don't actually need to know the ideal output values in advance to calculate the derivatives. Instead, we can guess what the ideal values should be and use those guesses to calculate the derivatives. Bam. Yes. Now, let's work through our example and see how reinforcement learning and policy gradients can use guessing to make this work. We'll start by assuming we are not hungry. And that means we'll plug 0.0 into the neural network. Now, we just do the math. doop and the probability that we'll go to norms P norm is 0.5 and that means that the corresponding probability that we will go to squatches P squatch is 1 minus P norm which equals 0.5. In other words, we can draw a line that is 0.5 units long to represent the probability of going to squatches and then append another line that is 0.5 units long to represent the probability of going to norms. And we end up with a line that goes from 0 to 1. Now to decide which place we will visit for a snack, we pick a random number between 0 and 1. In this example, we randomly pick the number 0.2. And because 0.2 is in the region that represents Squatch's fry shack, we'll go there. Now comes the part where we make a guess. If we make a guess that the correct thing to do is go to Squatches when our hunger is 0.0, then ideally we would want the probability of going to Squatches P squatch to be one. And that means that ideally we'd want the probability of going to norms P norm to be zero. Now given these ideal values that are based on our guess, we can quantify the difference between the ideal value for P squatch 1.0 and the actual value for P squatch 0.5 that's based on the output from the neural network. Note in the follow-up stat quest adding reinforcement learning to neural networks mathematical details will show exactly how we quantify this difference. Spoiler alert, we'll use cross entropy. For now, all you need to know is that we can quantify the difference between the P squatch derived from the guess 1.0 O and the P squatch derived from the output from the neural network 0.5. And that means we can calculate the derivative of the difference with respect to the bias that we want to optimize. Bam. How exactly do we calculate this derivative? That's a great question, Squatch, and we'll go through it step by step in the follow-up stat quest. For now, the most important thing is to know that the guess makes it possible. Anyway, given our guess that going to Squatch's place is the best thing to do, the derivative ends up being 0.5. And if that guess is correct, then the positive value for the derivative tells us to decrease the value for the bias and shift it to the left. On the other hand, if our guess is incorrect and we should have gone to norms instead, then we'll want to do the exact opposite of what the derivative tells us to do. In other words, instead of decreasing the value for the bias, we want to increase the value for the bias and shift it to the right. But Josh, how will we know if the guess is correct or not? In other words, how do we know if we should shift the bias to the left or the right? The answer is simple, squatch, we just order some fries. Bam. In this example, when we order some fries, we get this relatively small serving. Hey, Squatch, how come our order of fries is so small? I couldn't resist and I ate some of your fries. Well, this time I guess that's okay. It's okay because our hunger level is 0.0, meaning we're not hungry at all. So, it was actually a good thing that Squatch ate some of our fries. In other words, getting a small order of fries when we're not hungry means we made the correct guess. And since we made the correct guess, we set something called reward to 1.0. In contrast, if Squatch had given us a large order of fries, then we wouldn't be hungry enough to eat all of the fries, and that means we made the wrong guess. So, in this case, we would set the reward to negative one. Note, any positive reward will work for a correct guess, and any negative reward will work for an incorrect guess. And we'll go through an example with different values in just a bit. But for now, let's just set the reward to 1.0 for a correct guess and negative 1.0 for an incorrect guess. Okay, I get that a positive or negative reward reflects if our guess is correct or not. But how does that help us interpret the derivative that we calculated earlier? We simply multiply the derivative by the reward to get an updated derivative that points us in the correct direction. Bam. In this example, we were not hungry and squatch gave us a small order of fries. So, we made the correct guess and the reward equals 1.0. And multiplying the derivative 0.5 by the reward 1.0 we know means the updated derivative is the same as the original derivative and the positive updated derivative tells us to shift the bias to the left just like the original derivative. In other words, when we make the correct guess to begin with, then the derivative that is based on that guess points us in the correct direction. In contrast, if we were not hungry, but Squatch gave us a large order of fries, then we would have made the wrong guess, and the derivative would be telling us to do the opposite of what we should do. However, multiplying the derivative by a negative reward for making the wrong guess would flip things around and then the updated derivative would correctly tell us to shift the bias to the right. In other words, if we made the wrong guess to begin with, then multiplying the derivative, which is based on that guess, by a negative number will flip things around and point it in the correct direction. This is super important, so let me repeat it. When we multiply a derivative that is based on an incorrect guess by a negative reward, we flip the direction and correct the mistake. Bam. In summary, when we multiply the derivative by the reward, we end up with an updated derivative that tells us how to modify the bias correctly. Bam. Note, before we move on, I want to reiterate that the rewards don't have to be one and negative one. For example, if squatch didn't give us any fries at all, then we could set the reward to two org -2 depending on how hungry we were. If we were not hungry, then the reward equals 2.0. And the updated derivative will be twice what it was before, but still pointing in the original direction. And so the updated derivative would tell us to take a larger step in the same direction as before. In contrast, if we were hungry and we didn't get any fries, then the reward equals -2.0 and the updated derivative would tell us to take a larger step in the opposite direction of the original derivative. Bam. Now, let's go back to our example where we were not hungry and squatch gave us a small order of fries. So, our guess was correct and the reward equal 1.0. When we multiply the derivative 0.5 by the reward 1.0, we get the updated derivative 0.5. We then plug the updated derivative into gradient descent to calculate the step size. Now, in this example, the learning rate is set to 1.0. So, the equation ends up being 1.0 * the updated derivative 0.5. And so the step size is equal to 0.5. Then we calculate the new value for the bias by subtracting the step size 0.5 from the old bias value 0.0 to get the new bias value 0.5. Now we plug the new bias term into our neural network. Bam. Now that we have updated the bias, we have to get some more fries so we can keep optimizing it. So we plug in our hunger level 0.0. And when we do the math, the probability that we will go to norms is now 0.4. And that means that the probability we go to squatches 1 minus p norm has increased to 0.6. And the probability that we go to norms has decreased to 0.4. 4. Remember, the last time our hunger was 0.0, we went to squatches and we got what we wanted. So, it makes sense that given the same hunger as before, we now have a higher probability of going to squatches. Now, we pick a random number between 0 and 1, and we get 0.9. And because 0.9 is in the region that represents Norm's fry hut, that's where we'll go. Now, in order to update this bias, we have to calculate the derivative. And in order to calculate the derivative, we have to make a guess that going to norms was the right thing to do. And if going to norms is the right thing to do, then ideally the probability of visiting norms should be 1.0. Making the guess allows us to quantify the difference between the output from the neural network 0.4 and the ideal value 1.0. And with that difference we can calculate the derivative with respect to the bias. This time when we calculate the derivative we get0.6. Now we need to determine if we made the correct guess so that we know if we should shift the bias to the left or the right. So we order some fries and norm gives us a huge pile of them. Normally this would be awesome. Unfortunately we aren't hungry. And since we are not hungry the fact that Norm gave us a huge pile of fries means we made the wrong guess. So the reward equals -1. Now we multiply the derivative by the reward to get the updated derivative 0.6. Note the updated derivative 0.6 is positive. So that means we need to decrease the value of the bias some more. We then plug the updated derivative into gradient descent to calculate the step size. And remember the learning rate is set to 1.0. 0. So the step size ends up being 0.6. Then we calculate the new value for the bias by subtracting the step size 0.6 from the old bias value.5 to get the new bias value -1.1. And now we plug the new bias term into our neural network. Bam. Now if we plug 0.0 zero, which means we are not hungry into the neural network. The probability that we will go to norms is even smaller than it was before. Now it's 0.2. And it makes sense that the probability of going to norms is getting smaller because so far each time we've plugged 0.0 into the input, we should have gone to squatches instead of norms. And that means that the reinforcement learning algorithm we're using, ordinary policy gradients, is working. Double bam. Note, so far we've only plugged 0.0 for hunger into the neural network. However, in order to fully train the model, we have to use values from 0 to 1 as inputs to the neural network. And after doing lots of updates using all kinds of input values between 0 and 1, the value for the bias starts to hover around -10. And the fact that we are hovering around some value means we're done training. Now when we are not hungry and the input is 0.0, the probability of going to norms is zero. And that means when we're not hungry, we'll always go to squatches. That's because more often than not, I eat most of your fries. So when you're not hungry and don't want many fries, come to my shack. In contrast, when we are hungry and the input value is 1.0, the probability of going to norms is one. And that means when we are hungry, we'll always go to norms. And that's because more often than not, I don't eat your fries. So when you're hungry and want a lot of fries, come to my hut. Triple bam. In summary, reinforcement learning allows us to optimize a neural network when we don't have known outputs or target values in advance. We start by using the neural network to decide where to go. Then we make the guess that wherever we ended up going was the right place to go. For example, if we went to Norms, we would guess that going to Norms was the best option, even though we might later discover that we should have gone to Squatches. Once we've made that guess, we use it to calculate the derivative with respect to the parameter we want to optimize. We then determine the reward associated with that guess and we multiply the derivative by the reward to correct for any error that we might have made when we made the original guess. And that gives us the updated derivative. And we plug the updated derivative into gradient descent to optimize the neural network. Bam. Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StackQuest PDF study guides and my books, the Stack Rest Illustrated Guide to Machine Learning and the StackQuest Illustrated Guide to Neural Networks and AI at stackquest.org. There's something for everyone. Hooray! We've made it to the end of another exciting Stack Quest. If you like this Stack Quest and want to see more, please subscribe. And if you want to support Stat Quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. All right, until next time. Quest on."
Z-T0iJEXiwM,2025-03-31T04:00:25.000000,Reinforcement Learning: Essential Concepts,reinforcement learning it's just like me and you we learn and adapt from experience yes we do stat Quest bam hello I'm Josh starmer and welcome to stack Quest today we're going to talk about reinforcement learning and we're going to cover the essential Concepts this stag Quest is brought to you by the letters a b and c a all always B Be C curious always be curious imagine we were hungry and wanted to eat a snack and had to choose between going to squatch's fry Shack and Norm's fry Hut the problem is that we've never been to either place so how do we decide which one to go to specifically we'd like to know which place will do a better job giving us a satisfying order of fries to solve this problem we're going to use reinforcement learning reinforcement learning is a methodology that basically lets computers learn and adapt based on experience and it's used in all kinds of situations for example reinforcement learning has been used to help computers become better at playing games like Checkers and go and to help cars drive by themselves and it's even been used to make chat gbt sound more human when it responds to your prompts that said in this stat Quest we're going to learn how it can help us decide where to eat fries note what follows is just one example of how reinforcement learning can be done and the purpose is to focus on the main ideas of how this method works and for now we're going to ignore the copious amounts of terminology of associated with this field but don't worry we'll dive into the terminology after this example so with that said now let's see one way that reinforcement learning can help us decide where we should go eat fries since we've never been to either squatch's fry Shack or Norm's fry Hut we'll start with each Place having an equal probability that we will go there in other words we'll start with the probability of picking watches fry Shack for a delicious snack P Squatch set to 0.5 and the probability of picking Norm's fry Hut for a tasty treat P Norm set to 0.5 now to visualize how we will choose a restaurant let's draw a line that is 0.5 units long to represent the probability of going to squatches and then appin to that another line that is 0.5 units long that represents presents the probability of going to Norms combined we have a line that goes from 0 to one now to decide which restaurant we want to visit we pick a random number between 0 and one in this example we randomly pick the number 0.7 and because 0.7 is in the region that represents Norms fry Hut we go there and when we go to Norms we are served a large order of fries bam getting served a satisfying order of fries at norms is awesome but so far we've only been there one time and that means that we don't yet have a lot of confidence that most of the time will be served a satisfying order of fries at norms and it's possible that squatch's fry Shack also serves satisfying orders of fries so we still might want to try out squatch's place because we were just served an order of delicious and satisfying fries at Norm's fry Hut but it was just our very first visit and we're not sure if that will happen most of the time it makes sense that we should increase the probability that we will visit Norm's fry Hut in the future but just by a little bit so here's one way we can increase the probability that we will visit Norms in the future but just by a little bit bit first because these fries are so satisfying on a scale from 0 to 1 we give them a fry score equal to one and then we can use this equation to calculate the new probability of visiting Norm's fry Hut new P Norm this equation uses the existing probability that we will visit Norms P norm and combines it with the fry score that we assigned to our order of fries and a learning rate which helps prevent changing the probability too much too soon to calculate a new probability for visiting Norms fry Hut new P Norm so we plug in the current value for p Norm 0.5 plug in the fry score one and then plug in a learning rate which in this example will set to 0.1 note we'll show what happens when we have different learning rates in just a bit for now when we do the math we get 0.5 + 0.05 which is 0.55 and that means we increase the probability of going to Norm's fry Hut to 0.55 we then calculate the new probability of visiting squatch's fry Shack new P Squatch as one minus new new P Norm so we plug in the value for new P Norm 0.55 and we get 0.45 and that means we decrease the probability of going to squatch's fry Shack to 0.45 in other words we increased the probability that we will visit Norms again in the future just a little bit and we decreased the probability we'll visit squatches is just a little bit increasing the probability of visiting Norms the next time we want to snack just a little bit makes sense because even though we got a satisfying order of fries at norms so far we've only been there one time and it's possible that Norm's fries will not always be awesome and it's also possible that squatch's fries could be awesome so we still want to have a chance to visit squatches bam now before we move on and get hungry for another snack let's reset the probabilities to what they were at the start 0.5 for both Squatch and norm and talk about the learning rate learning rates control how big or little a change we will make to the probabilities each time we update them for this equation learning rates are set to values between 0 and one if we set the learning rate to to 0 then the whole second term is equal to zero and goes away and we end up with the new P Norm equal to P Norm in other words when the learning rate is set to zero we don't make any changes at all in contrast if we set the learning rate to one and then plug in the current value for p Norm 0.5 and plug in the fry score one then we will end up setting new P Norm to one and that means that the next time we get hungry we will go to Norms without leaving any probability to visit squatches and that's not really what we want to do right now since we've never been to squatches however when we set the learning rate to something larger than zero and less than one like 0.1 we end up setting new P Norm to 0.55 which is just a little larger than the original value for p Norm 0.5 and that small change does a better job reflecting the satisfying order of fries we got from Norms while also preserving a chance that we'll visit squatches note in practice the learning rate is something that we usually set to some default value like 0.1 and if it doesn't work we test other values small bam understanding the purpose of the learning rate and updating the probabilities that we will visit squatch's fry Shack and Norm's fry Hut is hungry work so now it's time for another snack like before we can draw lines to represent the probabilities that we will visit squatch's fry Shack or Norms fry Hut but now the line for squatches is a little shorter and the line for Norms is a little longer next we pick another random number between 0er and one and this time we get 0.2 and because 0.2 is in the region that represents squatch's fry Shack we'll go there and when we go to squatches we get served an unsatisfyingly small order of fries hey Squatch why are your fries so lame I needed a snack too so I ate some of them are you going to eat our fries every time I don't know I guess it depends on how hungry I am okay just know that it's bad for business anyway because squatch's fries are unsatisfying we give them a fry score equal to zero and now we can use this equation to calculate a new probability for visiting squatch's fry Shack new P Squatch this equation is like the one we used earlier but now we use P Squatch the most recent probability for visiting squatch's fry Shack instead of P Norm so we plug in the numbers using the same learning rate we used before 0.1 and we get 0.41 so because the fries at squatches were unsatisfying we reduce the probability we will go there from 0.45 to 0.41 with the new probability for visiting squatch's fry Shack new P Squatch we can update our probability for visiting Norms fry Hut new P norm and we increase the probability that we will visit Norms from 0.55 to 0.59 now when we draw out the probabilities the line for squatches is a little shorter than before and the line for Norms is a little longer so we pick another random number and this time we get 0.1 so we go back to squatches and again when we go to squatches the fries are lame double I couldn't help it I was hungry anyway like before squatch's fry score is equal to zero and we use that to update the probability we'll visit squatches in the future to 0.37 then we update the probability that we'll visit Norms in the future to 0.63 using this approach we can continue to visit norms and squatches to update the probabilities we will go to each Place most of the time Norm gives us a satisfying order of fries so the fry score for Norms is usually one and as a result we usually increase the probability that we will visit Norms however every now and then Norm just can't resist and eats most of our fries so the fry score is equal to zero and we end up decreasing the probability that we will visit norms and on the increase inre inly fewer occasions that we go to squatches we usually get a lame order of fries and as a result we reduce the probability that we will visit squatches even more however once in a blue moon Squatch somehow resists eating our fries and we get a satisfying order and we end up increasing the probability that we will go to squatches in the future anyway now we just repeat these steps we use the probabilities we will visit squatches and Norms to determine where we will go to get fries and then based on how satisfying the fries were we update the probabilities we will visit each place after repeating those steps a bunch of times the probability of going to squatches ends up hovering around 0.19 and the probability of going to Norms ends up hovering around 0.81 in other words every time we go out for fries we change the probabilities but after a while those changes stay close to 0.19 for Squatch and 0.81 for norm and now we know that when we get hungry most of the time we should go to Norms because Squatch gets hungry too often and eats our fries yum double bam now that we've seen an example of how reinforcement learning can work it's time for a terminology alert for starters using reinforcement learning lingo we would call squatch's Fry Shack and Norms fry Hut the environment the environment is something that we want to explore and can interact with in this example we explore and interact with the environment by visiting each place and ordering fries and because we are the ones exploring the environment we are called the agent in this example we the agent explore the environment by using the probabilities that we will visit squatches and norms and these probabilities are called the policy in this example we use the policy in a very simple way we simply pick a random number between 0er and one and if it lands in the area that represents Norms we go to Norms otherwise if the random number Falls in an area that represents squatches we go there however our policy could be more complicated by taking into account how many times we visited Norms or squatches in the past week or factor in how hungry we are lastly the pry score which we use to update the policy is called the reward in reinforcement learning the goal is to modify the policy so that we can maximize the reward and in this example that means updating the probabilities that we visit squatches and Norms so that we maximize the delicious fries we are served note in the example given here most of the time Norm gave us a great order of fries and the reward was relatively High however every now and then Norm ate some of our fries and gave us a small serving and the reward was low so the reward can change and doesn't have to always be the same to summarize we the agent needed to explore the environment which in this example means we needed to decide if we should visit squatches or Norms so we used the policy the probabilities that we should visit each place to help us decide where to go and depending on the order of fries we either got a large reward or fry score or a small reward and because our goal is to maximize our reward we update the policy so that we will visit the place that is more likely to give us a satisfying order of fries more frequently triple bam now it's time for some Shameless self-promotion if you want to review statistics machine learning and AI offline check out the stack Quest PDF study guides and my books the stack Quest Illustrated guide to machine learning and The statquest Illustrated guide to neural networks and AI at stat quest.org there's something for everyone hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
_kstkMF-lQQ,2025-02-12T14:20:19.000000,StatQuest on DeepLearning.AI!!! Check out my short course on attention!,the encoder model was used as the basis for Bert or the bidirectional encoder representations from Transformers which in turn is the basis for nearly all the embedding modelss that you use to create embedding vectors for rag or recommender applications today in this course you will learn about the ideas behind attention the algorithm itself and how to coat it in P torch each concept will be explained in a stepbystep fashion that is easy for anyone to understand you will learn about the query key and value matrices what they are for and how to produce them and how to use them in attention you'll learn the difference between self attention Mass attention and cross attention and how multi-head attention scales the algorithm this course is an opportunity to learn about this foundational technique I hope you really give this your full attention
fivdgj5w0K0,2025-02-06T19:12:02.000000,StatQuest with Josh Starmer is live!,I know that's always the question who is there is anyone there let me see the questions there's always a slight lag we we know we're live and then there's like it takes like 30 seconds before it propagates yeah it's true I still see the that we're oh there I think we're I think we are there we're live I can see us we're we're so let's yeah so let's see who's there who's here and tell us uh yeah tell us hello tell us hello where are you where are you watching from and where you're watching from yes always curious are you are you in North Carolina I'm in North Carolina nice how is it there it's gloomy but it's supposed to get really nice in a few hours okay nice I'm in Colombia which is where I come to escape the Canadian winter we got someone from Toronto hello uh someone from France yeah hello France uh India India late at night in India this morning yeah someone's not getting any sleep UK more India from India I give a talk in India this morning Vietnam Jo your B Network Egypt holy smokes wow someone wants to know when we're gonna when I'm G to post about or you might post about deep sea we uh should post about deeps yeah we'll talk about deepi today by the way we're going to talk about it today yeah exactly we're going to demystified definitely clearly explain uh Brazil oi Brazil ah Brazil Germany Nepal wow Nepal Germany nice very cool Bam from Sweden Greece Bangladesh Singapore wow Lithuania turkey Vietnam amazing yeah all right right perfect well let's uh how's your book going by the way I've heard great great they were supposed to do please show show the hold on you have it there Pakistan La Iran Canada India Ukraine wow it's here there it is amazing the stat Quest Illustrated guide to neural networks and AI love it what a great book yeah uh so the book is out I'm really excited about it you can get it uh most places you can get it on Amazon other places uh like in India for example you have to get it directly from the printer uh there are um I don't know there's strange regulations for for why I can't it can't be easily done in India but but it is available in India for a great price uh much cheaper than it is in other markets uh which is awesome um but regardless of the price it's an awesome book I love it it's it's got tutorials in pi torch so not only will you learn about neural networks you'll learn how to code them as well so that's my Shameless self-promotion I'm really proud of it I spent a lot of time on it and it's not shamess I promote it so I'll say it's amazing I had the honor of of actually checking it before and I loved it it's a I know Lou was one of my fact Checkers I've never seen a whole Transformer worked out with numbers I got I tried and failed so I very happy you did itth it worked out really well so great book yeah I did I did that work so no one ever has to do it again thank you for the in the sake name of humanity we we thank you uh glad yeah yeah so any new material coming up on your end well funny you should ask my deep learning.ai um lesson or whatever it is it's short course excuse me it's called a short course comes out uh a week from yesterday so this coming Wednesday my deeplearning.ai short course on attention everything you ever wanted to know about attention and coding attention fantastic comes out right on the heels of J Lamar's lesson his came out yesterday yeah J and Martin so we've got and I think his is all about sort of larger scale sort of the big picture of Transformers and things like that whereas mind Zooms in on on one of the key things the things that make Transformers so much different from a lot of other neural networks that's fantastic yeah I really I've been planning to watch that course very excited about it and definitely excited about yours yeah what's the name of the course I can't remember okay but it's about it's like it's like uh attention for deep learning or Transformers I don't know they came up with some catchy name uh that I can't remember so it's it must not be that catchy uh but it comes out it's all about attention uh it'll be with Josh starmer presumably so if you just look for anything by me you'll find it great yeah those deep uh deep learning AI courses are are really really cool yeah uh yeah I have some stuff on attention coming up to us I think I I finally understood a few things so I have uh I have a video coming up uh I see attention as gravity right like Words pull each other kind of like similar words pull each other the way two planets that are close pull each other oh and if they're far away they don't pull each other so I see it as like you have words flying in the embedding uh but if they're you know but based on what you're talking they pull each other like gravity yeah but I recently just find out just trying things that the pull is not symmetric sometimes a word pulls another one but another one doesn't pull the other one kind of like a big planet pulls a small one but not vice versa like Jupiter pulls it moon but the moon doesn't pull Jupiter yeah some words pull stronger than others and so and that's why you have a key and a qu Matrix because one tells you how much you're pulled and the other one tells you how much you pull that's right that makes sense for example a word I have this this example fisherman and net right if you're talking about a fisherman a net will come up so the pulls it but if I'm talking about a net I'm not necessarily a fisherman may not show up that's true because I could talk on the Internet or a tennis net so that one doesn't pull so strong and actually the bigger word that pulls less interest that's right fisherman because a big word gets pulled a lot a word that's very popular gets pulled into a lot of conversation a word that's very obscure it doesn't so it's it's the opposite but I'm worked that in a video so it'll come out soon oh that's very exciting yeah yeah yeah yeah right cool well let's see we're definitely G to talk about deeps and we're definitely going to talk about uh whatever questions people have I see a lot of a lot of one is it about the title of the book can you type it by the way can you can you put it in the in the chat I can type it uh it's called the because it was backwards oh Quest Illustrated guide to neural it's a mouthful networks and AI here I'll put a link to it too uh to where you can get it because you know everyone should have this book definitely put the link here is the link this has all the um it's it goes to my store but that has the links to uh different you know where you can get it on different Amazon so if you're in Spain it's got a link to the the Amazon in Spain if you're in India it's got a link to the publisher in India um if you're in the US it's you know in Canada all these things uh so it's got all the links and it's there's also if you're in you're in a region where you can't get the physical copy for one reason or another and I'm working on trying to get like Global coverage but it's always a struggle for me um I also have a PDF and I know a lot of people uh nowadays I think prefer the the PDF uh when my first book it's funny my first book I made as just a PDF and and I was like no one wants to read actual books anymore and and I was like what you know so I just made a PDF and then everyone's like this is just a PDF you should have a physical book and so I was like okay and so I figured how to do it I made a physical book and I ended up selling like 10 to one like physical books to PDF and I was like really surprised I was like this doesn't make any sense to me why are people buying these physical cops this time around it's kind of the back backwards it's the other way around where more people are buying the PDF and fewer people are buying the physical book I have mixed feelings about this um maybe people want what they can't get whatever you do they they would like the other one the mixed the mixed feelings though are like once I actually like I'm like PDF makes sense it's easier to search easier to navigate but once I um once I held the physical book in my hand I was like oh I love it and so like everyone needs this one right because it's kind of like yeah I still have that I mean I I still enjoy more the passing the pages and stuff I I would like to not I would like to have you know be more of the yeah and actually not all the time but sometimes like I'm starting to think of like maybe using two you know how you can if you if you if you you could draw a picture across two pages and that might be harder to see in a PDF so I guess I have to be careful not to do that but it's always very tempting okay yeah that that must be yeah huh right interesting anyways um cool I see some people showing love showing love I know and somebody people are just ordering it so hooray thank you very much yeah that's awesome um yeah so Le should we just Dive In The Deep End and talk about deep seek let's do it I I'm so excited I love it I've been using it so much I mean it's incredible tell meic what are the things you've been doing with it I I mean I do a lot of thinking it's my body for thinking I mean I I just bounce back ideas you know I go hey what do you think about this crazy idea of explaining I don't know a Transformers with a chicken and a you know and and it catches it right like it catches it and it starts going and then I ask it questions and questions logical questions I push it with the math it's much more like it it you know it still makes the occasional mistake but it makes it's just a lot more accurate than than Chad gptm it's crazy yeah the fact that it's so small like it I believe it you can get you can use it offline right you can you can you can use it locally because it's so much smaller well well they they have they have three different versions that's sort of the mystery of of deep C they have three versions they've got um one that's sort of like a prototype two the one that's like the the R1 that's the one that everyone's kind of going bonkers about and then they have these thing called distilled versions and distilled versions are basically um you know like so Facebook or Med excuse me post have have something called llama they have a model called llama which is relatively small and what they did is they they trained llama which is a relatively small model to behave like the full-blown deep seek and they call that a distilled model oh that's interesting yeah so it's it's an open source sort of model and it's relatively small uh but it's basically based off of another open- source model uh and I think they did two I one I know they did llama and there was another sort of open- source thing that they ALS that was a relatively small model that they trained uh to do these things feel like distilling is a genius way to do kind of like a dimensionality reduction right exactly it's kind of like you have a big model that knows a lot of stuff but it may have it may be over parameterized but you don't really know where they are yeah but if you use it as a teacher to teach a smaller model then it gives it the right knowledge and this one is forced to learn it with less parameters that's right and uh yeah and just like PCA or any kind of dimensional reduction you're G you might lose a little some some stuff some information is lost it's not perfectly lossless but the like the the loss to cost ratio is like Bonkers right so you get this really awesome model that you can run oh maybe on a laptop who knows um yeah and and I know um uh I know um for example so I used to work at a company called lightning and now they've got this whole thing set up so they're serving you can you can you can have your own version of the R1 model running and you don't you know you can actually you don't have to worry about privacy all of a sudden because it's on your own right exactly um and I think that's super cool I think that's like a huge um a huge change yeah it also kind of broke that Paradigm of like if you want this to work you need a ton yeah of of processors and gpus and money and resources like it all of a sudden I feel like a university could could train one of these like it's still big yeah but it costs uh I don't know in in the millions of dollars right not in the hundreds of millions or in the billions definitely to me it shows that yeah that you can that you that that human intellect fits in on a smaller place and and it just opens the door for for a lot yeah yeah so I think one one thing uh one thing I I I want to get out early on in our in our discussion of deep seek is is that there's a lot of people that are like you know this must be fundamentally different than anything we've ever SE before this has got to be a whole new technological Revolution yeah and and what's surprising is it's is it's not it's still a Transformer probably just a decoder only Transformer in it in its Essence the big difference is how it was trained um and it's not necessarily um so it was specifically trained to be a reasoning model and reasoning models are specifically trained to do things one step at a time and the way they're trained to do that isn't isn't rocket science they're just given training data that has you know answer this question and then the the known answer is this long one step at a time thing um and so um and so it sort of learns how to make answers that have the same format as what it was trained to do yeah um and the other thing is so it's fun mentally it's it's still a Transformer just like uh we've been we've had ever since 2017 yeah um they just specifically trained it for um uh for reasoning for answering questions in a stepbystep way rather than just answering factual questions which is a different process right if you want to say hey what's the capital of France you don't need it to do a stepbystep answer like France is a country in Europe on the planet Earth you don't need this step-by-step thing it so so this model Believe It or Not isn't actually that good at those kinds of questions those sort of like let me just get give me the facts it's really it's it's it's highly specifically trained to like solve complicated math problems and do stepbystep complicated things yeah yeah that that blew my mind and and and also the way you get this this Chain of Thought right because Chad GPT do the following I actually like this this analogy for example let's say you want to make a computer play chess so if you were this is a longtime solved already question but what would you do originally you would give it lots of chess games that are you play a lot we play a lot and give it the chess game and that's a huge data set so by the time it learns that data set it's hard and it may not be so good and it's change to what you gave it like it can't be smarter than what you gave it yeah on the other hand what they when they finally broke chess they just made it play with itself and they said you win or you lose I'm going to tell you just that and you figured out and it started backtracking the way a neural network breaks down things it started backtracking and saying okay this technique works this technique works and it started thinking sequentially but all you gave it is you made it play with your with itself right yeah I feel like this is a similar breakthrough that deepy had because Chad GPT does PR chain of prompts too yeah but it gives it the math problem for example the math problem a train is going at this speed how long is and then somebody actually types first you look at the velocity then you do this and then so this is huge data sets and it's like giving it solv chess games right yeah what did D do it it skipped that fine-tuning that that process completely and it just made it play with itself and these models are are better understanding that creating they it would just give it the problem math problem say it's good or not and it would make it play a game a reinforcement learning game to see if it if it wins or not and just like in the chess RL neural network it would start thinking sequentially by itself so I would mimic our sequential thinking not from the data but from the fact that it it's the optimal way to solve the problems that's right so all of a sudden fine tuning is the most expensive thing or one of the most expensive things in in training this big models you need lots of people lots of resources this did it by itself yeah uh so so that's what the r r zero model does that's the r zero yeah it's all it's all reinforcement learning so yeah just possibly a Shameless self-promotion but Lou you have some videos on reinforcement oh thank you you're so kind thank you I do I am very very kind of you so you want to learn more about that definely check out I have four videos on reinforcement learning one is rhf reinforce learning human feedback then po DPO which are two options of it and an original Deep learning deep reinforcement learning video I made a while ago so I may make something more on this on this tone of how how deep uses RL because I definitely find it so so exciting yeah yeah and uh and and um yeah so that's exciting the don't you have material on RL as well because I know you're talking about RL pretty soon in in got some videos coming out on it as well M are my early ones are going to be more fundamental where all I'm going to do the goal is to show the main concepts of reinforcement learning how it's basically uh my the example I have is I've got my two guys I got norm and Squatch and uh they're both they're selling french fries and I can't decide which place to go to get french fries I love it and so I flip a coin and I go to uh I go to Squatch and he gives me a small order of fries which is disappointing because you know I want a lot of fries right and so you know and so I update my the way I decide which place to go based on the order of fries I get and then so the next time you know I'm I'm a little less likely to go to Norm excuse me to go to squatches because he ha he he ate half of my fries and maybe a little bit more likely to try out norms and so I try out norms and and it's a and what it is is I show how this trial and error way affects how I'm going to make the decision in the future so it's basic concepts and then I show sort of in a very basic way how in the next video is how um reinforcement learning can be applied to like the most simple neural network I'm not talking about deep deep crazy Transformers and things like that and reinforcement learning with human feedback um it's just basic like what are the concepts how do they how does this where should I go to eat fries example that I gave before how does that then apply to a real basic neural network we kind of walk through that and then because I can't help it I've got a third video that deres all the equations so that you see how back propagation Works in that context exactly uh that when I was doing research on this that was one thing I didn't see was someone like doing the step byep just like my my Transformer you know where I show the math one step at a time I've got a video like that where I'm basically I'm show how one parameter in the model gets changed with reinforcement learning every iteration and I do all the equations because I'm compulsive that way I love that and I love that you leave it for a different place so like you could just go full conceptual thinking about French fries by the way I love food examples I have a special place in my heart because it activates some part of my brain that gets excited and I I learn more so a lot examp are food examples too and then you keep the math for for the end I think that's very because some people want to see the whole thing some people don't want to see the whole thing they they yeah some people don't want to see any of it that's fine yeah I think it's perfect yeah I've been trying to do that as well like keep the keep the math for the end um yeah yeah fantastic well I'm excited to see those to see those videos yeah so getting getting back to deep seek I think one a big thing with deep seek is that um they use a lot more reinforcement learning and and they use models to then also generate because there is some fine- tuning that happens but they use models to generate that fine-tuning data and that's reduces the cost dramatically right when when you know when you know and in some some respects you can say well that's kind of cheating right because the original chat GPT they had to collect all that data by hand and they did because it didn't exist but now that they've created these models that are I mean it's not cheating it's just like it's being practical models that can that can generate a data set like this rather than having to hand curate it go for it is what I say I mean that's ch's philosophy right it's like if I produced an image and it looks a lot like like something else it's it's it's not so I mean it's it's the same concept all over again and I mean I think it's human knowledge I mean yeah and there was there was a huge there was a controversy right when it first came out there was you know Chad GPT got all upset because they were like hey wait a minute you actually used our model to generate your fine tuning data said and that's cheating that violates our user agreement but then everyone was like yeah but you guys like took all this copyrighted material from The New York Times and didn't pay for it or ask for it or anything so there was a so there wasn't a whole lot of uhh what do they call that um alligator tears they had no moral grounds really yeah so but yeah it seems like reinforcement learning is is the big news of this time and that you can without with minimal fine-tuning and sort of spending more time on the reinforcement learning you can get models that are fantastic yeah it's pretty much the yeah the fact that they can you can train yourself play play with your own games and and train yourself I think that's that's genius uh and yeah also I think they did a great job with mixture of experts you know I feel like uh when you prompt uh deep sick and it and it's a math question it just activates the math part and and forgets about the rest yeah uh and so that's even on inference time that's good because that's not for training time but for inference time like it's faster because it it focuses on a certain part yeah of of knowledge um yeah exactly so I think that so I think I think to summarize sort of deep seek and the Innovations are are one it's the same fundamentally it's still just a Transformer model and that's the same as it's been since 2017 there hasn't been changes in that respect what there have been changes in are how it's been trained and how it got the data to use do the fine-tuning those are sort of the major things it's trained with a lot more re reinforcement learning and the FI most of the fine-tuning data was generated from other models uh rather than having to be hand created yep and it uses that strength that model has that that they can understand something better than they can create it you know there's like a thing about Chad GPT cannot tell good jokes these mods tell really bad jokes but they can explain them really well yeah exactly why is this joke funny yeah actually I have some material coming up on that but you cannot Chad GPD cannot tell good jokes be this is something I got from Jeff Hinton actually he said it once you cannot tell good jokes because when you're talking one word at a time you cannot plan a punchline right a joke requires a bunch of planning that's funny chpt or models start sounding funny immediately yeah right they start saying it's like a kid telling a joke they try to sound funny in every word and they doesn't come out well you have to plan it and be sneaky right not not turn on the funny to the max you can't stop can't start laughing at your own joke and exactly like you can't be like right um but it understands it really well right it understands it really well because it can look back these models I I think like like a car that you cannot see too much to the front because the headlights are low but the riew mirror is humongous you can look back and understand any joke you want right like you can understand anything yeah because it just looks back right yeah and so these models have a thing that they're good at understanding bad at creating which is you know good at understanding they've been forever because because they predictive AI means understanding yeah right creating is hard yeah and so when you use that strength and you say okay I may not know how to solve the problem so well right now but I can tell you if a solution is good or not yeah yeah yeah yeah that's and then you have a reinforcement learning problem immediately because I know how to I don't know how to play the game but I know how to grade it I I know how to get rewards and not so it's using its own weaknesses and strengths to build a reinforcing learn reinforcement learning framework MH and then just Skyrocket from there yeah so I think is genius yeah I love it and I I I I just can't help but think of like is how would if I had to train a neural network to tell a joke or create a joke how would I do that um yeah is there a is there like a way you could like pad the tokens or something like that like I don't know there's got to be some way where you say think of a punchline yeah first the punch line and then you're like okay now knowing that's the punchline come up with a with a big lead up to this thing that that is then funny and not funny before I don't know is there a way you can yeah I have to think the punch line first so I think about that so much and I talk to my friends about that and I think I have an I have a rough thought let me see what you you think I mean I I feel like I feel like um a joke humor yeah could could be maybe I'm I don't know but it's like vertigo in the brain like if I am if I'm in this part of the brain talking about something and then I take you somewhere really far away really fast yeah you laugh yeah Sur the element of surprise is a big deal right they combine two things far away and then unite them with something that was not expected yeah so I feel like you could take an embedding yeah and maybe find things that are really far away in the embedding yeah but somehow there's a third element that unites them somehow relates them yeah or maybe you're far in the embedding in a bunch of the dimensions but close in other ones yeah and then you're thinking two things that not related and then boom they're closed so I feel like you could search for these type of pairs or triples I just I can't wait to see the the title of the of the scientific article on how to create a joke you know it's published on the archive you know and it'll be like using using like semantic decompositional you know reconstruction Theory we were able to create something funny yeah but then if I ever write that paper the joke the title will be a joke that's generated by that because it has to pass the test right anyway so I feel like yeah I actually want to one day just start playing with it to see if I can prompt it to make better jokes because you can just prompt the the pairs and then and then be like okay um make a joke about this and and then works better because the jokes are terrible yeah well I think the irony of the scientific paper about how to make a joke would would be it'd be completely dry and no humor at all just make it the most boring paper in the world you ever read yeah definitely that's a very in tune with with academic and that that reminds me of the of the I don't know if uh if anyone read this but I I was I knew people were gonna be asking about deep seek and I knew we were going to talk about it so I decided to read the Deep seek technical paper oh and having just published a book the stack West Illustrated guy did neural networks in AI um I hired a professional editor and I spent quite a bit of money uh having them clean up my writing because um it it's it's nice to have that professional polish and have it easy to read and things like that yeah and you know I've been working with this editor for for basically months at a time you know going through every single page every single sentence with a real fine tooth comb um and then I read and but I but you know obviously you can hire a person but also there's programs like grammarly and there's all these AI sort of grammar checkers out there you you can just you can't even on like on a Mac nowadays you can just you can't help even type and it's like can I you want me to clean that up for you this is all this is not a good sentence you know so like all these tools for writing sort of like grammatically correct sentences and deep SE didn't use any of them in her tech technical report it is one of like the poor poorest jobs that's funny I've ever seen it's it's like there's no consistency with their nomenclature they're all they call they call things different things at different times they hyphenate some things sometimes and they stop H hyphenating at other times it's it's completely like random and there's no consistency across the document as a whole no one at no point did they have an editor AI or human read through that thing and go guys we need to fix some of these things they were like nope we just need to publish that thing now why they just put it through their own model fixed it completely fixes their own model would have done a better job writing that so we have a saying in Spanish that says the the uh in the house of a blacksmith the tools are made of wood yeah right kind of like if you're if you're the person who makes tools of uh of iron your tools are made because you don't to practice what you preach right that's exactly you don't apply it to yourself yeah uh so that's what they did yeah I I didn't read that but I I did read J J alamar's article is good it talks about yeah I'll put the link here but that let's mention that let's put that can you put that abolutely J as always and I don't he also sometimes a p gotic guest on our uh live stream he does appear often but he did it again um oh yeah and he's got this new thing called The Illustrated deep seek or something like that Illustrated deep seek yeah it's right there so let me let me see yeah so I yeah I read it earlier and I actually read it and I I actually had deep sick help me some parts of it I was like help me but no it's a it's a wonderful wonderful uh article yeah it can't go wrong with Alamar really yeah oh that works every time yeah and uh yeah what else did oh yeah and I saw a computer file video that was really good with a professor I will link it as soon as I find it he was also talking about I will I I hate to just make this just one big you you work on those links I'm going to put in another Shameless self-promotion absolutely please um uh both Lewis and I are going to be at the uphill conference in April in Burn Switzerland I know not many people live in that region relatively speaking uh but if you're in Europe or in a vicinity it's you can hop on a train and get to burn or hop on a plane and get to Zer and then get on a train and get to burn anyways we're going to be there in late op April at the uphill conference both of us uh we're both giving talks Lis are you going to be giving a workshop I actually haven't decided no it'll be it'll be a talk okay uh but we're still deciding topics but I think it'll be on the attention stuff that I was talking earlier on the gravity oh very cool very cool another thing that I I went there last year and I absolutely loved it so I I can speak from experience about how great a conferences is but one thing that's really cool is they also do these like Fireside Chats where they get two people and maybe me and LS will have like it'll live but it'll be a live live stream right like a live people just sort of riffing on each other wow I would look forward to that yeah fire side yeah that's right I would love that yeah we should do oh yeah so anybody anybody who's in Europe uh or anybody who wants wants to travel uphill conferences let me since I'm in the links Department let me let me share a link yeah just keep thanks my first time I'm very excited yeah Josh thank you you recommended me there the reason I'm going uh here it is uh Ray asked uh uphill com it's in Burn Switzerland April 24 and 25 yeah did you did you add the link I did add the links this thing add links right sometimes it blocks links but it it doesn't make them blue but you may have to copy paste uh just probably to remove a Spam or something oh yeah maybe I need to do something what questions are there you know these questions that people make 
qJrmQe8TOTw,2024-10-10T04:04:08.000000,Luis Serrano + Josh  Starmer Q&A Livestream!!!,hooray so it says we're live hello I'm G to chat hello hello everyone it says I don't see us oh yeah there we are all right do you see us on the on the I think we're yeah I think we're live okay and we have a few people hi everybody hello it takes a few minutes to get things rolling technology I see some people saying hi Jan gumar Adriana Rolf and we had a few technical difficulties getting the live stream connected so I'm like is it actually working I don't know but it looks like it is I I'm seeing some stuff in the chat that suggest it is working it worked out I'm very happy to hear that yeah yeah cool so how are you doing Josh I'm doing great how are you doing you just got back from a vacation yeah yeah it was fun I was in Europe for a bit and you got back from some interesting trips right yeah I was in uh I uh I guess the last time we did the live stream was in July uh so it's been a while so I've J what's that yeah with the one with J I think it's either June or July it's been a few months I've I've since traveled to India uh to talk at the data hack suic 2024 wow that's nice uh and then I went to uh Brazil uh South Paulo to talk at sender uh and that both trips were absolutely fantastic um that's awesome so but it's good to be home and it's good to be live streaming again it's good to be hanging out with lwis yeah I always like hanging out with Josh so this is wonderful um I think I'm not certain I think we're both I think we're both going to be at that uphill conference in Switzerland in Switzerland yes yeah so 2025 so I'm excited about that because if if nothing else happens between now and then I know I'll at least get to see you then in person I think so yeah definitely that's exciting because we never never met in person we talked a lot on maybe when we're there or whenever we meet up we can do a live stream but we can be together instead of being separate we can do that would a little we'll just live stream from the same camera yeah yeah that's very exciting yeah yeah let's see who's around uh so who wants want to tell us in the chat where they're coming from and I'll read that where they they are and I'll I'll just read them I see looks like we got venezuel someone from Venezuela which is cool hola a neighbor I'm from Colombia right there uh yeah well we can start I see some questions there yeah uh so we're gonna we have three things today for the audience and and more if we want we have a few questions that we're in the live stream that we're going to answer mostly about you know learning machine learning uh fields to work on Etc then we want to talk about this recent news about the physics Nobel Prize which was won by Jeff Hinton and John hopfield ai ai people which is very exciting uh it's huge for AI I never thought I was gonna say something is huge for a but this is huge for AI and then last but definitely not least we're GNA talk about your book which is wonderful and I got I got the chance to see it and I really like it so for people for people in the audience is this is a wonderful wonderful book that's right we have people from uh New York City Armenia Hungary Saudi Arabia Ireland I love it people all over the world um I mean let's dive into some questions and by the way people can always put questions in the chat and we'll we'll if if they're easy we'll get to them I'm kidding we'll get to them uh whether we answer them correctly or not we we'll definitely answer them no uh definitely put questions in the chat and uh we'll um and we'll and we'll get to them but well let's start with the ones that were put in the in the in the previous uh chat so let let me ask you one uh if you got stuck in learning what methods do you follow to break the problem and understand it yeah so uh that and this happens to me all the time so I'm actually very well practiced in in what to do when you get stuck um and what I always do is I you know if it's say it's like a topic I'm trying to learn something about I try to read uh everything I can about whatever that topic is and to be honest what that means is basically uh skimming through stuff and not understanding any of it um uh uh usually the doc you know say like I'm trying to learn about how PCA works and I'll read I'll read stack exchange post because there's lots of people asking about it there and there's people explaining it but I also read manuscripts um and a lot or I read the Wikipedia article and all that stuff and a lot of that time I uh I don't understand anything that I've UND you know anything from any of that stuff um and uh what happens though is I is I start seeing words that are used in all of these different Publications you know on the Wikipedia page they they might reference some terminology there might in the manuscripts they may referen in similar terms and I kind of build up a little bit of a histogram in my brain of words that I've never seen before and then it sort of becomes that rabbit hole where once I see what terms they're using in this field I'll start diving into those individual terms and and those terms lead to other terms which lead to other terms so it's just like like a like a landslide of terminology uh finally trying to get to the bottom of where of something where I finally understand things and even then I might get to the bottom where there's no longer any new terminology and I don't understand it so what do I do I just keep reading it and rereading it and this takes time takes a couple of days and sooner or later one little bit makes sense one little tiny bit and then with that it's almost like you know it's like a little finger hold on a cliff and I get my finger in there and I can hold my weight and then you know and that just lets me reach up and you know eventually I'm climbing and I get back out of this you know deep Cave of terminology and I bubble up to the top and usually then I can sort of read those top level AR uh articles uh but then what I'll do is I'll start looking at code I'll look at implementations um and then I'll start trying to either code it myself or I use somebody else's implementation and I'll see if the input and the output makes sense the way I expect it usually it doesn't usually I'm completely wrong in my understanding at this at this stage but then I can start trying to reconcile the difference between what I'm seeing with the implementation and what I thought I understood from the documentation um and then you know sooner or later things start coming together but but it's one of these things where um I just the the real key is just don't give up there's so many you know the first couple of weeks of this process is nothing but failure um and sometimes it goes on longer than that you hopefully it's not too much longer but it can go for a while um and I just don't ever give up because I I know fundamentally a lot of the math is just m you know simple math it's multiplication it's it's addition it's division if things get really funky they may take a log and they may take a square root uh but it's it's fundamentally small pieces that I know I understand and it's just a function of like uh getting to that level and then building back up to the top um yeah yeah so that's my story no that's definitely a a good answer yeah I I have done something similar and I feel you mentioned something which is kind of like being stuck yeah uh I feel like the day I made being stuck my comfort zone my life got a lot better because before I hated being stuck and now I say just know yeah um I'm going to be stuck and I feel like um when I read uh any paper or anything I I understand nothing even if it's stuff I already know as a matter of fact even if stuff I've wrote written in the past like papers I wrote in the past I don't understand them because they're written in weird language of formulas and I I can't do formulas never and so I my trick uh that I would uh to to what you said it's it's picking a super simple example so if it's Imaging for example 2x two 2x two monochromatic number two four numbers between zero and one is an image and I try to work everything out there so I do all the formulas in those numbers uh if it's text I try to have you know a language with four words and sentence with four words and try to do that or or a matrix of 2 by two like I always try to work out all the formulas there and then it turns out that it's always sums as you said sums and products and logarithms that only appear to turn a product into a sum that's the reason they appear right um so like it's always I have a few tricks but always just trying to get the simplest example and I've I've talked a lot chat GPT has been super helpful because now I before I used to bug my friends I bug people like I ask them questions and stuff like expert people I know but I but I now just ask CH GPT and I obviously if you ask it right away it gives you the formulas and the same thing you find in Wikipedia right but now now I just said okay no but I want I want an example with a dog and a cat and I want this or an image that is 2 by two or something help me understand this and it it's actually like after a while it starts getting understanding what you want and uh so yeah but definitely just just trying and trying and I feel like I I throw a lot of information in my brain and hope that it organizes it so I I watch a little bit of a video I read a little bit of a paper I scribble a little bit then I forget it I come back and I I don't have an organized way of of learning but I feel like if you just kind of throw stuff eventually starts clicking and sometimes things stay years to click yeah uh so now I just I just hope it clicks before the algorithm goes out of uh fashion you know because yeah I understand something old now like it doesn't help so I now we have deadlines but yeah yeah speaking of that what are the hardest topics you've explained and why that's another question we have oh man uh I'll be honest so uh my PCA video took at least a year to make MH um that was a that was really hard for me um and it's one of those things like once I made the video I like oh this is obvious uh but it you know getting to that stage was really challenging for me um what else has been really hard logistic regression took forever MH um uh neural networks was neural networks was interesting it was interesting in a weird way in that um I don't I don't I'm a big fan of three blue one brown he makes Incredible great content and I remember uh watching his videos on neural networks and being like oh these are awesome um why would anyone need to do another video and on neural networks and so I didn't I wouldn't even think about neural networks for a long time and then someone posted a comment in on one of my videos that said you should do a video on neural network work and I wrote back and I said why check out these videos by three blue one brown I sent him the link and the guy wrote back and he says I've seen those before you still need to make a video because I don't really get it and and so what made this hard is is I was like what else is there to say and so I watched what three blue one brown did I watched those videos again and they're again they were fantastic videos and so I but I was like but clearly there's something missing that this viewer wants MH so I had to watch those videos and go I see what he's saying I see what he's illustrating what's he not illustrating what's missing and I had to think of what was missing yeah and I had to and that was hard for me to figure out what wasn't in there um and that was but but I eventually figured it out I eventually figured out what what what three blue one Brown wasn't doing although he was doing a lot of things right MH what he what he was and and I'm not saying he was doing anything wrong either like he was just doing it the way he did it right um but what wasn't there was just like what Lewis was saying like a super simple example where he could walk through and see the math in action without it being really abstract you can actually do the multiplication actually do the addition and then plot a point on a graph and say uh based on the input value which is on the x- axis this is what the output value is so far and you can plot that on the y- axis and and once I started doing it that way where you could see how the neural network is transforming the input into the desired output you could actually see it happen one step at a time and it wasn't like abstracted into sort of generalized equations but actually we were doing the math one step at a time and actually coming up with real values uh and all of a sudden I was like oh that's how I'm GNA do this you know and I'm gonna so that but it took me a while to to figure that out as well like to come up with a new way to to point out something that hasn't been seen before yeah definitely I feel like with neural networks I had something similar because I was looking at them for a while and I was obsessed with the exor example right the one that's like this is positive positive negative negative and you can't cut them with a line cuz I was trying to look for something that you can't cut with a line cuz that reques a NE Network I saw some block post by chrisa where he bends space in a weird way and I thought that's got to be something and I and I I remember looking at it for months and months and months until one day finally something clicked which is if I have for example one logistic regression that's a line that splits the plane in two yeah and if I have another logistic regression that's another line that splits the plane in two and if I over impose them then I have two lines that split the plane in four and now I could use four regions and maybe one of those four regions is is where I want my data to be right and so when I started looking at over imposing I think there's some something called neural network tensorflow play ground where you can play with them and and you over impose the regions and then you joining lines you can start getting any kind of curve you want yeah that's when they finally clicked yeah and then yeah so that one that one took a while another one that took a while was reinforcement learning I feel like I I took a long time learning understanding that so much that I was once teaching a class in University on on on on machine learning and and I and I had reinforcement learning as the last lecture hoping that I would understand it and then I kind of had to like remove it without anyone noticing because I couldn't really click it my own way yeah and it took more years until one day it finally clicked I feel like that's one that's one that uh that took a long time to click yeah uh and then yeah a few a few other topics Quantum Computing to took some time like I have you just had some new videos on Quantum Community I have some videos and I'm dropping one pretty soon it's not it's kind of like starting I'm I'm like the preview of quantum Computing so I'm doing 4 a transform fast for transform and then we get to the quantum for transform so I'm still understanding it but fast f transform is my next video coming out soon awesome that's exciting so yeah if you don't subscribe to Louis make sure you do that subscribe to Louis yes I'll put it in the uh yeah yeah yeah yeah oh yeah we can put that in the chat holy SM let's put in the chat yeah let me put it in the do it right now uh can you oh I see a lot of questions I know we got all kinds of good stuff f up here let let's go through we have two more here that are quick I uh how do you understood how do you know you've understood a topic thoroughly or you still need to dig more yeah I feel like there's like a little elf on my shoulder or a little little tiny person or whatever that's like uh like I I well okay there's a couple of ways to do this one is the elf just tells me I'm stupid all the time you got the same elf as me and and that like you don't actually know what you're talking about so there's a lot of like anxiety and sort of like like what is it like impostor syndrome going on right there like the guy's like you know don't really understand it and that does push me to try harder to learn more and like I have to like then convince the elf I'm like no I do and the alha is like well what about what about this situation I'm like okay you're right I've got to go back and figure out that one situation um and but but also you know I I know I'm getting close when I can use the implementation of whatever algorithm I'm I'm I'm interested in teaching or learning about um and the input gives me the exact output that I expect right and I can look at all the things that happened in between I go yeah this is all exactly as expected and I can throw in more complicated things and I can I can really predict what's going to happen so that's that's a good sign that I'm at least in the ballpark of understanding um but the other thing is is is when I'm when I'm building the presentation when I'm making the slides for the stat Quest video you know I read it out loud and when I read it out loud it's like I'm talking to that little elf on my shoulder and and I'll I can like I can notice that there's there's like jumps I don't know somehow the when I'm read when I'm reading it in my head I don't notice the jumps but when I read it out loud I'll realize that I'm going from one concept to the next without talking about all the little important things that are in between and that'll go like what is going on in between those two things so when I something about saying it out loud points these gaps out to me and then I have to dive in and I have to figure out those gaps and when I can finally get to the point where I can read through the whole all the slides out loud and I feel like there's a smooth transition and there's no jumps in logic there's no like like all the steps are baby steps instead of there being like and then we start from point A and then we end up all the way over here once I and something about reading it out loud forces me to think in terms of baby steps MH um um and so that's how I do it what about you Louis I'm curious what your strategy I have the same elf yeah no I I I feel the same I have a superpow which I never know it was a superpow but it's I I don't understand anything like I'm very slow at understanding like if I'm in a talk look at my face if you ever seen me in a talk and I'm like uh I I I really have a hard time understanding stuff which find I found out later in life that is good for when you explain stuff yeah uh but I I find that I if I'm not understanding it's possible that I'm not understanding my own explanation so if my explanation contains math and formulas and matrices and I I I I know it's true I trust it I I see the steps and I believe it but I don't understand it and there's a moment where I feel happy uh and I don't know how to describe it but I just feel happy I think is when I've removed all the formulas if I if I managed to remove everything and have an example with little people or something or cats and dogs or cows or something like apples and oranges if I manage to do that and have it fully gometric fully visual then I'm happy about it and I and and and there's a moment where I clicks and I go finally and I just have a a few seconds of of of of awe and celebration and then I start you know then I start making the the video but I feel like I like it helps me that I'm so bad at understanding abstraction and technicalities that when I when it clicks it it has to be super simple so yes yeah yeah yeah great there's another question here and I think I see a lot in the chat so I think we can get through we can get through those uh but with so many fields and Technologies it's hard to cover them all which field of AI should I focus on to get a job computer vision time series llm reinforcement learning Etc it's that a good question yeah that is a great question um and I don't to be honest I don't know if I have a great answer for that I just know what my personal preference is and it works for me um so there's a lot going on right now and it's good to have at least a small part of your brain following that right make sure you're comfortable with chat gbt Lewis is a lot more comfortable with it than I am but make sure you're at least mildly comfortable with using using AI using sort of whatever the latest tour tools are just a little bit you don't have to understand all the details but you don't want to like completely fall behind in that area but what I would spend most of your brain working on is and this is going to sound ridiculous but the things that have been around forever like the linear regressions the logistic regressions these tools have been around forever and to believe it or not they're not going anywhere and they can actually be incorpor like even with the latest greatest AI you can incorporate a logistic regression using the variables generated by an in coder only Transformer to make do classifications these tools have been around for a long time and they're not going anywhere um they'll still be in fashion and one of the reasons why they'll be in fashion is uh they're very explainable uh there and and certain industries like for example banking that's all you can use you actually can't use uh sort of the most modern technology out there um and so these tried andrue methods you know have a place and have there's like careers out there waiting for you to use those methods because and they've been there forever and they're not going anywhere because they're they're super they're they're efficient you know you they you can you know like a any kind of regression model legit linear or logistic whatever you're using computation time is like zero right so they're super efficient but also super explainable and you can tell exactly what each variable is doing and how it's contributing to the output and in certain industries that's critical um and so I'm a big fan of just the try making sure you understand the basics uh rather than sort of like jumping to the top and being like I'm going to become an AI Master without any of the the foundational work so that and that's I don't know if that's the best solute I answer for everybody but that's just my personal bias and my preference yeah I think very similarly I feel like uh I mean one thing it's that transferability is is not that hard you know I feel like a lot of people when when llms became the thing a lot of people came from including me other other places in Ai and and move there because the ideas are very similar and if you're comfortable as you said with the basics like if you know what an your on N is that that gets you into anywhere because right now all advances are some different neur network maybe they'll change yeah but as of right now if you fully understand a neural network yeah and maybe how they work for images and how they work for text and like maybe the idea of a convolutional one recurrent even though they're not so much used right now but the those ideas uh if you understand that and how train models and how to evaluate models like it's all that right and so if something new comes tomorrow in imaging or video or something a lot of people can just switch there if they understand the stuff solidly it's always this same algorithms I feel like it's like sports right like if you're if you have a good you know physical state like if you're something's like running you can just do and that'll help you in all the sports right going to the gym helps you in all the sports and then if you're doing socer and then want to switch to basketball if you're a good athlete you can not if you're in the NBA you can switch to go to the World Cup but we're not talking about that we're talking about like do you do you play soccer for fun then you go play basketball for fun I feel like it's it's the same with AI and so I tell people to follow what they like the most most uh you know don't get stuck in one one algorithm but like you know try to have an open mind but like follow what you like you know if you if you love working with language just go for it but if you love images just go work on on that and if you if you're enforcement learning it's it's what you know you like because you want to use robots or make them play some game or something like go for whatever you love because that's really what where you have infinite supply of energy yeah um and then just keep keep an eye on the market uh but don't panicked I think that people panic and it's it's easy to panic because not many fields are as as as fast moving and wide as as Ai and and it's it's so easy to not to to look at all the stuff you don't know I mean every day I look at I open LinkedIn and then I has stupid because I I don't know a lot of all this stuff people are talking about but then you realize you know you're a few steps of from from knowing that if you know other similar stuff and you don't need to know all the stuff yeah so yeah pretty much that don't panic yeah I think that's great just in summary Don't Panic Don't Panic everybody else is just Hispanic yeah exactly everybody's freaking out I also like the advice of like just Chase things you're really interested in as well uh because that's where you're going to have most of your energy and most of your patience and endurance and all the things that are going to help you understand it um yeah yeah I think it's a those are those are some great great bits of advice yeah yeah definitely all right we are at a fork in the road we could have uh we see a bunch of questions yeah uh we're at about halfway point so we could uh you want to talk a bit why don't we talk about your book why don't you tell us a bit about Illustrated guiding your networks which great yeah so uh so I have a I'm I've got a book coming out in very early uh 2025 um it's called the stat Quest Illustrated guide to neural networks and Ai and it goes from the most basic neural network all the way up to almost the state-ofthe-art maybe the state-ofthe-art sort of depends on where we are uh Lewis is one of my technical editors I love it and so he got Early Access so he could find out all the mistakes um and I've got a handful of people doing that uh finding little heirs and little typos and also like conceptual things where you know when you're typing when I'm when you're writing a book I hate to like uh maybe like um I guess it's anything's anything is a draft right when you first when you first write or do anything create anything it's it's always a little rougher around the edges and sometimes uh sometimes I maybe uh you know things needed to be clarified or maybe I overlooked things and so people like Lewis are helping me like fill in all those gaps because H it's all at a point where I can't really see the mistakes anymore because even if I know that it would be a mistake I just can't see it on the page because my brain automatically fills that Gap um and so that is super exciting but we've got a little bit of editing to do um I'm going to spend the rest of the month going through Lewis's edits and a bunch of others and then we're going to send it to what's called a copy editor who's going to make sure that uh the grammar is correct um and and after that I think we're going to have it out the I I know some professors are already assigning it uh for their spring semester classes so I kind of have a deadline we got to make this happen oh so um so that's that that's the book uh I I know I know I haven't seen a question about it in the in and I know we've got tons of questions in the chat and we will get to those but I do want to address the Nobel Prize the Nobel Prize Louis do you have thoughts about this uh the especially the one that went out for physics I I don't know I'm not a physicist but I think it's having a physics prize going to an AI person it's it's big and I don't know what the physicists are are thinking I I'm definitely excited I I I I read the I was on RS Technica and I read the comments on that and they were I mean not everyone but so many people were so B out of shape about this uh they just felt like they were like this is just hype this is you know they're just getting on the AI bandwagon you know this a dis you know why didn't they you know they people were just B out of shape over this um and I actually have got some thoughts about this because I uh for I spent about 14 years of my life as a geneticist and so I used to pay very close attention to what was going on in uh medicine and Physiology and sort of biology and these things I never I never physics was always sort of like obscure random weird stuff but the Nobel Prize in uh medicine and Physiology and and I guess I don't know if there's a Biology one or maybe that's the medicine one um often would go to someone or a group of people that did not uh necessarily find something that gave us fundamental understanding of how the human brain worked or how the human heart worked but they'd come up with a method that then allowed other people to to go crazy with your research That Was Then Prof profoundly change the field a good example is is a method called polymerase Chain Reaction it's called PCR for short uh this was something I don't know if you remember but you used to have to get covid tests right and the PCR test was more accurate than the um than the antibody test and so and so PCR became something that was critical for making good covid tests uh but within the lab anyone who's doing any kind of genetics research uses this method of PCR pmer chain reaction and to and it's like used daily in almost every lab in the whole world and it the method itself was not like didn't provide a whole lot of insight you know on how the biology Works they already knew they just had to put the pieces together MH uh to make it work and that got them a Nobel Prize there's another thing called Green fluorescent protein which is uh a jellyfish protein that someone's like hey I can put this in a mouse and I can make their kidneys glow or I can make cells that are expressing this Gene glow green and that makes it easier for me to study this disease it didn't help me didn't on its own it didn't shed light on the disease or on the on the function of the heart or any of these things but it made it easier to study study those diseases and again that got a that got a Nobel Prize uh this guy Oliver smithies uh uh a guy I used to work with uh he got a Nobel Prize for this a method of called homologous recombination that was known how it worked but he was able to co-opt it into inserting pieces of DNA and so then again it's another method to help people understand biology and so uh in biology I know I've been rambling for forever but in biology the examples I just want to give you is that time and time again the Nobel Prize goes to someone not for necessarily finding some groundbreaking uh function of like how the brain works but more they came up with a way to facilitate researchers in general and their and their and and expedite and rapidly speed up the time we go from General hypothesis to testing that hypothesis to making conclusions and Publishing new insights into how biology works and I feel like this is the first time that's happened in physics right because uh neural networks are used for image classifications like like these satellite not satellit excuse me these radio telescopes they generate so much data all the time and they most of that stuff is basically run as a preliminary sort of like low pass filter on the data they put a neural network on there to identify things that are interesting if identify outliers identify Stars L classify things all that stuff is run through neural networks and it's a way of just like dealing with the flood of information that we can now generate on a huge scale and all branches of physics are becoming like this where they gen instead of like one measurement they generate tons and tons of measurement and to Wade through that a lot of people are using neural networks to do that and I feel like so what you know what they've got now is they've got someone who's who's created a method that has been that is now sort of permeated physics and it helps pretty much everyone do physics better and faster to get more cool results and so the first time ever that's happened in physics and people are getting all bent out of shape because it doesn't give us the neural network doesn't give us some profound Insight onto like the small force of an atom or something like that um but it just makes all of physics easier to do and more practical and and in biology we've been doing tha
DkmfIQRDyXc,2024-09-09T04:00:17.000000,Human Stories in AI: Nana Janashia@TechWorld With Nana,"[Music] hello I'm Josh starmer and welcome to human stories in AI brought to you by Stack quest in this series we'll hear about the career journeys of passionate AI experts from their humble beginnings to conquered challenges we'll be inspired by the real world experiences of professionals thriving in the ever evolving AI landscape today we have special guest Nana janesia of the insanely popular YouTube channel Tech world with Nana with well over a million subscribers Nana teaches all things Dev Ops an inclusive application development style that brings everyone together with a shared goal of building better products faster so without further Ado Nana can you tell us about your journey to where you are right now with tech world with Nana how did this all start um I would just go back to the very beginning of my it career which I started as um a software developer so basically I just um I had a marketing degree and I looked at the job descriptions out on the market and I was like I need a job that is much safer and much more demanded uh where um I'm not going to have a problem finding a job and that's what how I made a very pragmatic choice of getting into it without having any pre- knowledge uh knowing anyone in my circle who was in it so just like just blindly going into that direction just because I wanted job safety and that's how I started learning uh software engineering um I became a software engineer pretty quickly like in the second semester I already got an internship and that's basically how I started getting uh all the experience that I have from work um I even dropped out from the third semester because I realized I was learning way much uh way more at at actual project at work and then very slowly because I think also people don't realize it is such a vast thing like you have you know so front end development back end development you have uh data science you have devops cloud like these are so different uh so many types of different uh fields that you may be a completely good fit for one specific profession but very bad for another one um and for me it just turned out um I liked engineering but I what I was much more prone to the devops type type of task tasks even though I didn't know that it was devops back then so I was I was um uh for example the in our team um I always like to configure stuff to make the builds faster instead of actually programming some logic in the in the application and you know configuring the servers and this kind of stuff and then eventually you know I got into cloud and then I learned the kubernetes and that's how I immediately realized this is what I want to do this is much more interesting for me than web development or software development and I went just full full in in devops Cloud kubernetes was my like favorite technology for a while and um uh then very very shortly after I learned kubernetes I started making YouTube videos on kubernetes specifically because I just it was my intuition and I just tested it basically where I was like kubernetes is amazing but it's so difficult to understand and learn if you don't have proper resources and there were no resources out there I had to learn very painfully so I was like maybe I should create some tutorials because I have really good understanding of kubernetes now so maybe I should make this journey easier for other people other Engineers than I had myself and that's how our YouTube channel basically started and then the rest is history now we are here with over 1 million subscribers uh staying in the devops niche which was also interesting because very early on we decided I'm not going to do programming tutorials uh software engineering tutorials I'm just going to focus on devops and and Cloud Technologies oh wow that's amazing uh how long uh did it take to go from you know your first video to where you are now how long have you been on YouTube um it was so it was a very fast growth actually uh when we started it was just a site project for us I literally wanted to get a playlist of kubernetes out there and that was it so we're so me and my co-founder she was doing the the animations like graphics and kind of stuff and she was researching like googly like how to create a catchy title for a for a YouTube video and how to do the description and stuff um so before we were done with YouTube playlist so I had planned out the entire like uh the playlist and curriculum uh the channel already started getting a lot of traction and we saw this feedback from Engineers who were like finally I found a very understandable tutorial on kubernetes this is so helpful I need this for my job so that was the the feedback um pretty much at the the beginning because we were posting our videos um because it takes time on YouTube to be discovered so we were posting them actively in Facebook groups and uh Reddit groups we were of course you know um uh blocked by a lot of groups and admins because you know you can't self-promote or whatever but it before that happened like we would get hundreds of thousands of upwards and that's where we realized we're on to something here with our channel uh within first year we were at 90,000 subscribers and at the end of second year we were already by half a million wow um and this year in February so that was after four years of starting the channel we passed a million so for me I think it was a it was a pretty fast growth yeah that's amazing that's a great story um this is going to sound like a really stupid question uh but we have a very um General audience including me so this is really a question just for me can you in simple terms describe what kubernetes does oh okay yes uh I thought you I thought you were going to ask DeVos and I was prepared for that but I didn't I didn't think about kuet okay let me let me um think about this so very simply explained in a modern world of you know software applications which are much larger than before so you had like the simple web server now you have literally thousands or tens of thousands little containers and they have to run on lots and lots of servers so basically what kubernetes does is you take thousand servers and it has a it basically creates a layer on top of that to make those thousand servers into a one powerful large self-managing server so that's basically what kubernetes does and then you can throw containers like Docker containers on that huge powerful self-managing machine um and it will you know start and stop containers automatically if they crash they will restarted um you can put like load balancing and all sorts of uh cool stuff so that's my you know simple definition of kubernetes oh yeah so it basically treats a large cluster of computers as if it was just one and so it's a lot easier to manage you don't have to log into a thousand different computers and tweet a little file here and a little file there you can just treat it all like one and I I can obviously see how that would make life a lot simpler yes yeah so I also so one sentence description is I usually say it is an automated operations engineer for your it infrastructure where the the applications are running awesome very cool um so uh I guess the next question is um can you tell us about your relationship with give internet um yes so this was this is actually a startup that is doing a really cool thing um uh the F the the first Contact I had with them I actually created this little interview because they were doing a project in my home country of Georgia and the the main the main thing that they are doing which is actually really cool is um uh to provide internet access and basically laptop to connect to that internet uh to people who don't have that access and for me because what we are doing both of us are doing through YouTube is for example we try to make education in certain Tech areas which are very highly demanded accessible for everybody in the world right so it doesn't matter if you have money or not if you live in India or in North America it doesn't matter where you are everyone has access to YouTube and they can consume and learn these Technologies and in in a lot of cases they can find a job like they can make an actual career change through these tutorials so we make the like all the online creators who make education accessible for everyone but what they do what give internet does is basically go a step further because they are still which is very hard to imagine for a lot of the we you know lot of us basically with daily internet connection but there are millions hundreds of millions of people who don't have internet access so they don't even have access to those free resources online to YouTube education to Googling things or asking C GPT for for help and um when you think about the education is such a powerful thing and gives you such an advantage and when you don't have access to that education in any form whether it's internet or um off like on-site education that that puts you in such a disadvantaged uh position and the the shocking thing is that there are literally hundreds of millions of young people who should be going to school who should have access to education but they don't because they don't have internet they don't have laptops even if they live in the area where there is internet coverage they don't have that access so they're doing an amazing thing globally like worldwide which is amazing that so basically they are um offering on one side people who want to help and and fund these people um fund this next Generation to have access to Internet give them opportunity to very transparently very easily donate to their CA and on the other side they literally go and and uh help you know Buy laptops to to give to hand it over to those high school students to those childrens and give them internet access so they can use the same kind of exes that everybody else in the world has or their peers have in other countries who are more fortunate so that's basically their mission and I think they're they're doing an amazing job very cool yeah that's exactly why uh I've uh tried to endorse them as well um in that that's something that's always been important to me as well is trying to make education available to everybody and part just like you part of that is is making content but the other part is is making sure that content can get places um so yeah that's exactly right I have another very stupid question which is really just for me how do you st I mean in in devops uh how do you stay current how do you stay sort of up on things I I I a lot of that stuff I I focus a lot on statistics statistics hasn't changed in forever and so I feel like it's relatively easy for me um how do you stay current this is actually a good question and something that we get all the time when people see me like coming up with a different tutorial of a completely new technology every week um and so basically and I think it is a very uh intentional strategy because if you don't follow this strategy it may get overwhelming because as you said said devops is so Dynamic it's it's literally like a new technology popping up like every or new startup who is doing something in devops and cloud is appearing um so what I uh realized when I was back when I was learning about software engineering was that there are Concepts so basically programming Concepts and they are programming languages who Implement those Concepts and the same way in devops you just have more Concepts but the concepts are still very limited in in numbers right so you you don't have hundreds of Concepts you may have like a few dozen maybe 10 15 like main Core Concepts and then you have hundreds of Technologies who uh you know they implement the the same types of Concepts and the the most challenge people have is when they don't have a clear understanding of the basic concepts because they start by learning the tools and the features and they know terraform for example um but they don't know the the The Core Concepts of infrastructure as code why why do we even have this like what are the benefits what are the the disadvantages what would happen if you didn't have infrastructure as code tool so when they start by learning the the uh technology they're so deep into like the syntax and the configuration language and you know how to run the commands um and now when they have to switch the the technology or move to it something which is similar maybe doing something uh else on the side they're completely overwhelmed because you know they they are again starting learning the the syntax and uh configuration and so on but if you understand the concept it's just a matter of replacing the tools because you know because you start reverse engineering every time you um uh start learning a new tool you're like why is this tool there like who who invented it and why and the the minute you have that answer everything else kind of falls into place so that's how I actually that's exactly the so I literally would write five main questions before starting any new technology and those would like lay the context of okay I understand I just need to you know find this and this and this information and that that makes the learning much much more comfortable less frustrating and much faster that's amazing I love it um will you share your five questions yeah so the F the first one the most important one is why was this tool developed because you know you have lots of startups who are inventing new tools and because they so the founders the the group of Engineers they identified a specific use case or specific set of problems and if you understand those specific problems like the founders understood back then you have like 50% of the knowledge that you need about the tool like even if you don't know the specifics of the syntax and configuration I mean you can always Google them like you know the tool 50% already by answering that question and the second one is is what are the use cases because the the a tool may be solving a certain problem but it maybe fit for specific set of use cases much more than for others right for example a classic comparison between enzi and terraform they're both uh automation tools for infrastructure but they serve different use cases they have overlaps but you know one is uh has strength where the other has weaknesses so that's the second one again that would give you another 30% of everything you need to know about the tool and the syntax and commands are really the last the the final like 5% that you need to learn about uh about the the tools oh fantastic I love it so I absolutely love it um I'm a little jealous I've changed my mind I'm I want to move away from statistics into into death off I absolutely love it that's sounds I love it because you're focusing on the main ideas rather than the the actual details and you I I I love the emphasis that you put on on the on the concepts and and understanding those primarily and and then using those as sort of like your starting point for understanding everything else um that sounds absolutely fantastic um so you've had a uh kind of an interesting career you you gave us some tips like early on you were like I want to get a job where I can I can get a job and and then you found a problem that needed to be solved right there were terrible kubernetes tutorials out there and you were like yeah there's got to be a better tutorial and so you made that tutorial and then you posted it and holy smokes four years later you've got over a million subscribers um uh so I was wondering if there's anything else you can um if you have any other advice for people that might want to follow your footsteps either as someone in devops or I guess as a YouTube or holy smokes you've got these two very successful areas um so for YouTube I mean part of it was of course luck because when I started learning kubernetes and when I was so excited about it of course I didn't know that it's going to turn into a a YouTube channel because we had a perfect timing because um the kubernetes tutorials were basically non-existent so we were like one of the first ones to create content on the topic which was high demand but low like uh Supply which is a perfect combination if you want to grow or or anything so it was a luck uh involved uh for sure but uh on top of that like in terms of why we grew so fast was one of the strategies that we decided to follow at the beginning was uh we decided we're going to create few fewer number of tutorials so we were we were posting like once in two weeks or or one month uh so we're not like let's create a like a small video every every day um because I I didn't want to have a playlist of terraform or playlist of enable I want for every technology I wanted to have one video on YouTube that was your One-Stop shop like that was the only video you needed to to to consume to understand this specific technology and my my strategy for doing that was I would literally watch every single YouTube tutorial on that specific technology and I would literally make notes like what is this one missing what is that one not explaining what is that one not answering that I know that people would have question about and I would structure my questions based on the uh I would structure my videos based on the questions that I thought that a beginner would have um as they progress through through the tutorial and I would just write uh answers to those as a script and that was the structure of every every uh video and I also structured them the way I learned new technologies which was why was it created what problem doesn't solve what is a solution and how does this specific technology um Implement that solution and then you know some details on configuration um and I think that from my perspective that definitely contributed to people sharing our videos with their Community which again uh then tells YouTube okay this is something very important because it's been shared externally and then YouTube promotes that two um you know viewers in the um uh in that specific group so the the first one we we saw was uh our uh Prometheus tutorial was you know suddenly spiking in YouTube analytics it was like what is happening so YouTube basically realized that people were sharing the Prometheus tutorial so much off YouTube like on LinkedIn post and so on that it started com promoting that video to uh on YouTube homepage to lots of people and that's where we started okay now the channel is growing really fast and we just try to do that over and over again um so that's the the YouTube part with devops the only suggestion I have is just you have to be curious and you have to be learning constantly like that's the that's the only only way to do that because it is it is definitely very um time consuming and very resource consuming like as an engineer you have to be learning constantly if you want to be successful in devops so one last stupid question that is really just for me um so I do YouTube and but I just do it by myself I am the whole team uh you on the other hand have a a partner and and work as a team can you tell me about that what was the what was the decision to do this as a team can you tell me details of like how do you guys operate as a team I'm I'm very I'm personally I'm incredibly curious about this um you have great questions by the way um so we started the YouTube channel together because we were working on a startup together so as I said this was a site project so we already had like uh you know thought of this idea that we're going to Revolution I whatever you know the ambitious startup ideas that that you have um the first two startups that we worked together uh was were complete fa failures and that's we had like a small time window to start the YouTube channel um and because we are we were already a team we started working together it it is much easier and like completely life-changing if you have a partner with a very specific complimentary skills so something that doesn't overlap with yours which is the case with uh me and my partners so she is doing all the animations all the visuals posting on on YouTube I just record the video I do research record the video and just give it like throw it over to her and she does like the the rest of the stuff and um having this kind of um like partnership like you can brainstorm together uh like it has a lot of eded benefits but again if you don't have any overlaps because I think if if I had another partner who also wanted to like uh create tutorial videos and and talking to camera we we would need a third partner who would be doing the the backand stuff so um uh I mean I I see a lot of YouTubers who are doing it like you are which is you know they're the the Creator they're the editor um in you know in case you don't have the animations maybe you have a little bit less uh effort which are very very um successful so I think it is a you know obviously in your case you're a good example that it's absolutely possible to do this I wouldn't want to do it alone to be honest yeah I'm I'm I'm a little jealous about that too I think it'd be fun to have someone with very complimentary skills um but it's amazing I mean that's that's that's I mean it's it's very cool that you were able to find someone that kind of like had just the right skill set and just the right interest but also shared Vision in terms of like what you want the channel to do and where you want it to be yeah uh that's that sounds like magical I've played in a few rock and roll bands and that's you know we we have that shared Vision as well and that's one of the it's sort of like a magical uh sort of connection that you can have with other people of of of like we're all in this together and we're you know we're in this boat and we're gonna have a see what we can do so that's very cool yeah U well Nana thank you very much uh for being on the show I really appreciate it it was great talking to you I learned a lot personally he's a great so thank you I'm happy yeah very happy to hear thank you thank you for amazing questions by the way"
0QOm7Sn5uwQ,2024-09-04T04:00:00.000000,A few more lessons from my Pop!,a few more lessons from my pop hooray stack Quest hello I'm Josh starmer and welcome to stack Quest today we're going to talk about a few more lessons from my pop this stack Quest is brought to you by the letters a b and c a always BB C curious always be curious if you don't already know September 4th is global Frank starmer day and this year just like we've done in the past 3 years we're going to celebrate by going over three ways he influenced me number one do what you love when I was a little kid I couldn't tell you exactly what my pop did at work but I knew it had something to do with computers and health and even though I didn't understand the details of what he was doing I still thought it was the coolest thing ever and I hoped that one day I could work with computers and health I think one of the reasons I was inspired by my Pop's work is that he really loved it he was pretty much always excited about his work he'd always say the work I do always gives me a great excuse for getting out of bed each morning another reason I was inspired by him was it seemed like he was making the world a better place again I didn't know that the details but I thought my pop was helping people with heart trouble and helping people seemed like a really cool thing to do so at every step of my career I've tried to find jobs that excited me and made me feel like I was helping make the world a better place bam number two the world is awesome when I was a kid my pop used to do a lot of traveling all over the world and everywhere he would go he would make friends I mean you could probably name any country and my pop probably had a friend there and as I've grown older and started to travel more on my own I've realized that I can make friends in new places too the good news is that there is no magic to making friends people are just awesome everywhere but my pop was the one that showed me that it was possible and as a result I feel like my life is a lot better knowing that there are so many awesome people in the world double bam number three everything is an experiment when I was a kid my pop did all kinds of things some of the stuff he did made sense but some of it didn't so I'd ask what are you doing and he'd say an experiment and it turned out that no matter what he was doing if you asked hey what are you doing then he'd say an experiment thinking of everything as an experiment has had a huge influence on my life when I have to do something scary like give a presentation in front of a lot of people I think of it as an experiment when I think of it as an experiment success isn't just about giving the best presentation ever instead success becomes answers to questions like what did I learn from the experience and what could I do to improve so instead of getting scared or worried about a big complicated looking equation or algorithm or having to give a big presentation because it's just an experiment I get really excited because I'm going to learn something new triple bam happy birthday Pop now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stat Quest PDF study guides in my book the stat Quest Illustrated guide to machine learning at stack quest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stack Quest and want to see more please subscribe and if you want to support stat Quest consider contributing to my patreon campaign becoming a channel member by buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
wIGOnM6Cf_E,2024-07-29T04:00:38.000000,Human Stories in AI: Abbas Merchant@Matics Analytics,"[Music] hello I'm Josh starmer and welcome to human stories in AI brought to you by Stack quest in this series we'll hear about the career journeys of passionate AI experts from their humble beginnings to conquered challenges we'll be inspired by the real world experiences of professionals thriving in the ever evolving AI landscape today we have special guest obis Merchant founder and CEO of madx analytics which uses a combination of AI and analytics to transform Enterprise data into intelligent actions so without further Ado AIS can you tell us about your journey to where you are right now as the founder and CEO of madx analytics how did this all start so uh starting from I was in my 11th grade where it all started so everything was good life was good uh and suddenly from nowhere a thought random thought came to my mind that after doing all of this hard work like schooling exams boards college degrees phds of the world eventually I will be joining my family business so as I mentioned uh I come from a business background actually so that thought came to my mind and I was wondering what to do with it like if it is right or wrong or first ly it was a very dumb thought like dropping out of the school and it was something like right now if I think it's the worst thing that someone can think of like dropping out suddenly from school and joining the business but looking back I feel that everything happens for a reason so yeah I propose this great multi-million dollar idea to my family and they straight rejected like they also come from a business family but they know the importance of education and you need to be qualified a little bit qualified enough to handle those business right so to clar hold hold on to clarify um you're in high school yes and you're like oh I've got a business idea I want to drop out of high school to do this business and you tell your parents yes what do you guys think of my plan and they say that's the worst plan we've ever heard yeah okay okay just just to make sure we're all on the same page yeah so it was not a business idea it was joining my family business we are into retail and distribution of electronic consumer goods so joining that only there was not such any multi-million dollar idea that I have the idea was to drop out okay so the idea you knew you were going to be working for your the family business yes and and you're s you're in high school and you're like do I really need to do all this when I know that my destiny is to be working in the family business no matter what no matter what I study no matter how what grades I get I'm going to have this job and that's and I just can't fight it so I might as well just start now and that so that's what was going on yeah like I thought like all these years I should spend in real world doing some business instead of bookish knowledge that's right yeah that's I don't think that was right at that point because yeah when you think back uh it all makes sense but you need like education is the basic need like the fundamental of a human being like what we do in our life how we take decisions how we take actions it all comes from that like what what is your mindset where do you come from okay so yeah and just moving moving ahead so they just straightly rejected this idea and asked me to continue what studies and I was uh a little bit stubborn at that point of time like I was like no uh this was my mindset like it's clear I want to do this I'm not wasting my time more on studying and I I listen to No One to be honest and just uh dropped out okay yeah so after few months yeah it's uh like going back again it it's like a very bad decision but but it worked out I I don't know by the grace of God or something uh so after 3 to four months of convincing my parents they finally allowed me that uh okay now you are doing nothing and they Tred to convince me very much like my family uh like you should study you should get that basic knowledge business is year only it's going nowhere right but you should get that fundamental but I heard of no one and uh convince them somehow that please allow me to join and fast forward Josh three years I was in the business three years yeah yeah uh Working Day and Night the revenues were good the profits and I was learning a lot of new things but there was a sense of stability like I was not growing I want to grow but it was due to I don't know lack of qualifications or knowledge it was a stable like everything was already set up I was managing it and also I was uh using some of somewhat my brain to uh increase and some ideas and all of that so I was not growing enough and at that point of time I am talking about back in 2016 e-commerce was booming a lot of course e-commerce was quite before that but in in India it was booming a lot Amazon flip cart and uh and we are into Electronics retail distributor consumer goods business and I don't know how will we compete I saw that it's a major competition that we are going to face and I am helpless but I can't do anything so I started sitting with people business people uh from different Industries what are they doing what are their thoughts but somewhere I sense that feeling of missing education I don't know why that I'm either I'm not grasping things or I'm not able to understand or I afraid of the competition I don't know but there was some a little piece missing after thinking a lot or overthinking or anxiety or whatever you can say after 6 months of doing all this thinking uh I decided to switch gas and first complete my schooling so the base I I thought like I need to make the fundamental the base clear then only a building is going to be built otherwise yeah without base there is no building okay so I switch gas uh and take took that decision to pursue my at least complete my schooling again yeah and I proposed that idea and everyone was happy B but finally we were saying before three years now you are now you are understanding but yeah I I was bit I don't know so uh yeah when I I remember it was around 4 months left in final board exams that was that's what they called your 12th board exam after that after that we go to colleges uh engineering or anything so uh there was just four months left and I have no idea how in the world I will be completing after this great mind shift after three and a half years of doing business not touching the academic books how in the world I'm going to do this but uh yeah with family support with friends support and everyone came together and I like supported me so I was studying for 13 14 hours a day continuously for four months and fortunately I cleared the exams with somewhat good grades yeah congratulations that's awesome thank you but everyone was happy again that uh please feel free to stop me in between if you have any question no this yeah yeah everyone was happy again but I was not because I don't know how it will help me in growing my business because in back of my mind it was the only thing that ended up me here right I I ended up here so I thought of pursuing further like I need to get higher college degree a sort of educ education but this time it's a critical uh like it's a crucial decision for me because I don't want to waste any further single day so I started exploring what all business courses are available and what all things I can do all the bbas and mbas in the world I I explored and found that whatever I have learned in real world like three and a half years of doing business it was nothing different or I sense that it will not help me in what I want to achieve in uh standing in front of this High competition like high e-commerce thing yeah so uh again uh I asked uh one more question to myself that that changed the paths like that's why we are sitting here I guess so what I asked was I spent last 4 months studying day and night 13 14 15 hours a day doing this completing this schooling now how in the world these seven subjects or out of the seven subjects one subject will be helping me pursuing my pathway forward so I expl I started exploring so one of my favorite subjects you can say was or is my favorite subject is stats so it's all summing it summing it up all again I'm I'm fond of stats too yeah right so uh the question the question that game is how in the world is stats being useful how can I use stats in this real world uh I started exploring again and one thing that caught stats being used in multiple Industries but one thing that caught my attention was tech industry it because uh the tech e-commerce was the only reason I ended up in this situation like I am in this stage right now so why not why not understand Tech why not understand e-com so through stats so I pursu uh computer science as my career I decided to pursue computer science as my career and uh one year passed by I was in uh academics college degree there was just one subject in semester 2 which taught me the school level stats that I already know and somewhat of probability distribution some advanced level stuff but no one answered how is stats going to be useful in real world how we are going to use this knowledge that we are getting that I have got that everyone got in schooling in colleges how no one was ready to answer that I I don't I didn't get that answer to be honest I started exploring again Googling right now I'm with a different mindset when I'm researching because I have shifted myself from business from learning to now computer science so different Minds I started Googling YouTube and at that point of time uh you came into my life I'm not saying this because we are podcasting and we are sitting but it's it's a true it's true story fact so I came across your videos and how stats is being used the Practical implementation AI ml data science and I was like bam like sorry for stealing your love I was like maam i w I wanted to do this this is the thing I wanted to do uh like yes so I started learning ml STS maths gradient descent algorithms data science Python and whatever you can think of you won't believe in at the end of third semester which is six month of me learning this technology I have an offer letter in the hand with an AI ml R&D intern position awesome yeah yeah that's awesome yeah that's a very I don't know emotional or situation for me that I was nothing and suddenly in one and a half years I went into computer science and within 6 months of studying this I have this opportunity yeah now I have nothing in my mind just to grab it just to grab that opportunity no matter what I convinced my uh college that it's it's in different city actually so I convinced my principal being H ODS college that this is an opportunity that no one even gets after completing the college like people are looking for jobs right and I'm getting it so I am very grateful and blessed enough and I wanted to use this opportunity so somehow I convince them also yeah it sounds like a a consistent theme in your life when it comes to education yeah right right and I'm also I also mentioned this because today also I see people dropping out like with whatever excuses like Bill Gates was a Dropout Mark Zuckerberg was a Dropout education system doesn't teach entrepreneurship education systems does not have that curriculum no matter what excuses or what reasons you bring in at the end of the day it is important basic so in India there is a term like uh three things you need in life Roy Kaa makan which means uh food clothes shelter so I have added that like fourth thing you need is education interesting basic education that's all I don't know what excuse you you bring in so ah coming back so uh to our story yeah sorry if you have anything to add oh no no no no this is great so coming back to the story uh I I shifted back to that City and with that internship I also got a job there after 6 months and there was no looking back like I I right now currently I'm in this field for more than five and a half years working with multiple Industries different domains and I can't start to tell you how much today also how much learnings like I'm I'm learning uh new things how much new learnings I have how much new things I'm getting to know and I keep on like I wish I keep on doing this I am never uh what you say like you get frustrated by your job or something of course there are some points in time where I am also like that but I don't want to stop doing it I I'm I want to keep learning I'm enjoying it and this was all happening five years and one fine day one more question came to my beautiful mind sounds like time for change time for a change time for a change yeah that's where the entrepren path come into picture but let me let me elaborate more so one fine day so what uh the question was simple that I was everything was was going good but it was never the goal of climbing the corporate lead I will tell you a fact like until unless we know as a family like uh over from five generations never ever someone has done a job I was the one who in the SP generation doing a job but it there was no problem with anyone I was like it was not a goal to climb the corporate lead but I am enjoying it and I want to keep doing it it's 5 years it's 50 years the question let let me come to the point the question that came to my mind was in this five years I have worked with Fortune companies Fortune clients Fortune 100 Fortune 500 anyone and suddenly from nowhere uh the question was where are Fortune 5,000 where are Fortune 10,000 that doesn't even exist where are smmes small and medium Enterprises and that's where already started I started exploring again where are all this what are all these companies doing whatever knowledge and skill set I have how can I utilize it to make a larger impact no matter I am I'm making impact to that particular segment so along with my job I started doing market research validation now sitting with a different set of business people business owners uh industry networks all experienced people out there and I found a gap or I would I would like to say as an opportunity so what it was Fortune 500 Fortune 100 companies are all data aware AI aware they have all the infrastructures they have all the budget of the world there when it comes to the companies who are not Fortune 500 firstly they are not aware so I did this research for one and a half years with my job so I'm bit slow uh as you can and see see like three and a half years one and a half years of research yeah so uh yeah so three things I found out doing my this one and a half years research is firstly they are not aware how the data that is sitting ideally in the systems can bring value for their business can grow their business they don't have any idea second was affordability that we can understand third was all thanks to at GPT open AI because of that people are started to getting to know that AI is something before that AI was like robotics Terminators and all yeah yeah yeah now people have that understanding that AI is something that we are using in our day-to-day life so third thing was how they can utilize this technology in their business how in the world they don't know so I grab that this this is a gap this is an opport opportunity and I want to do this yeah so I took this step so before 6 month I Incorporated and found and Incorporated matx analytics that's what our company is called and from this year like from uh this new year I qu quit my job and Go full in uh like went full in you can say wow so you've started your own business yes started because I found in this one and a half years that it it will not be something that I can do as a side it's a problem that is going need to be solved and it will take some amount of time energy and everything and I it's just keeping in mind one thing that I want to utilize whatever skills and knowledge I have to make an impact to a larger audience yeah that was the mindset yes was that a scary step was that like you know when you leave that job and all of a sudden you're kind of floating right when you have a job with a company that's doing well there is a sense of stability there's a sense of just knowing exactly what's going to happen today but also six months from now you'll you'll be kind of doing the same thing making the same amount of money and then you said I'm gonna I'm gonna I'm gonna jump off a cliff I'm gonna say goodbye to all that stability and all that knowing of what to expect and I'm I'm just going to jump off and I'm going to see what happens was that scary very scary to be honest yeah because you right now you know how I have reached till here how I have reached it I was at senior lead data scientist position at Moody analytics is a listed company in us so uh I was like okay now like a family also like everyone that I like in one and a half years I have done this only like research and how will be how will I'm going to do this so I tried for six month like in my one and a half year research for six month I tried so I got one client also but okay uh I got to understand very early that it will not work out because I was the one working for it for that client yeah how will I B build a business I was doing freelancing that's what fre that's called freelancing that's not called established corporate company and this is not how we build it I was the one working in for that client now how will I going to build a business how will I going to build a team and five different departments you know sales marketing and all of that because I come from that I I I I got to know in this six month that it will not work out and also I have a client that also gave me some sort of confidence that yeah okay and due to covid so I I'm just saying it was scary but due to all these Reasons I'm able to make that step first is I got a client thumbs up that this thing is working out real world second that I I realized that I'm alone or if I will be the work one working for the projects or clients it's freelancing it's not business and I'm just trading time whatever I'm doing in my job I'm doing here and yeah third was like how in the covid yes sorry uh covid so because of covid I was working from home so I have some sort of savings from working so I can take that risk and also I'm privileged enough to come from that family or background that I like they are not dependent only on one income stream so and I also have a Runway of 2 years I would say right now it's just 4 months but yes so all this now I am very like doing calculated risk before you say as you said jumping off a cliff leing education dropping out uh taking computer science I I have no idea what how it is going to help me in my whatever I'm planning to do and taking this internship dropping out of college not dropping out but partially dropping out you can say I was I was not attending I was just going for college to give exams to be honest yeah because there was no such subject in my curriculum which I wanted to learn then that's no point so all these are jumping off a cliff as you rightly mentioned but this one I was like now I'm experienced bro now I I'll be taking calculated risks yeah okay so after small Cliffs small Cliff yeah small Cliffs not huge Cliffs correct and again I proposed this idea and they were like now you are talking my family were wow okay congratulations congratulations okay let's do it like they were like always supportive always like uh I'm uh grateful for that that uh if you think it's something you believe in if you think you are going to enjoy it uh like every day you need to wake up doing it then go ahead please yes because it's like they are also like that mindset that it's better to fail than regret mhm so I jumped in all in uh from this 3 to 4 months and you won't believe whatever I have not learned in this no no it's not a surprise but that's it from your expression I guess you are expecting but you won't believe from this 3 to four months I have learned so many things that I have not learned in my 9 years of Journey like 3.5 years in business 5.5 years in the corporate world and it's been amazing until now so far like how to build a team how to build a business uh marketing sales operations customer success and you don't ask like I'm really enjoying it and I want to keep doing it just to end just to end my like this is this that's all like like there's no more Clips or surprises for you yeah I love it I love it when uh so this also came from Once someone a reporter asked to Bill Gates that I don't know if you are aware but I saw that reel or something that what in this world you are afraid of losing it's Bill Gates afraid of nothing right and his answer was I don't want my brain to stop learning yeah and I was like yeah this is it I want to do the same I don't want to stop learning until unless I'm learning I want to be in this forever yeah I feel like that's what it means to be living is to be learning yes um yeah um that's an incredible story can you tell us about what your company is doing I it sounds like you're you're in the AI field but can you give us some more details about what you're doing sure I would love to Josh so right now we are working with couple of clients it's very early stage and with a small team so we are working with different Industries but uh mostly what excites me also is ml powered marketing uh that's what we are currently working in and also uh there is customer retention use cases so in real world how you prevent customer for from being churn before the churn right yeah so that's one and then there is some fraud detection in financial domain we are also there are a lot of AI and ml use cases but uh I can go deep on what we are working whatever field you are interested like ml power marketing or yeah let's let's talk about ml powered marketing uh can you tell us a little bit a bit about that and sort of what you're doing and maybe how you're doing it or can you give us those details yeah yeah sure I would love to so uh basically it's targeted marketing on based on ML so it's a financial company based out of us that we are currently working with so the use case I will be brief overview of the use case was how to select right target audience for our marketing campaigns this okay is the question that the company was facing like so marketing is cost to company that's it in simple words either you are putting out an offer 20% off 30% off 50% off either you are reaching out to anyone for opening a new credit card it's a financial company so if if if a person is not likely to interested in your credit card offer they will likely opt out right and when you need that person when you need that person in your marketing campaign he will not be there because he has already opt out due to that irrelevant offers so that's again a cost to company which is not in terms of money but in terms of lost opportunity okay yeah first is the offer which is like you can't randomly give offer to all 50% off right that's a cost to company you want to send offers to relevant people so let's say if Jos is buying from Nike Nike is a company and you are a regular customer you already buying so there will be a different offer for you I am a customer who have bought shoes from Nike before 2 years and now they want to retain me so there will be a different offer for me we both are not same here yeah and third one is customer acquisition if you want to opening a card or irrelevant irrelevant office that's also a cost to company so we are building machine learning models a multiple ml models to resolve this multiple issues so it's not just one because marketing campaigns as I mentioned are for different segments different scenarios so first is uh propensity modeling that's what we are building the model to be in simple terms it's uh predicting the probability of a customer opening a credit card with this financial plan looking at their past historical campaign and transactions and all of the data second was expected spend like what this customers are likely to spend in next 12 months so based on that uh they will be uh giving the offers propensity only and third one was Channel preference uh like what channel they prefer now uh to elaborating more on it before I dive deep do you have any questions just just to make sure I'm on the same page um so one is identifying customers two sort of categorizing them maybe since this is Maybe maybe if we're talking about credit cards you want to know who's going to be the Big Spenders uh and who's going to you know who who who might get the card but not use it and and that might create an expense for the company as well and then the third thing that you're interested in is umh what was the third thing uh the channel uh what channel oh what's the best way to reach that person if if you're going to because because you you know who they are and you know what they're going to spend but how can you contact do you call them on the phone do you put an ad on their email or how do you reach those people yeah that's the third thing correct okay I'm ready yeah sure so uh after understanding the problem statement of the client and so the major overview headline uh problem statement was how to select the right target audience for our marketing campaign you want to reach to the Right audience right yeah from that diving into de this three scenarios uh came in this three problem statements came into picture so we started looking into the data that's the first step like like after we understand the business problem and all so they have customer transactions data it's a financial firm of course it it's required right then products data items data and all of that data that we actually need for building that Predictive Analytics predictive modeling okay check mark That's it uh we have that data now second thing it second thing is how do we utilize so they have around four years of data with them like that's what we need past historical data to predict the future so we come up with a performance window and an observation window so for to elaborating more on that performance window is something for example there is a one year window that we will take that is a performance window in which we are a customer of of that brand in that window and what is our behavior in the observation window sorry observation window is the current 12 month window which we'll be using for model training and performance window will be the next 12 Monon window so 1 January 2022 till 31st December 2022 one year window will be the observation window so training data what is the uh user Behavior customer Behavior okay and performance window will be the next 12 months 1 January 2023 till 31 DEC what is this customer segment doing in next 12 months yeah okay now coming with the first problem statement so we started with all the data pre-processing getting all the data data warehouses and all then pre-processing feature engineering modeling all the step St first problem statement we for propensity modeling because why I'm saying propensity is it's different I I hope you might but for the audience it it's different somewhat different from probability modeling what we do eventually it's it comes with some sort of prising like why why you are taking this decision so right now we also need explainability so we also need to take care of that so this all come after our multiple discussions with the company so what we identify was the data was good like after doing all the feuture ranging we have all those features we also used after experimenting with of course when it comes to tabular data I'm a big fan of machine learning models algorithms boosting bagging models and I I like would not like to touch deep learning when it comes to uh tabular data to be honest like it's I don't know I'm biased or not but yeah well it's I I feel like the the the the methods you were just talking about are are lend themselves to easy explanations and and the the Deep learning and the neural network stuff is it's it's somewhat more opaque about how the decisions are being made and that makes it more challenging to explain at least to your client what's going on right yeah and yeah here's reasoning is also into picture so what we did was uh we choose like after experiment with multiple models and multiple feature engineering iterations we chose R boosting algorithm so final model was I think one one was exib and two were light GBM that where that that worked quite well now here so again making it more clear it's a prediction problem so binary classification problem what is the probability of Jos opening a credit card within the next 12 months and what is it if it is not so if it is towards one then you are more likely to open towards zero it's not likely to simple but because that's a challenge of Reason reasoning and all so what we did was uh I don't know if you are aware of this but we used first of all of course you are aware Au R Au to train our model the evaluation side because it matters a lot uh like the main part from my was Data pre-processing feature engineering data part modeling was just 10% and then evaluation how we are going to evaluate how it will work out uh so isn't that funny the way it is it's like most of the work is with data yeah creating the the machine learning model is a small part and then you evaluate the model to see how well it's performed and that stuff is almost like it's not an afterthought it's it's the you know something you really want to do but it's I found personally as well it's all about the data yep I I totally agree like it's all about the data we have spent I would say 70% of the time doing feature engineering data pre-processing and all how do we create more and more relevant features that for example uh customers spend in last 12 months customer spend in last 6 month customer spend in last three months what are the amount of uh where it where he is he or she is spending all those 11 features we come up with and finalize on those features and doing after the doing the modeling the valuation part that's what I that's what I was talking about so training matrices R binary classification works well so I'm talking General like whatever three models we have built the purpose was different but the methodology of course features were also different but I will not get into it because it's a lot uh but the purpose was different like it was at the end of the day classifying the likelihood of a customer opening a card spending so it's a regression problem how much reward is spending and again the binary classification Channel preference what channel prefer but because there are three different use cases I will not go in each and everyone otherwise it will be a long 3 hours already have taken so much of time in my journey but yeah yeah yeah so uh H after evaluating the model and R and all we different we come up with this different I don't know if you aware but lift and gain charts that's what I have learned from my previous industry experience where I also handson working with marketing analytics with can you tell us about these yeah sure yeah so lift and gain charts it's mostly use in propensity modeling to identify the output of your model let's say 90% au but we we want to select right target audience bro uh how we'll do with what is 90% a how you select the right Target on so that's where it come into picture what it what it did was uh this is a metrix of course uh you can like the audience can also learn about it Googling it but uh the major idea was splitting or segmenting this customer base into desiles and say Diles in buckets so there will be 20 Diles in which for for example we are dealing with in training data 1.5 million around 1.5 million of customers and testing around 700k I don't exactly remember but yeah so splitting this 700k or 1.5 million to test the train lift and gain and validation test lift and gain split this customers into segments 20 different desiles and check the probability likelihood prediction of this particular segment not at an individual level so whenever I also someone says personalized uh recommendations right or something like that so there are segments it's not like one particular individual so uh there are like uh segments in which we calculate lift and gain now coming to the lift and gain what is it lift is basically uh uh explaining if we are targeting that particular segment randomly as compared to we are targeting that segment with the help of machine learning prediction okay so let me more be more clear targeting that particular segment using the prediction of an ml model and targeting randomly anyone how much uh lift lift is let's say we got 2.5x lift so it's 2."
jp_NcF9Oagk,2024-07-13T04:14:01.000000,Luis Serrano + Jay Alammar + Josh  Starmer Q&A Livestream!!!,hooray I think we're live so hello welcome to my live stream with Lis J and me my friends I canot wait to answer your questions hooray maybe not my best song ever love those songs but it's done yeah so uh everybody Welcome to uh I we got to come up with a name for this live stream because it's definitely not the stack West live stream this is like the the Louis Serrano Josh ster and today J Alamar live stream Q&A uh awesomeness uh so uh I'm really excited about this I've got two of my biggest Heroes here with me today I cannot wait to hear what they have to say we've got some great questions coming up we're going to be talking about Transformers we're going to be talking about career path the big stuff and we're also going to be talking about other things uh and one of those things is uh I uh Louis can you ask the question I forgot who it is that uh that asked this question yeah definitely so first of all I'm I'm very happy to be here with you guys you are two of my AI Heroes and two of my favorite people so I'm very happy to be here we have a great crowd the first first question for people I just want to say for anybody joining us like where are you joining us from so I'll I'll start I'm uh I'm joining from Canada I'm originally from Colombia uh very excited about the soccer uh coming up uh and uh by I'm in Toronto Canada and uh you are and uh yeah you're in North Carolina I believe I'm in North Carolina Jay where are you right now originally from RI Saudi Arabia uh as part of my role I travel a lot I speak at develop our conferences at the moment I'm at uh coh's headquarters uh in in Toronto so that's where the companies sort of based so I spend some time here as well Tor fantastic yeah great Jay and I hang out in Toronto you more than invited Josh to come uh I know I have I can't wait to come up I'm gonna visit just you wait it'll be the coldest day of the year that's when I'm gonna come okay sure it's easy to pick yeah you have a lot of days to pick yeah all right well let me just start let me just feed a question that came on the live and by the way anybody here oh I see they're joining us from bivia Australia India Saudi Arabia like very nice um what time is it in Australia right now oh it's probably like the same thing backwards like 11 yeah so this is like late night TV exactly we're we're on late night it's awesome yeah yeah um so by the way for people in the live yeah we have some questions that people asked pre before so we're going to answer those but by all means uh ask any questions in here I see someone from Colombia hi Susanna from monreal from Cyprus uh I'm going to ask one question and it's for the three of us that uh Muhammad asked and it said um start with Josh for who is the target audience and what what are the reasons for you choosing a and dramatic constant style so basically you know you have a really cool funny Style with L running gags and stuff like that how did that how did that start or you know yeah where did it all come from so yeah so this channel I originally the original idea for the channel was to uh directly teach my co-workers uh what's going on over there Louis sorry I just got a just rearranging the furniture exactly anyways so started off trying to teach my uh my co-workers I used to work in a genetics Lab at the University of North Carolina at Chapel Hill and the idea was um they needed to learn statistics and so I started making these little PowerPoint presentations and I had a little Friday morning chat uh where I try to teach the you know genetics researchers statistics and so if you watch any of my early videos every example is given in the language of genetics and mouse genetics in particular because the people I worked with did Mouse genetics and so I was directly trying to talk to them very specifically uh I I had had in my mind that there would there would be like a virtual bookshelf in the lab itself I guess it's virtual so it wouldn't really be there but the idea was whenever they whenever these people my audience of six other people needed to know something they could just click on the link and they would learn about it and it would be in the language that they understood so it just so happened that other people started watching these videos and it was non-mouse Genesis and so then I started trying to like generalize my examples and make them a little um more approachable from different angles something everyone could relate to I talk about M&M's candies and things like that everybody likes candy I think to a certain degree um or most people um and so I tried to broaden it uh but the idea is um it's for you know I'm trying I'm still in my mind I'm still trying to teach these Genesis who aren't very good at math but are very smart people so if I can explain everything in a step-by-step way they can follow it and they're smart enough that they can figure it out and I've just I've just kept that as my sort of Ideal U audience is someone who is smart uh but maybe coming from a background that isn't mathematics and isn't uh genetics or maybe that is genetics or may you know it's it's it it maybe it isn't statistics maybe it isn't ml maybe it isn't AI um but they're smart people and so if you go through things slow enough and one step at a time they'll get it so that's my intended audience and that really has driven someone also they also wanted to know what my style why I I settled on my style um that's also been driving my style uh which is the original audience was trying to teach statistics to people who hated statistics so I came up with things like bam and double bam and I came up with little characters Squatch and Norm to kind of like make it a little bit more interesting and a little bit more exciting um than maybe statistics actually is I I personally feel very excited about it but I know a lot of other people are not so I try to inject things to to just kind of pump people up and that I think hopefully explains the style of stat quest uh yeah Lewis Jay do do you have anything to say about your I know you guys do things different like is there a story there we actually had a similar start because Jay and I worked at Udacity together we worked at different places worked at coh before we started Udacity like almost eight years ago or something D and uh I started basically making the videos to send to other people to teammates at at Udacity for for like this is what we want to do in the course Etc and then they just kind of became YouTube videos so the channel started getting views um but I also yeah so our our goal that Udacity was always to teach it to everybody and so I was trying to trying to make it as wide as possible so like beginners would would understand the stuff but an advanced one would would still be like oh I you know I see what you did there right like a different spin um so try to be as as as wide as possible and I yeah I also started with some I I think I had the only I don't I don't have a lot of running jokes but I I I had one with mountains at the beginning because I said grading descent is like descending from a mountain and so I call it Mount errorist and I felt like a genius when and then I realized that it's very easy to put error names in mountains because then you can Su Mount kill him and jeror and then I went to give a talk in Seattle Washington and I said mount rier and then it just became a challenge to make any any name into a mountain so I gave to talk in Spanish and I call it mount a kagwa that's a stretch but the biggest stretch was in Iceland because the volcano that stopped all the flights in 2012 it was called I may I may butcher this but it was called the aaf a like that hope there's no islandic people that correct butat Yul was the big volcano in Iceland and I then I looking on Google translate I found out that the word for AA in Icelandic it sounds like vidla oh again I may be wrong but and then I just call it the AF vidla yok and so I had that and that's when I ended with mountains because you can't talk that right like uh that's as good as it gets so that's that's the the only running gag I have I may I may get to more later but uh but that's the one yeah what about you Jay you got a good good style I started out in blogging actually in a lot of of my inial sort of content was was written but it was highly visual as well so the main artifact of it is the is the many visuals and my first videos I created when I joined Udacity with so a lot of it is just taking on what the team was um was doing there so in terms of scripting it explaining a specific concept um but definitely all of my influence is the the two of you and your videos like I've I've vlogged hundreds of hours sort of watching um both St and and and Lou's Channel and um in addition to YouTube I think maybe three brown one blue or I always get these mixed up um so those are the things that just made the the way for me to understand data science a lot more accessible because new Concepts are a lot of the times intimidating um but then when you have people who present it in this sort of style that's playful that's gentle that's a uh I feel very grateful to the people who sort of sort of do that and sort of feel um the need to to push it forward so with Udacity I think we created maybe 10 15 videos and I think I've had also a lunch with Louise where we it was the first time where I was scripting something with somebody else sort of not just working in my own on on a silo um and that one lunch definitely sort of Highly elevated my my video game and it it's when I transitioned to Apple keynote which I still use up until now which was just um dramatically improved my visuals game and animations and uh so it translated to both video and improved a lot of the the the written things that I do now video evolves and a lot of it has been a learning curve so YouTube was the big thing five six years ago um it still is very big for education but then I also sort of think about and explore other social um media so I found LinkedIn to be sort of interesting but then how would a world how would a video be different if it's going to be posted in LinkedIn versus YouTube for example and then you have all the vertical videos and the short videos with Tik Tok uh Instagram reals YouTube shorts that's another complete format that sort of needs to be figured out a little bit and it's it's it's quite different so it's a bit of figuring out how the the the platform sort of evolved with time G I need to talk to you about all of those alternatives to YouTube as soon as is over I'm like what am I supposed to do no I definitely think of them as just ways of repackaging like the hard thing is just creating the initial video and then clipping it changing the the formatting a little bit and just you know reuse of great content you've already created yeah yeah well excellent we keynote right do we all use keynote I think we are we're all keynot guys yeah I I've actually learned keynote from Jay I watch he's got a video on like how he does cool things in keynote and so I watched that and I'm like oh I can do cool things too we're all grandfathered in Louis then and I thought you K Wow that's definitely my greatest contribution that's right we should draw a tree and then Kino Grandpa yeah exactly yeah you should get like I don't know Apple should sponsor you or something like that they should yeah if you're listening app all right so I I let me say one to the to the crowd we are going to have we have a few career questions feel free to throw them in we're going to have a special time for career questions around the half time so about what would be the 30 the 30 minute Mark uh but feel feel free to throw them in I see a few new questions uh and uh you know let's let's do want let's do some questions before we get into the meat of the llms uh Kik is asking about uh can we use machine learning to predict time serious textual data anything you want to add there yeah so I'd say I don't know maybe uh I actually have very very very little experience with time series um but I have heard rumors in the hallways um that one effective use of one of XG boost is for time series a lot of people use it for time series so if that's something that could be applied to your problem uh that's something to look into I don't think that's a great answer to this question but I did want to throw that out there just in case that's good XG boost I didn't know yeah I would say uh throw in probably recurrent neural networks those are just the time thing hidden Mark of models uh if they're still used I I know they're good uh as convolution networks of one dimension those are good for time series and I uh I've heard a lot about ARA recently it's uh kind of combines a lot of things and it's one that's for example I I see it's native to a lot of uh um uh tools that that use it in the background so yeah but anything on llms Jay anything anything yeah I REM a lot of let's say the you know pre- Transformer models there are some Transformer time series applications but they tend to be if you're not just predicting one time series if you have a lot of them um that's when where Transformers tend to sort of outperform other methods that's when I last checked like a you know a year or two ago U but time series textual data like text generation can be thought of as that way GPT models um can be thought of as you know predicting over you know a thousand uh which let's say if your model has thousand dimensions in the in the the vector that that's one way of of thinking about time series for textual data but I think mostly we're addressing time series just in general so I have a um genetics right Josh excuse me oh oh go ahead oh I was gonna I was going to just posit an idea uh which is um if it can be solved with a recurrent neural network then it probably can be done with a Transformer better is that what do you guys think of that statement yeah a lot of approaches are uh what you get with rnns that you that Transformers are just gaining on it is context size so Transformers were sort of for the first few years were limited to in their context sizes but that seems to be going away and we have these very very long context models that sort of um seem to be gaining on on that context limitation what about ssms because they seem to be the Transformer version of a RNN right because I would say the Transformer is the Transformer version of a neural network and in my head the SSM is like the recurrent Transformer right do you think those will have a get the best of both worlds like the best of RNN and the best of Transformers they are showing promise uh we'll see how the adoption and how the scaling sort of uh continues but it's a it's definitely an active area of research and a lot of people are sort of bet betting on it but it's not yet gaining at least in the text generation it's not gaining on on Transformers yet but it's uh important to keep in uh on the radar and just because my brain is about the size of a small green P SSM is State space model is that right exactly okay I got it I'm so proud of myself pretty good I there's some of those acronyms that I always get wrong I can never get DPO rate what's that like I don't know I think it's direct preference it's either direct preference optimization or deterministic policy optimization or the combination there's four possibilities uh so yeah I mean they they get there's too many acronyms so yeah so in the chat I can paste uh yes a visual guide to Mamba and state space models uh this is an incredibly well-written article by my my co-author of book we're working on Martin clors yes I love it pasting it in the chat it's an incredible Illustrated guide to how these models uh that's awesome yeah put that in the chat by the way uh you just mentioned your book which I love I can't when is it coming out it's the best book ever thank you so much really appreciate it so we've been working on it for close to to a little bit more than a year now I think September is when it's going to be in print so we're that's soon that's a month and a half away yeah yeah we're wrapping it cannot wait I can't wait and the title of the book is is it's a Hands-On large language models uh it's going to be published by O'Reilly it's going to be in about 400 pages and it has maybe about 300 original images that explain Transformers their various applications how to fine-tune them how to understand how they work uh under the hood and but but just a lot of focus on applications yeah it's it's absolutely killer I just finished reading it a week ago and I just I absolutely loved every single page it was it was sometimes when you get review books you don't read the whole thing I read the whole thing this time I was like oh my gosh I felt like I was like if I don't read this I'm G to be left behind I I was like I was compelled holy smokes it's so great I cannot wait for it to come out uh Jay when it comes out we have to have like a whole episode just about your book just maybe going through some examples or something I cannot I just I absolutely love this thing that's great to hear really truly appreciate this absolutely yeah no would love to to to come and speak about it all right absolutely now anybody who's read The Illustrated Transformer which is the place to learn Transformers I mean like that's the book is basically like a a lot of a lot of that Clarity in exctly and chapter three is a is is a refresh of The Illustrated Transformer which was seven years ago oh my gosh but then how have trans have how have Transformers changed since then and so we update them that uh of modern day Transformers with a focus on text generation ones because that's the offspring of the Transformer that really took off yeah yeah all right Lis what's next well speaking of Transformers I I feel like uh we have so many questions let me start with the one that says can you as from r one can you effectively learn and Excel in generative AI without starting with NLP so let's start with the non NLP questions and then yeah what are other things we can do in generative AI that are aside from NLP I feel like images could be something good right well I mean you can think about the various modalities right generating video generating images generating uh audio um and understanding those as well so yeah understanding I find the term generative AI a little unfortunate because it just has people think about the generation side but the representation is just as important and the things they uh uh a lot of this rapid uh growth in AI is because we have better representations for for all of these um and so yeah you can pick a modality and um and and go with it um I play around with a lot of these products for video um and but they tend to be possibly a little bit more complex but if if you have more experience with computer vision for example so yeah doing things like stable diffusion or um diffusion models is is a good place to start I would say yeah yeah definitely understanding is something very important too right like I learned from jef I've seen two things that representation and the understanding I see it as like a like a human right you can't talk well if you don't understand a lot of stuff right so you need that the part of the understanding which is uh pretty much what was there before Generation right like classification all that stuff clustering like embeddings pretty much it's a good way to understand text and then be able to talk more true yeah because a lot of these models are encoders and then a decoder and exactly you know if you give a text generation model some text array prompt and or the image generation model it needs to understand that prompt somehow and so it uses a language model just to understand to represent that text and then it passes it off to an image generation component that sort of turns that understanding or representation into an image exactly I I don't know if this is part of the question but I I I'm going to just say this uh I I don't think you know natural Lang processing and that as a field has gone back years and years and years right you don't have to go all the way back to the beginning and learn like you know Chomsky normal form and all those like you know Linguistics and things like that um you can sort of like dive in at the at the transformer in the deep end with Transformers and sort of learn the basics of how they work and sort of just start from there you don't have to like in terms of if you're thinking about natural language processing as a field of study uh that has a long history and all these it's got its own language and of its own in terms of terminology and whatnot if if that's something you're worried about that's I don't I don't necessarily think that is needed um you can kind of sort of dive into the deep end with Transformers because they were uh in my mind it's it was sort of like a a a a you know like a like a like a paradigm shift with Transformers right before then people were acting ly trying to sort of like parse sentences and like come up with hierarchical structures and how sentences were were formed and like verb versus nouns and all this stuff and they could they could graph all this stuff and that was really sort of the state-ofthe-art um eight years ago yeah and then Transformers came on the scene they're like oh we don't care about any of that stuff we're just going to take tons of data and just sort of average over all of it and and that has worked exceedingly well and it's made all the sort of like the earlier work although interesting and cool and probably still has some um relevance and can be incorporated in cool ways uh I'm not saying it's a just it's all it's all garbage but but in terms of just if you want to dive into sort of modern Ai and things like that you don't need to have all that extra stuff you can just start with Transformers because that's when this era sort of began was in 2017 definitely you can start you can start here yeah uh I I I I find this interesting analogy like I would say that NLP models in the past learn languages the way we learn languages as an adult in classes like with adjectives and nouns and structure and then the Transformers learn a language the way a baby learns a language which is throw words words words and then eventually structures start appearing in the background but there doesn't even know what there is he just learns to talk well and and obviously you learn to talk better if you have the if you're lying like a baby right yeah um um so so yeah this goes into the next question which is you what's the suggested road map for learning gen and I agree with Josh completely that you don't need to go to the beginning you can if you want and it's you know sentiment analysis fun things that are interesting uh but definitely you can start with you can start with llms and you can start with just prompting Chad GPT or prompting you know goir model CL any any of those and and seeing how they work uh what about you Jay what what are suggestions that you have for learning I mean different people will have different paths and definitely work on what interests you and what intersects possibly with your current job or you want your job to uh to go so all of that knowledge sort of uh helps you and helps your your career um yeah for me understanding how how the model works and then building with it but also writing about it just docu learn in public uh show the people out there what you learn that will also help you sort of that's one of the when we're talking about videos like I create videos mainly for me to understand things and I write as well for for for that reason so as long as you sort of keep learning keep documenting and learn in public and publish the things that you learn and the things that you build um you'll you'll you'll pick your own sort of um path through it of what interests you and what sort of um next things interest you and there's not a shortage in what kinds of cool things you can build out there right now with the with the current state of of the models and how how many of them are um available for you to sort of play around with and and you know either fine-tune or just use pre-trained models uh as they exist and just you know chain them or prompt them in different ways and or do prompt chains that make them do interesting things so definitely experiment and learn in public and uh these are the two best things that I I can advise and watch uh Josh's videos on Transformers and read The Illustrated Transformer and and Watch Jay's video yeah exactly uh yeah I want to segue to another thing that I know Jay's working on how does it's a question by Callan how does the llm work as a GitHub copilot or office pilot or some other co-pilot what are things involved to work as a GitHub copilot is there difference in training data and output summarization basically you know you can segue this into tool usage in particular writing code but anything else you want to throw yeah so this I think this goes back to the point that both you Josh and Lis mentioned just now how how NLP sort of transitioned from 10 15 years ago being writing all of these you know code that does par 3es and this is a verb and this is a noun to the model the code being just 100 lines of code or 150 lines of code but then with that you have 100 billion tokens of of training data or a trillion tokens of of of of training data that is that is high quality and so what became becomes more and more important is that data how clean is it uh and then how it's shaped across the three stages of of of training so how you can get a Cutting Edge llm right now is three stages and the first one is called base training and that's where maybe 95 or 90% of of of the training and resources go there that's when you do the next token prediction uh predominantly the language modeling uh training um on it and that's where you need a lot of tokens but you need them high quality you don't want to have uh let's say spelling mistakes or grammatical mistakes because that's how the most of the model sort of knowledge U is available and that's for example a model like gpt3 is is that is just base trained but then if you ask a Bas trained model a question it might give you a question back um or if you you know might be thinking that this is a you know a list of questions which is something that it's seen in it in its training product once people started interacting with base models they wanted to say they wanted a different kind of behavior they were like okay when I give you a a request I want you to obey that request don't give me like say a list or a or you what's a possible continuation and so you have this second step of um supervised fine tuning or instruction tuning so if I ask you a question give me an answer if I give you a request fulfill it or obey it for me and so that's you need a different sort of set of data for that and you have you know annotators and data sets out there that do this kind of instruction tuning which that data looks like here's an instruction and here's how to obey it you know write me an article about um Mouse genetics for example and then the completion is the is the article and the contents and one that is sort of high quality or summarize this long document and then the continued completion is is the summary so that's the second step and then the third one is the preference tuning that's where you sort of get to align the model it's called alignment or sometimes it's called rlf Lou already mentioned o as as one of these methods that that do this um without needing to do very complex reinforcement learning and that needs a different structure of of of of the data as well um that tends to be sort of in pairs so this is a request to the model and then these two are this is a good let's say completion and this is a worse completion this translates into codes code in the same way and so to do things in in co-pilot uh like U manners you need the model to have seen a lot of code in the Bas training uh and then the tasks that you need the model to do need some some data in the in the supervised fine tuning so if you use copilot or I've or repet for example or any of these idees um you interact with them in a couple of ways so one is they can do autocomplete so autocomplete or generation for code you write for example a um the definition of a function and then you want them to complete it that's one way um or sometimes you want to ask them a question about the repo and they answer it and so the more of that type of data that you include in the in the supervised fine tuning phase the better um that model is going to be able to sort of do those use use cases that's a that's a little bit of you know an entry way to how how these mods are are built great thank you so good of you and and and then industry what you know what other like tool cases do you think are going to be big over the next uh few years that's Segway into another question so as still I think even if the developments in modeling stop today we still have a good five years of just engineering and product and business model Innovation for what the models are capable of doing uh today um a lot for a lot of cases you still now need to go to the model to do the work and then come back um I need we definitely need these models to come to our workflows so when I'm scripting in keynote I need keynote to sort of be able to generate all the audio or the video when I'm writing an email and Gmail um I need the model to be there for the common use cases that I need so we need a lot more of either the existing tools to incorporate these these use cases or for a new class of of tools completely uh that are built with with um gen in mind as as native um like we don't have the right interfaces to to to do a lot of these things and I think there's a lot of opportunity um in in innovating on on human computer interaction with generative AI being um let's say a native part of of of Designing that experience we still um we have a long way to go in in terms of building these user experiences well yeah a lot of cool stuff um thanks I think we're you know we're at the half mark and we uh promis we're going to talk about careers and we have a bunch of questions about careers so let's uh go into that a bit and we can go back to llms uh after this a lot of I see a lot of other questions in the chat about llms but uh yeah come a few questions talking about you know how to how to get into jna how to get into to the industry uh after taking some online courses etc etc uh there's also a few questions uh one by by Amit about a learning map for graduate students to start researching in NLP so I think we sort of take this as a general question and be like what are your sort of General recommendations for somebody to to get into this field and I like this because we probably all have three different paths that's true yeah uh I'm going to start uh because I just can't stop talking about it but my new thing my new career recommendation is as soon as Jay's book is available buy it and read it because literally I it's rare that I see read read something and I'm like oh I got to try this oh I got to try this you know it's like everything I read about that thing was uh it was all these kind of like cool use cases of of llms Beyond just like like you know can you write a poem for my you know girlfriend's birthday because I don't have time to do it um that kind of thing uh it goes beyond all that stuff and it's like like this is one example where he does um he takes all these documents and he clusters them and then he annotates them and he gives summaries and he gives labels for all the different clusters and it's and it's just like the coolest thing ever because when I was back in genetics that was a real problem we had which was there's so much uh so so so many researchers are doing so much stuff and Publishing all the time how do you keep up with it how do you categorize it and it's kind of a nightmare and I was like oh if only I could have done this back then that would have been super cool so I I remember just reading these things and and the way the obviously the way the book is written is it's it's you're not supposed to be an expert you're not supposed to know how to do all this I mean basic coding but not like crazy complicated coding because for the most part all we're doing is and I know I'm you know Jay could speak about this more than me but I'm just so excited about it but a lot of it is just make doing making configurations uh for the llms and not necessarily 
C9QSpl5nmrY,2024-07-01T04:00:05.000000,Coding a ChatGPT Like Transformer From Scratch in PyTorch,we're going to code a Transformer from scratch hooray stat Quest hello I'm Josh ster and welcome to stat Quest today we're going to talk about coding Transformers from scratch in pi torch this stack Quest is brought to you by the letters a b and c a always BB C curious always be curious I also want to use this opportunity to increase awareness of an awesome charity called give internet.org a platform that makes it simple and transparent for anyone to sponsor internet access laptops and education for underprivileged students note the code in this stack list is available for free so use the link and the pinned comment below to get your own copy and follow along lastly in this stack Quest we will be building a DE coder only Transformer which is the foundation for chat GPT thus if you are not already familiar with the concepts and Matrix math behind decoder only Transformers check out the quests the first thing we do is import torch to create the tensors we will use to store the raw data and to provide a few helper functions then we import torch.nn for the module linear and embedding classes and a bunch of other helper functions then we import torch. nn. functional to access the softmax function that we will use when calculating attention then we import atom to fit the neural network to the data with back propagation and to give us the tools to create a large scale Transformer network with lots of training data we import tensor data set and data loader from torch. yous. data lastly we'll import lightning as L to make it way easier to write our code and for Auto automatic code optimization and scaling in the cloud bam now let's create the training data set that we will use to train our transformer for this example all we want is for the Transformer to respond to two different prompts what is stat Quest and Stat Quest is what and in both cases we want the answer to be awesome thus the vocabulary consists of the following tokens what is stack Quest awesome and EOS and we map the tokens to ID numbers because the p torch word embedding function that we will use in in. embedding only accepts numbers as input and we save everything in a dictionary called token to ID we then make a dictionary ID to token that can go from ID numbers back to the original tokens these dictionaries will make it easy to format the input to the Transformer and interpret the output from the Transformer now let's talk about how to convert the prompts and the responses into a data set for example if the prompt is what is stat Quest and the response is awesome then what will be the first input token and ideally since we want each token to generate what comes next the first output token will be is then we want to use is as the next input token and we want that to Output stack Quest then we want stack quest to Output EOS to signify that we are done processing the input so these three steps process the prompt however we're not yet done determining what the inputs and outputs should be for the Transformer because we want the EOS token to Output awesome and we want awesome to generate a second EOS to indicate that we are done generating output thus the tokens to use as input during training come from processing The Prompt as well as from generating the output likewise the outputs that we use during training come from both stages thus if these are our inputs for training then we can code an input tensor using token to ID like this what is stack Quest EOS awesome likewise if the prompt is stack Quest is what then we can code an input tensor using token to ID like this stack Quest is what EOS awesome now going back to the first prompt what is stack Quest when we generate the output from each decoder unit is stack Quest EOS awesome EOS we see we should code the label for the first prompt like this is stat Quest EOS awesome EOS and we see we should code the label for the second prompt like this is what EOS awesome EOS s ultimately we have the input and label for the first prompt and response what is stack Quest awesome and the input and label for the second prompt and response stack Quest is what awesome now we just pass inputs and labels to tensor data set to create a tensor data set object called data set and lastly we pass data set to data load to create a data loader object called Data loader bam now we know how to create the inputs and labels for the training data set the next part is word embedding and we're just going to let nn. edding take care of that for us so the next thing we need to talk about is positioning coding position en coding commonly uses a sequence of alternating s and cosine squiggles to calculate values for each token and embedding value this is the equation for the first sign squiggle pause refers to the position or x-axis coordinate of the token in the input this is the equation for the first cosine squiggle this is the equation for the second s squiggle the two indicates that this is the second sign squiggle and D model refers to the number of values we are using per token this is the equation for the second cosine squiggle in general these are the equations we can use for as many tokens as we want with each token specified with pause and each embedding position specified with I note this + one simply means that the cosine comes after the sign and doesn't change the formula within the cosine function in in other words the formula inside the S function is identical to the formula inside the cosine function now let's work through an example so that we can see these equations in action if we had two tokens then for the first token pause equals zero and for the second token pause equals 1 now if each token had four word embeddings then D model = 4 then for the first token we calculate the position and coding values by starting with I equal 0 so we plug pause equals 0 and I equals 0 into the two equations we'll start with the sign function and we get zero so zero is the first position encoding value now let's evaluate the cosine function Bing and we get one so one is the second position encoding value now because we still have two more word embedding values to add positional encoding to we increment I the word embedding index by one and then plug pause equal 0 and I = 1 into the sign function and 0 is the third position encoding value then we plug pause equals 0 and I = 1 into the cosine function and one is the fourth position encoding value so I equals 0 is used for the first two position encoding values since we have two functions s and cosine and I equal 1 is used for the second two position encoding values for the next token we increment Pause by one and reset I so that I equals z and and then calculate the first two position en coding values just like before bam now that we have the first two position encoding values for the second token we increment I by one and then calculate the second two position encoding values beoop boop boop bam and anyway rather than use these equations each time we want to add positional encoding to a token we precompute the y-axis values and store them in a matrix this makes adding position and coding values super fast here's the code that we'll use to precompute and add position and coding values to the tokens we start by defining a new class position and coding that inherence from inn. module then like all always we Define an init method D model which is short for dimension of the model is the number of word embedding values per token and Max Len is the maximum number of tokens our Transformer can process input and output combined note for this super simple example we're setting D model to two and Max Len to six but in practice you would set them to much larger values then we call nn. modules andit method and now we start the code that will create a matrix of position encoding values we start by creating a matrix that we call PE for position en codings that is full of zeros PE will have Max Len rows and D model columns for example if Max Len equals 3 and D model equals 2 PE will start out looking like this now we create a column Matrix position that represents the positions pause for each token we're using torch. a range to create a sequence of numbers between zero and Max Len and Float ensures that the numbers are floats and UNS squeeze one turns the sequence of numbers into a column Matrix for example if Max Len equals 3 then we will get this column Matrix now we create a row Matrix embedding position that represents the index I * 2 for each word embedding just like before we use torch. a range to create a sequence of numbers but this time they are between zero and D model note setting step equals to two results in the same sequence of numbers that we would get if we multiplied I by two so by setting step equal to two we save ourselves a little math and just like before we use float to ensure that the numbers are floats thus when D model equals 2 then embedding index is just a single value zero but when D model equals 6 then we end up with three values now each value in position is divided by this term so we create div term to represent the divisor now we just do the math the first line assigns values from the sign function to The Matrix P starting with the First Column column zero and then this two means every other column after that the second line assigns values from the cosine function to The Matrix PE starting with the second column column one and then this two means every other column after that ultimately if we use the default values Max Len equals 3 and D models equals 2 PE will end up looking like this where the First Column has values from the sign function and the second column has values from the cosine function lastly we use register buffer to ensure that PE gets moved to a GPU if we use one now we create a forward method that takes in word embedding values and adds the position and coding values to the word embedding values and that's all we have to do to code positional encoding bam now that we have the position encoding taken care of let's talk about masked self attention the first thing we need to do is calculate the query key and values for each token and that means we need to code all of this math and in pi torch that means we need to use Matrix notation for example if we have the word embeddings plus position and coding values for what is and EOS then we can do all of the math required to create query values the multiplication by these weights and the summations by multiplying the encoded values for the tokens by a matrix that contains the weights associated with creating the queries that that matrix multiplication will give us a matrix of query values that we'll call Q that has one row per encoded token likewise we can multiply the encoded tokens by a matrix containing the weights associated with creating key values to create a matrix called K that contains key values for each token lastly we can multiply the encoded tokens by a matrix containing the weights associated with creating value numbers to create a matrix of values that we will call V thus when we code an attention class in pi torch we replace this treel likee diagram with matrix multiplication that creates the queries keys and values so we start by defining a class called attention that inherits from inn. module then just like always we create an init method in this case we're passing the init method D model the dimension of the model or the number of word embedding values per token we need to know the number of word embedding values per token because that defines how large the weight matrices are that we use to create the queries keys and values in this specific example we're using two word embedding values for each token so D model equals 2 and that means each weight Matrix needs D model equal to 2 rows and D model equal to 2 columns so that we end up with D model equals to two query numbers per token bam the next thing we do is call the parents AIT method now in order to create the weight Matrix that we will use to calculate the query values Q we will use nn. linear which will create the weight Matrix and do the math for us n features defines how many rows are in the weight Matrix so we set it to D model and out features defines the number of columns in the weight Matrix so we set it to D model as well lastly in the original Transformers manuscript they don't add additional bias terms when calculating attention so we won't either by setting bias equal to false as a result we end up with an object we're calling WQ with the currently UNT trained weights needed to calculate query values and because WQ is a linear object it doesn't just store the weights but it will also do the math for us when the time comes then we do the exact same thing to create a linear object WK that contains the weights needed to calculate the keys lastly we create a linear object WV to calculate the values and just to give us flexibility to input put training data in sequentially or in batches we create some variables to keep track of which indices are for rows and columns the forward method is where we actually calculate the masked self attention values for each token and another thing we're doing for the sake of flexibility is allowing the query key and values to be calculated from different token encodings for example encoder decoder Transformers have something called encoder decoder attention where the keys and values are calculated from the encoded tokens in the encoder and the queries are calculated from the encoded tokens in the decoder so allowing the encodings to come from different sources gives us the flexibility to do encoder decoder attention if we want to also since we want to be able to do masked self attention we can pass in a mask now we calculate the query key and values for each token by passing the encodings to each linear object and now we are ready to calculate a tension we start by using torch. matat mole to multiply Q by the transpose of K this calculates the similarities between the queries and the keys which we save in Sims then we scale the similarities by the square root of the number of values used in each key note this scaling is something that has been standard practice since the original Transformer manuscript in 2017 however it's not required and really only helps out when the model is relatively large the next thing we do is add the mask if we're using one to the scaled similarities masking is used to prevent early tokens from cheating and looking ahead at later tokens to understand how we add a mask using the mask fill method let's imagine that the mask is a matrix of trues and falses and the trues correspond to attention values that we want to ignore so the masted fill method replaces the TRS with -1 * 10 to 9th which represents 1 billion an approximation of negative infinity and it replaces the falses with zero to create the final mask that is added to the scaled similarities in scaled Sims the next thing we do to calculate attention is run the scaled similarities through a softmax function applying the softmax function to the scaled similarities determines the percentages of influence that each token should have on the others which is why we store the results in a variable called attention percents lastly we use torch. matat Mo to multiply the attention percentages by the values in v and that gives us the fin final attention scores stored in attention scores which we return bam now that we have coded the attention class we can create a class that puts the first three steps together and then adds the residual connections and then runs those values through a fully connected layer and then runs them through a softmax to get the outputs and we do that by creating a class called decoder only Transformer note unlike the position encoding and attention classes we created earlier this one inherits from lightning module doing it this way rather than having every class inherit from lightning module allows us to take advantage of everything lightning offers without the overhead of inheriting it multiple times anyway a first recreate the init method which allows us to specify numb tokens the number of tokens in the vocabulary D model the number of values we want to represent each token and Max Len the maximum length of the input plus output then as always we call the parents a nit method then we create an embedding object and we name it we for word embedding embedding needs to know how many tokens are in the vocabulary and the number of values we want to represent each token token then we create a position en coding object using the class we created earlier and name it PE and then we create an attention object then we create the fully connected layer with nn. linear inn. linear needs to know how many inputs there are and how many outputs there are then we create the loss function to quantify how well the model performs in this case we're using cross entropy loss because our model has multiple outputs and cross entropy loss will apply the softmax function for us now we put all the pieces together in the forward method forward takes an array of token ID numbers that will be used as inputs to the Transformer first we convert the tokens into word embedding values then we add the position encoding and then we create the mask that will prevent early tokens from looking at late tokens when we calculate attention we start by creating a matrix of ones with torch. ons for example if we pass four token IDs into this method then the call to torch. On's will make a matrix with four rows and four columns full of ones that Matrix of ones is then passed to torch. TRL and I believe that Tri L is pronounced Tri L where L stands for lower triangle because torch. Tri L leaves the values in the lower triangle as they are and turns everything else into zeros ultimately we save a matrix with ones in the lower triangle and Zer in the upper triangle in a variable called mask then we use mask equal equal 0 to convert the Zer into truus and the ones into falses and in the end if we pass in four token IDs this is the mask we will use for masked self attention once we have the mask we calculate attention note because the query key and value matrices will all be calculated from the same token encodings we pass in the same set of position encoded values three times for the queries keys and values we also pass in The Mask so that early tokens can't cheat and look ahead at later tokens and then we add the residual connections and lastly run everything through a fully connected layer remember the loss function we are using cross entropy loss does the soft Max for us so all we have to do is return the output from the fully connected layer bam now that we have coded a decoder only Transformer we need to write the code required to train it so the next thing we do is create a method to configure the optimizer we're using in this case we're using atom which is like stochastic gradient descent but a little less stochastic and we're passing all the weights and biases in the model that we want to train which is all of them to atom and in this example we're setting the learning rate to 0.1 because it makes training this specific model very fast however the default value 0.001 is probably more commonly used then we code a training step method which takes a batch of training data and an index for that batch we then split the training data into inputs and labels and then pass the input tokens into the forward method that we just wrote to compute the output and then we compare the output from the Transformer to the known labels using the loss function and remember the loss function does the soft Max for us lastly we return the loss bam now let's run the model before training it just to see what it does so the first thing we do is create a model from the decoder only Transformer class that we just created then we create an input prompt in this case we're using what is stat Quest EOS then we figure out how many tokens we are using as input we do this because our super simple model can only handle a total of six tokens so keeping track of how many tokens are in the input will tell us how many we can create as output then we run that through the Transformer which generates predictions for each token in the input this means that the model generates a prediction for what should come after the first token what and for all of the other input tokens however we're really just interested in what the model predicts will come after the EOS token so we use -1 to index the outputs generated by the EOS token the outputs generated by the EOS token are an array of output values one per possible output token so we use the ARG Max function to identif the output token with the largest value thus the token with the largest output value will be the first token generated as a response to the input note we don't have to take the token with the largest value if we don't want and this is something that is often configured in more complicated models anyway we save that token so that we can print it out later then we use a loop to keep generating output tokens until we reach the maximum number of tokens that our model can generate or the model generates the EOS token note each time we generate a new output token we add it to the input so that each prediction is made with the full context and then the model predicts the next output token using the full context which is the input plus the output token so far lastly we print out the generated tokens after converting them from ID numbers to text now when we run this code the response to the input what is stack Quest EOS is eos and that is not what we wanted ideally we would have gotten awesome EOS so that means we need to train the model the good news is that the code for training is super simple first we create a lightning trainer and tell it to only do 30 Epoch which is enough for our simple model and data set and then we pass our model and the data loader we created earlier to the trainer using the fit method and after we train the model we rerun the code we wrote earlier that generates a response to the prompt what is stack Quest EOS however this time we don't create a new model at the start we use the one that we just trained anyway the output is awesome EOS bam and now let's see what happens when the input prompt is stat Quest is what EOS using the same code we used earlier and the output is awesome EOS and that means our decoder only Transformer works exactly as expected triple bam now with time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stat Quest PDF study guides and my book the stat Quest Illustrated guide to machine learning at stack quest.org there's something for everyone hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
m8fvdRZb5CE,2024-06-17T04:00:34.000000,Human Stories in AI: Amy Finnegan,hello I'm Josh starmer and welcome to human stories in AI with stack Quest and lightning AI in this series we'll hear about the career journeys of passionate AI experts from their humble beginnings to conquered challenges will be inspired by the realworld experiences of professionals thriving in the ever evolving AI landscape human stories and AI is brought to you by lightning AI code together prot type train and deploy AI web apps all from your browser with zero setup personally I love lightning AI because it makes it super easy to use and learn from the stat Quest coding tutorials just go to the web page click on the Run button and Bam you get code that you can play with without downloading anything or installing any packages today we have special guest Dr Amy finnean the deputy director of data science at intra Health International Amy is a demographer and data scientist with over 10 years of experience working in global Health development and data science in emerging economies on four continents Amy is also an adjunct faculty member at Duke University's Global Health Institute so without further Ado Amy can you tell us about your journey to where you are right now at intra Health how did this all start yeah absolutely um I uh you know won't go back to all the way back but um I got my PhD in 2016 from Duke University and I was in the public policy program with a concentration in uh started out in political science ended up in sociology demography and at the time I was doing my PhD data science wasn't really a thing um but in the program um all my advisers told me you know don't collect your own data because you'll never graduate so we ended up having to uh use secondary data merge data sets together ask interesting questions do quasa experiments run AB tests like all of that stuff that is now commonly part of what a data scientist does is what I was learning in grad school um before data science was even a thing um and then within that as well we were in a multi-disciplinary program so you know everyone your committee has to be um an economist a sociologist and a political science and they are all experts in their field but they all approach the same question different ways so I realized pretty quickly that the one thing we had in common was regression and data right and descriptive statistics and like the the central limit theorem doesn't change based on which uh discipline you're in so I really doubled down on learning methods really well and um coming from that interdisciplinary or multi-disciplinary program I felt like I could kind of address any problem using those same methods and I also you know wasn't interested in like one problem specifically so I was probably you know I don't know if you've heard this saying before but like a mile wide and an inch deep okay sure that's not allowed really or not encouraged in PhD program right they want you to be the the expert in one specific thing and I was interested in so many things that I found the loophole was like being interested in methods okay awesome so yeah and then I went to uh the Duke Global Health Institute and I did a postto there in public health where I actually was starting to work on projects where we were collecting data in the field um I had a really great postto adviser who was automating all of his scripts to collect and clean data in R and I kind of sought him out because I really thought that those were skills that I really wanted to have um and I got this piece of advice kind of Midway through my PhD program in a class called computational political science which is probably now just called data science and they said it was taught by someone who had worked at Microsoft and then came as a developer and then was in the PHD program in political science at Duke and he said if you're a developer and you can program everyone's like great you're expected to program um but if you're a social scientist who can program then everyone is impressed and they're like this is like an additional skill that helps you stand out and then you can solve problems um that are you're coming from any direction so that's why I really um focused on that did you have a follow-up question well I was just going to ask uh and I think you may have answered this already but so you were in this PhD and then you moved on to a postto how did you choose that postto was it specifically to learn how to program or or or how what how did you make that decision yeah it was to go from using secondary data to Public Health where you're collecting your own data and to really be groomed on that track of like being an NIH supported research scientist and designing investigator initiated studies um and so I think the biggest compliment I've ever gotten is that I'm creative so I really doubled down on that right like a lot of people have skills you can be super good at something and follow all the rules but to kind of push the envelope you really do need to be creative other people CH grin at times I'm sure but how did that work in as a graduate student that that did that chafe you because they didn't they didn't want you to gather your own data or um I really loved the demographic and health survey like I can still remember when my advisor said some of these questions that you're asking you could probably answer with the demographic and health survey and I was like okay look let me check it out and that's what I used to write all the papers in my dissertation That's the basis of like creating this big data for Reproductive Health um bass connections group at Duke University for about four years we were working with Duke students to use Big Data methods on the contraceptive calendar from the demographic and health surveys and then my dissertation was on the mat using the maternal mortality module from the demographic and health surveys so these two kind of gnarly we can say data sets that people don't use because they are so hard to Wrangle we saw that as like here's the Big Data opportunity and we can um use these new methods that are becoming um more popular to answer questions that we have about reproductive Health oh I love I love this so let me just make sure I get this straight though uh it sounds like so this data set is Big complicated hard to use and you saw that as an opportunity yeah absolutely I love that I absolutely love that fantastic Okay so we've got you as a postto learning R where do things go from there yeah so I finished up my postdoc and then I had a job as a research scholar at the Duke Global Health Institute and everyone will tell you there is no track there's no like track from research scholar to like research Professor or you know there's no real career pathway in that job but I had gone from a postto at Duke and really wanted to stay in the triangle area so I'm based in Durham North Carolina and uh kind of this was the The Next Step from post do it kept me at Duke University um where I would had already kind of like figured things out I felt like seven years you know I kind of understood what was expected of me as a researcher and like I know where I'm going I had my my career development plan and my Five-Year Plan and all of that um and then someone I worked with actually on big data for Reproductive Health had gone to work at intro Health International where I work now and they had uh at the end of the fiscal year they would always have some what they call Budget dust like okay left leftover money that they could use before the end of the fiscal year right like the sound of that yeah right and it's it's dust it's not like big project material but they would use it for kind of innovative projects that they could see if they had any legs right and so this guy David poeni if you're listening David he said that um uh he had they had a bunch of data uh he was an informatics guy um I think he was pushing 70 when um I ran into him and he had seen like how health informatics had unfolded in the US and then also supporting Global projects and he was like we've got all of this data it's not connected like it's siloed into different Data Systems but the methods exist to link all this data together and then the machine learning methods exist to like make sense of it right so that was his project proposal and through this person I had worked with at Duke they found me and had me as a consultant for um two months uh and it turned out to be a two-month long job interview so I didn't know that uh they were GNA hire someone um at the end of this so when my when the job ended actually this is a funny story um so the job ended they posted this job they sent it to me and I was like you know I don't think I'm who you're looking for like I think this is like requesting too much experience and like too too much of all of that and they were like no just like come in for the interview and I was like okay sounds good so I came in for the interview and I met that director and David was there and other people on the team and the interview went really well they followed up for the second interview and I was like you know like I feel like I I finally figured out what I'm doing at Duke and um this would be a big change for me and uh I had just got off the waiting list for parking it had been two years that I was on this waiting list I was like oh decisions um so the director of digital Health at that time called up and he was like what do I need to do to give you this job so I accidentally negotiated a pretty good deal for myself oh that's awesome I love this technique yeah it was definitely an accident but I I kind of bumbled my way into this ex really great job I love it I love this story um well I mean to be honest it sounds like uh it sounds like you were given an opportunity to do this sort of Consulting and you you took it you didn't that was the key right if you bumbled on that and been like I don't know if I want to do this they you may have never heard from them again but because you you took that opportunity and you went with it they fell in love with you and they were like we have to get you and that's I think there's something to be said about that you know just sort of going out on a limb and and just saying oh I think this will be fun and let's see what happens yeah and I you know I gave up my weekend for a couple months to work on this duke it and Duke also rules how much Consul you can do and so it to fit within the rules of that but yeah um made it work out really fun yeah we made it work and I love it yeah and then they hired me as a senior data scientist and then uh that was September 2019 yeah that brings me to something that I did want to say like advice for students from reviewing a lot of student applications for our bass connections team reviewing a lot of like like I've hired three data scientists now at inal like reviewing cover letters and all of that the one the people who say like this would be a great opportunity for me are less appealing than those people who are like here's what I can deliver for you okay okay yeah nice so that's very important distinction as people are applying for jobs oh I like that yeah it's uh it's it's it's basically doing the exact same thing you did in Africa like here's what we can do for you bam yep selling it right and here's how we mutually benefit and this is the time it's going to take and getting a little bit of trust right and building up your previous projects you can show them the results you've had and then they're willing to um trust you for the the bigger dollars right and that's how the Uganda Consulting project happened is that that turned into bigger dollars from that same project that ended up funding a lot of my time in that first year because we were given the runway to just like see what can happen and then when we were able to solve a problem the cheve a party of that project was like can you solve this problem too uhhh this is the one that I really need solved so so you you said you said a word that I think is super important to just data science in general uh which is trust building up trust um I can think a few things more critical to sort of the role of a data scientist than establishing and building trust um because you know what we do can somewhat seem like magic you know like machine learning AI all these things there's a lot of buzzwords in our field that I think within the field we know what we're talking about we know what these things are but outside of the field I feel like a lot of people are intimidated uh or or they're or or you know or just like they don't know what's going on and there's that fear of the unknown or the or the fear there's just fear I think um yeah it's like every dollar we spend the opport Unity cost of spending on something that might not work is it's a risk that a lot of people aren't willing to take yeah and I think what you said was how you start you know simple and you build up trust over time uh both by like completing tasks and showing utility but I think also a lot of that is uh has to do with clear Communications in terms of like how this can benefit you yep and also walking people through what you're doing and making it intuitive for them because a lot of machine learning it's it's an artificial intelligence it's supposed to approximate like how the brain works right like in like creating that intelligence so if you can get those really simple examples that help people grasp that intuition then um yeah they can kind of wrap their heads around it I really do think that's one of the Gap areas in our field is that it happened so quickly that um you know software developers started doing data science and in ouri field strategic information and m& people started doing data science but the Gap I think was teaching the the technical people how to like get the intuition and be able to interpret these things and then ask for it right because it's only as good as the questions you're asking from it as well and kind of thinking about their specific um problems and how data science could be used to solve them and I think upskilling some technical people would be a really good investment yeah that makes sense makes sense to me so could you tell us a little bit about what you're doing at interel right now yeah um so I've been here for four years and one of our major projects is called Ready rapid efficient and data driven implementation um because there was a need to um kind of unify all of the data coming in from our our projects and how that was getting fed up to our um technical experts at the global level and then up to the executive team and up to the board so I spent the last couple years building that system um helping uh run a technical working group where it's not just data science building it it can't just be data science um you know we have the technical advisers who tell us what should go in there we've got the m& people who tell us how to define those indicators and make sure they're measured well and then the data science team can build those structures and data pipelines to bring them in and then display them powerbi dashboards which is what we do so that's been our major project and our probably highest visibility project at the organization um one of the really cool projects we worked on um in Tanzania was on um voluntary medical male circumcision so what we're seeing is that donors are asking us to circumcise more men with fewer dollars right so your targets are going up your budget's either staying the same or going down and you still need to reach your targets okay so what we learned after implementing for a while is that the the more men you circumcised the fewer men there are to circumcise right you only need to do it once um and so we had older population data from um the Bureau of Statistics in Tanzania that was at a higher level so it was at like the the region and the district level for example so we had a mobile testing site and we had kind of roving teams of Health Providers who could go to health sites and and perform the circumcisions and then upskill providers while they were there um but what ended up happening is that these teams would go out and stand around because they didn't go to the right place there weren't as many people as they thought they were there were you know the even these districts and um regions can be so big that even the the health officers there might not know where all the people are especially if they're um populations that are moving around a lot like pastoral communities and things like that so this was a request from the Project Director of that project it was toara plus funded by the CDC in Northern Tanzania and she said look look like I'm having this problem we we need to go more granular so we found some data actually from Facebook created these 1x1 kilometer um high resolution population density maps and put them out into the wild uh so we could see which it had been satellite data that they had collected for free and then estimated where people were by different age groups um so we had that which we could aggregate to the the district level and then also below the district to the W level and so we could show where the people are where the men are right and then we could use the demographic and health surveys which you know I love um to estimate you know if we've got this many 15 to 19 year olds the DHS is telling us that by age 15 x% of them are already circumcised so let's take the number we have and reduce that by the percent we think are circumcised and then pepar who funds these programs it has been collecting like the number of circumcisions done every quarter over the last several years so then we could also decrement from that number the circumcisions already done and then add in the 14y olds right and like cuz people age one year at a time that's the best thing about populations um so then we were able to Overlay that with data we had on HIV prevalence which we got from the testing and the positive test from pepar and the first time I made that map like popped up on my screen and it just lit up like a Christmas tree right it's like here are the wards we need to go to that have higher than average HIV prevalence and we can sort those by the number of men that we need to circumcise so we know exactly where to go W so yeah just to just for my benefit uh not being in the health uh business it sounds like uh so it sounds like the the end uh was or the the purpose was to reduce HIV rates yes and and the way you could go about it was uh increasing the uh proportion of the male populations that's been circumcised and so that's what you guys were were were tackling uh which makes sense so you're trying to get it before you know at an early you know trying to like prevent it before it happens yeah it was a prevention project for sure yeah that sounds fantastic um can you tell us so this data sounds really cool uh is it I mean how was it working with the data was it is a huge cumbersome data set or is it how do you how do you work with it yeah it wasn't too bad like you need to know some of the basics of geospatial data we were working with it in R so you have to know the library that you can work with geospatial data um and then there are some really good packages for summarizing those pixels essentially like you give it the the ward boundary and then you sum up the pixels in that boundary and that tells you how many people there are and then you merge on the data from the demographic and health surveys or from pepar that's also at the ward level so you do have to know like what are those identifiers that you're going to use to link the data together um and then the issue can be that one data set says this is like a sunshine district and the other data set says this is sunshine a district and like which one is correct you know and is this really the same place so in health plug for INT Health has a tool called gopher which is the uh Global open facility registry that helps you link those two lists together using some um matching Theory so there there are ways around it but it was very similar to something I did for my dissertation which was using data from Indonesia to map on um deliveries and facilities and to evaluate this program called um Desa Yaga where uh they had this kind of Rippling implementation of the program and we want to see if things were better after the program what was the program supposed to accomplish daaga means alert Villages so it was supposed to have a a post in the village that didn't exist before or was kind of like repurposed to um like if a disaster struck like the big tsunami people were designated to be the First Responders um someone would volunteer to be like you can call me and I will be the ambulance like if a woman has complications so that was the suami siaga that's the uh alert husbands program so it was really about um making sure that people were aware and planned for things that could go wrong just like disasters in general disasters maternal complications and childbirth um probably malaria Deni fever co could activate this network of Health posts to provide information cool um that sounds very helpful um uh so it it sounds like you do a lot of your work in R do you do all of it in R you said you got this for doing dashboards you guys use powerbi yep we use powerbi for our dashboards because we're a Microsoft shop so it nests really well with all of our other tools um we use qgis for our geospatial work because it is free okay that works good for me R also because it is free and open source so are the people that we serve uh you know they can't they can't afford support even the charity pricing that we get from Microsoft so yeah if you're looking at like what do I spend my dollars on licensing is not at the top of the list when you've got um women dying in child birth right yeah yeah that makes sense um very cool um are there any other projects going on right now um we're trying to get one project started um because uh I'll go I'll go on my open source software tangent so uh about 10 or 15 years ago when um open source software started to become developed for Health Information Systems uh to replicate like what we have in in the US to make that open source so that it would support a whole country's health information system like the facility registry uh identity manag management Supply Chain management Health worker management electronic health work electronic health records um all of those tools are they're open- Source versions of those tools available um but they were produced or initially funded under projects that were never intended to be permanent so you've got these pieces of software that um you know everyone assumes because they're open source like everyone is watching and there's a community but what it really means is that no body is taking responsibility and there's no funding for um improving these core tools unfortunately um so one project we're working on is using large language models like chat GP um to develop our software right because it's it's open source so we can just say like chat GPT read our software and then you know how you know vue.js and no. JS and open search um help us document this code right or look at this bug and suggest how we can fix it or um write tests for us right because our developers don't always have room or time in the budget or things move so quickly or priorities change that sometimes testing falls to the Wayside but it's a really important part of software development um so we have an uh an intern now working with us on using large language models to improve our our Iris Software which is Health Workforce uh basically headcount planning software for governments so yeah and do you run your own version of chat GPT or do you just use the we're using github's co-pilot right now uh it is like 20 bucks 10 or 20 bucks a month um but it's a context aware so you can have this um application where you open up all of the supporting files for IRS and then it can read through any of them um and then it's also polling from um uh what it's learned from the web about software development and like VI and node and and all of that so we're really hoping that it can accelerate adoption of the software that it can make them more robust with less money um sounds cool sounds very cool well uh to wrap us up I know you already gave us one bit of advice uh that you've learned along the way but do you have anything else that you've learned that you think would be a bit of wisdom that you could share with our audience yeah one piece of wisdom I think yeah realize you can't do it on your own you need a team of people to do it the data scientist has that kind of vend diagram of skills of like technical hacking skills and statistics but you still need a full team of people to make this work so yeah get get Buy in create value for other people so that they will benefit from your solution um students should be doing projects and creating portfolios so that they can show potential employers um what what they're capable of doing you know put everything on GitHub document your code really well show show teams that even at an entry level like you can fit in and start producing value on day one I love it thank you very much Amy it was great speaking with you today and uh have a great rest of your day great thank you
QhWZq8oBpVw,2024-06-03T04:00:15.000000,Human Stories in AI: Xavier Moy,hello I'm Josh starmer and welcome to human stories and AI with stack Quest and lightning AI in this series we'll hear about the career journeys of passionate AI experts from their humble beginnings to conquered challenges will be inspired by the realworld experiences of professionals thriving in the ever evolving AI landscape human stories and AI is brought to you by lightning AI code together prototype train and deploy AI web apps all from your browser with zero setup personally I love lightning AI because it makes it super easy to use and learn from the stat Quest coding tutorials just go to the web page click on the Run button and Bam you get code that you can play with without downloading anything or installing any packages today we have special guest Javier gadoo the director of customer experience and automation at hbx group in mayorca Spain Javier has had a career driven by curiosity and a desire to learn more while simultaneously making sure that customer satisfaction is always the focus of his efforts so without further Ado Javier or Chavi can you tell us about your journey to where you are right now at hbx group how did this all start so H I decided to go back to to the university you know ER for the ones well a bit of a story about H our company H we based the headquarters although we are Global is based on on mayorca which is an island off the coast of Spain H and mayorca is well known since the 60s or 70s for being a great place for H tourists all around the world maybe uh most popular in in the UK or in Germany H for taking vacation so uh I suppose that this influenced me and and what I studed at University is Hospitality so I was kind of H charm by by this world and uh then so despite I've had uh I started working on the on the hotel industry H then uh quickly I went into the more the distributions some the technology site of of travel and and tourism which is basically what our company does no so being a tech player ER playing in the in the distribution of uh travel uh products and services so I'm not in the hospitality industry and what you just said kind of made sense but kind of didn't could you could you give us a few more details about it to to to fill in the blanks yes so uh quick example no so we've got a a Hotel H and the hotel needs to sell the different uh rooms that they have no so let's say that we have a hotel with 100 rooms and they have different options to ER to distribute these these rooms uh so the first uh option would be uh if someone uh walks in no from the from the street then H you can uh tell the uh rack rates no the prices and uh and you can secure a room right know so people H stay in at your hotel but with this uh strategy H possibly you could get to maybe 5% of the occupancy of your hotel so this is why you need to H expand no and and er the hospitality industry is TI to travel no so if you only rely on the people that are going to walk by your hotel then this is not going to work no this is why having players like us is important because uh we ER fill this Gap no of the of selling the these hotel rooms to travel agencies T operators Etc across across the globe no so that H the hotel can have a higher occupancy with this Global r that we enable Okay okay that makes a lot more sense that totally makes sense so instead of just like hey I'm going to go to my Orca but I have no idea where I'm going to stay and you get there and you're like walking down the street and you're like oh that looks like a huge Hotel I'm going to go to that one um you know and they're like you know if a hotel that depends on that kind of business will go out of business pretty shortly so what you guys are doing is you're connecting that hotel with other ways to Market and sell that those H hotel rooms so travel agencies presumably websites uh correct travel agencies as well and we are a B2B player no so H our name won't be known to the general public uh it is known in in the industry but basically what we're doing is uh to create this uh so through technology to create these bridges that allows this uh distribution of uh of services so B2B you mean business to business so so you guys are yeah you're the in you're the invisible connection between the hotel and sort of some end website where someone's going to like ex I don't know if you guys use Expedia but you know what I'm talking about there some travel website okay very cool um uh or like what is it trip advisor or something like that um PL plenty of them I mean ER you can uh you can name hundreds and thousands of uh ER both tour operators travel agencies so uh the the size of the business is quite V so uh a question is so I'm not very familiar with Hospital ality industry but when I think of hospitality industry I think of you know restaurants or managing a hotel you know you would you could go to school for and learn how to manage a hotel I wasn't expecting you to go to school and learn how to like program and and do all this software stuff so H did you learned that in school or you learn that on your own or how did how did you develop those skills so I've always uh been very Keen to learn things about technology know and uh I would say that it was kind of my my well not my first uh option but because uh in my heart what I wanted to be Was An Architect no h but my my uh physics grades were that good okay so I had to decide for kind of the mainstream option here in in in Maya but when I entered into the into business what uh I managed to do know and thanks to a lot of opportunities that also I've been given is to H move closer to UH responsibilities around technology and uh as uh I am very curious now and always trying to learn also thanks to the different colleagues that uh we've been engaging uh with know here throughout the year I have been able to learn a bit no about technology about uh AI as well and this is why I'm currently uh holding that position that's excellent and you actually said a word that I I just want to repeat because I think it's like a little nugget right there like curiosity you had curiosity and that's I think that was maybe the driving force right you you're like oh I want to learn about this stuff let's do it and you went and you did it um which I think is fantastic um so that that's cool um so now can you tell us about customer experience automation so um in our so in the name of our area is a bit a misnomer no because uh you would you could argue that customer experience is uh is or fights against uh automation no or that the two walls are kind of uh collapsing one one with the other but at the end what we are what our aim is is we need to embrace Automation and we need to embrace H the transformation no so we need to H become more efficient because we've got er advances H in the industry and and and in the general world so now a lot of advanced are generally available for everyone H so basically we need to adapt to become more efficient but what I think that is good is that we by Design are embedding the customer experience into this transformational process so that our or we try to balance the the the objective no so yes we need to be more efficient but without losing this customer Centric customer Centric mindset yeah yeah you want the customer to feel special but you want to kind of streamline whatever process it takes to make them feel special correct so that it's as efficient as possible um so can you give us some examples of of how you do that or yes sure so uh the first things on this we started a few years ago no when so kind of the first uh implementations of um machine learning mods Etc are around personalization so uh one uh thing that we have very clear is that we wanted to uh back in the day I was in marketing and uh so we we ER identified that our Communications needed to be more relevant no so one one simple example and H this is super widespread now is how uh by personalizing our content and something that you cannot do manually know so you need uh some kind of system behind to to do it uh so being more relevant at the end you are going to uh provide a better uh experience for the one that in this case are receiving your newsletter or or your emails and also H looking it from the business perspective you're going to have uh better engagement and better conversions so this is a very clear example Al Al quite basic yeah uh but I love basic examples um personally uh so can we dive a little deeper into this basic example so um so say like for example you were creating a a newsletter and you wanted to personalize that that newsletter and you said you were using some sort of machine learning uh to help with that personalization can you can you talk about that how that how how you might do that I'm going to fast forward a bit and talk about something that is uh a bit more actual no something that actually my team is working on at the moment which is um uh an Opportunities Center for the customers that are uh using our website now when we're talking about H distribute Distributing the the hotel rooms the example that I provided before uh we've got uh certain distributions that are based on uh in Integrations no interconnectivities of systems but we also have a quite big customer base that are travel advisors so travel agents um working with their computers and they looking into our website and uh a tool that we providing to them H is a a center where they can see the uh the the hotels that perer travel agents in their uh in their geographical zone are buying okay so here is when H machine learning comes into play because applying er uh so grouping uh this travel agents uh by geography is is basic but uh imagine a travel agent that is um selling is specialized no and sells trips to Japan yeah ER so this Trav that happens to be in New York uh is closer to a specialist H travel agent that also sells to Japan that is in Paris okay so in this case this travel agent uh in Paris and this traveling in New York would be closer will be more similar one to each other compared to the ones in their neighborhood this is H so in this case what we applying is uh a clustering mechanism so that we can find these similarities and offer the rant products I love it I so so yeah so using a clustering or or and it doesn't even have to be like the most fancy thing can I can I take it I mean are you using something akin to CU to K means clustering or or something like that or um or do you or tell can you tell us about that or the this rings a bell ER but as you know I'm not the so I'm more on the business side H luckily I've got uh a team of uh uh data Specialists no and and these are the guys that are selecting the the right uh the right Implement systems yeah yeah you you you did you come I I don't know who comes up with the idea someone comes up with the idea you decide it's good um and then um and then the team works on it how big is your team So currently uh my team is uh close to 25 people no way that's huge wow um that's quite a big team uh so I I'm presumably there's lots of projects going on uh among uh other the team members and and I'm I I still think your journey I want to go back to your journey because I now that I know your team is so big holy smokes how did that happen did you start off with one person and then grow it or did you transfer into an existing team or or how did this work so my background uh in in my company uh I come from from marketing no so uh I did kind of a a transition of my role and speciality and uh I started working in the marketing department here at HBS group and what we so we were a small area in the marketing department ER that was uh kind of the technological area Al so the nurs that are working in marketing more focused on the digital stuff also around websites H around data Etc uh versus the creative uh site no which is of course uh very relevant no but uh so the kind of um this team so this te within marketing was the the embryo of it um and basically so our company is not a digital native so we our activities in marketing back in the day many years back ER were pretty ER let's say offline no so H with relying a lot on printed materials uh and offline interactions no and uh uh we we didn't uh we weren't um um uh working a way that wasn't work before so we were kind of catching up because uh so we were kind of lers in terms of applying certain H digital techniques and this is H this was our H Focus back in the day and basically what we drove was a data transformation within marketing no and then uh we ended uh into a point where uh the whole D digital Department sorry the whole marketing department was already digital and then the company decided that we were going to uh focus on uh expanding these uh these capabilities into the commercial side okay so basically to construct a digital uh a digital attention model no for for specific customers and specific suppliers and then last my last move H until where we are now or the last move of the of the team uh is now in operations so this is when H we H realized that we were talking we were calling ourselves customer experience but we had experience on sales and marketing and we were lacking a lot of experience on Customer Care on operations okay and now the team is in operations so we can so that we can embrace all this knowledge uh because there is no better way to H speed up ER your learning that be immersed in in this uh in the department that you need to learn this about yeah so it sounds like you were in marketing originally and it was you needed to mod modernize like your your the techniques that that you know had worked before weren't going to work forever because other your competitors they're moving fast too right so you had to modernize and in that process it just sort of organically formed this team that eventually became this this this large group that the company then decided to strategically move uh into operations ultimately is that is that sort of what happened that's correct in terms of the size of the team initially so we were kind of the as I said no the the NES of this of this area and then uh our uh uh director of marketing back in the day H believed that we needed to uh have a greater impact uh to the company and uh the our first uh Endeavor was uh around acquisition Okay so er uh our acquisition effort was uh driven mainly by our Workforce on the terrain okay that and continues to be so Al so we are not losing uh this uh this Personal Touch what we're doing is complimenting this and and uh back in day what we uh started is to construct a a digital way to become our customers and our partners and this originally H so imagine know that the first uh year when you put the these together H you start to I mean if you see the the metric no and the number of customers that you are H getting possibly 2% 5% of customers are only coming from the digital channel uh this uh over time and uh I have a lot of bad things to say about the the pandemic know but I think that ER it had changed I mean we were changing the paradigm already no so we were getting more digital we were uh the the customer experience that J L Amazon no has H said is something that we all needed to play catch up with no uh but the pandemic especially for us in the travel industry has been a a total change no and and it has speed up a lot uh of the a lot of the this modernization at the moment uh the The Rao has uh has swi no so the vast majority of customers that we are getting in our business H not all type of customers no so I'm talking more about our um our partners in in the travel advisor uh segment uh they are coming uh through through digital because it's a very uh effortless frictionless uh uh 6 feet minimum distance yeah when you do it online yeah U that's fantastic oh one thing another thing that that keeps popping up that I think is amazing is you keep talking about the customer experience and trying to like optimize that and make it as there seems like there's a very Human Side of everything you do like when whenever you do something you're always thinking about the effects it will have on real people you know in terms of their experience which I think is is phenomenal like you're right at this interface of of of of trying creating a technology but creating it in such a way that it's personalized that it's that it's that it creates an experience that that people resonate with and and respond to positively that it's all about them it's all about the customers uh I just think that's that's fascinating because often sometimes sometimes there's ML and there's data science and it's and it isn't so people focused um so I think it's it's very interesting that it just keeps coming up that you keep talking about how how the goal is are these people you know and they're real people and you want to make them as happy and as comfortable as possible and be sort of adaptable to their needs I think that's fantastic I can tell you two two stories uh about uh about people no or how we embed the people component into into what we do the first one um is uh is around the the systems that we use no when we are designing a certain um Journey that we want to our customers or Partners to to flow across uh is always customer Centric I mean and for me there is no other way and what does customer Centric means this means H that you need to do uh an effort to uh take to bring the customer uh uh to together with you when you are designing the the process uh for them no and H I mean we still need to go a long way but the first thing that you need to uh to to establish is the mindset no and uh uh we have very clear that there is no other way no sometimes uh yes you lack the time H you have uh committee decisions that you need to uh apply ET Etc but if you have this mindset then in most cases you're going to have the customer with you and is a matter of uh PRI prioritizing that this happens no I mean uh if we tell ourselves that uh I'm not doing this because I don't have time Etc at the end if this is this is four parts of of your mindset you're going to make this happen no so this is the the first point no customer first H and ER the customer needs to be part of uh your design process the second story as and this is a a topic that I repeat often is when we're talking about uh virtual assistance no so virtual assistance is another a solution application that we are also uh working on uh and sometimes you get this uh normal uh argument or this normal discussion about uh artificial intelligence or V assistance H displacing uh human human customer care agents and uh for me the the way into looking at at a custom at a virtual assistant is that they embody the best of the best of the human talent that we have uh in the company no so humans are still needed of course and uh there are there is going to be and there is uh there are many many uh cases where a human is going to do a better job okay than a than a viral assistant H what we need to achieve is that the virtual assistant is or takes the best traits of the let's say human colleagues so that it can be useful for the customer and then when H we decide that the virtual assistant H is not good for the customer experience uh a human takes over it can hand it off so I I think what you're saying is that the virtual assistant is useful for kind of like maybe some simple things and and maybe uh clear you know making it so that the actual people can focus on the more interesting problem so if it's a real simple thing that doesn't you know people don't want to do to begin with uh you know the virtual assistant can take care of it but as soon as it gets interesting soon as it gets complicated the virtual assistant can then hand that problem off to an actual person so that they can get that Personal Touch and they can get that um sort of knowledge and wisdom that the that the actual person's going to bring that that makes very that makes a lot of sense right I I think that's what you're saying right yes that's it and uh and possibly so I think that you said this correctly no so the assistant H takes on the simpler activities and uh and then uh our so the people takes on the the topics that are more complex more complicated uh we need also to think that uh uh sometimes the dealing with simpler uh problems or simpler request is a way to uh uh lower no the the cognitive uh tension that on one person has no while working H and if we remove these H simpler uh problems then what we are uh leaving Our People Too is to deal only with complex cases so we need to make sure that yes whil we uh uh use uh machines to solve the simpler problems we also use H machines and use these kind of Technologies to provide not to give tools for the people to be also much more effective in their in their day-to-day and and so that they er have the tools to to perform better I love it I love it I'm a I'm a personally I'm a coder and to me it and and I like coding but the fun part of coding isn't for me from my perspective isn't like typing in everything correctly the the the the fun part of coding is solving this problem um and there's all kinds of AI assistance now for coding and and they don't they don't ruin the experience at all for me because I'm still solving a problem uh but the little tedious things uh I don't have to worry about those things about about getting them right all the time I can let the AI take care of those details for me and I can still think of this big picture sort of what's the real problem and solving it it sounds like that's what you're doing the same thing but for sort of customer service which I think is fantastic uh Chavi uh before we go uh it's been great by the way it's been great talking to you but before we go I want to uh find out if you've got if you've learned anything kind of any bits of wisdom or nuggets or advice that you've learned over the years during your journey um that you think uh maybe some other people might benefit from um well I have a quite a bad memory so H what I can uh explain is is possibly the the the topics that ER possibly I've learned uh in the very recent past no h so I would say h going back to the the start of the uh of the call when we when you said curiosity know this is a a key um trade that you have to uh to Foster and to and to H put in practice Etc so I would say ER be curious no and and curiosity ER is a driver for ER learning a lot of new things and moving H into new areas of knowledge new areas of the business or or whatever and second oh go on I was I was going to say yeah I was actually I had my own second already because and it's based on your story not only were you curious you were flexible right as things changed you went with the changes you weren't being like no I'm I'm going to be in marketing forever you know so but don't let me get in the way I was just getting really excited I love curiosity well you you gave me a a third one not that these also fix ibility that I think that is also a very very good trade to have H if I have to say the second one that I have thought about is uh is proactivity no I think that you need to be proactive H because if uh you are reactive yes things may come uh your way but uh but being proactive and er facing problems uh looking for problems as well no so H so sometimes uh uh exciting problems are not going to come your way so possibly you need to uh make the effort to encounter them and then solve them I love that one too I a lot of things came to mind when you mentioned being proactive it reminds me of like one of the first jobs I had I worked in a hospital and I had a a job that like in all honesty took a very small amount of time for me to get done uh so I would just wander around the hospital and see what needed to get done and I found all these cool problems and had a lot of fun and did really well and you know it it actually kind of pushed me in in the direction that I am now so it really paid off but it also reminds me of just when you were describing your company how uh the hotel could just sit there and wait for someone to walk by and go oh I think I'll stay in this hotel that hotel's going to like there's no way that hotel is going to do well right it's going to go out of business within the year and that's because they're not being proactive right they're not going out and finding the people and and connecting in ways uh that need to bring bring the business just to keep the hotel afloat and and so it's sort of like so I that's all these like cool ideas is like yes yeah of course be proactive not just if you're it's like it works for the business but it also works for people right if you go out and you find the problems that need to be solved you find the customers that you need to be reaching that maybe you weren't reaching before and you can come up with a new way to reach them um that's that's problem solving I love it that's it yeah well Chavi I just gotta say it's been fantastic talking to you I want to thank you very much for joining me today um I've learned a lot about hospitality and kind of the problems you guys have to deal with and how you're uh solving those problems and it's very cool um keeping the human touch as part of the of the solution I love it well thanks for having me joosh this has been a real pleasure
JV0S5f89-Q4,2024-05-20T04:00:10.000000,Human Stories in AI: Tommy Tang,hello I'm Josh starmer and welcome to human stories and AI with stack Quest and lightning AI in this series we'll hear about the career journeys of passionate AI experts from their humble beginnings to conquered challenges will be inspired by the realworld experiences of professionals thriving in the ever evolving AI landscape human stories and AI is brought to you by lightning AI code together prototype type train and deploy AI web apps all from your browser with zero setup personally I love lightning AI because it makes it super easy to use and learn from the stat Quest coding tutorials just go to the web page click on the Run button and Bam you get code that you can play with without downloading anything or installing any packages today we have special guest Tommy Tang the director of computational biology at utas Therapeutics Tommy is a computational biologist with over 10 years of computational experience and 6 years wet lab experience committed to reproducible research and open science at EMAs Therapeutics Tommy employs a single cell sequencing platform to dissect the biology of immune cells and human tumors by using machine learning approaches and cloud computing before all of this Tommy was a lead scientist at the Dana Farber Cancer Institute so without further Ado Tommy can you tell us about your journey to where you are right now at utas Therapeutics how did this all start it dates back to uh my my childhood back in China so I'm from China I was born in China in the South part South part of China is in a it was in a small town I was born in a lowincome family and my dad was a middle school teacher and my mom didn't have a job she was caring for me and my sister and that was more than a full-time job and we could never make ants meet and I remember uh there was this one day uh we couldn't afford any meal and we we could only have a a soup with soy sauce and green onions that that was really hard my mom always told me Tommy you need to study hard and only knowledge can change your life so I started really really hard so I actually got into the rest high school I was ranked number one in the county and then I and then later I I uh got into one of the top universities in in China that's there in Shanghai and that's where I had opportunity to to learn a little bit more like what's out there in the world and and because it's in a big city and we also had a really good environment to go abroad and I applied and many graduate schools in the US so 2008 August 8th I came to the states uh uh at University of Florida to pursue my PhD at the UN University of Florida and like I was actually trained in a wet uh molecular biology cancer biology lab and I didn't know any data analysis skills and at first year of my PhD my advisor asked me to analyze this public available trip sequencing data set and it C crashed my Excel that was the first time I realized okay no matter how good my hands are in the lab I I need to uh learn some computational skills and I started to learn uh computational biology online like taking courses on adex Kera and Udacity in early days and uh yeah like I just enjoy like doing that because after the busy days uh in the lab I Dash back back to the apartment and then start to to like do the exercise homework do the homework and also Google a lot online and kind of uh learned a little bit computation skills uh in the last two years of my PhD then I moved to uh mdn and Cancer Center as a full computational biology post and there my uh advisor was uh Dr Roa hak who was leading the cancer Geno Atlas uh gasta project so I had the opportunity to analyze uh real large scale uh genomic data sets TV size yeah now that sounds like a really big step of you were a lab biologist and a self-taught computational biologist this is all stuff you did on your own I mean you took some online courses but no like no formal degree no Masters nothing like that and now you've got a postto which is basically a job full-time job in computational biology correct was that stressful was that like oh this is just no big deal I you know I I took these online courses I'm qualified to do this at the at this huge Cancer Center tell me about that yeah yeah it was definitely stressful right although like so I took me six years from my PhD and the first four years like I start I mean I already had two first all the like Publications but they are all purely uh web lab based so that's why I spend spent two more years like to kind of learn a little bit more like unix commands basic art and python so I was kind of uh better prepared for my posttop but of course I mean even after two years I was still a beginner so uh I learned quite a bit actually during my post at MD Anders and Cancer Center and they are also friends uh good colleagues in the same lab also help me a lot just to learn a little bit more like Advanced skills but again everybody is busy and even they teach you something you still need to spend a lot of time just by yourself and uh do a little more studying like after after after they teach you something yeah and from MD Anderson what happened oh yeah so I stayed there like for for three years like for my post start and at there I learned quite a bit on like different sequencing data analysis like different types of sequencing data like whole Aon whole genome bani sequencing chip sequencing ACH sequencing some reduced bofi sequencing to study DNA mation so I I learned quite a bit of different data types and then uh I also learned uh this workflow language called snake make okay so and because like in my later uh uh years at MD Anderson I was in uh Dr Kunal rice lab and his lab actually developed this High throughput uh chip sequence in a and we we were profiling actually six different history modifications across hundreds of samples and if you time six and plus like input control time seven and that's like thousands of actually samples and uh we had to actually process those samples uh uniformly and that's how actually I started to learn uh snake make and wrote this pipeline to process all this data in the lab so to to summarize for sort of a general audience um you guys were uh collecting data uh sort of genetics related data and you were collecting a lot of it and what you did is you created a pipeline for processing that data and analyzing that data and used it using something called snake make uh which sounds fascinating in enough it's just a python extension yeah okay so so anyway so just a just a summarize so that everyone we're all on the same page um okay so you've created this uh cool pipeline for um processing large amounts of data yes yeah yeah so then I extended like my snake pipeline to process all kinds of different assets that I've mentioned before so I looks like I have a a good tool like on my belt and then I moved to to uh Harvard uh faculty of Arts and Science informatics division as a senior by informatician there I started to analyze single cell an sequencing and single cell attack sequencing data uh so in collaboration with Neuroscience lab so I stay there for one year and a half and then moved to Dana Institute to actually lead a big uh NIH founded consultion project called cidc cancer immunological data comments so so in that project uh four different cancer centers in the states MD Anderson Dena Farber uh uh Mount Si and Stanford they carry out uh immunotherapy clinical trials and they profile those patients using Next Generation sequencing technology like hoxon hog genome and not not H like TCR sequencing and bani sequencing and it maybe it's by coincidence that they were using snake m so that a per that was a perfect position for me is I like you never know what you learn can be useful hilarious yeah yeah and uh and in collaboration actually with E theis Group uh at Dana Faber so they are team they are more like a software engineer team so we actually process all those clinical trial data security on the cloud and then they will actually package our uh pre uniform process data set into a really nice web portal and deliver it back to the clinical trial team for them to do Downstream analysis and uh two years there and I joined immunitas Therapeutics and as as director of computational biology to establish their computational biologic capability and immunitas is a single cell uh genomics company uh trying to use single cell only sequencing data and single cell T sequ data to find new therapeutic targets for uh for cancer patients and we also have a antibody engineer team to generate those antibodies so we so the biology wet biology team can test those two antibodies and eventually get clinque yeah so again just to like bring everyone up to speed um what Tommy says when he says single cell uh that means that say like you can take so like your body is made up of all types of cells they can take an individual cell and look and see what's going on within it rather than say say like a whole bulk of cells and just sort of take the average of it they can look at things on a single individual cell basis and then they it sounds like you guys can then develop potential Therapeutics based on those results so you've got sort of the the the basic research to identify targets and then there's a there's a component that develops things that would bind to those targets and hopefully cure cancer or some disease correct yes exactly yeah well that is fascinating well I can see why you'd be excited about being there yes and just to to not like our leading program it's called im9 it's a cd161 antibody so cd161 is a T cell and NK cell modulator and hopefully if we can block it then it can act with both T cells and NK cells and just to benefit the uh patients and now it's in clinical trial phase one we doed like seven eight patients now it's safe and I'm really excited to see whether it's going to like benefit like patients and that's amazing so just to just just so I understand make sure I hear what you're saying because this sounds fantastic and super exciting you guys did some some exploratory research some single cell stuff you came up with a molecule that can then Target things that you discovered and that molecule is now in clinical trials and it looks like it's working is what you're saying just just clarify and this molecule specific molecule was actually discovered by our co-founder but using the same single cell on sequencing uh single cell TCR sequencing platform and when I joined the uh company like uh this this molecule actually was already identified what what I were was doing is to actually using the same platform and trying to identify new targets okay oh so that's what you did okay fantastic yes and also now we have the this druck on the clinical trial we're dosing the patients and we also generated data from the patients so we are also supporting the uh biomarker data analysis from those clinical the clinical trial so uh so the moleculees there but what you did is you found some new targets for it the no new targets for like new antibodies to to Target okay yeah okay fantastic well that's that's very exciting um would you mind uh sh Shing sort of like what techniques you guys are using I mean you've talked a little bit about snake make um but is there any sort of like do you guys use machine learning at all or artificial intelligence or what how do you how do you interrogate your data yeah that's a great question so at immunity test we actually have a machine a 10 from 10x genomics to generate single cell on sequencing data and single cell T Cell T T Cell receptor sequencing data so what what it can do is that for a single cell you can have the genic expression profile but you also know the T Cell receptor sequence and what you can do is okay you can correlate the Gen expression profile with the chronal expansion or t- cell phenotype because when a t- cell recognized by some antigen expanded for example it could be a a tumor antigen so when a t- cell recognize it it kind of expanded and try to destroy the the tumor cells and when the t- cells expanded all those same te- cells they will have the same actually exact TC sequence okay so so by just counting how many times that TL receptor occurs in in the cell population you know okay how many how much expansion that t- cell is is expanded and then you can correlate the uh gene expression profile with the T cell expansion phenotype and you were asking what kind of machine learning approaches we use then we always start with simple interpretable like machine learning approaches for example in this case you can use like we we actually use single uh logistic regression for example you can designate whether a T cell is expanded or not expanded then you can uh essential find correlation or anticorrelation of your gene expression to that phenotype so either random forest or or uh largest regression are used for that purpose fantastic um just out of curiosity I mean I'm going to be honest I'm no immunologist and so a lot of what you said sort of just went right over my head but let me let me again let me see if I can uh summarize things so you know like when I get a vaccine uh for something what happens is um one I get a sore arm but then two I guess there's a an immune response that ramps up against the vaccine correct and and that's that's that te- cell expansion thing is that what you're talking about how my body is making more cells that can attack whatever I'm getting immunized against correct so I think I'm not an immunologist either but I learned quite a bit just by working with them oh I bet yeah and I I think because the the vaccine like previously people use those like uh maybe just died virus like uh in Act activate virus and you inject into the body and the T cells yeah as you said they actually recognize them right and then they will expand and try to clear clear them and then those T cells they get memory this called like t- memory cells like and so after it clear all this virus and they become like dominant but they they keep the memory but next time when the real virus come and those t- cells have memory so they can recognize it and spend really quickly and get rid of them yeah oh fantastic and and so what you're doing is so you you're one you're you're checking to see if the te- cells have expanded or not and you can do that just by counting the number of I guess T Cell sequences that you that you're getting and you're correlating that with single cell RNA gene expression correct and and what what cells are those are is that a different cell type or is that the same those are the same tea cells same same te cells okay and so so with so so when you have and you're like okay we've got a bunch of tea cells and they're all uh we sequenced the the the receptor thing so we know that that the immune response is kicking in and we're and we're and we're building a large collection of these te- cells you can then say these genes positively correlated with this transition to expanding and sort of building an immune immune response yes that is very cool yeah there are many actually nuances like under like under this like for for example not necessarily like for example one famous example is uh pd1 is the most famous uh checkpoint for immunotherapy okay so tell us about pd1 I I this is all new for me so pd1 is a immune checkpoint on T cells so when the te- cells are activated so then the t- cell the pd1 expression will will also increase actually and what uh the tumor cells can do is that they express its liant called pdl1 PD liant one and the tumor cells will use the pdl1 to block the pd1 oh so what they do is so there's this pd1 thing yeah and that does that promote does pd1 promote uh an immune response so yes and and no so pd1 is more like a it kind of uh pd1 is more like active so when the T has expand pd1 actually is up and it's trying to actually but then when pd1 is too high and it's also trying to damp the immune system because you don't want to over activate the immune system okay so and and so is so when the tumor has a defense against pd1 mhm and you're saying it creates something called a liend yes and and a liend is something that that sort of fits fits into like like imagine pd1 is like a glove and the and and the the Lian the pd1 Lian that the that the cancer creates fits into that glove um and what is that what is the effect of that so the effect will be the tea cells will will actually bypass those uh uh cancer cells oh that's very sneaky yes that's very sneaky yeah it's that's is that breaking the rules I mean is that fair it's not fair yeah but but tumor cells come up with those uh mechanisms like they up they up regular pdl1 then then then binds to the pd1 on the te- cells and T cells will actually just yeah will not work on those W okay so so are you looking for ways to bypass that checkpoint or what do you are you guys trying to figure out a way to to prevent the tumor from from tricking the T cells or that's already a drug in the in in the market like it's very uh like well-known drug and it's npd1 uh antibody so the antibody will actually binds to the pd1 but only binds to it but so occupy that pocket so then the pdl1 from the tumor cells or from other immune cells they cannot sort of bounces off yeah it's already occupied preoccupied by this antibody so the te cells will remain active and then try to kill the the two cells okay yeah um and so that's some that's one of the things you're looking at when you're correlating uh with you're doing this logistic regression or you're doing a random forest and you're saying uh we're we're looking at expression of genes including maybe the expression of pd1 yeah and in fact pd1 show up in our analysis and that shows okay our analysis is is making sense because okay pd1 actually is up in those highly expanded T cells yes so that's a sort of a proof of concept it's a way of calibrating your test to make sure it's actually working correctly yes yes oh fantastic yeah um so that's that's a way to um is that sort of used to to to train your random forest and to train your your your your logistic regression and figure out what thresholds to use for for classification or so so in this case we're not using it for classification we are looking at the uh the weights of of the the features or the genes so then we say okay which Gene is uh posate or anate with this T Cell expansion okay but again think about it here biology is more complicated so pd1 is is like POS correlated with T Cell expansion and t- cell expansion is a good phenotype we want to have yeah do you want to inhibit pd1 or do you want to activate pd1 so in this case actually we are inhibiting we're not uh trying to activate pd1 Okay yeah so this so biology is kind of complicated because the t- cells they up regulate pd1 yeah like when they try they're expanding but that the immune system actually also trying to like damp down okay the the immune response because it's a balance within the yeah so so it wants it to grow but not it doesn't want it to take over the entire body it wants it just to just a you know it's a way of of of promoting something but then saying okay okay we enough yeah exactly I I don't I don't need to eat anymore I've I've had enough I've had my fill let's let's start toning things down and yeah and also PD wi is also like immune immune immune like exhaustion marker so immune exhaustion means the t- cells they are activated for too long okay and then PD wi's High really high but then they lost the ability actually to attack the the tumor cells so they're exhausted so that's why if uh we want to try to actually block pd1 and some people say what try to reinvigorate those like exhausted tea cells yeah yeah well so it sounds like you've got relatively large data sets uh is that correct um and the the the question uh that follows up from that is how do you guys manage all that data do you do it inhouse do you have like a huge Data Warehouse on site or do you lease space in the cloud or or how do you manage all that data and how do you manage your your your pipelines in terms of where's the Computing happening and yeah we're using uh snake uh Google Cloud okay and uh so Google Cloud so the good part of that you can spin up any size of uh virtual machine and with big enough RAM and disk space so single cell I mean you can sometimes have millions of celles in a single data set it can be huge so we can't all uh process the data on our desktop so we have to use the Google Cloud but then and I told you we sequence those uh we actually make libraries uh in house but then we send it to uh bu for sequencing then we grab all the fastq files and then uh send it to one of the virtual machine and we actually also write uh snake make pipelines to uh crunch those data in house we have a a pipeline for single cell preprocessing yeah so once you get the data all on the cloud you then run this uh Snak make pipeline data analysis pipeline yes uh to sort of do all the analysis for and that's also done up in the cloud correct correct yeah how just out of curiosity how long does a does a typical analysis take to run you know for for pre-processing it doesn't probably take that long also but that depends on uh What uh tools are you using from 10x genomics they have their commercial solution called cell Ranger is a uh pipeline they developed so you can run that but that can take you like tens of hours just to do the pre-processing from the fastq files to the final account Matrix like rows of jeans colum of the cells so and if uh you use some other tools called like Crystal bus tool or like um Salman those are so-called alignment free uh tools and they also have single cell version for uh for quantification and usually that takes like within one hour or something that it's much F it's not too bad yeah yeah but then the challenging really is for Downstream analysis also uh for example for uh uh cell and cell type annotation this is more challenging uh for examp because single cell like first the size is Big second they're many in zeros so the data Matrix is kind of sparse you you have to treat them a little bit uh specially and for cell annotation that's one of the most challenging problem because after the most usually people do okay the sequence a bunch of cells for in the tumor but the tumor Mass contains so many different cell types different immune cells cancer cells and you want to actually uh classify them uh using either like marker based method or like machine learning approach and but the thing is no matter like how much work you you do try to automatically predict those cell types if you work with the immunologist they will always is actually change your annotation because oh for real yeah because the the can the immune cell state is very complicated you can't there's no there's no one fet all actually annotation Nom clature uh different cell States and uh that's one of the big challenges there yeah so uh so I guess a question I have is how do they know there there's no ground shoes here yeah that's the hard part like I think for simple data sets like some people like they they can do some like synthetic like data sets like more like artificial data set for example they have different cell lines they know they're kind of homogeneous for each other but then they mix for example three different like uh cell lines together and then you sequence them then you class them okay they probably are separate into three different clusters they kind of very clear but biology is always uh more complicated because the cells can have different cell state from this state to this state is it's a continuous like a process you can cannot like just make a discrete whatever cell type uh based on yeah the gene expression is very tough yeah so so when you're looking at individual cells rather than the average of a big pile of cells when you're looking at individual ones they may all be from the same cell type but they could be doing different things or or a different stage in their life cycle or something like that and so uh they may not all they're not like I mean in some sense they're clones but they're but they're not all doing the same thing they're not all like we're doing the exact same thing and as a result it can be tricky so when when they're doing different things that means they're using different genes and activating different things so when you're looking when you're doing the anal is it can be difficult to say yes these two cells are the same type they're just doing different they're different stages in their lives yeah yeah I think exactly you're right I some cells they are of the same cell type per se but they uh they of different state they like different they work they actually exact their function a little bit differently and it's also a little bit philosophical uh question like when you define cell types you're trying to to class the cells right but then when you class the cells you're defining a distance Matrix like between the within the cluster right okay how how close are they but then in reality this single cell is different from this single cell like it's like two like two cells those are two different clusters right so and when you do single cell analysis actually there's a parameter that you need to find that you need you can tune okay what's the resolution you want to look at if you increase the resolution in that tool you get more clusters if you decrease then you get like a smaller number of clusters so it it it's kind of tricky yeah yeah sort of like uh sort of like you and me in theory in well technically we're both computational biologist but we both do very different things right and so you could lump us if you in a really low resolution way you could say Josh and Tommy do the same thing uh but it but the truth is a lot more complicated than that in that we we really do very different things yeah it's a good analogy that's very interesting yeah uh that's very interesting um well I think it's fantastic and I love the fact that you guys are making some progress on that I think that's very exciting uh and to be honest I'm gonna say I love random Forest I know uh people ask me all the time what my favorite ml method is and I'm going to be honest I love random Forest because they allow you they they're just they're easy to interpret yes um I know what they're doing they can do they can they allow all kinds of data tabular data not just numeric data you can kind of throw whatever you want at them and I also love that you can use them as a clustering algorithm right right um and I just think that's the that thing right there a lot of people were like well how do I cluster if I've got tabular data most clustering algorithms only allow numerical data but random Forest allow you can throw anything in there and it's like I'm going to Cluster this data for you I think that's super cool you actually remind me of that video that video I know that's one of my all-time favorites nobody not well I say nobody watches it very few people relatively few people watch it and the one of the big problems the Achilles heel of this is that uh that clustering is as far as I know it's only implemented in the r implementation of random Forest the the python implementation does not feature it even though I think it's so cool so maybe one day someone else will implement it for me because I'm not a good enough coder to make that happen um but before we go I was wondering Tommy if you could give uh me and uh B anyone else in the audience you know if you have any advice of things that you've learned along the way you've you you came from basically the most humblest of beginnings and you went all you were all the way at the top you know you went to Harvard and then beyond um and I just I there has to be a nugget in there of something like uh that you learned along the way that you think could help other people sure sure like I I would like to share so I think you need to learn meta skills which means the skills that enables you to learn anything so like just uh take uh like deep learning for example like I'm like learning deep learning recently and the how I learn it so first I will read a book uh deep learning with or deep learning with python they books like that so I will go through the examples the co with code so of course those will not be related to biology will be it will be like sentiment analysis movie review or like product U recommendation but just go through the code it helps me to understand okay what parameters that I need to uh pay attention to understand the the architecture of of the neural network okay after I go that I finish that I will watch your video oh just even get a high level understanding of uh what's going on like for for different architectures convolutional Neal Network I thank you again for making those videos and then you understand those right then because I'm working on uh computation biology so I will apply what I just learned to a specific biological question and I will use a example data sets biological data and and implement the same NE neon network and on that data and and just get familiar with how how it works and then eventually you need to actually uh do a project with what you just learned and that's essentially How I Learned bi informatic 10 years ago 11 years ago so that's how I learned because only if you can code it by yourself or or the other trick is that you want to actually share what you learn so that's why I also write blog post on on my website dive into genetics and genomics. comom and you will see I share how I actually learned how I actually write those uh neural networks and if you can teach other people that means you are uh uh you already like good uh well understand uh the problem so that's that's also one trick that that that I use yeah and actually you just reminded me you do have this incredible blog um and you are currently writing posts about learning deep learning correct so I'm yeah I'm writing one block post how I am learning uh uh deep learning yeah it will be out I think next week sometime yeah okay and can you just just tell us what's the name of your blog so if anyone wants to learn deep learning and they want an additional resource what's the name of your name is diving into genetics and genomics. comom diving into genetics and genomics. comom correct fantastic well we should all check that out especially if you're interested in genetics and genomics and the application of deep learning uh which I think has a great potential uh to yield some awesome some stuff so I hope some people can check that out um well Tommy I got to say thank you very much for being on my podcast it's a real honor to have you here um I've just I've enjoyed sort of talking with you and and learning from you over the years and so it's it's a it's great to have you here and yeah thank you very much thank you Josh I'm so humble like it's my H to be here yeah a thanks
UC5jflMmubs,2024-05-11T04:01:07.000000,Luis Serrano + Josh Starmer Q&A Livestream!!!,[Music] hello everybody oh there you are hooray hello everybody it's me and Louis we're doing a live stream today hip hip hip hip hooray hoay back Quest and Serrano Academy here we are bam double bam double bam all day long um anyways um uh is anyone there hello everybody I see we have some people joining why don't you tell us where you're joining us from in the chat and meanwhile we'll just be here chatting so hi Josh hello very happy to be here with you in your wonderful Channel I am a huge fan so this is very exciting um Chap Hill andc you're in Chapel Hill North Carolina and I am in Toronto Canada um yeah let's see I'll be here oh 56 people watching so Missouri oh great that's close to I'm from Colombia the the other one the South America one I'm in Toronto Canada we have from India we have coliac holy that's awesome DRC wow lashi ISO Alto ra Greece wow cool people from everywhere I'm so happy to see everyone here Ling Switzerland swiland I'm actually going to be in Switzerland next week yeah you will Warsaw wow cool so wow we have a very International uhad nice bam s you might have to do some Spanish for him I need I need a catchphrase in Spanish you know like Kabam in Spanish I wanted something like uh like Cara or something like that I feel like I should do that when I when I when I reduce the the log loss I just go I uh got Saudi Arabia Brazil Italy Argentina Taiwan bistan Bangladesh D wow this is from literally everywhere in the world tunia South Africa I am very excited great Syria that's awesome well Louis all right should we just Dive Right In look I mean I'm super excited we got a completely Global audience it sounds like H let's Dive Right In because we got people from Palestine from Greece from Morocco from Vietnam usbekistan Vietnam hi aring uh anyways so uh we took some questions uh we're going to try to get some questions in the chat as well but uh we've got some questions that people posted in advance and uh the first one comes from temporary for study yeah and they they ask how do you get a job in AI um Louis do you have any tips on how to get a job in AI yeah I feel like AI is interesting because got people from every background right I've worked with psychologists I've worked with philosophers I worked with any kind of of people coming in from with their ideas so I think if you have a field that something that you like go through it from there yeah um I would say there's a lot of courses out there corera has courses Udacity has courses videos like obviously your channel check out my channel as well that's a good way to learn uh Hands-On I would suggest kaggle for example is pretty good for for Hands-On uh but I would say yeah go go in with your with your goals and your the things that you do in life and try to see how to apply them uh try to try to you know if it's llms you know just go on llm chat GPT and things prompt them and basically get get handson so a combination of courses and and handson is pretty good do you have any I've I'm gonna be honest I've got a YouTube channel and so I haven't applied for a job for a little while but I used to be uh at lightning Ai and um I know one of the things that they were looking for uh were like super solid GitHub um repositories so if you uh obviously you need to learn about AI but another thing you need to do is kind of create a portfolio of work uh that demonstrate that that you know what you're doing and also maybe demonstrates that you know how to collaborate with other people uh on group projects and so uh what I've been told is that you know you create a good Hub uh account and you use to develop a portfolio of projects that you've worked on and code that you've written and you can kind of show off like how good you are at commenting and explaining the code uh but you can also collaborate with people um and I I know recruiters are looking for these things or uh where they kind of see this as like self initiative but also able to work in a group and and understands um what of those things GitHub that seems to be an essential thing on in addition to understanding the actual AI and large language models and all those things as well um so those are some things that I think of of like a way to like make you stand out from the crowd and I know everybody does those things but it's but it's worth reminding and and just you know treat it like it's your resume it's something you want to look really special and you want to be very professional um and that doesn't mean you can't have your own personality uh but but treat it as if like You're Expecting potential employers to be looking at this thing I dropped in some good good answer I dropped in some resources in the chat for practicing for interviews hacker rank is really good kaggle lead code uh Oiler project in order to study for my first interview in in AI which was at Google I just did a lot of hacker rank problems okay so those those really help damn you've work all over the place I you were like you were at coh here yeah I didn't know you were at Google I was a Google and apple Google and Apple so I mean holy smokes just here and there I tried I tried different places in zigzag uh it was fun yeah it's amazing very cool yeah thanks uh so actually I see some math questions but I I have one for you that I want to ask you okay I want to ask you how you come up with uh stat qu and the normal sarus and it's a loaded question because the next part of the question is and can I suggest one yeah so um so stat Squatch and normal sorus um I uh go like normal Source came from a t-shirt design that a friend of mine's daughter created a long time ago she was really young and she but she was artistic and I asked her to draw something for St for stat Quest and she came up with this uh dinosaur that had a normal curve as a backround and I just loved it and and so that's where the normal Source originally came from uh and Stat Squatch came as a um I obviously I thought it was hilarious the way it sounded but also um I felt like when I'm creating these stat quests um I'm learning as as as well and I ask I ask when I'm learning these the new material I ask a lot of not dumb questions but I ask a lot of question um and and it kind and basically sat squatches me sat squatches me like and that's that's who I was when I was learning the material to begin with and so and so it's a way for me to sort of like take a step back and go okay assume you don't know the material what kind of questions would you have and I go well these are the questions I had and then I go and I give those to uh to stat Squatch so those are that's the story of of stat Squatch and the I love it I feel like I identify with st question as well because I have ask a lot of basic questions still I just yeah with my friends I just keep asking them stuff asking asking asking so here how about this let's say you have a logarithm function let's say you have like a a log loss or something and you have a hidden like logarithm so how how about this the less monster oh my gosh and then the neck is the logarithm function graph right like so feel feel free uh it's yours oh I I'm I'm going to have to do that I'm going to have to use that I'm going to actually write that down and the loges masterer lives in in what country um Scotland in in Scotland that you have in your which you have in your videos I forg it's flag is got it's the flag is the letter X I feel like Scotland has so much potential for math pond it's just in Limitless but anyway so yeah I love it that that was the suggestion that's so good I love it I love it I love it cool all right what's the next question so the next question and I get this all the time and I almost have videos that explain this and leis I think you might have a video coming out that explained this so the question is why do we divide the sample variance by n minus one instead of n yes that has driven me crazy for many years I've been thinking actively for 10 years about this uh because it's weird to divide it by n minus one right and I've asked everybody and I bring it up in conversations with friends who like hey how about that vessel correction term and people are sick of me asking this I I I and there's two schools of thought so basically what's the question right like what's the variance of a distribution is a measure of how wide it is right so this distribution very narrow has low variance this white one has high variance and the way to measure is take the middle the mean and see how far points uh are from the Distribution on average so you take the average square distance there's a problem there which is we don't know the mean asking for knowing the mean is too much yeah right and the two schools of thought are of of the and and the and the the the Bessel correction term says if you want to estimate the mean Take N points take their average distance from the mean and don't divide by n divide by n minus one yeah and that drives me crazy yeah and there's two schools of thoughts the one that says that you just work out the math and it cancels out and it's n minus one which doesn't make me happy at all and there's the other school of thought of people who say well there's n minus one degrees of freedom because you have the average fixed and so therefore you divide by n minus one which I don't get that big jump yeah yeah so here's what I think i' like to I I think I want to define the the variance in a different way okay so don't think of how far you deviate from the mean cuz you don't know the mean yeah think of how far points deviate from each other so pick two random points their distance squared and take the average of all the distance squared that tells you how wide the curve is right and that's not the variance but if you work it out it's twice the variance oh so I can take the average distance between two points divide it by two and I get the variance for a distribution now for a data set take end points take the average of the differences between them how many pairs of points are there in N points well the the first point you can choose in N ways yeah and the second one in another n ways so n squar but I don't want repeated points because I don't want x- X yeah so instead I pick the first point in N ways the second one in N minus one okay ways yeah that's the vessel correction term so I shouldn't divide by n^ squ I should divide by n * n minus one so if you define the variance like that like differences squares then the vessel correction term is obvious but if you div if you do it in the other way I still haven't been able to to see that in minus one yeah uh so a video is coming up pretty soon in in my channel which is in the link canot wait to see it's going to be awesome thank you a so cool um yeah I love that um yeah moving on I guess I mean because there's nothing I can add to that thanks yeah yeah I'm definitely I'll let you know when the video is out because I be out in the next few days there's a next question by NH sun1 1902 wondering about real life applications of igen De composition in machine learning yeah do you have any thoughts well um so uh when I think of I decomposition I think of principal component analysis and even though principal component analysis doesn't act I mean it's it's they used to do it with ion decomposition and now it's done with singular Val decomposition but it's still two rootes roads to the same end I think of that and I think of uh PCA as a d noising application it's it's a way to I mean data sets are are if they're large enough tend to have a lot of noise I mean even if when they're small they're noisy as well um but we get these uh really high dimensional data sets these days and by high dimensional I just mean we've measured a lot of different things for example for like for me you could measure my weight you could measure my height you could measure my hair color you you could measure my vision you could measure my blood pressure my you know you could measure all these things about me how long each finger is um you know the size of my nose you could have all these measurements um and and how do we you know some of them are going to be useful and some of them aren't going to be useful um uh in order to you know for whatever your AI uh or machine learning application is um and we don't want to feed in sort of useless information into our ml algorithm or AI um and so what one thing we can do is um we can um we can Denise that data with PCA what PCA does is it basically takes all the data all of the things we've measured all the variables and it kind of like uh reduces it down to just a handful of very useful things where every it's it creates new variables in the process it creates new igen vectors um that and the igen vectors are most likely going to have a lot more information in them sort of per variable than maybe the original variables so I think of that as like the classic use of igen decomposition in machine learning yeah yeah that makes a lot of sense I think PCA is the whole um umap and tne these clustering Al or not really clustering algorithms but they're like way of visualization algorithms that are that allow for nonlinearities in the in the data even though have a PCA sort of noising Step early on um and so it's just sort of like a classic approach um yeah that that's definitely the one yeah it's yeah and I think you yeah you can find like correlations between the data right like if you have a measurements in you like if you figure out that the noise is the same as the forehead then you only need one of them right so you kind of remove redundancies uh so yeah yeah yeah um what else uh next question yeah uh we have joy Jester use of multimodal llms in embodied intelligence and agentic workflow so I can I can add something here yeah I think I think uh so the question is about agent agents and workflow in in llms um I think that's the next step and it's happening right like it's it's the next because llms what they do they talk to you and they talk back right uh it's like having a friend who talks and tells you what to do and then you take that and do something thing but agents are definitely the next way to go right like which is when they actually go and do something so rag rag is the first sort of agent which is it you ask it a question and it doesn't just answer because it may hallucinate okay very likely uh it instead just go search on Google or in Wikipedia or on your own database of and then comes back so it does one thing that's a huge step right does one thing if you ask it a sum uh it could answer it with text or it could just go look for the calculator app and do this some and come back right much easier it goes and writes you know code and compiles it in a certain place or tests the code writes a unit test those things are not done at the language level they they should be done at the tool level because we have tools for that right so so yeah opening mail for example send a mail to somebody add this to the calendar app uh like that that is definitely the the the way to go and it's operating with agents and then you can do things like uh you can even even make answers better like that for example if I ask at code and it writes code that could work but if it goes and runs it and comes back writes a test runs it comes back uses a tool to give me a response that's that's even better too right and then when we're looking about multimodality we're looking at sort of what you know the the the models they only is text in text out but we don't learn like that we we learn from images we learn from sounds we learn from all these things and we have workflows right like you could have a uh you could put in a prompt that says make take this figure extract something and make a sound about it a voice say it in voice right so you you have like sort of chains of of thought uh and and agents collaborating with each other so yeah that's that's definitely the next big thing we're going to see is is after languages is Agents very cool very cool um I'm going to skip around on the questions um sure go ahead uh Ara sh bager I'm really bad with names so I'm going to mispronounce everybody's name but anyways they asked um uh could we talk about the use of Transformers in omix technology so omix technology is is my background a lot of people may or may not know this but I used to be a professor in the genetics department at the University of North Carolina at Chapel Hill and in genetics uh we have something called omix data and it comes from the omix comes from uh the genome we used to call um you know the human genome or we could say and and we say genomic data it's a genomic data it comes from the genome um but then there was um what happens inside of a cell is the DNA that genome that we were talking about uh is transcribed into RNA uh and RNA is a different type of molecule and you don't need to know too many details about it but you get a lot of it in your cells and we can measure the RNA to understand what the cell is doing um and so they would call that transcript omix and so there they kept adding this omix and and if they were studying proteins uh and and and then they would say that's proteomic and so we would just stick this this suffix this omix onto whatever they we're studying and basically the word omix meant lots of data uh lots of data in that subject um so if we got lots of Gene data genetic data we call it genomics if we have lots of RNA or transcript data we call it transcriptomics if you have lots of protein data we call it proteomics or proteomics um so someone asked about the use of Transformers in omix data so anything biological where we have lots of data and actually I was just talking to a friend of mine uh Luca Panella who's a professor at Harvard um and he recently made a publication using uh a language model to generate sequences that could control genes and I know this is like very exoteric and we're going to dive into a little bit but but uh in our in every single cell in our B body we have genes that that do everything so in my in these skin cells I've got genes that can create parts of my eyeball I've got genes that can create parts of my teeth my tongue my internal organs so my skin even though all it does is is hey I'm just skin all I do is skin I don't do eyeball I don't do stomach I don't do tongue um but it has all the genetic information in there and one of the problems with say cancer is that a cell forgets who it is it's like oh I I used to be a skin cell but now I don't know what I am and and in that state they can transcribe anything they can turn on any gene they want and as a result they kind of can start taking over and doing things they shouldn't be doing and what he's doing is he's using a large language model to create sequences that can turn on and turn off and regulate genes now uh so for example say like um I've got a skin cell but what I'd like for it to do is I would like for it to create sort of stomach secretions I don't know why I would want to do that but he's got a way of of basically coming up with a code a sequence of code that's based on an llm so that's one of the I don't know too many things but that's one of the things I know that's going on in terms of Transformers and language models and how they're being used uh with omix data and I know that was a lot of explanation for a little bit of like answer but uh but that's the best I can do right now that's fascinating I didn't know so you can use an llm to like sort of uh study big sequences of DNA yeah exactly so yeah so the Bert I don't I don't know if you're familiar with the Bert llm it used to be you know was originally trans uh sort of trained as sort of a language specific llm um but now it's used for because it because it's really good with relatively long sequences they're they're just feeding it tons and tons of genomic data and by that I just mean long sequences of DNA from our chromosomes uh we've got billions and billions of these uh of of these base pairs connected to make a sequence of A's T's G's and and we can feed that in and and say some of these sequenc do this some sequences do that and we feed it in and then we then once we've trained it we go or fine-tuned it we say Okay based on what you know about these other sequences can you create something new that will cause this stomach enzyme to be transcribed in this new you know situation and the new environment um and it's you know it's a it's a and it and it and the amazing thing is it works it totally works it creates these novel sequences and you can you can you can you can you can have a cell do something it shouldn't you can also remind a cell to not do something you know it shouldn't and you can make it and you can also do it tempor you can say I want you to do this in in this setting or in this time frame or something like you can do all kinds of crazy control over how the gene is used and uh it's pretty remarkable stuff so it's that's fasc I feel like I have hours and hours of questions for you yeah we'll save that for another I'll save that for next time but if anybody has followups on that I would I would uh I would love to hear more yeah yeah by the way there's a couple of comments your I hear you very well but a couple people your microphone could be turned up a little bit is that possible uh and let us know in the I I hear you very well but if anybody wants to let us know in the in the chat if you hear us both well loud you know for you that's I'm putting that in the the chat yeah that loud enough hopefully people can hear me better now um I think so yeah uh anyways uh I we have one more L I have a question for me and it's also a question from brain eater zombie 3981 okay that's a pretty good question I better answer it properly yeah exactly watch out the question is Laura how it works yeah uh yeah great great question brain eater zombie 3981 um I've been reading up a little bit on it and it seems like a pretty clever uh I'd love to hear your thoughts on it too it looks like a pretty clever way to uh train in your own NW because when you're training the Transformers you could have trillions of parameters which is awful way to train and to fine tune and anything like that yeah but they started noticing that they are lower in dimensionality uh uh so for example what's what you talking about a PCA right like I could have a data set that is flying in in two dimensional space that I need two two variables to explain it but maybe it's all in a line so I really just need one variable to explain it yeah I started noticing that these layers on layers of the neural network have which have many parameters uh we're kind of flat in some regions of space so they're easier to you can explain them in in less variables as in like if you have a big 10x 10 matrix I can I can explain it as I Factor it as a you know 10x2 and a 2x 10 and all of a sudden I have a lot less parameters to get really close to the Matrix so it seems to me that what they do in Lura is is that they just pass it through a funnel pass it back don't lose most of the information and train a lot less parameters uh so it's low low rank what is it low low rank uh uh something um I think that's what what it pretty much does yeah and so my understanding is um so you have a large language model um and and yeah it might have 375 billion parameters right and so training or fine-tuning like LS just said is incredibly timeconsuming and expensive uh so what they do is they basically create sort of an adjunct that has relatively few parameters and they fine-tune that and you can have this model running whatever your big model running uh but in order to have it you know because one thing we want to do when we're fine-tuning is we take a general sort of like Foundation model something that's been trained on all of Wikipedia and all of the internet and we want to but we want to do a bunch of different things like one we might want to do a customer service chat bot another thing we might want to do sort of like a pargal assistant bot uh which would be a slightly different training another one we might want to use for translating English to Spanish uh or something like that like there's all these different uses of this Foundation model and we don't want to have to train it for each specific thing uh because that would take forever and so instead they have this sort of like adjunct that they add to the model that's got relatively few parameters that can learn the specifics and that it's the it's this adjunct plus the full model uh that kind of work to give you the the desired uh responses and so and so when you're using it uh and so say like I could have one model but just like a bunch of these little small things that are specially trained um rather than many copies of this gargantuan monster um and so I think that's one of the things about how Laurel Works um and why it's why people are all excited about it um because it allows you to with with relatively modest means do a get a lot more mileage out of whatever large language model you're using um yeah yeah yeah I love that um should we dive into some questions from the chat I saw a bunch yeah can I ask you one that came up here which is also I'm looking at my own on somebody asked for you was watching your XG boost videos last night which I love those four videos by the way I learn XG boost like that and in and I couldn't learn in another way uh one thing I'm unclear on are all the weak learner trees fit on all predictors or a upset like a random Forest yeah so uh in theory um it's it's the answer is both uh the original manuscript um describes a testing every single variable you've got um you know for every split uh but in practice what you do is you is just like in random Forest you have it select a random subset of uh variables and you use those instead um and so that's you know so it's a little bit of both um but yeah they're parameters just like you have in with random for us there are parameters when you use XG boost to sort of configure uh how many variables you want to use per tree and and things like that ways to kind of like reduce and add a little bit of basic random Forest style building to XG boost so yeah the answer is both great yeah uh so um let's see what else I see a lot of questions about career so I think maybe we can ask her we can can answer a few technical questions and then delve a little more into like the PHD career Etc like I yeah I see a question about internships uh just starting out um yeah uh uh more yeah more career stuff what courses should you take what what topics should you focus on yeah uh let's see trying to see if there's a a couple technical questions we can can answer quickly um in eval when evaluating variable importance in tree models is there a way to understand the sign plus minus of the importance does plus or minus sign come out in three models I feel like in regression models they come out right like if you have a I'm say that's a new one for me too so I don't know if I can answer that I feel like in regression I see a lot of line negatives right like if it's not correla I'm trying to figure out the price of a house and obviously size is positive is a positive coefficient because it adds to the price but for example something that's not you know crime rate in the neighborhood or something that's negatively correlated would have a negative effect I don't know if that's the question or it it could be if you're using say a regression tree maybe that's what they're talking about regression Tree in that context is is this creating a larger uh output value or a lower output value what's the effect on the output y uh maybe that makes sense that's a guess I would say your guess is yeah good mine probably good as mine great uh I see a question someone says as uh this is from Mark uh Watney as as time goes is it possible to get get a complete clean data set devoid of hallucinated data content order to avoid the Xerox copy of a Xerox problem that's a good question comp what's the zerox problem I actually don't know what the zerox problem is uh I feel like I'm stat squash now I'm asking I'm asking the question mark can you help us out what's the zos problem but uh but I I wonder if if if maybe they're just asking is like is is it possible that large language models are contaminating our our say data set right because as uh you know D they they large language models have been have learned and on the internet but now they're generating internet content content and some of that internet content could be hallucinations like I have a friend who's got this website that I think is pretty hilarious called reads. me and it's basically chat gbt that that creates these listicle articles like top 10 things I love about November or the the 33 things we need to do to celebrate the Ides of March um but it had this it it had this article that that came out like a week ago and and I made me laugh because it said 10 reasons Homer Simpson is still relevant in 2018 I love it of course and so part of the problem is my friend's website is just generating garbage some of it's true some of it's not um and so I guess the idea is uh when we train the next level of foundation models that are trained on the whole internet right they're going to be training on a lot of this garbage generated that's hallucinated right yeah and so so are we just like are we just going to end up with things that are worse and worse and more and more hallucinations until like all information is is garbage uh is that a possibility I Vanishing gradient of information that's the singularity right the singularity is when there's no information wor anything on the Internet it's all it's all videos it's just just just a a void yeah know I don't know yeah I I feel like as humans we're pretty good at generating a lot of garbage biased biased data sets and stuff so if if there's one thing we don't need help and we're getting unwanted help is that but I feel like yeah I mean hallucinations are hard to avoid because they're not a bug they're more a feature yeah like when a model I I like to say that a model doesn't just say the truth and sometimes hallucinate it it always hallucinates sometimes it accidentally hallucinates the truth because you can't store data on the nodes of a neural network right these are continuous right these are you it's not a database yeah so you have to do things like rag or heavy fine tuning uh and Tool use it's very important but I feel like you can count on language models to hallucinate because they're learning how to speak they're learning the language but that they're not they're not learning facts and and you know we've seen things like training data like having a neural network um produce data and then train on that and in Federated learning right when you want to be secure you want to train a model with people's personal banking data but you don't want to the model to release personal banking data accidentally and so you just do a few steps of like you you generate fake data based on this one train them all on that and then so we we may it it may be that that's a solution for for the language model not to accidentally spit out something personal uh but at the same time we run into the problem that you say which is the you know the the vanishing gradient of of of everything ends up uh being garbage so I think it's yeah it's something to be very careful um and that hope whoever's doing it is doing it well yeah yeah yeah pretty much um so here's a question uh Mark T asked what's the pros and cons of having a big bigger model compared to a smaller one if you just use rag or fine tune it to get it to work the way you wish uhhuh so I'm going to answer with the way what I would have answered a few years ago and they asked me how to train a model I would have used this analogy of saying you can you can train a too small that's too small or too big it's hard to pick the right amount but if you're going to on the side of too big kind of like if you're going to wear pants with too small you can't do anything with too small but if you do too big you can put in a belt right so like you can do regularization you can like train a allas too big and then do Dropout or regularization or kind of like make it smaller yeah um I feel like that's the way to go if you're doing it yourself right like I don't know when it comes to large language models with trillions of parameters there's probably better techniques but I feel like if you're going to just put a belt on it just just go big and see how much you can do with how much computi
Oekl7K1iwxY,2024-04-29T11:50:30.000000,Human Stories in AI: Simon Stochholm,hello I'm Josh starmer and welcome to human stories and AI with stack Quest and lightning AI in this series we'll hear about the career journeys of passionate AI experts from their humble beginnings to conquered challenges will be inspired by the realworld experiences of professionals thriving in the ever evolving AI landscape human stories and AI is brought to you by lightning AI code together prototype type train and deploy AI web apps all from your browser with zero setup personally I love lightning AI because it makes it super easy to use and learn from the stat Quest coding tutorials just go to the web page click on the Run button and Bam you get code that you can play with without downloading anything or installing any packages today we have special guest Simon Stockholm a lecture at UCL Denmark Mark Simon applies machine learning especially deep learning to images video and time series in a wide variety of settings and by wide variety I really mean it Simon is fearless when it comes to seizing opportunities that come up and somehow turns them all into success stories so without further Ado Simon can you tell us about your journey to where you are right now at UCL how did this all start well Josh um I think I was was about 10 years old uh when my father who was working as a psychologist he came home one day with uh a software called Eliza and for those who don't know Eliza Eliza is uh the jet gbt of the 9s oh really yeah so um it was supposed to imitate a psychologist and it was supposed to replace psychologists actually by asking people questions okay so so he come comes over and I try the software and it says what's your name well I type Simon and it says okay Simon tell me about yourself and so you start saying stuff to the machine and and the Machine picks up specific words and in response to it it never gives you any answers just keeps asking questions it sounds like a good interviewer yeah exactly and I was so fascinated about this technology that I was like wow I I need learn this so um I decided I wanted to program um but there was no internet back in uh 1990 uh so I went to the library and I picked up a couple of books uh not on how to program but just entire programs written and then I just started copying them letter for letter uh onto the computer and some of them worked some of them didn't I had no idea what I was doing um so fast forward uh a decade and now I'm ready to go to university and I decided to study Linguistics uh which is the study of languages um and I've forgotten all about my uh First Love um and so um while I'm studying Linguistics uh I come across this thing called computational Linguistics and I'm like wow that sounds interesting it's how to computers understand can't speech how do they speak you know I'm thinking okay I want to do that um but that they don't have that course at the University I'm enrolled in so I have to switch to a different one um so I uh I get enrolled in the different uh University and when I'm about to start nothing really happens I don't get any responses from the University so I call him up and uh I say I'm enrolled in this this class um do you have like a curriculum or something and they're like oh man we forgot to tell you that uh we shut down the uh the education after you got enrolled I'm like what so um they said you can call this guy he used to be a professor there okay so call up the guy and there nobody answers I called back and they like oh yeah we forgot to tell you he he got fired because of drinking are you kidding me I'm not kidding no so his yeah so his replacement uh they say you can call the replacement call him up and nobody angelist I go back to the university turns out he's uh on vacation so they're like okay call his replacement and I called the third guy and he he picks up and he says what can I do you for I'm like uh I just want to study computational Linguistics um but since you shut down the the education how am I going to do this and he says well there's a different University um and you can go there and I'll just you know uh get your uh course credits moved to this University so um so I go to that University I study computational Linguistics for for a couple of years uh that was my my Master's Degree um and then one day this guy comes into our class this this this business guy and he's like does anybody in here um know anything about speech recognition cuz we need somebody who does I like sure I I do um and so um we set up a meeting and he says well I'll give you half a year to build us a switchboard where you can um get the person you want by speaking their name and I'm like I only know it theoretically I don't know it I mean I can't code I I coded as a 10-year-old but I didn't know what I was doing I hadn't coded since so I had no coding experience at all um but but um I actually get the job and I I finish the switchboard in those six months yeah how's that possible I don't know uh I guess he said if if you don't finish it I'll just fire you so you know on once you get this you're in a dilemma or you know it makes you makes you work so you learned how to code and not just did you learn how to code you learned how to code something quite complicated yeah were like there there were libraries that you can just apply to it so so um I don't know if you've heard of Dragon speech by newans uhhuh those were the big guys back then and those were the libraries I was using so it wasn't that difficult but I had to adapt it to the Danish language uh there were a lot of things that Dragon speech could not figure out about Danish I had to type in uh the actual pronunciation of different words and stuff oh interesting like phonetically you would type in phonetic pronunciation yes exactly okay um so that was training back then yeah it was um and so everything back then was more um rule based it wasn't uh machine learning so so at this point I still don't know anything about machine learning I just kind of know some algorithms and I know some programming and then the financial crisis comes uh and I get fired along with the rest of the company it almost collapses uh and that's I've been in the company for five years at that point okay so I I start to uh indulge in in teaching um I teach uh people with Aspergers Syndrome oh okay and I teach them how to code and and turns out these guys are much better than I am oh really yeah so some my my boss decides to give me a lot of courses so I I start taking all these certifications uh in in coding uh and I end up becoming fairly good at coding and then I apply for a job at UCL uh and that was 9 years ago so that's where I've been ever since oh wow at UCL I I teach computer science uh for the first four years okay and then uh I start gaining an interest in in machine learning and I come across a site called Data camp and data Camp is brilliant I mean it's really really good stuff and it's free for Academia um and then the next thing I come across is actually your channel so so I learned a lot from that and I still use your videos for my students now because now I uh five years later I'm teaching a um introductory course on uh applied deep learning um to to my students um and I also come across a fast AI um and and that's where I uh you know the coin really drops from me I I really get hooked on this and I I noticed it's not that difficult really I I always had this expectancy that it would be impossible to actually uh do machine learning you had to be you know really really brilliant uh not just an average show like myself but it could be done uh I would say I started out being very much Hands-On and I'm getting more and more theoretical more and more mathematically uh engaged now so does that mean you were coming up with new architectures or what does that mean to become more mathematical it means I'm starting to study the math more I'm starting to study the algorithms more and yes I've actually looked at something like unit um and I I thought to myself myself why are they using those skip connections and I started testing it out and it turns out on the data set I was trying it actually didn't help oh really yeah and then uh afterwards I found another article uh by some other people who had discovered the same thing oh really okay so so I guess for some data sets it may work yeah but but what surprised me was the way they actually use those gift connections they kind of just take and concatenate and I was like why don't you add them together instead of concatenating Cu if you're concatenating then you're not really looking in the image right you're you're you're stacking it on top of each other so that's uh that's why I started looking at it and that's what I mean by getting more involved into the mathematics and yeah cuz because now you now you're in a position where not only do you use these algorithms and and you could use them you can customize them and maybe improve them which is pretty cool yeah um well cool uh would you be willing to tell us about some of the projects you're working on right now sure um I have been involved in in four different projects I um I the first project I was on was with uh I had to help a fairy company they wanted to they have to uh go away from using diesel engines and use electricity instead so oh batteries um so they wanted to be able to be in the harbor for long enough that they could recharge between each trip uh so I have to figure out how the different people are are are controlling the ship and um it turned out that um in the fast AI course I was following at that time I noticed that you could take time series and turn them into images and then uh use that to identify patterns can you tell us a little bit about that um you just kind of blew my mind you're taking time series which is like you know how things change over time and you turn that into an image and then you proc that as an process it as an image yeah so what you do is you use gamy and angular fields and a mar of transition Fields uh and then you kind of create is uh colorful image it doesn't look like anything but it's just you know a color pattern yeah but it represents your time series and uh you the march of transition Fields make sure that you you kind of symbolize how much the the effect is so if if the thrust is going up then you see a a different color right okay and uh the Gan angle Fields represent uh each time step along the diagonal axis more or less okay um so it it kind of got all this information packed in there and then if you use convolution and neural networks you can uh look for a similar pattern okay and what happened was that I actually were able to identify who was sailing the ship only based on the amount of thrust they used no way a given point time yeah and they not just the amount of thrust the image of of the amount of thrust over time yes that and you could figure out who was who was at the who is the captain yeah that is fascinating that is pretty cool yeah I actually uh wrote a a couple of articles about it that have been published um did you come up with this technique or is this a kind of a standard practice in the well I I just saw Jeremy Howard use it okay going from time Z series to to images and I just thought well why not try it maybe it'll work here and it did and and forgive me because I the the methods you're using to create the image I've never heard of them but I'm gonna can I ask a question how is are there libraries that you can use that can just do this you don't have to be an expert in uh this field Theory thing or I can't remember I I I'm not an expert in it myself it it is about you know using polar coordinates and then mapping them onto uh a regular coordinate system really so um so I'm not actually a l whole lot into the math behind it but I'm using the libraries because I've seen other people do the same okay so it's so as fancy as it sounds doing it actually doing it isn't that hard because you can just use a library to do it and it works yes that is so cool uh can I ask a few questions about the actual convolutional neural network I'm just a plugging a Shameless self-promotion I have a I have a video that shows the conceptual uh or excuse me it gives a conceptual explanation of how conv convolutional neural networks work but I've simplified it to the most simple version you could have so you can see all the parts uh but uh but it's not like overwhelmed with complexity the question I have for you is how complicated is your convolutional neural network because I I know some of the ones that win these image classification contests are really quite the the pieces are all simple but they just have lots of them and they just stack them and yeah and and and what did you end up doing I I I went with the simpler possible neural network I could get away with okay so what I built was an autoencoder really uh so you have the a neural network you're taking the image and it just kind of downsize it all the way to until I couldn't downsize it anymore and I get this Laden representation and then I I train it to identify uh similar images you know um and it's very very basic it is convolution Max pooling convolution Max pooling uh okay as far as you can go that's it and and and uh I'm almost embarrassed to ask a question about this because I'm supposed to know these things really well that by the convolution does that mean you're you're you're just running a you're running a filter over it yes and and what that filter does is it kind of shrinks the image a little bit Yeah and and then you do this Max pooling where um where wherever that filter matched the best it probably has you know the output is going to be a larger number than places in the image where the filter did not match and so the max pooling sort of consolidates those those high ranking places and then you run the filter over it again and see if it matches you know or maybe a different filter that matches things yeah and actually several filters at time so you you go from a 3X3 by uh or a 3 by uh let's say 28 28 image my images were bigger than that but just for for sake um then you may go to uh 16 by or what is yeah could be 16 by 16 by any number of filters that you want so the the number that you get the next time is actually the number of filters that you have uh so you get a lot of filters and I don't remember the exact numbers I used but that it was yeah it wasn't something I put a whole lot of thought into I just I was just trying make a simple network uh even though it was a relatively simple model was it computationally intensive did you need to run it in the cloud or on what kind of Hardware were you using I used uh Google collab um oh you did it all on Google collab yes no way yeah so and and I think that means you were running on a single GPU yeah yeah yes and how long would that take it would take a couple of hours oh okay it wasn't too bad and I think I have I had I had data for five years oh wow okay so but these images are are very simple yeah and not very big yeah they may have been 512 x 512 or something okay so so not big images simple images um yeah uh the other question I have is going back to sort of the original goal was to I I think the idea was to was it to figure out how long they need to stay they docked to recharge yes they so sometime we knew that sometimes they only had 8 minutes and sometimes they had 17 minutes in Harbor and both of those were problems because if you only uh if you only have eight minutes you don't have time for recharging if you spend 17 minutes then you're actually punishing the next guy because he has less time to do his trip oh I see so we wanted to make sure that uh everybody uses 14 minutes um so we had to figure out what patterns can we identify where this is the optimal amount of time and and that actually but just led to well we can actually create the this watch schedule okay so shift schedule is the right word right so I was I created the shift schedule and then we got access to the actual shift schedule afterwards and we could kind of compare them and see if there was an overlap and some people were easily identifiable others could be confused with other people okay so uh so what I guess what you're saying is you used a convolution convolutional neural network to establish a shift schedule for who should be a captain Wi on this boat or who was the captain or who was yeah historically going backwards yeah and then from that you could you could then you could you could optimize on something or yeah well at that time we still didn't know what constituted a good sailing actually or good Crossing we still don't have any idea about it because we don't uh work with the project anymore so so we kind of we're stuck there at at the current uh point but you you could definitely try to go into the nitt gritty and see if you could do more okay um well still very cool that you could that you could predict who is who is sailing the boat when I think it's fantastic yeah and you mentioned some other projects so you said you you got about four so we've got one yeah that was that was the first one um so the second one um is uh is about feet um we had a case here in Denmark where people got their legs s off or amputated if you will uh because of uh bad blood flow so apparently there's not a lot of prestige in uh science within legs and even less within feet so if you don't have enough blood flow in your feet nobody knows what to do about it so just amputate your leg and it kind of became a big thing in Denmark that maybe these people shouldn't have had their legs amputated um this is actually a a project that I'm starting to work on right now so we're we're supposed to kind of see if we can identify the blood flow in Mr uh scans and so we compare different images um and see if we can identify anything that can prevent you from getting your leg amputated that sounds like an incredibly Noble effort uh a noble cause um and and what what methods are you using you using additional convolutional neural networks because this sounds like it's maybe image based as well yeah it it is image based and it will be uh instance segmentation is the the procedure will go for what's that say say that again instance segmentation inst segmentation yeah so you create a mask of where are the muscles where are the um the bones and you just sort of identify those in Mr image uh and then then maybe you can if you know where those things are then you can also know where where the blood vessels are okay as as I understand it I'm I'm no doctor so oh wow how long you been working on this one um just about a couple months and um I'm still waiting for some data so we're not that far into this project is is this similarly being run in Google collab on a single GPU uh no this will I have a um a machine we call the Beast it's actually not that that crazy it has a uh Nvidia um what is it 3090 RTX uh graphics card uh 10 GB of RAM uh it's way too little for most of the stuff I do but uh we're trying to get a a bigger machine soon uh but for purposes like this it's it's you can use it to test stuff off and then we'll probably go to the cloud afterwards and see okay so you can kind of debug your algorithm and make sure it's sort of Performing as expected locally and then uh once you have everything ironed out all the all the wrinkles you can upload your data set and your your algorithm and train it in the crow in the cloud excuse me exactly yeah and then the the third project is one with pigs um this one um came about because one of my colleagues who is in agriculture saw a a magazine and in that magazine there was some pigs in there uh that had been marked with anry or playful and stuff like that and she she came to uh the head of um of science uh at my at my workplace and she said can we do something like that and he he kind of took a hold of me and he said can you do this and I'm like sure we we can do that sentiment analysis for pigs yes so but but more than sentiment analysis she was interested in are they drinking are they eating are they lying down or are they standing up uh because this tells you something about the welfare of the pigs okay so if a pig is not if it's drinking a lot and it's not eating and it may be a little more tired uh then it probably has Di if a pig gets diarrhea uh it it's contagious and everybody gets diarrhea and then you have to use antibiotics for the entire farm and this is what Farmers Pig Farmers fear the most it's getting infections in the pigs and you cannot sell the meat at all so they want to avoid avoided this at all cost um so I I had to identify these different postures is it drinking is it eating and so on uh so we we did U what we did was we we created this case with a raspberry pie in it uh and a siia uh box that so you could hook up through the internet and then a big hard dis and we put a camera in there and we started just recording the pigs from the ceiling of uh of the Pix okay and that gave us a lot of images of of pigs um and then I had a student helper who drew all the lines around uh and annotated all the images oh wow yeah that sounds like a lot of work right there it is um and then I actually just I used the T the TR Tron 2 uh to begin with is which is an algorithm that Facebook has created for instant segmentation okay uh it worked fine but um I switch to YOLO V8 afterwards because it's much easier to work with it's much easier to uh configure I think okay and it allows you to sort of um classify the pigs in different yes while tracking them yeah and so this is the point if you can track that this pig has been drinking a lot it's returning to drinking all the time then that pig has a problem of course once you go down there you cannot see it anymore there's no way to identify the pig what you see it on the camera you go down where is the pig now you you'd have to paint a large number on the back of each animal so you could but you could track it you could create graphs you know uh maybe do it for the entire pix ey or something so that project worked out pretty well but then afterwards I was like H maybe um well actually uh the drinking eating and lay laying down pigs are kind of under represented in the data set okay and so I thought to myself maybe we could do something with stable diffusion okay um because I had heard so much about stable diffusion I had to try it so you can create a synthetic data set to help better train your algorithm yes exactly so so that's what I went for next and and uh this gave me a bit of problems um I uh first tried uh just m asking stable diffusion to create pigs but of course those pigs didn't look anything like the pigs I had okay so I tried to use dream Booth uh in a collab uh environment from hug and face uh every time it finished it will always tell me we can't show the the pictures we're afraid this is nudity cuz you're showing a lot of pig skin yes pig skin looks like people skins so maybe this is nudity so I never knew if my if my uh model worked yeah so I thought to myself all right well we'll try textual inversion um and textual inversion gave so and so results not too good um and so I tried uh going image to image so I took one of the images uploaded it and and kind of use control net to see if I could just slightly modified uh the best results I got was if I told it to go from standing pck to sleeping Pig it would just close the eyes on the pig that's not how pigs sleep yeah yeah yeah exactly so that this is a problem yeah and it's very very difficult to actually get it to do what you want I mean you can create all kinds of beautiful stuff but to make it do exactly what you want that's difficult yeah an art form in its own right yeah and and I I tried to you with control mod you can use poses you can input a pose in an image um but there are no poses for pigs to LY down sleeping pigs yeah yeah they don't exist you I can get a dancing man or something but you know pigs don't do that um along the way I I created some very strange looking images uh mostly because um the the I had uh kind of looks like a grill because the floor is has this weird pattern okay so when I use stable of fusion it would create an image of a a barbecued pig without a head you know lying on laying on the side again not super useful for training your algorithm unless you're you know you're trying to figure figure out what's going on at the barbecue yeah or it would create pigs with two heads or it would create you know pigs with fins and I'm like where you get the fins from if I Tred to reverse the prompt you know you can you can reverse the prompt it will tell me a a bird a white bird flying over something I'm like white bird so it seems like uh stable diffusion doesn't have a whole lot of knowledge of pigs seen from above in a PIX okay well uh noted maybe maybe you can provide them with a training data set I mean that your student graded maybe you could license it to them and get a lot of money maybe for all the all the Pig I mean because I mean it's joking I joking a little bit but but you know there's a lot of money to be made in in in this right because what you're saying is is it is a big problem you don't want to have the the the whole you know a whole Farm's worth of of pigs uh go to waste or some other bad thing happen right there's there's there's probably a significant demand for this because pigs are one of the most popular you know food animals on the planet um so joking aside it actually sounds like a very useful and a helpful thing to to get sort of advaned warning that maybe there's something wrong with one of the pigs and then we can do something about it in advance of of of it becoming a real big problem actually sounds very cool yeah yeah I I agree that's uh so so that was the third project yes yeah the fourth project is um satellite images uh of cloud movement and this is something I saw Thomas Chapelle do uh Thomas Chapel Chapelle is the guy from uh weights and vises okay he did an article on stable diffusion and Cloud movement and I thought that was very cool cuz now you can create time series indefinitely into the future uh so kind of video prediction actually okay cool yeah yeah um and I I tried that out and I actually got hold of a company that has solar panels uh and it's a huge problem to them that clouds are are coming in and they want to know 20 30 minutes into the future how are the clouds going to be how much sunlight is going to come through um I only did this on small scale okay but this is where I really need a supercomputer because satellite images are are they're big they're real time they're I mean I need a lot of ram um to to kind of keep all that data in there so you need a way to scale this up to a pretty sizable sort of cloud uh infrastructure it sounds very cool predicting Cloud movement yeah uh that's I mean that that sounds fascinating I love it uh Simon so all four of those projects are super cool and I and I'm glad we were able to cover all of them um but before we go I was wondering if you had uh any advice for people that that might want to be an associate professor any any lessons that you've learned um on on during your journey that you think people could benefit from from sure um I I'll say just do it I mean it sounds like Nike uh exactly yeah uh but what I've come to find is that I I'm just an average Joe and and I could do it um so I learned not to be afraid to just kind of do it you know the guy who came into the class you know I just raised my hand and said I'll give it a shot you know what's the worst thing that can happen that's right six months yeah learning how to program something relatively elaborate uhhuh I'll do it call on me I'll do it yes and uh that's amazing I mean I I I mean you I would be intimidated in that context I'm sure I was I'm not a very outgoing kind of person uh really uh I'm kind of introvert uh so so it's not like me to do stuff like that but sometimes it just happens you just got to go for it and and when when you have the opportunity like it's it's not just about having the opportunity it's about taking advantage of the opportunity when it comes your way and and that takes a little bit of courage and a little bit of you know getting out of your comfort zone but it it can really pay off it sounds that's what it sounds like it's saying to you like your whole career is basically based on that true yeah a couple other lessons I think I learned from you are um if at first you don't succeed try try again it's trying to get into computational Linguistics sounded like such an Epic Journey um you know like how I mean it was three schools later or you know how many schools did you have to try until you finally got there yes um I I usually tell people I went to five different universities all in all yeah exactly I mean that the Persistence of like I'm I'm just going to keep calling I'm gonna call um the original Professor the backup the backup's backup and I'm just going to keep going down the list until I find someone it sounds like sounds like you are a combination of persistence and just being brave and just being like I'm going to try it I don't know how to do it I'm going to do what I can I it it sounds intimidating but I I I just have to go for it that I think those are the two lessons other than the technical lessons that we learned about how you're actually getting things cool things done but uh maybe sort of like uh tips for for for me and living I think those are two great lessons and follow your heart you know yeah follow your heart yeah your passion and and be determined uh be stubborn uhh uh to to to get to get where you want to go um I love that I absolutely love it well Simon I got to say it was fantastic talking to you today uh thank you very much for being on the show and um I'm looking forward to hearing what comes next it was my pleasure
iujLN48gumk,2024-04-26T04:00:31.000000,Log_e Song - Official Lyric Video,from slams import J stor and statistics and machine learning when you take the log you use base e and statistics and machine learning when you take the log you use base E when where you take the L you log base e base e is for you and for me you can use it with the log function and you'll see the math will work out in your feel ecstasy that's the thing that I love when I use b e [Music] [Music] l b [Music] e yeah e it's approximately 2.72 [Music] in statistics and machine learning when you take the log you use BC in statistics and machine learning well you take the log you use BC where [Music] [Applause] you where you take the you Bas [Music] e long b e [Music] l [Music] b yeah I've heard of people using other bases and that's fine as long as you're consistent but check it your numbers are going to work out the way mine do so don't complain [Music]
KphmOJnLAdI,2024-04-08T04:00:09.000000,"The matrix math behind transformer neural networks, one step at a time!!!",we're going to do a lot of math row by column aray stat Quest hello I'm Josh starmer and welcome to stat Quest today we're going to talk about the Matrix math behind Transformer neural networks and we're going to go through it one step at a time this stat Quest is brought to you by the letters a b and c a always b b c be curious always be curious note this stack Quest assumes that you already understand the basics of how a Transformer neural network works and that you are already familiar with the essential Matrix algebra that is used with neural networks if not check out the quests in this stack Quest we're going to learn how the math associated with an encoder decoder Transformer can be written in Matrix notation hey Josh why would we want to learn this Matrix notation because understanding The Matrix notation will make it infinitely easier to understand how to code Transformers and neural networks in general bam specifically we're going to walk through the Matrix arithmetic used by a Transformer neural network that can translate a simple English sentence let's go into Spanish vamos note we're specifically going through an in encoder decoder Transformer and not an encoder only or decoder only Transformer because encoder decoder Transformers cover everything we'll need for any type of Transformer so the first thing we need to do is convert the input phrase let's go into word embeddings actually in this example we're going to start our input phrase with the SOS or start of sequence token so that means we need to determine the word embedding value for SOS lets and go for all three tokens we use the exact same word embedding Network that looks like this where each input has two separate connections to each activation function and each connection has a weight a value we multiply the input by and then we add those products up before using the sums as inputs to the activation functions in Matrix notation the word embedding Network starts with a matrix that corresponds to the input values multiplied by a matrix that contains the weights associated with each input the first row in this Matrix corresponds to the weights associated with the input for the SOS token the second row corresponds to the two weights associated with lets the third row is for two and the last row is for go so when the input tokens are so s lets and go then the first row in the input Matrix has a one for SOS and Zer for everything else the second row has a one for lets and zeros for everything else and the third row has a one for go and zeros for everything else now we just do the matrix multiplication row by column boop boop and we end up with the word embeddings for the input tokens bam now let's move the W embeddings for SOS lets and go over to the left and add the position en coding for the SOS token which is the first token in the input we add the corresponding y- AIS cordinates from the S and cosine curves then we add the corresponding y-axis coordinates for the second token lets and then we add the corresponding y- AIS coordinates for the third token go note in practice these y-axis coordinates are precomputed and stored in a lookup table so figuring out what values to add doesn't take any time also note we'll talk about how the S and cosine curves are calculated in a follow-up video on how to code Transformers anyway and now we just do the element by element addition to get the word embeddings plus positional encodings for each token now we have encoded values for the inp put tokens and that means we now need to calculate self attention specifically that means we need to calculate the queries keys and values using the encoded values for each of the input tokens for example to calculate the query values for the SOS token we multiply the first encoded value 1.16 by 2.22 and we multiply the second encoded value 0.23 by 0.17 before adding those two products together using Matrix notation we can accomplish the same thing by multiplying the row of encoded values for the SOS token by a column that contains the query weights likewise calculating the second query value for the SOS token corresponds to this equation which corresponds to multiplying the encoded values by a second column containing 0.41 and 0.51 in other words we can calculate the query values for the SOS token by multiplying them by a matrix of query weights and when we do the math we get 2.61 and 0.36 likewise we can calculate the query values for all of the tokens by multiplying them by The Matrix of query weights bam now we've calculated all of the query values for the input tokens note before we move on I want to point out that I labeled the query weights Matrix with the transpose symbol this is because Pi torch prints out the weights in a way that requires them to be transposed before we can get the math to work out correctly small bam anyway we can use the same type of matrix multiplication to calculate the key values for each token and the values now that we have the query key and values for each token we can use them to calculate attention we'll start by multiplying the query Matrix Q by the transpose of the key Matrix K Josh why do we need to transpose K when we do this multiplication well in this specific case the obvious thing is that the multiplication wouldn't work if we didn't transpose k then the two numbers in the first row of Q could multiply the top two numbers in the first column in K but then the bottom number would be left out of the fun so in this case multiplying by K without transposing it is a bad idea for technical reasons however there's actually a much more important reason to transpose K and to understand it let's go through the multiplication one step at a time we start with the first Row in Q the query for the SOS token and the first column in the transpose of K the key for the SOS token the matrix multiplication gives us the sum of these products which is -4.4 n this process of multiplying pairs of corresponding numbers together and adding them up like we did here is called calculating a DOT product so - 4.49 is the dotproduct of the query and the key for the SOS token dot products can be used as an unscaled measure of similarity between two things and this metric is closely related to something called the cosine similarity the big difference is that the cosine similarity scales the dotproduct to be between -1 and 1 in contrast the dot product similarity isn't scaled so that makes - 4.49 an unscaled similarity between the query and the key for the SOS token likewise the unscaled dot product similarity between the query for SOS and the key for lets is 2.05 and the unscaled do product similarity between the query for SOS and the key for go is - 27.4 likewise we calculate the unscaled dotproduct similarities between the query for lets and all of the keys and the unscaled dotproduct similarities between the query for go and all of the keys thus by multiplying Q by the transpose of K we end up with the unscaled dot product similarities between all possible combinations of queries and keys for each token bam now the next thing we do is scale the dotproduct similarities by the square root of d sub k d subk is the dimension of the key Matrix and in this case Dimension refers to the number of values we have for each token which is two so we scale each dotproduct similarity by the square root of two and that gives us a matrix of scaled dotproduct similarities note scaling by just the square root of the number of values per token doesn't scale the dot product similarities in any kind of systematic way as a result our scaled dotproduct similarities still have a really wide range of values between 5.01 and - 66.2 that said even with this limited scaling the original authors of the Transformer said it improved Performance Small bam the next thing we do is take the soft Max of each row in The Matrix of scaled dotproduct similarities and taking the soft Max of each row gives us these new rows note the soft Max function makes it so that the sum of each row is one so we can think of these values as a summary of the relationships among the tokens for example the SOS token is 1% similar to itself and 99% similar to lets and 0% similar to go now let's put these new rows back together to form a matrix the last thing we need to do to calculate attention is multiply the percentages by the values in Matrix V to understand exactly why we do this multiplication let's go through it step by step when we multiply the first row of percentages by the first column in V we calculate 1% of the first value for the SOS token and add to it 99% of the first value for lets and then add 0% of the first value for go and that gives us 1.54 the first attention score for the SOS token likewise we scale the second column of values to get the second attention score for the SOS token then we scale the values by the percentages for lets to get the attention scores for lets lastly we scale the values by the percentages for go to get the attention scores for go go and at long last we have calculated the self attention scores for each input token bam now that we have the self attention scores for the three tokens we add the residual connections and that just means that we add the values we had right after positioning coding and this last Matrix contains the output from the encoder double bam now that we have finished doing the Matrix math in the encoder we can start doing the Matrix math associated with the decoder note to make this example more complete and interesting we'll go through the math used during training specifically we want to train the model so that the input tokens SOS lets and go are translated into the output tokens Vos followed by EOS where EOS which stands for end of sequence or end of sentence is the token that tells the decoder to stop generating output first just like for the encoder the decoder starts with a word embedding layer however in this example because we want the output to be in Spanish the decoder uses a different set of tokens specifically the decoder uses these tokens SOS which is what we will use to initial ize the decoder ear the Spanish word for to go vamos one way to say let's go in Spanish e the Spanish word for and and EOS which tells the decoder it's done generating output also just like the encoder our decoder uses two embedding values per token now because we want the output from the decoder to be vamos followed by EOS then that means we want the first decoder unit to Output Vos however in order for the first decoder unit to Output anything at all we need to initialize it with something so we'll use the SOS token to initialize the decoder now like we said ideally we want the first decoder to Output vamos however because we are training this Transformer regardless of what the output of the first first decoder is the second decoder will be initialized with the vamos token initializing the decoders with known output values during training is called teacher forcing and it makes training go faster anyway once we initialize the second decoder with the vamos token we want it to Output the EOS token which tells the decoder that it is done generating output thus our decoder has two input token tokens SOS and Vos and that means we multiply the weights in the word embedding matrix by a matrix with five columns one for each token in the decoder's vocabulary and two rows one for each token and doing the math extracts the word embedding values for the SOS token and Vos now let's move the word embeddings for SOS and vamos over to the left and add the position en coding just like before bam now that we have the encoded tokens we can calculate the self attention scores during training calculating the self attention scores in the decoder starts out exactly like it does in the incoder using weights that are specific to the decoder we calculate the queries keys and values then using the Q K and V matrices we calculate the unscaled dotproduct similarities between the queries and the keys for each combination of tokens then we scale the dot product similarities now before we move on I want to emphasize one specific detail when we multiply the query for SOS by the key for Vos we get 0.12 as the unscaled similarity and ultimately 0.08 as the scaled similarity between the query for SOS and the key for vamos this detail that we calculated the scaled similarity between the query for SOS and the key for vamos will be very important in just a bit so let's label it so it is easy to keep track of okay going back to the scaled dotproduct similarities now we do something we didn't do in the encoder this has to do with the fact that during training meing the input let's go should translate to vamos and we know that vamos should come after the SOS token and rather than doing all the math for SOS first and then sequentially doing the math for vamos our knowledge that vamos follows SOS lets us do it at the same time so we can calculate the word embeddings for both tokens at the same time and then we can add the position and coding value values to each token at the same time and we can also calculate the Q V and K matrices at the same time doing all this math at the same time rather than sequentially one token at a time means we can train relatively quickly on Hardware that allows for parallel computation however after training when we want the Transformer to translate something all we know is that we start with the SOS token and that that means when we calculate the self attention scores for the SOS token it only knows about itself and nothing that comes after it as a result during training even though we calculated all of the scale dot product similarities for both tokens we don't want to use the similarity between the SOS token and vamos when calculating the self attention score for the SOS token so during training we only use the similarity between s OS and itself at the start of decoding and then the decoder does the rest of the math not having any idea of what might come next until it gets to the output in contrast because Vos is the second token it will incorporate what came before it when calculating the self attention scores and that means using both similarities that we calculated when we determined the self attention scores for Vos in other words we don't want tokens to cheat and Peak at what comes next when calculating self attention but it is okay for a token to look at what came before it the way that Transformers keep track of what values should and shouldn't be used to calculate self attention for each token is to take the scaled similarity Matrix and add a mask to it the mask Matrix adds zeros to the values we want to include in the attention calculat ations and negative Infinity to any value that we need to ignore what that means is that when we apply the soft Max function to each row the first token SOS has 100% similarity to itself and 0% similarity to anything that came after it and thus when we finally multiply the percentages by the value Matrix the self attention scores for the SOS token do not include anything that came after it now that we have the masked self attention scores for SOS and vamos we need to add the residual connections so we take the self attention scores and add the original word and position encodings to each token now we calculate encoder decoder attention the good news is that incoder decoder attention is the same as calculating standard self attention except that the V and K matrices are created with the encoder's output Matrix and the Q Matrix is created with the values from the decoder so we start with the decoder values we just calculated and multiply them by an incoder decoder attention specific Matrix of query weights to get the Matrix Q then we take the output values from the encoder and multiply them by encoder decoder attention specific weight matrices to create K and V and now we have q calculated from the decoder values and K and V calculated from the encoder values then we just calculate the attention scores just like we did when we first calculated attention in the encoder and now we have the encoder decoder attention scores bam the next step is to add the next set of residual connections then we run those values through a fully connected layer in this example the fully connected layer looks like this it has two inputs for the two values we have calculated for each token and five outputs one for each token in the output vocabulary as we saw with word embedding the multiplication and summations that take place in this fully connected layer can be done for both token at the same time using matrix multiplication and doing that math gives us this new Matrix we then add the bias terms to each column in the new Matrix and that gives us the outputs from the fully connected layer now we run the output from the fully connected layer through a softmax function and we end up with a matrix that has the outputs from both copies of the decoder the first row contains the outputs from the first decoder and because the output for vamos is one and everything else is zero vamos is the first token generated by the Transformer the second row contains the outputs from the second decoder and the EOS token has the highest value so it is the second token generated by the Transformer indicating that it is done with its work triple bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stack Quest PDF study guides in my book the stack Quest Illustrated guide to machine learning at stack quest.org there's something for everyone hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my Pat Pon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
l2hro8DemsM,2024-04-01T04:00:26.000000,Human Stories in AI: Fabio Urbina,"hello I'm Josh starmer and welcome to human stories and AI with stack Quest and lightning AI in this series we'll hear about the career journeys of passionate AI experts from their humble beginnings to conquered challenges will be inspired by the realworld experiences of professionals thriving in the ever evolving AI landscape human stories and AI is brought to you by lightning AI code together prototype type train and deploy AI web apps all from your browser with zero setup personally I love lightning AI because it makes it super easy to use and learn from the stat Quest coding tutorials just go to the web page click on the Run button and Bam you get code that you can play with without downloading anything or installing any packages today we have special guest Fabio urbina an associate director at collaboration Pharmaceuticals Fabio combines computational tools and machine learning with classical small molecule molecular and cell biology techniques to address previously difficult to probe scientific problems specifically Fabio finds solutions to drug Discovery with machine learning so without further Ado Fabio can you tell us about your journey to where you are right now at collaborations Pharmaceuticals how did this all start okay if we go as far back as possible so uh my great-grandfather back no I'm kiding far um no it's a great question so um yeah I guess as a kid I was always really interested in science it was something I was always wanted to do something that I really enjoyed you know I'm sure many people uh Like Me grew up watching Nature Documentaries and science documentaries and really enjoyed the process of learning and so since I was little I essentially wanted to go do something in the Sciences um you know didn't know what that meant exactly as a kid you know you really don't know what you're looking for but something in that realm and so uh another sort of component to my I guess upbringing is that I was really into computers um so you know I remember we get our first computer as one of the apples you know Green text Oregon Trail and floppy disc it was great and so that that sort of became one of my hobbies essentially on the side as I was going through school and whatnot so in college um I decided to get a biolog a Bachelor of Science in biology and a minor computer science and so I sort of just dabbled in um biology and that's really where a lot of my interest lies I remember taking my first like cell biology class and thought it was like the coolest thing ever and so sort of where I decided to to keep my focus in that area and um yeah so essentially you know got a Bachelor's of Science and biology and uh there's a really interesting internship program that I remember seeing uh on a door of one of our buildings and it was to come up to um Massachusetts General Hospital and do like a couple of years of lab tech research once I graduated in a um lab that worked in a really rare disease called familial disautonomia which I'm sure most people have not heard of something like 400 people in the world have ever been affected by it um so it's a very very rare disease and so not knowing what exactly I wanted to do once I graduated college which I think a lot of people find themselves in that that sort of interum what to do um I decided to go there and become a lab tick for a couple years and that's when I got really invested in essentially rare diseases and uh early stage drug discovery which is is what I focused on while in that lab so we were essentially trying to find a drug of any sort that could potentially be a therapeutic for this rare disease um this famili dis anomia and so that experience made me realize that I really wanted to kind of dive head first into biology and really into the cell biology of things and really into the the nitty-gritty of of what makes cells kind of tick and that sounds a little bit quite different from my current area of research but it'll come back around in a minute here and so um I joined a lab at UNCC Chapel Hill which was the lab of Stephanie gupton and there I worked on um neuron cell biology and neuron development and essentially how do neurons grow and make the connections they do in your brain from you know individual Blobs of cells to this very very complex structure and one thing I wanted to bring to that experience is some of my computational background so throughout my time of working in Biology one thing I did was I always kind of weaved in some computer science or computation or statistical sort of analysis kind of grouped in to the actual experimental work and so that sort of is what I did over the course of my PhD is um applied computational image analysis to a lot of the cell biology Imaging we were doing so once I finished my PhD I I kind of realized I didn't really want to go into Academia it wasn't really what I was that wasn't really the career trajectory that I wanted to go so I wasn't quite sure what I wanted to do so you you hear there's a little bit of luck in this for sure when it comes to finding out you know where you go in life but I started looking for internships and Industry positions and so essentially started looking around for um you know posts for internships that looked really interesting and by biotech um companies sort of in the area and um so good was this while you were still a student or um or is this after You' gotten your PhD so this is while I was still a student so this was about my last year um I knew I was going to graduate soon now of course my graduation schedule is a little messed up because I graduated in 2020 which is uh right when yeah and you know that sort of delayed a lot of things and there was a lot of un things I weren't quite sure there um but uh yeah essentially towards the last year of my PhD I you know you start thinking about what it is you want to do and usually the next step for a PhD when you finish it if you're going to stay in Academia is to go on and do a postto in another lab somewhere and um you know I I really like the area here in uh the triangle area of North Carolina didn't want to move um I wanted to sort of jump more into my career stage rather than jump into the postto life and then sort of then go on to try to start a lab so industry just seemed like the kind of thing I wanted to do so yeah during my last year of um my PhD work looked for internships um and you know you get permission from your pi to go and do an internship if you're going to do it in the middle of your PhD work and so we worked that out and uh yeah I just happen to find this um company collaborations Pharmaceuticals which is where I ended up for the majority of this far and they just had a lot of inter really interesting postings of what you could work on if you wanted to be an intern with them so I essentially emailed um Sean who's the CEO and he he I think on a Thursday off to double check but then he he got back to me within an hour and was like oh can you start tomorrow wow very different experience than most um yeah wow it's yeah I will say there's a lot of yeah one of the nice things about the sort of company I work at I kind of try to push this perspective a little bit is as I think we have this sort of concept of what a biotech or drug Discovery company looks like and it's usually from a very large pharmaceutical company point of view and you know the pluses and minuses that come with that sort of Industry viewpoint but um when I joined collaborations as an intern I think we had less than 10 people including the CEO um so it's very very small um I almost want to call it a mom and pop operation because it's literally um sha and his wife are the ones who run it and so really yeah so it's very very different and I I think that's what really drew me to it is just how different it was from the general sort of you know drug Discovery monoliths that I think of in my head um the the other thing that really drew me to actually applying as an intern for them is that they focus specifically on rare neglected diseases oh really wow mhm so they they actually foro a um the typical funding model of of Biotech where you go to get VC funding and raise capital and you have investors their actual original investment strategy was through grants from the government in order to create Technologies and in order to find Therapeutics for rare diseases which are generally considered not profitable and so that was a real big sort of draw for me to them so okay you know I I really like that they had such a very different model and that their focus was a lot more on the actual therapeutic side and wasn't completely driven by sort of like an investor type um model which mostly pharmaceutical companies are um so yeah so I I started the internship and um essentially just had a lot of random interesting problems thrown at me and uh you know tackled them and it was it was a lot of new learning I will say mhm um about yeah because it's so different from a from the biology that I studied it was very drug Discovery focused it was very machine learning focused and while I was fairly good with the statistical background machine learning was fairly novel for me so there was a lot of learning in that sort of first uh few months um completed the internship decided I really wanted to work there so I you know essentially applied afterwards and they were happy to have me and then I spent the next few months finishing my PhD defending and then joined the company essentially as a um a postto okay and we could go through the whole process but essentially over the next few years I sort of rose to the company until I eventually became an associate director uh mostly overseeing a lot of the machine learning a lot of the software development and um sort of the creative experiment mental computational side of the company can you tell us a little bit about uh what the machine learning what you guys are doing machine learning what the uh what are you trying to accomplish with machine learning yeah so um we focus on what's generally called early stage drug Discovery and so what that means is uh we may have a disease of some sort so one of the ones just to pick is a malaria for example and maybe we want to try to find new antimalarials and so what we do with the machine learning is or I guess I'll start with the traditional drug Discovery approach is usually you would take a current antimalarial compound or drug and a medicinal chemist might take it and try to alter the structure of the molecule in order to try to find some uh maybe a better antimalarial that's kind of similar um the sort of alternative is this naive strategy of you just kind of brute force it by getting very large number of diverse compounds and you create an assay that can tell you whether a compound has antimalarial activity or not and then you just Brute Force this whole entire set of compounds and try to find something randomly okay and that's actually been a strategy that's been one of the more successful which kind of tells you how um how non-specific and kind of random drug Discovery is a little bit yeah so that's that's got to hurt the ego of the um what did you say structural yeah the the structural medicinal chemist yeah so finding new structures is difficult problem yeah um and so machine learning is sort of this way to to dive in and kind of bridge the two gaps there and the way we use machine learning is we'll take our known uh drugs or compounds for example we may we have a list of antimalarials um okay as well as you know compounds that do not do anything to malaria they're dcri negatives and then we can give these structures to machine learning model and train the model to essentially take in a New Drug chemical structure and decide or predict whether that new molecule is likely to have antimalarial activity or not okay and so then we can take this new machine learning model and we can we call it virtually screening all we have to do is take the structures virtually and essentially put them through this machine learning model and predict which of these you know thousand 10,000 100,000 virtual compounds might be antimalarials and then we can follow up the predictions by actually testing those compounds and this way instead of testing thousands and thousands of compounds kind of randomly we can narrow it down to 10 or 100 compounds and test those and that sort of accelerates our ability to find new drugs or new antimalarials in that case off the top of your head like of those 10 that you actually test do all 10 show some efficacy or or how how accurate is this method yeah it's a great question so um we've had success rates where we've picked three compounds and those three were all active and those are actually three that we've taken forward so that was a really I think that was I forget exactly which which um viral we originally tested in but I think it went on to become we found like three anti-ebola anti- some sort of antivirus but yeah we've had success rates upwards of three out of three to totally novel structures that oh wow ended up being efficacious we've had uh something I think I think like another project we had something like seven out of 10 were very efficacious okay and then we have had projects where we tested 10 and zero out of the 10 were actually efficacious so we tend to have a very nice enrichment rate meaning 10 to 100 or a thousandfold a better chance of finding new compounds new drugs but it is still a um challenge once you build a machine learning model to decide is this going to be applicable are we going to actually find what we are looking for but yeah we have had some pretty good success rates using these machine learning models have have you learned anything you know when you when you get zero out of 10 hits has there been like oh um you know this disease has this characteristic that wasn't part the original training data or is there some indication as to why uh it didn't work yeah um one of the things and this is something we deal with is um we call it a coverage or sort of an applicability domain which is this idea of our you know training sets for these models are compounds and drugs themselves but sometimes those drugs and compounds in the training set cover a very very narrow chemical range and all I mean by that is they all look very similar to each other structurally yeah and so when we then build this machine learning model it might look really really nice on paper because what it's actually doing is just learning a very narrow chemical space and then when we go on to predict on maybe something way outside that chemical space the model is much more confident than it actually should be because it's only learned on such a narrow chemical space so that's that's one challenge we generally face is how diverse is our training set uhhuh and um that tends to be the main problem in in our field unlike you know text generation or image generation is it's really really expensive to generate data sets usually have to do full experiments um on a single compound in order to get even one data point so usually it's the spareness of the data that ends up being the the issue for the most part which uh kind of leads to another question which is usually when you do when I think of machine learning I think of Big Data huge data sets um how large are the data sets that you're working with I I just assume that they must be much smaller because obtaining them is relatively expensive it's you don't just suck down the entire Wikipedia um and then work from that or or or yeah so what can you tell us about that yeah I mean you're 100% right the our data sets are considered tiny by comparison to what do you think of as like you know maybe what open AI is doing with chat GPT they're like you said consuming terabytes of data we work on the order of hundreds of compounds or hundreds of data points up to maybe I think the biggest data sets we get are generally about 100,000 data points and so we are in kind of a small range and that that does tend to present challenges that we've been investigating a lot of uh potential answers for especially on the the extreme end we actually recently completed a project which we're hopefully going to publish on soon where our data set was actually 15 compounds in size oh wow holy smok we managed to engineer a model that could actually give some predictive power and actually found some new compounds on on a training set that's only 15 data points now that is fascinating that's almost like a nano data set especially for machine learning I think for traditional statistics 15 is large but for machine learning that is the smallest data set that I other than like the really simple examples that I use in my little videos uh which I intentionally make as small as possible just so we can see the math but I didn't actually think it was possible to have a data set that small I'm going to be honest that that just sounds like you're you're kind of blowing my mind yeah it's a it's a field that doesn't get as much attention because it's you know you don't get these massive impressive models but a few shot learning models or even zero shot learning models is is kind of what these are generally called and the idea is if you approach from the extreme end what how can you extract sort of a maximal information from these tiny data sets and they're not going to be super impressive but where you can focus their application and their predictions they tend to be pretty accurate so yeah we were we were very excited to kind of get those results um can you share any of the tricks that you used uh can you tell us about the model one the type of model that you trained and share some tricks that you Ed to make it work with such a microscopically small data set yeah um so the the model type that we use um it's called a prototypical network I'm sure most people haven't heard of it unless they've actually worked in this area so I've never heard of it yeah so it's a it's a very you know it's actually a fairly simplistic model underneath the underneath the hood and it's designed that way on purpose because the more the simplistic models tend to have a lot of um bias within them which we generally think of as as hurting modeling but actually um bias can actually be a good thing if the model's bias is sort of correctly aligned with what you think the data set structure is so so the simplest um I guess uh example of this is for example a linear model um anyone done yal MX plus b you know that's the line and slope that's a linear model if your dat set is actually linearly correlated that model is most likely going to outperform just about any other model you could throw at a small data set because it's going to draw a straight line which is your actual Association so that's for example a bias of a linearity within your data um yeah with the prototypical networks the general bias is you know we want to give it two classes of compounds one that we know are uh I'm going to stick to my malaria example antimalarials one we know that are not and so the thought is it's going to try to figure out what is the maximum difference between these two classes what is so extremely different between these very small data points and so we you know we tried a couple of strategies um one of them being can we choose negative compounds that look very very dissimilar structurally from the positive compounds because when we have a data set of uh you know 15 compounds it we could essentially give the entire data set and sometimes if the structures are kind of on a spectrum where they all kind of look similar with ones and zeros uh or sorry with um the uh antimalarials and non- antimalarials looking too structurally similar that could be a difficult problem for the machine learning model to do so we actually even pruned away some of the data a little bit to give it only the things in the negative that look as dissimilar as possible from the things in the positive which was the antimalarials and it sounds almost a little bit like cheating with your data but when you apply it to outside data test set you know finding new compounds it tended to work really really nicely in that in that fashion so the so the it's sort of like the ends justify the means like maybe it looks like you were cheating but as a result you had something that generalized a little better uh than it would have otherwise um absolutely I mean that sounds good to me yeah um and that's fantastic and can you tell us a a little bit about the T I've already forgotten the type of model you're using uh can you tell us a little bit more about that type of model I've never heard about it so I'd like to kind of understand sort of what how what what how does it work yeah so the Mel type again is a prototypical network which sounds kind of funky um yeah and and there's actually a second part of the model which I won't get into too much detail cuz it gets a little too into the weeds okay um so the way our model is set up is it's composed of of two pieces one is what we call an embedding model and the second is the actual prototypical Network so what the embedding model is trained to do is to take in a compound structure and generate a vector of numbers uhhuh that represent that structure yeah and the important thing that it's required to do when we train it is that structures that are similar to each other so if you have two molecules that look really close to each other the vector of numbers they produce should look very very similar so the numers should align really really well um whereas structures that are very dissimilar should look very different so if you okay lined up their numbered vectors they should look different they should have very different numbers so this allows us to we call it embedding our molecules into Vector space which is to say we just numerically put these molecules in a in a high dimensional number space where structurally similar are numerically similar and you know the opposite so once that model is trained in order to do that we take all of our molecules we embed them into this Vector space and the prototypical network the way it works is it takes these number vectors and it sounds so simple because it is it takes the two classes our antivirals and our non- antivirals and it essentially finds the average of these Vector numbers in this space called a prototype what you're doing is getting the average Vector of the antivirals and an average Vector then it's so simple but it's so powerful when you apply it to small data once you have these uh sort of they call prototypes these mean vectors that represent the average antimalarial non- antimalarial then in order to predict a new compound as you know maybe having antiviral activity is you would put it through this embedding vector and you would defin which prototype is your vector closest to so is your a new molecule closer to the zero class where there's no antiviral Properties or to the antiviral vector or prototype yeah and it's very simple but very very powerful so it's almost like a nearest neighbor algorithm yeah it I believe it's based off of a k nearest neighbor actually is a very is essentially the the origin of that strategy so from my perspective um so I've got a I hate to I'm like tooting my own horn here but I have a video on something called word embedding uh which is essentially uh what you described but applied to molecules instead of words and it does the same thing and then you take those those vectors of numbers that come out of the embedding Network and you obey basically apply K nearest neighbors to it uh to find and this to find sort of to to then classify whatever your new molecule is this sounds fantastic I absolutely love it I love the Simplicity of it um I know I know right now when people think of AI and they think of ml they think of chat gbt and they think of these huge monolithic monsters um and these from what I know are incredibly expensive to run they like just you know they've got so many gpus just chewing up electricity generating heat and need to be cooled down everything about them is expensive expensive expensive and it's and and that's awesome and whatever fine but it sounds like what you've got is incredibly simple uh and I'm I'm going to guess that the hardware you run this model on is relatively modest compared prepared to say what chat GPT runs on yeah that's understating it I I've run these on like a 2015 Mac like there's not not even gpus involved in half of this um it it actually is yeah you you hit a point there which is um something really nice in I'm going to say our field my field of drug Discovery and machine learning is the one plus side of these data sets being so small is that we can run them on pretty much any machine so while we do have um you know fairly large GPU clusters for running deep learning models where we try to aggregate much larger pieces of information um when it comes to some of the smaller models uh yeah we tend to just be able to run these very uh simply locally on our own machines most people would be able to run these machine learning models themselves on their own you know Hardware desktop you just need a your your modern day CPU and it'll work just fine I love it I I just love it it's this to me it's one of the unexpected things like like for me I just assume that everyone is and maybe other people are like this too but I just assume that everyone is doing something more fancy and more complicated than I could ever wrap my brain around and this sounds in a way it's like oh I could do this it's like it's and and I know that I I don't want to make it I don't want to belittle what you're doing I'm in a way it's like I feel like I've been empowered by listening to what you've said I I felt like like the crazy world out there isn't as crazy as I assumed it did you you kind of like brought it down and said hey get rid of all those silly extreme ideas that you have let me let me tell you what it's really like and it's and it's not so scary yeah I I completely agree I I don't think it's belittling at all I think I think you're right it's it's something that I wish more people would realize is you don't have to go and try to chain train your own transformer model with 96 layers in order to do machine learning or in order to you know even dabble and get something useful you know uh a lot of it's no secret that a lot of fields still have a very small amount of data you know the reason these uh image generation models and um text generation models are so big is because they're essentially the only Fields where you have a huge amount of data and in any other application you know it's a I'll say it's actually a common thing for when uh someone kind of new wish to machine learning comes into our field that the first thing I want to do is the latest and greatest published model and what's really funny is a lot of times if you just train some of the simple old models support Vector machines you may have heard of from decades ago random forests even um they outperform some of the newer model types M very easily actually in our field and a lot of that has to do with you know how much data there is and how um each of the algorithms sort of treats the data so so it is a bit humbling because I did the same thing I came into the field I said oh this is how what we're using these are the featur we're using random forest for Vector machines oh I could I could out do this come on you know and I start all these models and nope I completely failed in my ability to outperform what was there so that it was very humbling to come in and do that and to really have to sit down and really engineer the problem a bit well hooray for the old models I'm I'm a secret I'm a secret fan of all those old models I love it um um well to be honest that's a cool that we could end on that if if if if that's all we got but if you do have any you know final words of wisdom uh anything for just you know a a student or someone who's maybe changing their career if you have any advice for them um we can you know we can just let me know what you got okay um you might have to cut me off because I can talk forever but so yeah you're doing great you're doing great I guess you know the the biggest thing I think I see people struggle with is thinking that they either don't know enough or they're really uncomfortable with you know encountering a problem they can't understand right away and you know I I've trained a number of um students under me and I do see I think the biggest struggle is yeah you you encounter something you don't know what it is and you shy away from it because it makes you feel bad because you don't you don't like that uncomfortable feeling of not knowing of not understanding it makes you feel like maybe you you're you're not smart enough to understand something and that's such a small thing it sounds like but I think that's actually one of the biggest sort of barriers to people kind of going on and being a lot more successful I guess or a lot more um a lot more willing to plunge ahead into the unknown um and I say this only because I think I have something a little broken in my brain that I've never had that issue where if I don't know something if I don't know something I I just don't know it and if I try to understand it and I don't understand it I don't understand it but I'm going to keep trying until I eventually do and so that you know I I don't think I guess what I'm trying to say is it's the work you put into finally understanding it not how long it takes you understand it that's really important and most people who are in fields who understand and know a lot I think most of them they're not necessarily smarter than anyone else I think they just are okay with realizing they don't understand it they try to learn it they still don't understand it they try to learn it again they still don't understand it and they just keep going until they finally get it or know it enough that they can give it to somebody else to take overation SS but yeah um so you know I think getting comfortable with feeling really uncomfortable can be a really big Boon to people especially when they're thinking about changing Fields one of the difficulties I know especially if you're coming from a very very different background than what you're transferring into is you're essentially starting over from scratch you become an expert in your field over the course of say your PhD or even your undergrad you feel like you become kind of an expert in the general area and so you feel like you know a lot and you've done this whole progress for four plus years six and a half if you're doing a PhD sometimes and you feel like you know it a lot of stuff and then as soon as you switch Fields you all of a sudden are almost starting from scratch you don't know the lingo you don't know the acronyms you don't know what people generally do in the field and that can be pretty jarring and so that is also one of those pain points where as long as you're find sort of going back and struggling through things again like you did at first you know it won't take long before you eventually pick it up and are able to sort of run with it and because it's not your first rodeo going down the research path it tends to be a lot easier it'll still take you a while but it's tends to to not be too difficult and so that's sort of what I found when I switched over from cell biology to sort of a drug Discovery machine learning well yes I had some experience in the past felt like I was sort of reading the original papers in the field again it was consuming so many reviews and so many papers right I didn't quite understand it and then having to find the you know go back into the uh literature and down more references rabbit holes all that um but you know it's one of those things where I think if you can get past those feelings of almost an adequacy and can kind of push through it and recognize that you know you can learn it you just have to put in the time and you just have to be comfortable with the uncomfortable then you can switch Fields pretty much at any point in your career I think it's scary but if you have the capability of learning once a very specific set of you know topics you can do the same same thing over and over again there's nothing different about your brain from when you first started except maybe you know maybe you're a little bit older and takes a little bit longer but you know there's nothing different about it from from the first time you did it so it you know jumping Fields can be scary but it's 100% wo"
fx0GGAANIus,2024-03-18T04:00:18.000000,Human Stories in AI: Khushi Jain,"hello I'm Josh starmer and welcome to human stories and AI with stack Quest and lightning AI in this series we'll hear about the career journeys of passionate AI experts from their humble beginnings to conquered challenges will be inspired by the realworld experiences of professionals thriving in the ever evolving AI landscape human stories and AI is brought to you by lightning AI code together prototyp type train and deploy AI web apps all from your browser with zero setup personally I love lightning AI because it makes it super easy to use and learn from the stat Quest coding tutorials just go to the web page click on the Run button and Bam you get code that you can play with without downloading anything or installing any packages today we have special guest cushy Jane who works in data analytics development M at John Deere she's also in the master's program at the University of Illinois Urbana Champagne working on a master's in computer science data science having recently graduated with her Bachelors kushy participated in the data science club and also completed several internships at John Deere so without further Ado kushy can you tell us about your journey to where you are right now at John Deere and the University of Illinois how did this all start absolutely so um I guess a fun fact um my whole life I I was never that student who's like I'm going to be code you know from middle school and I love Tech in fact I was very like oh I'm terrible at Tech and I defined it based on things like can I just like you know do computers what people often don't realize there's a lot to computers like there's the hardware side there's software side of course I didn't know that as a student and I was very much into science I was that you know classic Science Olympiad geek um you know and I just loved all kinds of Sciences and then my junior year of high school my dad said why don't you try an a computer science class I really think you'll enjoy it and I I didn't want to initially but I was like fine Dad I'll try it um and I ended up feeling a very I don't know crazy kind of a rush in that class I just uh I mean I'm not to say that I'm very instant gratification person but I just loved when code worked like it made me feel like intensely satisfied and somehow that was enough of of a feeling to just make me want to pursue computer science or something with coding um and so yeah that was the initial start of it but I ended up you know really changing pth so I didn't get into computer science at uvi unfortunately um so I ended up taking information signs okay um not being sure about it initially but I ended up loving it and I I truly believe that everything happens for a reason um and information science is that really balance where you do get to focus on the technical side of things but also the human Centric side and how do you make things customer oriented and how do you present yourself how do you present your data how do you present your technology um and that's how I kind of got into data science um and not just learning you know how to just code but how to apply your knowledge and present in a way that appeals to different kinds of audiences so it ended up working out really well but I hope long story short that answers to some degree how I got here I absolutely love it I mean I'll be honest I I have a similar sort of Love of coding where uh to me it's like solving certain puzzles like little you know it's like doing a Sudoku or doing a crossroad puzzle you know when it works it you just feel fantastic about it and to be honest for me even when it doesn't work I almost always learn something from that and and I also just like love that process of learning so even even even a failure is still kind of like mildly a success for me um and I also love that you like uh sort of the the data science side of it to me that's always been super important personally uh when I first got my a job uh after my PhD I got a job in a biological genetics lab and I knew very little about those things they were all doing experiments I was just the numbers guy but I loved trying to communicate to them and trying to relate sort of the data analysis that I was doing with them uh so I'm in in some ways I feel like we might be kindered Spirits in that kindred spirit with Josh that's so cool I don't know people know but I a huge huge fan of stat Quest like stat Quest got me through a lot of Concepts like radiant descent so um huge fan so it's amazing that you're saying something like this it feels so special well I mean I mean I guess you get the idea right the all the whole idea of of what I love is is I love the communication aspect uh and I love that it's part of a bigger picture and it's not just sort of like coding for coding sake you know it's it's about Community it's about solving big problems and that's what I loved about being part of Science and I and and you being kind of a also a science nerd like me um you know it gets us it gets us in it gets us to be a part of that but in a in a in a way that that for me I've discovered a lot of people are scared of a lot of people are scared of the analytics and so if we can do it for them we're really doing a great favor or a service or just really being helpful and and I just and I like being helpful absolutely yeah I completely agree with you on that and I think uh that's also part of why um we run the Illinois data science club here at uvi but we believe that every field can apply data science to op optimize decision making so I completely agreed can be a very interdisciplinary field I love it would would you be willing to tell us about the Illinois data science club yeah sure so um our very first although I did not found it um we kind of started our first semester together with the pH with the founder my my roommate Ria sha who's also a student here at uvi with the same major um and our thought was you know we wanted to find a community that was not doing some kind of like hardcore hackathon or hardcore you know Tech Consulting where you have a client and a deliverable and there's just a lot of pressure wanted to make it a little more I guess for lack of better terms a lowkey okay where people of different you know levels of experience can come to a safe community and learn some data science um and yeah we're not expecting that people will be experts at the end of this there's different levels of expertise of course but they might have learned how to use data in a useful way and in a subject of their interest we didn't want to force a project or how they want to do something we just want to create some sort of guidelines and guide them through a project of their choice and they get to do a showcase and uh present their idea um and yeah the key I key thing behind our Mo uh motto was that uh we don't just do data for data we do it to solve a bigger problem whether it's business or societal so that is kind of like the uh key underlying theme of our club but yeah that's a little bit about us I love that I love that motto too you don't just do data for data you do data to solve bigger more important problems I I I I feel like if there's one takeaway from this podcast episode it's right there that's a nugget of Awesomeness I love it um well can you tell us a little bit what what you're doing now I I understand you're a a senior about to graduate uh yeah sure so so now well yeah of course this is my last semester I'm graduating and if all things go well I plan on pursuing my masters in data science here at uiu it's called The mcsds Masters in computer science data science and I want to work full-time while I do it because it's online um and get all the experience that I can that is the plan so far I love that plan uh you also had a summer internship at John Deere can you tell us a little bit about that uh what you did how did you even get the internship to begin with um any details I would love to hear all about it yeah so uh I'm very thankful to John Deere because they've given me numerous amazing opportunities not just one internship I interned there twice and I even did part-time there um yeah so how I got my first internship in soft or actually end of sophomore here um well I don't know if I even had a good resume or not but they liked all the python stuff I had in there that's what they told me um I think all credit goes to one main class and it's kind of funny it's not my hardest class it wasn't like data structur or something it was a very basic data Discovery class and they taught you the basics of python and how to apply in a statistical sort of way um for those who are from uvi shout out to the class stat 207 um yeah it's called data science Discovery and yeah I basically learned like the absolute basics of you know how to do like SK skarn packages and what to do with your data um how to do feature engineer to some degree a bit of feature engineering um that whole Pipeline and that I guess was enough knowledge to get me my first internship it's fantastic um I love all that stuff too that to be honest that was also my Gateway into sort of data science in the python realm was was I saw that in in scikit learn they had um all these machine learning models and what I thought was super cool about it is once you got your data which took forever but once you got it you could then just try a done a ton of different models on it without like having to do a whole lot more work um and so I love that well that sounds fantastic so you got these internships which are which are great can you tell us a little bit what you did during those internships oh yeah I forgot to answer that sorry it's okay um yeah so my first internship was a very classic analytics one felt um we did like customer call sentiment analysis and like you know grouping calls into red yellow or green based on you know how they're feeling um my biggest takeaway from that internship was Data I that's when I truly realized data is what matters the most we tried a bunch of you know ml models and we came to the conclusion that you know this is probably not going to work out so well as of now because um the audio transcriptions the quality of those were not so great um and humans you know on their own were not like people at the company were not sure how to you know classify calls because it was not clear so what better is a machine learning model going to do and I learned that ml is not magic um if a human can't do it at all or not so great there's a high chance an ml model won't do it that well either but it was a great learning process and um yeah I learned what all it might take to make a good model um all the stuff ahead that you do also just yeah go ahead sorry to butt in a little bit it sounds like what you had right there was a was a problem basically just getting good training data right absolutely I feel like that's that's a big theme across a lot of these podcast episodes where uh 90% of it is getting good training data and when you can't get good training data I mean you do the best you can but you really just it's it's just that's all you can do yeah Absol you can't make miracles happen yeah and also like labeling your calls uh like for example we realize a lot of our calls are not even actual like calls from the customer end they're sort of like workers at John Deere who are kind of working on the field okay um they're not the real customers and of course they speak in a different tone um or way and it might seem negative or you know it could be anything and the reality is actually you should not even be looking at that call in the first place and uh that tone is all relative to the type of people that are talking to one another yeah that's interesting that's I mean I think there's a lot to be learned from that right is is how do you how do you when you're getting data when you're focusing on data how do you get the data you need and sometimes that's really hard because it's usually all just put in one big bin in one big box and they go here's our data and and you're like oh yeah got to sort through all this stuff and sometimes that's easy to do and sometimes that's like next to Impossible absolutely and all like the the efforts to label the data that can be yeah they might have tons and tons of calls but we need people to painstakingly sit through them process them and label them as red yellow or green um and that's not always the easiest thing to do we couldn't get a ton of training data so that was also another problem um but yeah that was my first internship okay uh I guess I won't go over everything John Deere because it'll take too long but my second internship um was is actually a um very uh natural language processing heavy one actually the last one was also natural language processing because I had to process calls using natural language techniques and I don't know if you've heard of TF IDF term frequency inverse document frequency I don't but I'll ask essentially trying to look at your calls to gauge um how important is a particular word with reference to a particular so like for example the word the uh-huh is likely to appear in everything yeah so you look at also not just how frequently does the word the appear in a specific call and what sentiments associate with but how how frequently does occur in all the calls and if there's like a particular word that has that has you know is associated with a bad sentiment and it's not like a normal word we're like oh okay so that word is associated with bad sentiment that's kind of how you know um things like naive phase algorithms Works um yeah yeah yeah sorry I'll move on to yeah my next I love it I I know you know this is great I one of the highlights of this whole podcast is is a learning opportunity for me and so so I love it yeah so thank you very much for teaching me about a new term and a new technique yeah um sorry yeah so my next internship was more natural language heavy and it this is the give context this was after chat GPT came out the whole generative AI boom and what they wanted to do I was working in Factory Automation and they wanted to find a way to improve the completeness and accuracy of um machine defect documentation at a factory um and to give context you know operators they be working on the assembly line and when they run into a defect besides just figuring out what base machine and what model and what not you have to figure out look through like hundreds of thousands of parts and figure out which part do you have the issue with and what exactly is wrong um and memorizing it well and that can be a very tedious process so you wanted to create a sort of like a sub automated um solution to this um this is also another important point that you don't want to always completely rely on AI sometimes merging it with human intelligence can give you the best output over trying to f- tune and get perfect accuracy in your model um and what this approach essentially did was we used our bill of materials um and we put it into a vector database type type of a setup and we started out broad it's a it was a hierarchical bomb so you have the broad level machine parts and the more specific machine parts within those broad levels and you kind of go through each level until you get to a specific part okay um and it's like a series of questions like you'll be presented with the top five parts to the like on the user end and he or she would pick one and then you would get deeper um is it almost like a decision tree to a certain degree like or like a CL you know like like where you know there's all these sort of if this part then this or or am I completely off on that with the decision tree it's more hardcore ml where um well I guess in term if the the one characteristic that's similar is narrowing down your options uhhuh um however it's this one is very like um embeddings based so are the um the parts they could converted to something called embeddings which is like a numerical version okay of the words but also as context right it's not like if something contains apple and this also contains Apple then they're similar apple is also associated with red so it has knowledge about the English language and you do similarity searches with the users initial description and you find the closest thing but the nice thing is you don't have to do it um you don't have to really do it with all 200,000 Parts is one they're categorized so you find the closest bigger category yeah so how many of these parts um match it the most and what do most those parts fall into and that's how you select the category oh I love it I I I I love it because it's it's sort of like this divide and conquer strategy for identifying what the real problem is uh rather rather than trying to tackle everything all at once you you've broken it down into smaller pieces and I it makes a lot of sense to me I think it sounds fantastic thank you yeah um yeah yeah we that I guess it's really this is my favorite project cuz um it's in a field that I really like and it kind of brought in the core of my major which is understanding how to integrate human intelligence with AI intelligence how to incorporate the human Centric aspect I love that that my solution really really you know was true to that um that you know it's not just AI picking one out of 200,000 Parts the user itself is picking it gradually and getting to the right decision yeah and you're just helping them make that decision fast fter and more accurately absolutely I love it I love it um uh so other questions I I've got lots more questions um I'm curious so you say you're gonna uh start a master's program tell me about the master's program and also uh I'm also curious as to um well we'll just start with this tell me about the master's program master's program well the one that I've applied to and hopefully get into crossing my fingers um it's a data science ones as I said and there's like you know their classic beian statistics type of courses um your ml type of courses there's also like a cloud component to it um and I can't remember everything on the top of my head but a very like you get like the big picture of a lot of different you know from the stat side from the a ml side from the cloud side from the industry side how to apply data science very is how I'd like to summarize it um and why I'm doing it uh initially my plan would be oh work full-time only and figure out what you want to do but I realized I didn't really want to do classic software engineering I wanted to go into applied sciences um and their requirement was get a master's at most places so I was like why wait if I know what I want to do yeah um I might as well you know do the job that I want and not wait if I already know but and so the are you going to be you said it's all online are you going to be doing this full-time this master's program or are you going to be working as well or how's this going to work out sure yeah it's self-paced okay um so you can take one course or you can take three it's up to you um and you can finish it uh whenever as long as you meet the deadlines so yeah I'm going to be working full-time that is the plan um fantastic um and approximately how long do you think it'll take to complete the the Master's Degree um so usually it takes around 2 years however there's a way to transfer courses in from undergrad if you took them uh so I'm transferring in two courses and I plan on taking it's an eight course program so I should plan on being done in about a year so fantastic congratulations that's exciting than you um so I guess the question I have now now that we know what we know where you've been we know what you're what you're doing and we know you're headed uh do you have any words of advice or things that you learned along the way that you think uh would be worth sharing with other people yeah absolutely um well I guess an overall advice that I'd like to give college students in general is don't be like be willing to experiment and try new things um I think the best part about my education that that it wasn't just hardcore CS it wasn't just is I took like I had a I forgot to mention I have a minor in CS so I got that whole you know nice package I tried courses that you know I might never apply ever again like computer architecture but exposed me the things like lower level programming and how does programming even work um and how do operating systems work and that kind of context can really help you do better in the tech World um and also the importance of you know having personal projects or being part of like for example I'm part of the disruption lab um tell me about that yeah it's a tech Consulting academic unit okay well yeah they' like to call an academic unit within the geese College of Business um and you're essentially you're put into groups as software engineers and you work for a client usually like a startup or a small company um that's the usual thing and that really helped me you know get you know hands-on experience with like actual projects applying things in the real world versus applying things in the class are very very different yes yes that's very true for sure yeah so that and actually how I even got into generative AI how I got my project at John Deere was because of disruption lab um I was initially going to do like a very classic analytics type of internship for my second one but I'd created a um or with my team i' created a live crypto search engine um that essentially yeah it was really cool like you can like look up stuff so chat gbt you know it has stuff 2021 and before um so you can't really get live summarized data that's right so what we want to do is control the information Source uhuh um to answer the questions that we're looking up okay um in a search bar and this is this concept is called retrieval augmented generation okay um and we use this technology called Lang chain to do it um and we put apis on the back end things like I don't know if you've heard of coin gecko or defi Lama and they fet that information for you okay um so they have all these like API calls it tries to figure out which API call to pick to answer your question it takes the Json output and summarizes into something that a human can understand W that's fascinating um so there was one word that that really threw me you said this was um some it was you said crypto sorry I'm so sorry so it was a crypto Search bot it's like basically you can ask any questions that you want about crypto whether it's like oh what are the top coins in India or the US tell me it's market trends stuff like that that I mean that sounds fantastic I I love it so you've got you've got this large language model or like a chat GPT type thing that rather than being limited to information prior to 2021 what you've trained it to do is select one of these apis from which it can get it can get live information about Bitcoin or whatever cryptocurrency someone is interested in and and you've trained it to select those apis and then get the information and then when it gets the information it summarized it which I this sounds amazing I mean that's one of the coolest things I've heard thank you yeah and the thing was at that time we didn't have things like being AI that could give live information so back then it was even cooler um but yeah it was but the for me the interesting part was learning how to use Lang chain and Lang chain became the foundation for my next internship so um and that whole concept of being able to send text to chat GP basically chat GPT um in an automated manner yeah so so another thing I love about what you're saying and this is actually kind of Echoes uh what we what I've we've heard in other podcast episodes which is you learn and then you apply so you learned a new technique this Lang chain technique uh as part of this sort of organization on campus and then you took that directly to your John Deere internship and you applied it yeah I love that thank you I'm actually surprised you know it's crazy I didn't I never picked the path of generative AI or natural language processing one project sort of built on the other my first project at John Deere I did natural language processing um and daab liked that so they put me on a generative AI project and then John Deere liked it again and put me on a generative AI based project success build on success I love it I guess so absolutely I'm fortunate uh that it paved out so nicely but I would like to say on that note that if you don't know your path it's completely okay it's okay to go to work for a bit figure out what you want do only reason I'm doing my masters once again is because I had that Clarity and I know I'm not going to be able to get the job that I want if I don't do this Masters so oh that totally makes sense I love it well Kushi I want to thank you very much for uh taking the time to be with us today and giving us some advice and sort of just telling us about your journey and your quest in data science I absolutely love it uh so thank you very much for joining us absolutely thank you so much for this opportunity Josh I once again I'm a major major sta Quest fan so this is like surreal doing a podcast with you right now um but yeah thank you so much I really really enjoyed this conversation and would love to keep connected with you he we'll do we'll definitely keep in touch"
lA2baUE00uY,2024-03-04T07:00:17.000000,Human Stories in AI: Achal Dixit,"hello I'm Josh starmer and welcome to human stories in AI with stack Quest and lightning AI in this series we'll hear about the career journeys of passionate AI experts from their humble beginnings to conquered challenges will be inspired by the realworld experiences of professionals thriving in the ever evolving AI landscape human stories and AI is brought to you by lightning AI code together prot type train and deploy AI web apps all from your browser with zero setup personally I love lightning AI because it makes it super easy to use and learn from the stat Quest coding tutorials just go to the web page click on the Run button and Bam you get code that you can play with without downloading anything or installing any packages today we have special guest achel Dixit a data scientist at delivery the largest fully integrated Logistics services in India achel solves problems using data statistics and machine learning with a focus on business and people before delivery acho was a business technology Analyst at Zs and before that achel was a research assistant at Imperial College London so without further Ado achel can you tell us about your journey to where you are right now delivery how did this all start I've been always intrigued by the feel computer science since I was a kid so even even though when I was in school so I I had that in mind that I had to do something with ML or AI when I kind of venture into whether it's academics or in Industry so I think it was pretty much clear for me in the beginning but I didn't know how to do that so the best advice I got from my teachers from my parents it was just to pursue computer science because doing Engineering in computer science has been the most uh I would say straightforward path for anyone in India as of now so a lot of folks are enthusiastic and they are looking forward to pursuing engineering specifically in computer science because as we all know it's a field that's booming not only with not only in like monetary and fiscal terms but also in kind of the Innovation that we keep seeing every day that's true so that has been a pinnacle for everyone so mostly it started in first year I got into an engineering college and rather an Institute so it started with me doing pursuing Bachelor's in computer science I took some mathematical course but the if I have to pinpoint at one thing that uh started my journey towards data science and statistics was the moment that Co hit oh really okay yes tell me about that so what happened is uh lockdowns were announced Nationwide so we had a lot of uh kind of lot of migration from offices to home from schools to home so similarly for us also all our uh teaching went online all our studying went online and I got a so I have been I have been really passionate about doing things so in putting in terms of RL I would say it's like exploration versus exploitation dilemma okay so what I it's it's a simple thing where you explore with n coefficient and N minus Delta times you exploit what you have whatever you have learned okay so so you're you're learning just one step ahead of how you're of of and you're exploring and then you have tools and and and you can use those tools to do new things and new exciting things exactly that's how I would Define my thought process behind learning and doing things it's like I learn and I quickly move the coefficients around for my n and for my Delta so then I exploit with n and explore with Delta yeah so that's how I would put it so what happened is I I was very enthusiastic as a student to participate in maons because they they are challenging they kind of I like to take challenges and solve them in in in ways that I don't know it's just it's just fun for me it's like a puzzle it's like a kid solving a puzzle that has always been the case for me I love it I I have a confession to make I find nothing more intimidating and scary and frightening and I have nightmares about hackathons there's just I'm my hands are sweating just thinking about it so it's it's actually cool that I'm talking to you as someone who's like I love hack they scare me to death okay so I have I have participated in a couple of them and yeah it was the MIT covid-19 hackathon which started the whole uh research I ideally I got into research because of that and from research I got into data science and a job so let's take one thing at a time so I so I met with a couple of researchers from University of Michigan okay so they are very good friend of mine and we all decided to participate in this haathon organized by MIT okay and there I slowly realized like how clinicians actually use statistics okay what was the problem you were working on can you can you describe that yes so the problem that uh we were working on was kind of uh predicting the ICU times okay so how much time a patient would be blocking an ICU for because there was a huge queue if we go back to 2020 there was a huge queue for patients lined up to get admitted to icus yeah so we started from solving that problem but we ended up solving another problem what we did was yes yeah that's that's isn't it the way it always is exactly exactly and it was so much fun for me and it was such a great learning experience because I was able to see firsthand how the clinical data looks like yeah so how the clinical data looks like how researchers and staticians uh and specifically clinicians look at that data and make use of that data yeah so we had demographies we had symptoms we had data on symptoms demography and uh previous comorbidities of the patients so they were all lined up and we simply ran a logistic regression we wanted to predict if you were to be sick from Co yeah how sick you would be yeah whether it will be severe for you or not okay that makes sense so given your comorbidities symptoms and your previous background all these attributes we were able to predict with pretty good accuracy and kind of other metrics that if you were to Catch Co how bad it would be yeah so you need to take more precautions yes so that's the end goal of it and then we ended up kind of publishing that whole Research into one of the very good journals the American Journal of emergency medicine oh awesome that's fantastic so so that was the first publication I got and I was in my second year that's amazing yes can I ask a question yes how many people participated in this hackathon so over 800 people particip ated I guess and we won that haathon as well and you won it out of 800 people a seconde student with your team won the hackathon yes I mean most of the work like on the research side was done by my friends who are Scholars at University of Michigan uh Charles thank you I mean if you're seeing this so that's one thing and uh yeah so that got me into research and I was really excited that okay this is research it's problem solving because I had a very different idea about research in data science machine learning or or surrounding subjects being in computer science I was like okay it's it kind of seems intimidating boring probably probably it was kind of make once you go into research you feel anxious yes but to me it seemed like okay you have a problem you solve it you give you proofs that you solved it it's research that's it bam and I was I was I was like okay so whatever I've learned so far how how can I exploit it to probably do more research on the similar topics that I have done and that's how I got really very interested in uh research and data science okay so that's how data science became the center of my interest and something that I really wanted to pursue okay because for me being a computer science student it was two things were very simple that I had that core technical understanding I had programming uh experience I had my technical side pretty fundamentals pretty strong so I can virtually enter into any domain at that point and make use and exploit my tools and learnings in order to make some progress in that field yes and I just love the fact that the interdisciplinary approach that we have in terms of using statistics and data science yes and AI it was so good so that's how I ended up and then later on I took officially the course of machine learning learning through my third year in engineering and there also we got a project but I take my project seriously as I said I love hackathons I ended up doing couple of uh I I started with literature review I read a lot of papers and that's how I found that okay I can solve one problem and the problem being there's a gray area when when clinicians diagnose heart failure okay so there's a gray area because the metric that they use to define heart failure is called ejection fraction say that again ejection fraction so how much blood does The ventricle pumps out of the heart got it that makes exact sense okay I got it now yeah but the but this metric has a flaw that they don't know how to classify the ejection fraction between 40 to 50 or let's say 35 to 45 so it is it is considered a gray area and I found this just by reading a lot of literature on it and then I thought okay can we can we Sol Sol this if I have good enough data okay and by chance I got a good data set from one of the physionet websites which it host open data sets you just need to site them if you use that's so what so what you're telling me is there was just a public data set that you could just download and play with on your own you didn't have to you didn't have to like network with a bunch of cardiologists or anything you could just get this data set and just start seeing what you could do yes okay okay so I I figured out okay so by in back in the back in the course the goal was to kind of just build a machine learning model that does a classification but I took a different approach I went like okay let me go ahead if I'm doing it why not solve a real problem okay oh wow I love it I went ahead did a lot of literature review somehow I gained a good enough knowledge of a little bit about little bit knowledge on Cardiology as well how they diagnose how the prognos is so I listed the attributes I went down I did uh nice exploratory analysis and I figured that there was some features that were highly correlated with uh how the gray area looks like okay and hence I was able to use some AI techniques like Active Learning and semi-supervised learning to classify that gray area into primary types whether less than 40 or more than 50 which is heart failure with reserved ejection fraction and a heart failure with preserved addiction fraction oh okay and I titled the paper demystifying heart failure with mid-range ediction fraction uhhuh and and I I I published it in one of the i e scoped uh conferences which is Computing in cardiology it's a pretty old conference like 48 or 49th Computing in cardiology conference I got really good feedback that okay and it to me I don't know it was simple because I was able to find a problem yeah was able to quantify what methodology I know that I can use to solve that problem and then I just needed to do experiments yeah so that's another paper in my heart that I just got because I was simply interested and this is your third year so you're in your third year you've got two Publications yes I mean this is pretty amazing right I mean it's just it's just me being curious so I think I was genuinely interested in the subject and things kept aligning for myself I somehow I got lucky I read right papers found right problems to solve I would say can I ask a question so when you were experimenting with different methods to analyze this data did you kind of know specific methods you wanted to try uh or did you were you just like well I've got a whole bag of methods I'm just going to try them all what was that process like actually I was from if I look at it look at that problem again from from where I am am now yeah I was quite immature I did not know what a parametric test is what a non-parametric test is okay I did not know what this distribution is and what test can be applied on it so I kept learning like I I just intuitively thought that if I were to visualize this kind of data what one transformation I need so I went back read about it saw your videos yeah on PCA umap uniform man for approximation projection and then I visualized and tried of gain insights from what the data tells me okay and once it kind of uh showed a good uh inclination towards my hypothesis I went ahead ahead for different techniques so yeah I just proceeded like learned applied learned applied that's was like learn visualize and then once you can visualize what's going on you had a better sense of what the next steps were to do exactly oh fantastic I love that yeah that's that's what I always tell people and they're like what what do I what should I do and I go visualize you know try to graph it try to plot it do whatever it takes so you can look and see um what the data look like yeah because uh two of the features that I really uh that that kind of clicked the uh clicked and lit the bulb in my head was when I plotted Crea and KES versus one of the other features that I had and the gray area the people falling under the other the patients falling under the area of mid-range rejection fraction they were not isolated how you would how you would imagine on a linear scale of addiction fraction okay they were evenly spread out so I made a hypothesis that maybe it's just maybe it's just the flaw of a metric yeah because metric makes us visualize it in a linear scale yeah and those people do not lie on a linear scale so can we classify yeah so can we classify and I converted into a binary classification problem and run some semi supervise validation and it worked oh that's f fantastic so that that was one of the preliminary things I did uh and that's how it just started by itself I was like okay this is it I feel I'm good at it I'm going to continue doing this okay okay so you're in your third year what's next so basically I I can so I was also quite a lot oriented towards applying what I've learned as I've mentioned so I try to convert this whole Research into an actual application that can be used in India so India is not heavy on your use of ehrs or electronic health records so I try to build a product where where you can build a EHR on fly you can load all the patient data on there and all these AI algorithms are going to run on back end on those patient datas and they going to give recommendation to the clinicians that probably hey you should look at this this is what I suggest so I'm not doing anything it's just suggesting yeah uhhuh sure hey probably you should look at this so it's kind of a suggestion and that that idea went into another hackathon we did not win it but when I mean we did not ended up winning the hackathon but we ended up in top four finalist at Microsoft imagine cup and how many participants were in that uh probably more than a thousand I guess top four is pretty amazing wow so you're you are like not only do you enjoy hackathons you ex you they're it's like something about it like yeah I you know fires you up in a in a very creative and productive way like maybe maybe for you the pressure of like get it done in the next 24 hours or 48 hours or however much time you're given is exactly what you need you need that like yes that time constraint and all of a sudden you can focus all your creativity and all of your energy and and Bam get it done I mean one thing one mindset that has really help me in doing that is I don't think about the outcome okay I just do uh evidence-based stepping I step if I have good enough evidence to move ahead yeah and as I see it I keep moving ahead and somehow the product just finishes itself and something comes out yeah well so it sounds like it sounds like you enjoy the sort of the quest the Journey of I'm not gonna this isn't going to be predetermined outcome I'm going to see what the where the data takes me what it's almost like a like a like a detective right where you're just Gathering Clues and and it could lead to anything and you don't want to like you don't want to make a decision before you have the data and before you see what's going on and yes and that's that sounds sounds very healthy yeah I mean in a sense that has really I mean that's one thing that a lot of uh companies when they hiring for data scientists they want to look for because in business I mean apart from academics one thing that I've seen is uh people are more obsessed with the method meth theology sure and the outcome rather than the impact it makes yeah what you just said right there let's repeat it because I think it's one of the most important things that can be said right now especially in the field of ml data science AI statistics let's can you just say it one more time sure I mean I would uh rephrase it like people are people get more obsessed with the methodology and the outcome rather than the impact the outcome causes that's amazing that's I feel like that's so true people are like oh what method are you using well I'm using this super fancy neural network oh that's really cool and you're like no no no no it's not about the method it's about the outcome sometimes sometimes that fancy neural network is the right thing to get us the outcome that we need and that and and and when that's the case we need to use that fancy thing but sometimes it's a logistic regression sometimes it's K nearest neighbors sometimes it's like the most simple method that's GNA have the most impa impact and let me give you an example here so um what I mean by this is U if you go to a very enthusi if let's say we have a very enthusiastic team of data scientists and analysts andl Engineers they are so somehow I would I don't know if it's appropriate to say they are seduced by the technology they have that's right yeah they they they are completely logged in what they're doing and they believe in it and in all good faith it's perfect yeah but let's say you're using something you're using a neuronet based method called Tabet yeah you train it it takes a lot of time to train and on top of on the other hand you have a simple tree based method let's say exus random for the training time of the two is widely different yeah and let's say you are able to get 5% more accuracy on the other part on Tabet because it's a deep neural network and it's a better methodology and you get get end up getting better results but I think being in industry and solving problems for real world you need to include other factors as well like cost yes how much it costs for me to train tablet it needs GPU it needs more time probably the probably the cloud compute that you're using might ended up ended up costing ,000 which you might not save by having that 5% that's right that's right that 5% may not uh compensate for the extra time time is a big factor money is a big factor we need to keep track of both those things and sometimes it's better to to sacrifice a little bit of accuracy or something like that if if it means we can uh save time save money and just with like with any model I think a lot what people also forget is that it's not just about the initial training it's about maintaining that model and retraining it every so often and if you have something that's really expensive and really really timeconsuming to train you're not going to want to like update it every month or every couple of weeks or whatever because it's going to be too expensive yeah the time to production the maintenance that goes in the amount of uh learning for different teams so let's say the development team needs has a higher learning C for method even though it's better so we tend to trade trade it off for uh underperforming model if it if it does not require a lot of uh different infrastructure to be pulled in a lot of different teams taking a lot more time to bringing it to production so I think those are most of the things that companies are looking for data scientists who understand yeah I'm just having very good grason theory yeah and they they don't just want to use the latest greatest they want to use the tool that's going to solve the problem and that could be the latest greatest but it could also be something much simpler so in a sense in a sense a good data scientist I would be I would say is not the one who not only they understand the theory fundamentals methologies the different uh algorithms that are out there because somehow the open community and open source has democratized a lot of different algorithms to be used by people who don't know much about Theory MH so that does not sets you apart what sets you apart is how you can translate a business problem into a maths problem and then use those tools in the most optimal way to solve without incurring a lot of costs yes I love it I love it well um can you tell us a little bit what about what you're doing now now that you're uh now that you're a data science at delivery um what are the kind of things you're doing now so most of the things that we de so the business is simple you take a box from one place and you send it to the other place yeah yeah super simple there could be no no complications to that exactly I mean so most of the work that goes in uh Logistics has to do has to strongly correlate with opt optimization research okay so there are a lot of um MPS mixed in dig linear programming problems that kind of approach is used to optimize the facility location to optimize uh the rider assignment who is going to deliver which packet where and how many packets at once so assignment load assignment facility location and vehicle routing for all these uh kind of stuff milp is still the best approach because you can explain it because it's a it's not a black box it's a white box okay I'm G to be I'm going to be honest I don't really know much about optimization right now so I'm just GNA like maybe let me see if I can see if I understand what you're saying you're talking about so there are some I guess these are relatively old methodologies for optimization that you can use to figure out everything from where things should be warehoused to how many packages each deliverer should be taking um and and that's those are some of the tools you're using right now are these are these maybe not state-of-the-art but relatively tried and true simple Tech technique excuse me relatively simple that are explainable some methods like neural networks these days are like black boxes and it can be difficult to understand why it's making a decision or why it says we should do this thing so you're using these explainable things so you can understand and make choices and decisions based on that yes so to put it simply into a single sentence you have let's say x variables X number of variables you form a linear uh equation out of it you form a linear programming problem out of it and then either you maximize it or minimize it okay that's that's all no problem yeah that's all simping it I think I need to do a stat Quest on these things for sure it would really help a lot of people yeah so now where I'm using uh machine learning it's mostly statistical machine learning because we have tabular data here mhm now I would not uh directly talk about the problem but rather I would take an analogy to explain like what kind of problems you're solving so one of the problems that is that is really interesting to solve is let's say you have an ice cream business okay and you have open your ice cream stalls in 10 different cities okay are the most crowded places that would be the first place you want to go mhm how do you know uh how many ice creams each ice cream paror needs to sell optimally so let's say the ice cream stall a is selling 70 uh ice creams a day okay is that optimal or can we improve how do you Benchmark that because it will be the problem will look like okay in in City a let's say in California you're selling 100 a day but in New York you're selling only 70 a day so is this a function of the city or can we do better in New York because we are selling 100 in California yeah that makes sense uh so for example maybe in New York it's colder and people don't want as much ice cream but maybe in sunny California where it's you know they're in LA and it's hot and they like I need an ice cream so maybe you're just going to sell more because of the climate differences and you could maybe put a lot of effort into trying to sell more in in in New York but you may not get much return because you've kind of saturated the amount of ice cream people can eat there now let's complicate the problem a little bit you you don't make your own ice cream you buy it from some third party so you have 100 different flavors 100 different brands o I see so probably the client mix is different that's why the issue is coming yeah every do you have a different person selling the ice cream yeah a little bit some people are friendlier than [Applause] others some people might not want to sell where the previous person was selling they might want to put it in another block yeah so you see how this can be directly mapped to Logistics so many variables yes so many variables now two problems come yeah first problem is how do you Benchmark it second if it is not meeting that Benchmark then why the cause the causality what's the causality yeah so this is a very interesting problem that we are trying to solve because the business needs to understand whether the performance in that specific area for that unit is good or not yeah if not how can we drive that up yeah you do that you instantly help business grow and optimize their costs and efforts putting uh being put into that unit so how we are solving it you take all the variables you get a good enough data set because see modeling is not the center of it okay again I'll say how you translate business problem to a mathematical problem how you find good enough proxies for the data you don't have mhm mhm how do you estimate different variables empirically and substitute it for something that you don't have because more more than often you'll be able to map your problem but you won't have good enough data that's right yes get I feel like that 99% % of data science is trying to get good data yes so I think most of the work and the mental trigger that goes into that is figuring out how can we solve with the limited data that we have or how can we substitute and get better proxies for the things that we don't have yeah and what reasonable assumptions we can make yeah with that fall within a tolerable threshold of error that we are except to uh that is admissible so what we did is we simply take a nice good data set you have all your variables in people people places they have been in past some historical figures how you plan that City how you map that City mhm youv use uh it's a open public thing called Uber H3 hexagon you try to create feature out of it that each individual locality whether this this locality likes ice cream or not this locality likes ice cream or not how much ice cream I have sold in this local it whether I visited this locality in past or not and how the people have been in response to it whether these people order more ice cream expensive ones what brands do they like what brands after eating ones they have never came back to us probably the brands that I'm buying from I need to get rid of some of the brands because they are bringing RS down yeah so all of those factors you turn into features and we it's a it's a it's not a it's not a very standard approach Ro this is a very shy way to do things mhm so you train a tree based model out of it it understands a pattern how the ice cream has been sold where and it give it understands the patterns of client mix ice flavor mix people number of ice cream sold frequency of it and then we do sharply analysis on top of it okay okay which is a game theoretic approach it tells you how much each feature contributes towards that prediction mhm the big assumption that we make here is model is good the data it's strained on was optimal yeah and then you simply compare it with current rate and you can kind of get some directional inputs where the Improvement needs to be made and what benchmarks to set okay so uh um just a quick question so how do you I mean if you're going to assume that your data is optimal how do you how do you know you're in at least in the ballpark of that so for the that we that's the that's another myth that a lot of people who are trying to get into data science have that it's all about modeling it's not modeling is just 10% of it that's right so you do a lot of exploratory analysis okay you have a hypothesis let's say I say affluence affects sale of ice cream yeah how fluent that area is if people are rich they're going to buy more expensive ice creams yeah so I made that hypothesis now I go back I look at my sales six uh six months of historical sales and I try to see what Rich localities are there and whether that correlates with increase in average order value okay if that's true it's a good indicator and let's include it yes oh I get it I get it I see I see what you're doing I see how so for each you come up with hypotheses of like what do I think would affect ice cream sales then you can test each one and the ones that come out as positives you include that in your model and the ones that don't say like uh um T-shirt color uh that didn't correlate with with different ice cream choices or anything so you're like well we're going to get rid of that variable and we're just going to use these essential variables and and use that as sort of the gold standard let's say Josh was selling the ice cream so we made a million dollars so that Gand be strongly correlation strong cor I think the problem if I was selling the ice cream I'd eat all the ice cream well AEL I want to thank you very much for for uh being on the podcast and giving us not just one nugget of wisdom but several throughout um and I want to also want to congratulate you on all your successes so far it's it's amazing what you've been able to accomplish um and anyways I just want to thank you and um yeah and with that thank you so much for having me on your podcast and I don't know I'm doing it for the first time so I don't know what I did right or what I did wrong if my for people to understand for the audience and I'm sorry if it is not a come on you great fantastic oh we I mean the important thing is I think we all learned a lot and that's that's why we're here that's the quest"
QdXF69-EGEI,2024-02-19T12:41:38.000000,Human Stories in AI: Rick Marks,hello I'm Josh starmer and welcome to human stories in AI with stack Quest and lightning AI in this series we'll hear about the career journeys of passionate AI experts from their humble beginnings to conquered challenges will be inspired by the realworld experiences of professionals thriving in the ever evolving AI landscape human stories and AI is brought to you by lightning AI code together prot type train and deploy AI web apps all from your browser with zero setup personally I love lightning AI because it makes it super easy to use and learn from the stat Quest coding tutorials just go to the web page click on the Run button and Bam you get code that you can play with without downloading anything or installing any packages today we have special guest Rick Mars a professor at the University of North Carolina Chapel Hill School School of data science and Society before UNC Rick was a director at Google's advanced technology and projects group exploring new interaction approaches for ambient Computing environments and before that Rick founded the PlayStation magic Lab at PlayStation R&D so without further Ado Rick can you tell us about your journey to where you are right now at UNCC how did this all start sure I actually I just covered this a little bit in my class that I'm teaching with my students I kind of gave them my full Journey perfect starts in high school I was really interested in using technology to try to augment my uh social insecurity so like I I tried to make a program that would maybe like come up with the perfect joke to tell or the perfect quote to have on hand and I was thinking it would be like maybe in my ear and it could tell me what to say and I could you know so I started typing in all these quotes and all these jokes having no idea how I'd ever like get it to do the right thing at the right time but I started doing all this work of course I abandoned that project at some point and decided it was infusible for me in high school to figure out how to do that but nowadays it's much more possible and so in class we talked about a lot about the ways you might do something like that now what are some ways to do that now well I mean basically with chap GPT now you can type in kind of something if so if you had what had been said for the last 30 seconds it would probably have something pretty interesting to add to that which would be relevant it's very good at that now there's a bunch of issues about you know recording and listening and things like that that might not be easily solved but the actual technology part of coming up with something to add and along the way there was multiple times when I have been interested in the same project so I at Google I was interested in that project and we we gave it a name and stuff so it's uh but anyways that's kind of my journey it started in high school wanting to figure out how to use context around you and Technology together to augment your life give you you know enhance your life oh that's fantastic I actually love that uh because it's kind of a I love the origin right it's it's it's a it's such a such a small and simple thing and I completely uh identify with that like I'm one of those people that always thinks of the right thing to say about two days later you know like I'll be laying in bed and like oh what I should have said this um uh so to have anything that might help me uh in those situations just I mean I'll sign up and buy that whatever it is the only issue is if everybody has it it kind of levels the plane field again so if you're vastly successful in making a product like that then you no longer have an edge that's true that's true and then the people that are just a little better uh will all of a sudden rise to the top again oh well um but anyways I'll be an early adopter and have like a few months of bliss and feeling like I'm witty and uh you know can talk in the spur of the moment um that sounds fantastic I also am really interested in the idea of combining it with some of the ideas from video games so just having a dry Oracle let's call it or something that has these answers for you isn't I mean it's nice that's information but it doesn't excite me as a person as much as having maybe a character that I have a relationship with that mhm shares that information with me and so the project I worked on previously was we called it Daxter and there's a game called Jack and Daxter it's a video game and you play Jack and Jack is the hero and goes around and Daxter sits on your shoulder and Daxter is all the color commentary I mean he's not really doing anything except for constantly commenting on everything that's going on around you so we called it Daxter and you can imagine this thing is just observing your life and adding little comments in when it wants or maybe you know you could implement it different ways it could be your screen saver on your phone or you could glance down and see if Daxter has anything to add at that moment yeah and maybe it could be colorcoded so you could glance and you could see it's just going to be something funny or it's going to be informational like different ways you could implement this to not interrupt your life I didn't really want it to you know take over your life or something or be something you look like you're constantly distracted by but yeah yeah yeah but maybe at the end of the day you could like replay the highlights you know plus plus color commentary it's sort of like watching sports center but it's like your life center um that that sounds pretty interesting um so can you tell me so let just maybe let's back up a little bit so you're back in high school you're trying this coding thing you're realizing uh that it's maybe a little too difficult uh to do at the time uh so did you just abandon that thing and you're like I'm never going to do that again or did you did you say oh I need I need to in order to achieve achieve this dream I need to take this path or this path or or what happened next I mean it wasn't like that's been the driver of my whole life but it's been a something that's always been there I ended up using all the code that I wrote for a science project in my senior year or junior year where I made a robot and the robot was really just a frame holding a computer out and the computer had a face on it and it had different personality so you could switch to the joke personality or the famous quote personality and it would its voice changed a little bit and it was mostly just a computer program that was running on a TV that was like but yeah that I kind of repurposed the code for something useful yeah and then I got very interested in graphics and things like that so I I definitely did stuff that was more along the lines of 3D Graphics virtual reality for a long time in my career also got very interested in computer vision and understanding context from you know what the what the player or user is interested in and doing with their body or their hands or their face to try to enhance experiences was that just sort of like a real smooth Evolution from one thing to the other or were there certain events that kind of drove you know your interest in computer vision or was there like a certain problem that needed to be solved yeah definitely I started my PhD thesis with not the intention to do computer vision but we needed some sensing for the robot we were working on an underwater robot with the montere Bay Aquarium Research Institute and in the first summer I worked with them we needed some sensing for the robot so we got a camera and a a frame Grabber board and we needed to understand what was in the scene and we wanted to just do a really simple thing where we'd move a a very simple Target like a bright dot around and the robot would follow it and then make that work underwater and my focus was going to be the control theory part of it and moving the robot with control but the sensing part of it turned out to be very interesting and I got kind of absorbed in the sensing part and everyone kept saying this should be your thesis this I'm like oh no I'm doing control and they're like no no you should do computer vision I'm like eventually I saw the light and pivoted switched over to computer vision yeah and so your thesis was in computer vision and were you like I'm in love with computer vision and I want to keep doing this or what happened next after that so computer vision and I still did the control part I really wanted to do it for the sake of like making the robot move but the robot moving was a pretty simple part of it compared to understand what to follow so we we did a thing where we follow kind of a a fish which we use like a plastic Turtle to move it around and it could follow it had a pan and tilt system we also um did a mosaicing of the ocean floor so taking multiple photographs and stitching them together and that's something that's you can't really see the whole ocean floor because you can't back up because Marine snow makes it you know you can't see that far underwater so it gave you like a new capability to see a bigger part so using these ing things to give you new capabilities is what I kind of got excited about and so going I went and worked for a startup that did a lot of computer vision and then moved on to Sony and used computer vision for video games um I one little technical term uh uh ocean snow Marine snow Mar Marine snow what is that well it it it's kind of a general term for all the different particulates that are in the water and make it so the water is no longer transparent when you look through it what what kind of depth were you guys working at in terms of like presumably that mean you had to go pretty deep to to uh to photograph the the floor what was the depth so the the the montere Bay Aquarium Research Institute is right off right off of the coast of um California there's a thing called soel Canyon which is a deep underwater canyon and so they can go not very far off the the coast and get really deep really fast and that's kind of why they pick that area to do a lot of the work so you can get really deep and get very hard to see and there's also no light given to you you have to bring your own light and when you try to shine the light on it there's no ambient light so it's only your light and that's the worst part CU that basically all those particles are just you know reflecting off so you don't have any extra light around to help you so uh yeah I I had like a mathematical model for the Marine snow and it was very I was very proud of that part of it but that's the more theoretical part of my thesis and I think I was in a robotics lab so they're more excited about the Practical parts of my thesis do you do you still do any robotics at all these days I'm just out of it's just out of curiosity well at at Sony we actually used a robot to do some of the um some of the testing for like we had a the controller the PlayStation move controller which is also the VR input controller mhm we wanted to be able to move it around and see how it would behave the sensors inside of it and things so having a robot do that for you you can repeat do repeated tests and things so using it more for the kind of practical side of things I don't do a lot of the other side i' I've played around with like vacuuming robots a bit and things but you don't have robots all over your house doing all the things no but right next to me is the robots group in UNCC so oh fantastic so here's right at home you're in uh just to just to uh just to tell the the listeners you are based in this I'm off in the school of data science and Society they hired me and I have an appointment in computer science and I started by teaching in computer science because there aren't any classes in the data science school yet because it's just been formed so so I I intend to teach data science classes as well but I started with computer science I guess that leads us to where you are right now you are you're doing this stuff uh can you tell us about your current research I'm under the impression that you're getting into generative Ai and I wanted to hear about um what you wanted to do in that direction sure maybe I'll I'll back up a little bit and like okay when I was at PlayStation I was really interested in graphics and virtual reality and I love that work but I wanted to do something a little different I got pretty excited about AI started taking some AI classes machine learning and uh I got one of the smart speakers for my father and I gave it to him and uh he said he wanted to return it it was a gift and he said he wanted to return it cuz it didn't work and I was like what do you mean it doesn't work what did you do and he said I I asked it when is Golf and it didn't know I'm like well that's I mean I can understand as a end user you might think it should be able to answer that but there's he need it had no context for what he was asking me but I started thinking about that and I'm like if he was wearing his you know cleats and had his golf bag on his shoulder he probably means when is my tea time but if he's flipping through channels on a television he probably means like when is the Masters on right now so like more context is all you really need to answer that question effectively and so trying to get that context became very interesting and I'm it's very related to computer vision anyways again just trying to get more context so that's what I did I went to Google to try to work on that and I my team did a lot of work in trying to understand context and more about machine learning a lot more about um real time inference of things gesture recognition lots of different things so yeah and so and that leads us to generative AI uh and your you know your the research that you'd like to do in this area and generative AI is is very new so it's like even hard to say that I have had a plan to do anything with it but I I've been interested in combining Ai and virtual reality for example for a while and a lot of my students are actually interested in the same thing I'm teaching a class right now my focus has been on teaching but I do intend to do start some research one of the areas is and some of my students made a final project actually in the class I'm teaching that did this so the idea of creating things within the virtual world that are dynamically based on what's happening or what the user wants to happen so some of my students they made a uh one of their projects was they called it like a a magic um painting so you or magic canvas so you say what you want and then the it appears on the canvas and then you can like hang it on your wall in VR and so you can just keep asking for this canvas to make pictures and of course it's using something like these uh generative AI image creation things so you talk you have a text to spe or speech to text system and then the text is fed into one of these very cool that's pretty cool for a two-e project oh my gosh yeah two weeks how big were the teams out of curiosity two people team two two person teams okay that's a nice size yeah and the issues start to come up I mean right now generative AI is is moving fast and they really wanted to make a 3D model generation but the generation for 3D models right now takes about a minute okay and images take only a few seconds so they decided to Pivot and do images instead of 3D models and that is one of the things generally trying to use any of this in a user interface kind of situation is there's some latency and I mean everyone experiences it right now you type something in and then you wait a little bit to see what it comes back with and that's okay in kind of a text transactional interface but it's not so good in a voice interface and it's definitely not good in a kind of uh real time situation that that's actually when you were telling me about your original plan to make a little earpiece that could give you witty comments the first thing I was thinking about was the latency there and how there everyone else could be smooth and and I I might be able to come up with the right phrase but there'd be always be that lag and and everyone would be like is is Josh a robot I mean that there's there's nothing more robotic than that little lag right that little delay that it seems completely unhuman and unnatural yeah you can see a little happening in the meantime right exactly I actually worked on I mean video game developers are very good at dealing with these kind of issues and they hide things that are problems it's lots of smoke and mirrors kind of like Hollywood sets you know they're all and so we talked about how you could hide this like latency so we one of the concepts was like if you ask for a 3D object there's a bunch of boxes behind this they're going to just use a robot but there's a bunch of boxes behind the robot and the robot would turn and go and pick up a box and shake it and then say no and like it go like and eventually it you know after a minute or whatever time it would give you the box and You' open and there would be the 3D object that you'd requested so you kind of like put it into the story maybe yeah I love that I actually really like that because it it it uses kind of Storytelling as a way to um sort of solve the problem of of latency and it makes it it makes the makes the latency sort of like not not necessarily a problem I mean the way you solve it is a way to um to tell the story of there's all these boxes in a room and and I don't know I love that any any opportunity there is for storytelling I'm is something that I'm really interested in yeah there's a great conference called the future of Storytelling that I've gotten to go to a couple times and they really look at how storytelling and and Technology kind of meet together it's a very cool conference I would love to hear more about this conference what can you tell me about it so it it it's uh in New York and they have a very unique uh process so everyone gets on a ferry at the in the morning and you go to somewhere that it's not accessible any other way really and so you're kind of isolated you're there and the whole experience kind of begins right then and they bring in different storytellers and different technologists and they each give talks and there's a lot of lot of Art and different things all merged together and there is a very small number of people because it is like a so it's it's not easy to go every time I don't know what happened during the pandemic it must have been difficult but hopefully that conference is still going actually I don't know sounds amazing it sounds absolutely amazing another connection of uh of generative Ai and uh virtual reality for example there's a a fantasy book series that I read a long time ago and there's these kind of special people that have the ability to manipulate reality and so they just start to want the reality to be a certain way and then they can walk through it and it changes to match what they want and they kind of just think about it and sometimes they kind of talk out loud and it happens and I I mean we could kind of do that now with generative Ai and virtual reality you could basically start to say what you want and the world around you could start to form into that space it's a yeah it's very cool uh book series but also we can almost Implement that in that sounds cool I'm looking forward to it um yeah so I was wondering uh if you can tell us a little bit more about the class you're teaching um uh I mean the project that you described sounds pretty cool what are the other kind of topics that you cover and and and tell us just more about the class sure it's uh introduction to virtual reality and 3D Graphics so we start at the very lowest level basically the mathematical process of getting pixels on the screen from a model representation so you have projection and transforms and it's a lot more math I think than some of the students expected it would be a lot of linear algebra but then after that once they kind of understand how you would render something on the screen we stop and then we use a package like we used unity in my class that lets you do a lot of that still have to understand transforms and how to put everything together and then they use that for a little while and then after they get comfortable with that then we go on and add virtual reality on top of all that so you use that package to build a virtual reality experience so teaching a lot of different things in one class actually maybe but it it's we go sound fantastic um I love it uh um I love it cuz when I I used to work at UNC a long time AG or not that long ago but a long time ago I had a summer job in in the computer science department at UNC and we did a virtual reality project so it's kind of cool from my perspective to know that virtual reality at UNCC is still going strong yeah it's one of the birthplaces of VR for sure I mean that's partly why I was drawn here uh Henry fuks is one of the people that was really early in that space and Fred Brooks before him again and those two gentlemen I got to meet when I visited here a few years ago and a big part of why I'm here oh fantastic I was going to ask um actually that kind of leads to a question of like uh other than Fred Brooks and um and the virtual reality what brought you to UNCC Chapel Hill uh since you know you were at Google all way on the I mean presumably you were in California or were you in Durham or where were you yes I've been in California a long time actually so from from my PhD all the way to till I came here so I had decided when I was really young that I wanted to spend some time learning and then spend some time in industry and then eventually come back and kind of share the industry experience in through teaching so it kind of finally was felt time to become a professor I have you know looked at a lot of different places there's many good universities I had a lot of connections to UNC through friends that I worked with and conferences I'd go to they're very welcoming the professors I they'd invite me to lunches and events and things so I got to know some of the professors I came out and visited here a few times and gave talks and met some of the graduate students so it it was just I I decided I didn't want to be in a school in California I wanted to be somewhere else uh I had and I guess UNC was the kind of the top of my list of all the places it's a little bit strange because I I am very you know I've worked in Industry so long it's a little bit of an odd shift they it's not easy to find a position that matches you perfectly sure and actually the computer science department it was challenging to figure out exactly how I could do that they you know they have certain things they need taught and they so but the data science school was starting up and they had more flexibility and they are very tightly connected to the other schools within within uh UNCC so basically they suggested I talk to them and we kind of figured out a system that would I think work for everyone that's awesome but we have to go back so you said at a young age you decided you wanted to learn then work in industry and then go back and teach how young and how did you make that decision that's uh and and why I mean of of all the the the things in your life you know you've you've gone through and some things have changed but that vision of of of living that way apparently never changed you you stuck to it so can tell us more about that I would say I'm a very like not a planner I'm I'm very like go with the flow person except for this one thing when I was I don't know like 16 years old I just decided and I even like framed it with like 25 years of learning because I thought I could get my PhD in 25 years probably um 25 years years of industry and then 25 years of being hopefully being an effective Professor teacher and then I don't know hopefully that's not the end but yeah let's hope not uh but but I love that um and you just came up with that out of the blue I don't I think I read something somewhere I don't know what motivated it I can't remember why it was but it seemed good to have a plan like that somehow where you had an intentional I approve I think it's fantastic uh I mean and I'm trying to think I I mean I think there's something to be said about about having a dream like that and um and to be to be honest it sounds like you're living the dream I mean you're fulfilling this this dream you had at age 16 and is and you're doing great I think it I mean it it sounds corny when you say it out loud sometimes but like you know basically like learning and then putting that into practice and then trying to share it's like in a more service way maybe is kind of how I envisioned it would make sense this logical I mean it's it's inspirational to me I mean and I feel like and something I that maybe I hope uh might come out of this podcast is other people thinking yeah that is a good dream to have and maybe I'll do the same thing I actually think it's fantastic because I my my personal experience is that there's often a a a a gap between sort of practice and education and and and I think part of the problem with that Gap or the cause of that Gap is a lot of people that go into education started an education and just stayed there uh there's not a ton of people that go to Industry and then come back to education to teach um and so but but people like you have seen both sides you've seen sort of what happens um in an academic setting uh when you're getting your PhD but then then you saw sort of like what happens in a corporate uh industrial setting and you can you can see what the differences are you can compare and contrast sort of not just like what's being done but how it's being done and how people what people wait as import you know how people view the things as like you like in academics they might value certain things and then in Industry they may value different things and you've seen both of those and so when you come back to teach you can teach from that knowledge of having been on both sides in a way that a lot I feel like a lot of Educators don't have that ability so that's I think if there's one thing I hope people come away from this podcast thinking is like maybe that's something I want to do too is go into industry and then come back to academics to help teach and and share what I've learned yeah I and I think it it is valuable I I don't mean to say there are definitely some advantages to teaching your entire life I mean some of the teachers are very good at teaching because they've been doing it for quite a few years and I it's new to me so I'm still trying to figure out how to be a good teacher as well so it it actually I'm I was surprised at how challenging it was to prepare for a class and things oh yeah yeah teaching is hard it's hard work believe me but I have felt it's been very useful to be able to share stories with the students about things when they're learning them I show an example of how that showed up in something that we made as a product often or at least as a prototype of something yeah and that's got to be inspiring to them cuz I feel like a lot for me when I'm learning uh a bit just like you're saying about Vision a lot of learning is learning context you know like I might learn how to do Matrix algebra but unless you give me the context uh it's completely abstract and not very interesting to me and I feel like one of the things you can bring is is you've got that industrial context that you can say and this is you can say here these are abstract equations but look at what they can do this is what we did with them and this is awesome um it's also particularly easy with video games and students because they have a they relate very strongly they know them and any consumer product really is easier because people can directly relate to it if you're in some kind of more uh business setting that the students don't relate to it's not quite as easy I think I imagine it wouldn't be yeah well I think it's super cool and it's inspiring to me um anyways uh so before we wrap up I was wondering if you had any advice uh that that you've learned along the way that you'd like to share uh with our listeners yeah I get I get asked a lot by the students like what should I study or what should I do and how should I you know make myself marketable and I when I was hiring people definitely what I always looked for was somebody that had some deep contribution they could bring like something they were deep in but also people that weren't only one-dimensional and they could relate to the rest of the team so and I think that that's probably been what has helped me the most through my whole career is I had something deep that the rest of the you know my group didn't have I was a computer vision person and I had that part but I also understood a lot of what they were doing so that I could bring it in and connect the things together and and work with them so I think they call it a t-shaped person often in Industry they have this like some a lot of breath but some depth as well and saying like you should just be deep in everything is not a reasonable thing to say hard yeah so maybe like a PhD you know T it can be very dangerous where you can get very narrow easily and maybe uh other things you can be very flat and Broad and not have any specialty and that's also challenging to find like what do you bring to the table then so having a this kind of shape I think is a very simple framework to think about it in I I I absolutely love it uh um and the way I relate to that concept is um it allows you uh because basically every field every other group out there like you might be very good at computer vision but you're probably going to be a part of a team of people that are experts in other things and in every field they have their own what I found they have their own language of how to describe things and they we may all be talking about the exact same thing but using slightly different words and having that t-shape gives you that expertise in one area but it also gives the ability to communicate effectively with everybody else who's on the team and I just found that you know that seems to be like a super helpful thing that's been very helpful for me and that's actually how I got started was just being able to communicate sort of what I know to people that are completely afraid of mathematics you know I could find the language that worked for them well uh Rick I just want to thank you very much for being on the show I really appreciate it uh thank you for your time no it's great I've watched a lot of the videos they're good good for me to just actually reframe again you know sometimes again solidify especially if you haven't used it in a while well well thanks again uh thanks again Rick I really appreciate you thank you very much oh thanks for having me it's been fun
ZTt9gsGcdDo,2023-12-11T05:00:25.000000,"Essential Matrix Algebra for Neural Networks, Clearly Explained!!!","If you can add and multiply, then you can do matrix stuff! Hooray! StatQuest. Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about essential matrix algebra for neural networks and it's going to be clearly explained. Start small but then build as big as you want. Lightning! Bam! This StatQuest is also brought to you by the letters 'A', 'B', and 'C'. 'A' always, 'B' be, 'C', curious. Always be curious. Not that you need to, but if you watched any of the StatQuest of videos on neural networks, then you already know that even the most state-of-the-art models, like transformers, can be explained with relatively simple diagrams and flowcharts. However, if you try to code neural networks using any of the tools that make the job easier, like PyTorch and PyTorch Lightning, then it won't be long before you need to look at the documentation and you'll see something like this. Ugh! Or you'll have a small bug in your code and you'll see an error like this. Double ugh! Or you might just want to read up on the latest trends and neural networks and not wait for StatQuest to explain it,and you'll see stuff like this. Triple ugh! All this math looks complicated and hard to understand. Don't worry, 'Squatch. We're going to learn how to read all of this gobbledygook one step at a time. Bam! All of these things are based on matrix equations, which provide a very compact way to describe neural networks. So if you want to understand the documentation, debug your code, or just read the latest manuscripts about neural networks, you need to know how to work with matrix equations. So in order to understand how matrix equations apply to neural networks, we're going to show how this specific neural network translates into this compact matrix equation. Ugh, that equation looks weird. Are you sure I'm going to understand it? Don't worry 'Squatch. We will go through it one step at a time. Okay. However, we'll start by learning some basic terminology and build up an understanding of the strangest part of matrix equations, matrix multiplication. And in order to do that, imagine StatSquatch just got a ticket to see Taylor Swift at the Friends Arena in Stockholm, Sweden. 'Squatch,are you ready for it? Yes. Where is your seat? My seat is close to the front on stage left. Now, if we put an x and y axis centered on the stage, then in terms of those axes, the coordinates for 'Squatch's seat are 2, 1. Now, for reasons that we will never, ever, ever understand, Taylor decides to flip the direction of her stage 180 degrees. I knew she was trouble. Anyway, rotating the stage means 'Squatch's seat and everyone else's needs to rotate as well. To determine the new x and y coordinates of 'Squatch's seat, we can use these equations. For example, if we plug in the original coordinates, x = 2 and y = 1, then the new x and y coordinates are -2, -1. In other words, these two equations transform the point 2, 1 into the point -2, -1. Oh no! It's the dreaded terminology alert. Because these equations, which transform the original x and y coordinates into new x and y coordinates, only multiply and add stuff to the original x and y coordinates, this transformation is called a linear transformation. For example, if we focus on the first equation and plug in x = 2 and y = 1, then we get -2. If we then increase x to 3, then we get -3. Likewise, increasing x to 4 gives us -4. Thus, each time we increase the value for x by 1, we decrease the output by -1. In other words, a constant change in the value for x results in a constant change in the output. Because the amount of change in the output is always the same, just like the slope, or the amount of change in the y axis, of a straight line is always the same, this transformation is called a linear transformation. In contrast, if the transformation was 2 to the x, then plugging in x = 2 would give us 4. Plugging in x = 3 would give us 8. And plugging in x = 4 would give us 16. And we see that each time we increase the value for x by 1, the output increases by a different amount. Because the amount of change in the output changes. Just like the slope of a curve changes, this transformation is called a nonlinear transformation. Anyway, going back to our original linear transformations, the reason I'm making big deal about linear versus nonlinear transformations is that matrix algebra is especially useful for linear transformations. Thus, using matrix notation, we can write out our linear transformation like this. In this example, we've put the old x and y coordinates into something called a row matrix or row vector. We can also call this row matrix a 1 by 2 matrix because it has one row and two numbers in that row. So we say it has two columns. We can also say that this matrix is one-dimensional because everything is on a single row. The coefficients, the numbers we multiply x and y by and define the transformation, go into a matrix. And we can call this a 2 by 2 matrix because it has two rows and two columns. Or we can call this a two dimensional matrix because it has multiple rows and columns. The coefficients in the first equation go into the first column in the matrix and the coefficients in the second equation go into the second column in the matrix. Josh, this seems really weird and I have no idea how I will remember this. Don't worry 'Squatch. We will explain exactly why we're doing all this in just a second. And knowing why we do this will help you derive it on your own. Bam! Anyway, if, just like before, we let x = 2 and y = 1, then we can multiply the row matrix that contains the original coordinates by the transformation matrix with the coefficients. Hey Josh, I know how to multiply numbers, but how do I multiply matrices? We start by multiplying the row of x and y coordinates by the numbers in the first column in the transformation matrix. So, in this case, we start by multiplying 2 by -1, then we multiply 1 by 0. And lastly, we add the terms together. And this gives us the original transformation for the new x coordinate that we started with, which tells us that the new x coordinate is -2. Then we multiply the same row of x and y coordinates by the numbers in the second column in the transformation matrix. So that means we multiply 2 by 0 and 1 by -1. And lastly, add the products together. And that gives us the original transformation for the newycoordinate that we started with, which tells us that the new y coordinate is -1. Combined, the matrix multiplication transforms the old x and y coordinates to the new x and y coordinates. Bam? It seems like a lot of trouble to do the same thing as before. Well, what if Taylorrotated the stage 90 degrees clockwise from where it was before? Oops, she did it again? No 'Squatch, that's Britney Spears. Anyway, where should 'Squatch sit? To determine the newest x and y coordinates for 'Squatch's seat, we can plug the coordinates we just calculated into these equations. And just like before, we can convert them into matrix notation by putting the x and y coordinates we just calculated in a one-dimensional row matrix with one row and two columns, and putting the coefficients into a two dimensional transformation matrix with 2 rows and 2 columns. The coefficients in the first equation go into the first column in the transformation matrix, and the coefficients in the second equation go into the second column in the transformation matrix. Now we plug in the x and y coordinates we calculated earlier, -2 and -1, and do the matrix multiplication just like before, row by column. We start by multiplying the row of x and y coordinates by the numbers in the first column in the transformation matrix. So we multiply -2 by 0, and we multiply -1 by 1, and then we add the terms together. And that gives us the newest x coordinate, -1. Then we multiply the same row of x and y coordinates by the numbers in the second column in the transformation matrix. So we multiply -2 by -1 and multiply -1 by 0, and then add the terms together. That gives us the newest y coordinate 2. Thus, 'Squatch moves from -2 , -1 to -1, 2. Josh, I still don't get why we multiply matrices in such a strange way. Well, the reason why we multiply the numbers in a row by the numbers in a column and then add the products together, is that when we use matrix multiplication to multiply the first transformation by the second transformation, which means we multiply the first row by the first column and add the products, and we multiply the first row by the second column and add the products, and then we multiply the second row by the first column and add the products. Note, because we are now using this second row of the first matrix, we put the result in the second row in the output. In other words, when we use a new row to do multiplication, we start a new row in the output matrix. Anyway, then we multiply the second row by the second column and add the products. And we end up with a new two-by-two transformation that combines the effects of both transformations. The new combined transformation transforms the original point directly to the newest point without having to take the intermediate steps. In other words, matrix multiplication is funky so that we can easily combine a sequence of transformations into a single transformation. To convince ourselves that we can use the combined transformation to transform the original point to the newest point without having to do the intermediate steps. Let's multiply the original coordinates by the combined transformation. So we start by plugging in the original coordinates 2 and 1. Then we just do the row by column math. Beep boop boop boop boop beep boop boop boop boop, beep boop boop! And we see that the combined transformation transforms the original coordinates, 2, 1, to the newest coordinates, -1, 2. Bam! Note, matrix multiplication is such that no matter how many times Taylor Swift changes her mind about which direction the stage should face, we can just multiply the individual transformations together to get a combined transformation. And that combined transformation allows us to go directly from the original location to the final location without having to do the intermediate steps. Bam. Okay, now that we understand why matrix multiplication is so funky, let's talk about another confusing thing about matrix multiplication. Because matrix multiplication is applied row by column, we can't just reverse the order of the matrices and expect things to work out the same. In fact, we can't even multiply these two matrices together because each row in the first matrix has two numbers and each column in the second matrix only has one number. For example, if we try to multiply the first row of numbers in the first matrix by the first column of numbers in the second matrix, then we can multiply 0 by 2, but there is nothing to multiply this 1 by. In contrast, it's possible to multiply these two matrices because we have the same number of values in this row that we have in these columns. In general, the number of columns in the first matrix has to match the number of rows in the second matrix. If, for some reason, we want to switch the order of these matrices, then we need to change each row into a corresponding column. Now we can do the multiplication correctly, row by column. Beep, beep, boop, beep, boop, beep, boop, boop. And after turning each row into a corresponding column, the math works out just like it did before. Note, because we turned the rows in the original matrices into columns, the output is now a column instead of a row. Oh no, it's the dreaded terminology alert. Changing the rows in a matrix into corresponding columns is called transposing a matrix, and we notate the transpose function with a superscript 't'. So this matrix is the transpose of the original coordinates. This is the transpose of the combined transformation matrix, and this is the transpose of the newest coordinates. As we saw earlier, the original coordinates were in a one row by two column row matrix, which is also called a 1 by 2 matrix. But after we transpose the original matrix, we end up with a 2 row by 1 column column matrix, which is also called a 2 by 1 matrix. Likewise, the transformed coordinates were also in a 1 row by 2 column row matrix, or a 1 by 2 matrix, and the transposed matrix is a 2 row by 1 column column matrix, or a 2 by 1 matrix. In contrast, the combined transformation has 2 rows and 2 columns, and thus is a 2 by 2 matrix, and the transposed matrix also has 2 rows and 2 columns, and is also called a 2 by 2 matrix. Now, because writing out matrices can be cumbersome, people often replace them with variable names. Matrices with only one row or column are usually referred to with lowercase letters that are sometimes italicized and sometimes not. And matrices with more than one row and column are usually referred to with uppercase letters that are also sometimes italicized and sometimes not. Out in the wild, you'll find additional notation styles, so just be prepared to be flexible. Anyway, given these variable names, we can rewrite the matrix equation like this 'a' times 'W' equals 'b'. Or we can refer to the transposed matrices by simply adding a superscript 't' to each variable name and rewrite the matrix equation like this: the transpose of 'W' times the transpose of 'a' equals the transpose of 'b'. Either way we do it, we will get the same transformed coordinates. Note, we could have just as easily called this column matrix 'a' and called this row matrix the transpose of 'a'. Likewise, we could call this matrix 'W' and this matrix the transpose of 'W'. Lastly, this column matrix could be called 'b', and this could be called the transpose of 'b'. Thus, we could also write the matrix equations like these: the transpose of 'a' times the transpose of 'W' equals the transpose of 'b', or 'W' times 'a' equals 'b'. I mention these alternative ways to write the equations because out in the wild, you'll see all kinds of notation, so just be prepared to be flexible. Lastly, if we want to refer to an individual value in a matrix, like this -1 in the bottom left-hand corner of the matrix we originally named 'W', then, if the variable is uppercase, we convert it to lowercase and add subscript row and column indices like this. Bam! Now let's talk about how all this applies to a neural network. This neural network may look really fancy, but all it does is take two measurements from an iris flower, the width of a petal, which is this part of the flower, and the width of a sepal, which is this part of the flower. And with that information, it predicts the species either Setosa, Versicolor, or Virginica. Anyway, let's start with a petal that is 0.5 units wide and a sepal that is 0.4 units wide. The first thing this neural network does is multiply the petal width by -2.5. Hey, wait a minute. Where did that -2.5 come from? All of the numbers in this and pretty much every neural network come from something called backpropagation. And there's a lot to be said about backpropagation, so if you want to learn about it, check out the Quests. Anyway, then we add that term to the sepal width multiplied by 0.6. And at the same time, this neural network also multiplies the petal width by -1.5. Then we add that term to the sepal width multiplied by 0.4. Now, even though we are looking at these equations for the first time, they may remind you of the equations used to transform 'Squatch's ticket to see Taylor Swift. Those memories follow me around. In both cases we have two variables multiplied by numbers, and then the terms are added together. And just like how we converted the ticket transformation equations into a matrix equation, we can convert the neural networks transformation into a matrix equation, where we put the petal and sepal width into a row matrix and multiply them by a matrix containing the coefficients, or the weights, in a neural network. Now we just plug in the petal and sepal widths, 0.5 and 0.4, and do the math by multiplying the row of input values by the coefficients, or weights, in the first column and then adding the terms, and then multiplying the row of input values by the weights in the second column and then adding the terms. And that gives us a 1 by 2 row matrix of transformed values, -1.0 and -0.6. So this first part of the neural network transforms the input values. The next thing the neural network does is add bias values to each transformed value. So that means we add 1.6 to the first transformed value and we add 0.7 to the second transformed value. And when we do the addition, we end up with a row matrix with 0.6 in the first column and 0.1 in the second column. Now we are ready to use 0.6 as the x axis coordinate for the activation function on top and 0.1 as the x axis coordinate for the activation function on the bottom. Now we run both values through the ReLU activation functions. Note, there's a lot to be said about the ReLU activation function, so if you're interested, check out the Quest. Otherwise, just know that the ReLU activation function outputs zero or the input value, whichever is larger. Thus, because the input for the top ReLU is 0.6, which is greater than 0, the output is 0.6. And because the input for the bottom ReLU is 0.1, which is also greater than 0, the output is 0.1. Now the network has three transformations that it applies to the ReLU output. And doing the row by column multiplication and summation gives us this one-dimensional 1 by 3 row matrix. Then we add bias terms to each transformed value and we end up with the output values for Setosa, Versicolor, and Virginica. Bam! Thus, when the petal with is 0.5 and the sepal width is 0.4, this neural network predicts that the measurements came from Versicolor, since its output value is closest to one. So, now we've seen how this neural network can be written out as a sequence of matrix multiplication and addition. Bip bip bip bip bip bip bip. Now, if we call the row matrix, that contains the input values 'a', and call the first transformation matrix, which contains weights, w1, and the first row matrix of bias terms b1, then we can use 'a' times w1 + b1 as the input for the ReLU activation function. We can then multiply the output of the ReLU by the next transformation matrix, which will call w2, because it also contains weights. And then we add the final bias values, b2, to get the predicted species. And at long last, we've seen how to transform a neural network into a super compact matrix equation. Double bam! Now to review, let's go back to the matrix equations we started with and see if we can make sense of them. This first bit comes from the PyTorch documentation for the nn.linearclass. And we just saw that an equation like this corresponds to a part of a neural network that looks like this. This 'x' corresponds to the input values, and like we saw before, the input values can be put in a one-dimensional row matrix like this. This transpose of 'A' corresponds to the multiplication of the inputs by the weights and the sums of the terms. And all that math, the multiplication by the weights and the summation of the terms, can be accomplished by multiplying the inputs by a transformation matrix containing the weights. Note, the transpose 't' of matrix 'a' just means that nn.linear assumes the weights are in rows. But in order for this math to work out, they need to be in columns. Lastly, this 'b' corresponds to the addition of the bias terms to the result of the matrix multiplication, and that just means that we put the bias terms in a one-dimensional row matrix and add those values to the transformed values. Thus, this simple neural network multiplies and adds these matrices, and that corresponds to this matrix equation. Thus, the nn.linear class builds a very common part of a neural network. And do you remember how early on in this StatQuest we made a big deal about linear versus nonlinear transformations and how a linear transformation means a constant change in the input results in a constant change in the output? Well, this part of a neural network and its corresponding matrix equation is a linear transformation. And now we also know that the nn.linear class gets its name from the fact that it performs a linear transformation. Now, let's talk about this error message. Although there are some technical differences between tensors and matrices, they have a lot in common and we know that a 1D, or one dimensional matrix, is just a single row or a single column, but the function that generated the error was expecting something with at least two dimensions like this matrix. So the error just means we need to think a little bit more about the data we are giving the function. Note, there's a lot more to be said about tensors in the context of neural networks. So if you want to learn more, check out the Quest. Lastly, this is the equation for attention, which is used by transformers, the neural network architecture behind ChatGPT. Attention requires three matrices, 'Q', 'K', and 'V'. For example, given these three matrices, the first thing we do is multiply 'Q' by the transpose of 'K'. And given these example matrices that gives us 11.7. Then we divide by the square root root of d sub k. And for the sake of this example, let's assume d sub k equals 2. So we divide 11.7 by the square root of 2, and that gives us 8.3. Note, if we had started out with matrices that had more rows and columns, we'd end up with a larger matrix at this point. And to be clear, attention is expecting larger matrices, and that is why these variable names are capitalized. However, we're just using row matrices because they keep the example as simple as possible and they work. Anyway, now we take the SoftMax of our little tiny matrix and we get 1.0. Note, this is probably the world's lamest example of the SoftMax function in action, but if you want to learn more about it, check out the Quest. Anyway. The last thing we do is multiply our tiny 1 row by 1 column matrix by 'V'. And that gives us these two values. And those are the attention values for the example matrices. So given these matrices, we might not know what attention means, but at least we can do the math. Bam? Well, if you want to know more about what attention does and means, check out the Quest. Triple Bam! Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StatQuest PDF study guides and my book, the StatQuest Illustrated Guide to Machine Learning at statquest.org. There's something for everyone! Hooray! We've made it to the end of another exciting StatQuest. If you liked this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, consider contributing to my patreon campaign, becoming a channel member, buying one or two of my original songs, or a t-shirt, or a hoodie, or just donate. The links are in the description below. All right, until next time, Quest on."
Qf06XDYXCXI,2023-11-06T13:03:55.000000,Word Embedding in PyTorch + Lightning,[Music] wood edding with po torch and lightning hooray stack Quest hello I'm Josh starmer and welcome to stack Quest today we're going to talk about word embedding in pie torch plus lightning don't stress out about the cloud use lightning bam this stack Quest has also brought to by the letters a b and c a always b b c curious always B curious note this stack Quest assumes you are already familiar with word embedding if not check out the quest also note you can download all of the code in this stack quest for free the details are in the pinned comment below in the stack Quest on word embedding we created a simple neural network that converted the word or input tokens Troll 2 is great and Jim cata into numbers which we call word embeddings we also showed that these word embeddings allow words that are used in similar contexts like Troll 2 and Jim Kata to appear close to each other when we use the embedding values to plot each word on a graph bam now in this stack Quest we'll learn how to build and train this simple word embedding network with P torch plus lightning first we'll do it from scratch using just tensors and some basic math and then we'll simplify our code using the pi torch linear function lastly we'll learn how to use the pytorch embedding function to load and use pre-trained word embeddings the first thing we do is import torch to create the tensors we will use to store the raw data and to provide a few helper functions then we import torch.nn to create the weights we will use in the network and to bring in some other helper functions then we import atom to fit the neural network to the data with back propagation then we import uniform to initialize the weights in the network and to give us the tools to create a large scale embedding network with lots of training data we'll import tensor data set and data loader from torch. ts. data now we import lightning as L to make it way easier to write our code and for automatic code optimization and scaling in the cloud lastly we import pandas matte plot lib and Seaborn so that we can draw some pretty graphs now we will create word embeddings for these two simple sentences Troll 2 is great and Jim Kata is great so we will need to create an input for for each unique token in the training data and these inputs will eventually connect to this word embedding Network now if we wanted to run Troll 2 through the network we would put a one in the input for Troll 2 and a zero in all of the other inputs oh no it's the dreaded terminology alert when we specify the inputs like this where one input gets a one and everything else gets a zero it's called one hot encoding so if we want to run the word is through the network we specify the one hot en coding with a one for is and a zero for everything else likewise this is the one hot en coding for great and this is the one hot en coding for Jim cata now going back to the one hot en coding for trol 2 in pi torch we can create a Four Element list with one in the the first position for Troll 2 and zeros in all of the other positions to recreate the one hot encoding for Troll 2 likewise we can create similar one hot encoding lists for is great and Jim Kata and once we have a list for each possible input we make a list of these lists and because we're using pi torch we convert the input lists into tensors with torch. tensor and save the tensors in a variable called inputs now the goal is to create a simple Network that can predict the token that follows a specific input for example if the input token is Troll 2 then we want to predict the word is and that means we want the output for is to be one and the outputs for all of the other tokens to be zero in pi torch that means we want to predict the one hot encoding for is likewise when the input is is then we want to predict great and thus we want to predict the one hot encoding for great great is at the end of each phrase and nothing comes after it so in theory it shouldn't predict anything but if we had a larger training data set it probably would predict something so in this example will just pretend that it predicts Jim Kata lastly Jim Kata just like Troll 2 predicts is now let's combine the output lists in a list and convert the output lists into tensors and save them in a variable called labels because in machine learning that is what we call the known or ideal output values and now we are done encoding the training data now that we have the inputs and label we can combine them into a tensor data set that we'll call data set and then use data set to create a data loader called Data loader data loaders are super useful when we have a lot of data because one they make it easy to access the data in batches two they make it easy to shuffle the data each Epoch and three they make it easy to use a relatively small fraction of the data if we want to do a quick dirty training for debugging but Josh we don't have a lot of data why are we using a data loader you're right Squatch we don't have a lot of data but in a more realistic setting we would so we might as well do it now okay anyway now that we have the data taken care of let's write the code for this simple word embedding Network when we create a neural network in pi torch we always start by defining a new new class now because we're coding our first word embedding network from scratch we'll call this word embedding from scratch and in order to make coding super easy We'll Inherit from lightning module then just like we always do we create an initialization method for the new class this method will create and initialize all of the weight tensors that we need to implement the embedding Network and it will also create the LW function that we'll use during training then we'll create a method called forward that makes a forward pass through the embedding Network then we'll create a method to configure the atom Optimizer and lastly we'll create a method called training step to calculate the loss which in this case will be the cross entropy loss the cross entropy loss function will quantify the difference between what we want the out output to be and what we actually get for the output note there's a lot more to be said about the cross entropy loss function so if you're curious check out the quest now let's start by coding the anit method the first thing we do is call the initialization method for the parent class lightning module this is simply required whenever we inherit from a class in Python in this case this will allow us to take advantage of all the features that lightning offers now we need to create and initialize the weights for the network and we're going to do this by using a uniform distribution to randomly select an initialization value for each weight specifically we're going to use this uniform distribution that goes from 0.5 to 0.5 to Generate random numbers for the weights the shape of this distribution shows the that all values between 0.5 and 0.5 have the same likelihood of getting randomly selected hey Josh why are we using values between 0.5 and 0.5 to initialize the weights good question Squatch we're using this specific range of values in order to match up with what we will do in the second part of this tutorial when we use the pi torch linear function to do the math and the L linear function selects a range of values based on the number of inputs okay anyway in order to use this uniform distribution to Generate random numbers for us we create a variable called Min value and set it to 0.5 the minimum value we want to randomly select and we create a variable called max value and set it to 0.5 the maximum value we want to randomly select now we create a parameter for the first weight associated with the first input and use uniform. sample which we imported earlier to initialize it with a random number when I did this the first weight associated with Troll 2 was randomly set to 0.38 then the second weight associated with Troll 2 was randomly set to 0.42 now we just create an initialize parameters for all of the other weights associated with each input bam now we've created and initialized all of the weights associated with the inputs likewise we create and initialize parameters for all of the weights associated with each output now we have all of the weights initialized for our word embedding Network bam now the last thing we need to do in our anit method is give our class access access to the Cross entropy loss function and we do this by calling nn. cross entropy loss and saving it in a variable called loss now that we're done coding the anit method we can use those weights to code the forward method to make a forward pass through the embedding Network for the forward method the input is a list that contains the one hot encoding for one of the input tokens for example the input might be the one hot encoding for Troll 2 however when it is passed to the forward method it comes wrapped up in an extra set of brackets so the first thing we do is remove those brackets by setting input to be the first element now we multiply each input value by its corresponding weight that goes to the activation function on top and add the products together and we save that s in a variable called inputs to top hidden then we multiply the inputs by the weights to the activation function on the bottom and add the products together and save the sum in a variable called inputs to bottom hidden so now we have the code for the first part of the word embedding Network and since the activation functions are identity functions which means the input is the same as the output we can multiply inputs to top hidden and inputs to bottom hidden by the next set of Weights directly for example we can multiply inputs to top hidden by the first weight going to the top output and then multiply inputs to bottom hidden by the second weight going to the top output and then add those two products together and save the result in a variable called output one then we do the same same thing for the other outputs bam now we've done all the math up to the softmax function and that actually means we are done making a forward pass through the embedding Network because the loss function that we're using for back propagation nn. crossentropy loss does the soft Max for us so the last thing we need to do is package up the output values using torch. stack and save everything in a variable called output pre- soft Max note if instead of using torch. stack we just returned a list of the output values by wrapping them up in square brackets then the gradients would get stripped off and we would not be able to do back propagation so by using torch. stack we can return a list that preserves the gradients anyway the last thing we do in the forward method is return output pre- soft Max now that we have the forward method we are ready to configure the optimizer and configuring the optimizer in this case atom is so easy we can just replace the pseudo code with the real code we pass atom the parameters we want to optimize and we set the learning rate LR to 0.1 hey Josh why did you set the learning rate to 0.1 because our example is pretty simple and I wanted to train relatively quickly I tested out a relatively large learning rate 0.1 and it worked okay now let's talk about the training step method which we'll use to calculate the loss the training step method takes a batch of training data and the index for that batch and the first thing we do is split the batch of training data into the input and the labels which are the ideal output values then we run the input through the network up to the softmax function by passing it to the forward method for example if we run the one hot encoding for Troll 2 through the untrained Network these are the values that the forward method will return we then run those values along with the ideal values through the loss function nn. cross loss then runs the output values through a soft Max function and quantifies the difference between the soft Max output in the ideal values and we save that difference in a variable called loss and then return the loss now at long last we've made it through all the code needed to create word embeddings from scratch we create and initialize the weight tensors and create the loss function in the init method we make a forward pass through the embedding network with the forward method configure the atom Optimizer with configure uncore optimizers and last but not least calculate the loss with training step bam now let's use the new class we just wrote to create a new word embedding Network that we'll call model from scratch and let's print out the randomly selected weight values that it starts out with here we just have a for Loop that iterates over all the named parameters in the network and for each parameter it prints out its name and value and here's the output now to be honest this list of numbers is kind of hard to read so let's organize it into an easyto read data frame so the first thing we do is put the weight values into a dictionary the first first part label W1 contains the weight values for each input that goes to the activation function on top note we're using the item method to get the weights because it Returns the tensor values as python numbers the second part label W2 contains the weight values for each input that goes to the activation function on the bottom then we just label the tokens and inputs and save the dictionary in the very aable called data and then transform data into a pandis data frame called DF and print out DF and this is what the data frame looks like now we can easily see that the weights for Troll 2 and Jim Kata are relatively different even though they both represent movie titles that are used in the same context this table is pretty helpful for making the embedding values easy to look at but a graph would make them even easier to look at this graph has the weight values to the top activation function W1 on the x axis and the weight values to the bottom activation function W2 on the Y AIS with a graph it's super easy to see that the embedding values for troll two are very different from the values for Jim Kata to create the graph the the first thing we do is call the caborn function scatter plot and we pass scatter plot the data frame DF that we just created and we tell scatter plot that we want to use the weights that go to the top activation function W1 on the X AIS and the weights that go to the bottom activation function W2 on the Y AIS now if all we did was call scatter plot then we'd end up with this scatter plot and while this scatter plot is super cool it would be much cooler if each dot were labeled with the word or token that it represented like this so in order to add the tokens as labels to each point we call the Matt plot lib text function and we pass in the X and Y AIS coordinates for the point in the first row in the data frame and the value for the token then then we specify how we want the text aligned the font size the font color and lastly the font weight then we do the same thing for each row in the data frame and then we call PLT doow and we get this super cool looking scatter plot like we mentioned earlier we can now see that the embedding values for Troll 2 and Jim cata are pretty different and that means we need to train our embedding Network we start training by creating a lightning trainer called trainer and tell it to train for at most 100 Epoch which means we will do back propagation for every weight using the training data at most 100 times now we call the trainers fit method and pass it the embedding Network called model from scratch and the training data called Data loader in theory it should only take a few seconds to train our simple embedding Network and when it's done we recreate the data frame that has the weights or embedding values for each token and we can either stare at the embedding values in the data frame or we can draw a scatter plot of the tokens just like before note because the labels for Troll 2 and Jim cata are overlapping and hard to read I then added little offsets to where Troll 2 and Jim cata were printed and now we can see that after training the embedding Network the embedding values for Troll 2 and Jim cata are very similar which is great since they are used in similar contexts bam now that we have trained our embedding Network we can see what it predicts when we use Troll 2 as the input remember from when we created the training data that we want Troll 2 to predict is so the first thing we need to do is create a soft Max function because we didn't have to explicitly use it in our model note we set dim equal to zero so that we can apply it to rows of output values if we set dim equal to one then we would apply it to Columns of values now we pass Troll 2 as a one hot encoded tensor into model from scratch and we run the output values through the the soft Max and then round the output of the soft Max to two decimal places and finally print out the result and we get the one hot encoded tensor for is which is correct bam likewise we can verify that all of the other inputs to our embedding Network create the correct outputs okay now that we know how to create and train a simple word embedding Network from scratch let's make our lives a little easier by using the P torch linear function to create the same network so let's create a new class called word embedding with linear and again in order to make training super easy will inherit from lightning module then create the anit method that we use to create and initialize the weights and like always we'll call the anit method from the parent class now comes the interesting part instead of calling nn. parameter to create and initialize each weight in the network we only have to make two calls to nn. linear the first call creates the weights between the inputs and the hidden layer in features equals 4 means we are connecting four inputs to two nodes specified with features equals 2 in the hidden layer in other words this call to nn. linear will make four weights for each of the two nodes in the hidden layer and since we don't need any bias terms we set bias equal to false the second call to nn. linear creates the weights between the hidden layer and the outputs it creates two weights within features equal to two for each of the four outputs without features equal to 4 and again since we don't need any bias terms we set bias equal to false now the last thing we need to do in our anit method is give our class access to the Cross entropy loss function now we need to code the forward method that makes a forward pass through the network the cool thing is that all we have to do to calculate the sums before the activation functions is pass the input to the linear object input to Hidden that we created in the anit method and save the sums in a variable called hidden the linear object input to Hidden does all of the multiplication and addition for us bam note now that we are using linear to do the math we no longer have to strip off the extra brackets from input like we did before anyway because the input to these activation functions is the the same as the output we can just ignore them and pass hidden to the second linear object we created hidden to Output hidden to Output calculates the output sums from the activation functions and we save those output values in output values now remember that we don't need to calculate the soft Max because the loss function nn. crossentropy loss does it for us so all we have to do is return the output values bam the next thing we do is create the configure optimizers method and just like before we use the atom Optimizer and pass it the parameters we want to optimize and set the learning rate to 0.1 the training step method is also similar to what we did earlier and that means that this embedding network is contained in this class definition by using inn. linear we significantly reduced the amount of code we need compared to when we did everything from scratch we can see the difference when we shrink the original code for the word embedding from scratch class down so that we can fit it on a single screen and compare it to the size of the word embedding with linear class on the same hardto read scale bam now we can create a new model model linear with our new class word embedding with linear and just like before put the pre-trained word embedding values into a data frame called DF note because we used nn. linear to create the weights we access them with weight and we call detach to remove the gradient from the tensors and we use zero and one to index the weights that go to the top and bottom activation functions and lastly convert the tensor to a num Pi array with num Pi now when we print out our data frame DF we get this nicely formatted table and we can draw a scatter plot of the tokens just like before and that gives us a scatter plot that looks like this and the graph suggests that the embedding values or weights are not yet optimal because Troll 2 and Jim are so far from each other so just like before we can train the model for 100 Epoch and after training we end up with these weights and the weights going from the inputs to the hidden layer are the new embedding values and when we redraw the scatter plot with the new embedding values we see that the values for Troll 2 and Jim cata are similar double bam now that we we know how to create word embedding networks from scratch and within in. linear and we can create word embeddings that put words and tokens used in similar context near each other let's learn how we can load and use pre-trained word embedding values with nn. edding Note in this example we're just going to load and access the embedding values we created but in practice we might want to load and use the word Tove embedding values which have 100 values per token and millions of tokens or the embedding values created by a Transformer like chat GPT however before we get started let's just print out the embedding values from the last Model we trained model linear we access the embedding values in model linear just like we did when we created the data frame except now we don't have to worry about the gradient an and we get two lists of embedding values specifically the weights are arranged in two rows the first row corresponds to the weights that go to the top activation function and the second row corresponds to the weights that go to the bottom activation function now the problem with this is that nn. embedding expects the weights to be in columns just like in the data frames we created the good news is that converting rows into columns is super easy so with that said let's load and use these pre-trained word embedding values with nn. edding we start by creating an nn. embedding object and we pass the pre-trained weights in with from pre-trained and we use T to transpose the rows of Weights into columns lastly we save the the new embedding object in word embeddings we can then verify that we did things correctly by printing out the weights and we see that the weights are now arranged in two columns now we can print out the embedding values for the first input Troll 2 by passing in a tensor with the first index value zero and the embedding values match what we expect so we know we did things correctly accessing the embedding values by index is fine but we can also make our lives Easier by creating a dictionary that maps The Tokens to their indices and now we can more easily access the embeddings with the token itself rather than the index and that's all there is to loading and accessing pre-trained weights into an nn. embedding object we can now use our embedding object word embeddings and connect it to a larger neural network like a Transformer also before we go I just want to remind you that you don't have to type this code yourself instead you can download it and I wrote tons of comments that explain every little detail just like this stack Quest the link is an app pinned comment below triple bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stack Quest PDF study guides and my book the stat Quest Illustrated guide to machine learning at stat quest.org there's something for everyone hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
rC9vw2dSpQo,2023-10-07T00:30:48.000000,"The Golden Play Button, Clearly Explained!!!",hello I'm Josh starmer and welcome to the opening of this box let's get right down to it today we're going to talk about what's in this box and it's going to be clearly explained what's in this box it's well sealed that's nice to know what do they do on the bottom oh there we go open holy smokes we have a card that [Music] says congratulations on your subscriber Milestone we're honored to take part to recognize your thing and here's a letter from Neil Mohan the CEO of YouTube who I'm sure wrote this personally and it says congratulations One mission one channel and one more thing 1 million subscribers that's pretty cool thanks Neil and here we have holy smokes this is kind of bigger than I was expecting the golden play button and it says present it to stat quest with Josh ster for passing 1 million subscribers I can you see that zoom in on that well ho hey this is a triple bam I want to thank um everyone who obviously who subscribed all 1 million of you uh thank you for subscribing to my channel that's a bam uh everyone who has watched my videos and then shared my videos with friends and colleagues uh I want to thank you guys as well um that's a double bam and I also want to thank everyone who supported me through the years this is this is not something that happened overnight uh so if you've been supporting me through patreon or uh Channel membership or or just whatever um including lightning AI I want to I want a big shout out to them because without them none of this would be possible so thank you very much triple Bam um this is the golden play button for 1 million subscribers for stat quest with Josh charmer thank you and hooray we've made it to the end of another exciting stack Quest um and that's all I have to say Okay bye
Ka04Dj7DxGk,2023-09-04T04:00:16.000000,Another 3 lessons from my Pop!!!,[Music] another three lessons from my pop stat Quest hello I'm Josh starmer and welcome to stat Quest today we're going to talk about another three lessons for my pop and they're going to be clearly explained start small then scale then deploy with lightning this stack Quest is also brought to you by by the letters a b and c a always b b c curious always be curious if you don't already know September 4th is global Frank starmer day and to celebrate we're going to talk about three lessons my pop taught my older sister Rachel and my older brothers Mike and Jack number one to solve problems draw a picture dad was the go-to person for word problems and you'd think I'd be really good at that kind of thing because you know I like to read but um anyway you know the trains are going in different directions and we need to know when they meet and I'd go to dad and say I have no idea what I'm supposed to do here and he'd say have you drawn a picture I'd say no and he go go draw a picture and come back we'll talk about it so I'd go and I'd draw a picture and then take it back to him and we'd look at it he'd tell me about we'd talk about whether it kind of fit the story that was in the word problem you know was it actually depicting what was in the world Pro word problem or not and how to fix it and by the time we were done talking about the picture and modifying the picture then I knew how to solve it and so then I would go off and solve it and that method of thinking about things has proven to be just the sort of the My go-to Method now for just about everything can I draw a picture and if I can't draw a picture it means I don't have enough information um I'm not able to really see what it is the thing that I need to see and I need to go find out more so I use that all the time I use it when I do my counseling work I use it with my own self in life um and with all sorts of things and so I am so glad that dad taught me how to solve word problems by drawing a picture thanks bam number two to remember draw a picture uh I had a really difficult time learning and the issue that I had is I just forget stuff so I think today they call it like add but my dad dad was say hey Mike can you go get me a glass of ice tea and I say sure and I'd walk in the house and just forget what I was supposed to do um and then he picked up on this and teacher started telling him that I had bigger issues and so he said Mike I'm going to teach you how to think in pictures and so what he did is he he said Mike can you get me a glass of tea and I said sure I can I can do that well draw a picture tell me the picture so I said all right and he says tell me the picture and I said well I'm walking in the sudden glass door I open this glass door I go into the counter the kitchen I get a glass I get some ice and iced tea I pour it in I come back and I give it to you and he said okay go do it and I did and that went on for you know a few months minimum but he'd always talk about Mike what's the picture you see in your head what do you what when I said this what do you see and so that's then now so now all of a sudden you know fast forward you know 40 50 years and what do I have is my head only think in pictures visually that's all I see when I'm speaking to you or I'm talking to people in a business meeting I'm thinking about the story that they're telling me I'm visually talking so I'm talking to somebody they're not just words in my head they're just actually a real story and I'll tell you this I am who I am today because pop taught me how to think in pictures double bam number three to solve a problem start I remember how helping dad fix things around the house I learned one of the best strategies for fixing things is to understand how they work and then methodically troubleshoot based on this knowledge and the symptoms but sometimes we didn't know how something worked or where the problem was so we picked a course of action and began taking things apart like the time we fixed the dishwasher it was a lovely shade of olive green this dishwasher I was chasing my sister around the house and she accidentally sat on the dishwasher door and benett oh no I got in big big trouble and was panicked to try and fix it Dad and I didn't know how we just knew it needed to be fixed so I watched as dad started taking it apart we studied the door mechanism and found some parts we could straighten out and we fixed the door I took this strategy to heart and started taking most anything apart around the house to see how things would work even things that didn't need fixing my mom told me she freaked out one time when she walked into the living room and found me in the middle of a pile of vacuum cleaner pieces understanding how things work is immensely helpful in problem solving but there is a more fundamental rule I learned from Dad years later I was sharing a story with him about a team I was asked to join at work to solve a complex problem they had been working on the problem for a few weeks but hadn't made any progress I told them we have to pick something and dig in my dad says the first rule of problem solving is start triple bam now last but not least happy birthday Pop now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stat Quest PDF study guides and my book the stat Quest Illustrated guide to machine learning at stack quest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
bQ5BoolX9Ag,2023-08-28T04:00:30.000000,"Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!",decoding is all that you need stat Quest hello I'm Josh starmer and welcome to stack Quest today we're going to talk about decoder only Transformers and they're going to be clearly explained trust me whatever Transformer you want to use it's better with lightning bam right now people are going going totally bananas about chat GPT for example stat Squatch might type something into chat GPT like what is stat Quest stat Quest is awesome bam now in order to learn more about how chat GPT Works let's learn about decoder only Transformers which is the specific type of Transformer used for chat GPT note if you're not already familiar with basic Transformers don't worry we'll go through each Concept in a decoder only Transformer one step at a time that said if you are familiar with basic Transformers then you might want to just skip to the last chapter normal Transformers versus decoder only Transformers anyway in this stack Quest we're going to show how a decoder only Transformer can take a simple input prompt what is stat Quest and generate a simple response awesome now since a Transformer is a type of neural network and neural networks usually only have numbers for input values the first thing we need to do is find a way to turn the input and output words into numbers there are a lot of ways to convert words into numbers but for neural networks one of the most commonly used methods is called word embedding the main idea of word embedding is to use a relatively simple neural network that has one input for every word and symbol in the vocabulary that we want to use in this case we have a super simple vocabulary that allows us to input a short phrase like what is stat Quest and we also have an input for a potential response awesome lastly we have an input for the EO symbol which stands for end of sentence or end of sequence because the vocabulary can be a mix of words word fragments and symbols we call each input a token the inputs are then connected to something called an activation function and in this example we have two activation functions and each connection multiplies the input value by something called a weight hey Josh where do these numbers come from great question Squatch and we'll answer it in just a bit for now let's just see how we convert the word what into to numbers first we put a one into the input for what and then Zer into all of the other inputs now we multiply the inputs by the weights on the connections to the activation functions for example the input for what is 1 so we multiply -2.3 by 1 to get - 2.38 going to the activation function on the left and we multiply 0.10 by 1 to get 0 .10 going to the activation function on the right in contrast if the input value for the word is is 0 then we multiply 0.61 by 0 to get 0 going to the activation function on the left and we multiply 0.17 by 0 to get 0 going to the activation function on the right in other words when an input value is zero then it only sends zeros to the activation functions and that means is stat Quest awesome and the EOS symbol all just send zeros to the activation functions and only the weight values for what end up at the activation functions because its input value is one so in this case - 2.38 goes to the activation function on the left and 0.10 goes to the activation function on the right in this example the activation functions themselves are just identity functions meaning the output values are the same as the input values in other words if the input value or xaxis coordinate for the activation function on the left is -2.3 then the output value the Y AIS coordinate will also be - 2.38 likewise because the input to the activation function on the right is 0.10 the output is also 0.10 thus these output values - 2.38 and 0.10 are the numbers that represent the word what now before we move on I want to mention that all of these weights and all of the other weights we're going to talk about in this Quest are determined using something called back propagation to get a sense of what back propagation does let's imagine we had this data and we wanted to fit a line to it back propagation would start with a line that has a random value for the Y AIS intercept and a random value for the slope and then using an iterative process back propagation would change the Y AIS intercept and slope one step at a time until it found the optimal values likewise in the context of neural networks each weight starts out as a random number but when we train the Transformer by getting it to predict words in known documents back propagation opt optimizes these values one step at a time and results in these final weights also just to be clear the process of optimizing the weights is also called training bam note there's a lot more to be said about training and back propagation so if you're interested check out the quests anyway we now know how to calculate the word embedding values for the first word in our phrase what so we'll keep track of those values with this diagram in the upper right hand corner corner now we reuse the exact same word embedding Network to convert the remaining words in the prompt what is stat Quest into numbers thus the word embedding values for is are 0.61 and 0.17 and the word embedding values for stat Quest are - 2.38 and 0.10 and now we have converted all of the words in the prompt what is stack Quest into word embedding values note reusing the exact same word embedding Network for each word in the prompt allows the decoder only Transformer to handle prompts that have different lengths because we can just copy the network as many times as we need to convert all the words in the prompt into numbers also note there's a lot more to say about word embedding so if you're interested check out the quest anyway now that we know how to convert words into numbers let's talk about word order for example if Norm said Squatch eats pizza then Squatch might say yum in contrast if Norm said Pizza eats Squatch then Squatch might say yikes so these two phrases Squatch eats pizza and pizza eats Squatch use the exact same words but have very different meanings so keeping track of word order is super important so let's talk about positional encoding which is a technique that Transformers use to keep track of word order although there are several ways to keep track of word order with positional encoding one of the most commonly used methods uses a sequence of alternating s and cosine squiggles each squiggle gives us specific position values for each words embeddings so now let's see how we can add positional en coding values to the word embeddings we created for the prompt what is stat Quest the first word which in this case is what has an xais coordinate all the way to the left on the green squiggle and the position value for its first embedding is the Y AIS coordinate zero the position value for the second embedding comes from the orange squiggle and the y- AIS coordinate on the orange squiggle that corresponds to the first word is one now to get the position ition values for the second word is we simply use the Y AIS coordinates on the squiggles that correspond to the x-axis coordinate for the second word likewise we use the corresponding y AIS coordinates for the third word stat Quest note because the S and cosine squiggles are repetitive it's possible that two words might get the same position or Y AIS values however because the squiggles get wider for larger embedding position and the more embedding values we have then the wider the squiggles get then even with a repeat value here and there we end up with a unique sequence of position values for each word anyway now we just do the math to get the positional encoding for all three input words and we end up with the word embeddings plus positional encoding for the prompt what is statquest now because we're going to need all the space we can get let's consolidate the math in the diagrams and let the S and cosine and plus symbols represent the positional encoding now that we know how to keep track of each word's position let's talk about how a decoder only Transformer keeps track of the relationships among words for example if the prompt was the pizza came out of the oven and it tasted good then this word it could refer to pizza or or potentially it could refer to the word oven Josh I've heard of good tasting pizza but never a good tasting oven I know Squatch that's why it's super important that the decoder only Transformer correctly Associates the word it with pizza the good news is that decoder only Transformers have something called masked self- attention which can help correctly associate the word it with the word Pizza in general terms masks self attention works by seeing how similar each word is to itself and all of the preceding words in the sentence for example masked self attention starts by calculating the similarity between the first word the and itself then masked self- attention calculates the similarity between pizza and itself and the preceding wordthe and then masked self attention just keeps calculating similarities like this allowing each word to look at itself and the words that came before it but not after until it gets to the end of the input once the similarities are calculated they are used to determine how the decoder only Transformer encodes each word for example if you looked at a lot of sentences about pizza and the word it was more commonly associated with pizza than oven then the similarity score for pizza will cause it to have a larger impact on how the word it is encoded by the decoder only Transformer oh no it's the dreaded terminology alert because masked self attention only allows access to the words that come before it and not the words that come after it is sometimes called an auto regressive method anyway now that we know the main ideas of how masked self attention Works let's look at the details so let's go back to our simple example where we had just added positional encoding to The Prompt what is stat Quest now since what is the first word in the prompt its masked self attention values will only reflect its similarity to itself and ignore everything else in contrast the masked self attention values for the second word is reflect the similarity to itself as well as a similarity with the first word what and the last word stat Quest takes into account its similarity with itself and everything that came before it now I know that what is the first word in the input but it will make it easier to understand how masked self attention works if we start with the second word is so let's move the position encoded is over a little bit to give us some room the first thing we do to calculate the masked self attention for the word is is multiply its position encoded values by a pair of weights and then add those products together to get Negative 2.4 then we do the same thing with a different pair of weights to get 2.6 we do this twice because we started out with two position encoded values that represent the word is and after doing the math two times we still have two values representing the word is Josh I don't get it if we want two values to represent is why don't we just use the two values we started with that's a great question Squatch and we'll answer it in a little bit G anyway for now just know that we have two new values to represent the word is and in Transformer terminology we call them query numbers and now we're going to use the query numbers for is to calculate the similarities with itself and the first word what and we do that by creating two new numbers like we did before to represent the word is and creating two new numbers to represent the word what in Transformer terminology both sets of new numbers are called key values and we use them to calculate similarities with the query for is one way to calculate similarities between the query and the keys is to calculate something called a dotproduct for example in order to calculate the dot product similarity between the query and key for is we simply multiply each pair of numbers together and add the products to get 5.9 likewise we can calculate the dot product similarity between the query for is and the key for what by multiplying the pairs of numbers together and adding the products to get - 25.7 the small similarity value for what relative to is - 25.7 compared to the large similarity value for is relative to itself 5.9 tells us that is is much more similar to itself than it is to the word what that said if you remember the example where the word it could relate to pizza or oven then the word it should have a relatively large similarity value with respect to the word Pizza since it refers to pizza and not oven note there's a lot to be said about calculating similarities in this context and the dot product so if you're interested check out the quests anyway since is is much more similar to itself than it is to the word what then we want is to have more influence on its encoding than the word what and we do this by first running the similarity scores through something called a softmax function the main idea of a softmax function is that it preserves the order of the input values from low to high and translates them into numbers between 0o and one that add up to one so we can think of the output of the soft s Max function as a way to determine what percentage of each input word we should use to encode the word is in this case because is is so much more similar to itself than the word what we'll use 100% of the word is to encode is and 0% of the word what to encode the word is note there's a lot more to be said about the soft Max function so if you're interested check out the quest anyway because we want 100% of the word is to encode is and 0% of the word what to encode is we create two more numbers that will cleverly call values to represent the word what and scale them by 0.0 then we create two value numbers to represent the word is and scale them by 1.0 lastly we add the scaled values together and these sums which combine separate encodings for both input words what and is relative to their similarity to is are the masked self- attention values for is Bam now that we have the masked self attention values for is we can go back and calculate them for the first word what remember because what is the first word it only needs to know how similar it is to itself so we can get rid of most of what we did for the word is but keep the key and value numbers that we calculated for the word what now so we have a little more room to work let's move everything over a bit and just like we did before we create a query for the word what now we use the query and key for what to calculate the similarity with itself and we get -12.4 so we plug 12.4 into the soft Max function and we get 1.0 last ly we scale the value numbers for what by 1.0 and -2.9 and -1.3 are the mask self attention values for the first word what it might seem a little silly to do all this math just to end up using the value numbers for what as the masked self attention numbers for what however doing it this way gives us a unified method for calculating masked self attention so so far we've calculated the masked self attention values for what which only required taking its own value numbers into account and we calculated the mass self attention for is which required taking the value numbers from what and is into account now we need to calculate the masked self attention for the third word stat Quest and that means we need to take the value numbers for stat Quest into account which we calculate like before and we need to take the value numbers for For What and is into account so the first thing we do is calculate the query numbers for the word stat Quest and then calculate its key numbers and then we bring back the keys for what and is and calculate the similarities between the query for stat Quest and the keys for stat Quest is and what now we run all three similarities into the soft Max function and the output from the soft Max tells us what percentage of each word's value numbers to use when calculating the massed self attention for stat Quest now we bring back the value numbers that we calculated earlier for what is and Stat Quest and scale them based on the similarity scores lastly we add the pairs of scaled values together to get the masked self attention numbers for the word stat Quest bam note before we move on I want to point out that we read use one set of weights to create query numbers for each word in other words the set of Weights we use to create the query numbers for what is the same set of Weights we use for the query numbers for is in stat Quest likewise the key numbers are calculated with a different set of Weights that are shared for each word and the value numbers are also calculated with another set of Weights that are reused for each word reusing the sets of weights for the the query key and value numbers lets the decoder only Transformer handle prompts that have different lengths because we can just keep reusing the weights as many times as we need now that we understand the details of how masked self attention Works let's shrink the diagram so that we can keep building our decoder only Transformer bam Josh you forgot something if we want two values to represent what why don't we just use the two positioning encoded values we started with first the new masked self attention values for each word contain input from all of the other words that came earlier and this helps give each word context and this can help establish how each word in the input prompt is related to the others also if we can think of this unit with its three sets of weights for calculating queries keys and values as a masked self attention cell then in order to correctly establish how words are related in complicated sentences and paragraphs we can create a stack of masked self attention cells each with its own sets of Weights that we apply to the position encoded values for each word to capture different relationships among the words in the manuscript that first described the original GPT they stacked 12 masked self attention cells bam okay going back to our simple example with only one masked self self attention cell there are a few more things we need to do before we start generating a response to the prompt what is stat Quest First We Take the position encoded values and add them to the self attention values these bypasses are called residual connections and they make it easier to train complex neural networks by allowing the masked self- attention layer to establish relationships among the input words without having to also preserve the word embed and position encoding information bam lastly we need a way to use the encodings we have for each word in the prompt to generate the word that follows it and then generate a response in other words we want to use these two numbers that represent the word what to generate the word that comes after what and we want these two numbers that represent the word is to generate the word that comes after is lastly we want these two numbers that represent the word stat quest to generate the word that comes after stat Quest hey Josh I don't get it why do we want to generate the word that comes after what when we already know the next word is because this is a decoder only Transformer we need one thing that can both encode The Prompt and generate the output thus even though we are not yet generating a response we need to include the parts that will do it also we can compare the known input to what the model generates when we train the model so now let's see what the model generates given the first word in the prompt what and that means taking these two values - 5.28 and 0.2 and plugging them into something called a fully connected layer this fully connected layer has one input for each value that represents the current token so in this case we have two in inputs and one output for each token in the vocabulary which in this case means five outputs note a fully connected layer is just a simple neural network with weights numbers we multiply the inputs by and biases numbers we add to the sums of the products also note in the original GPT manuscript instead of a new fully connected layer they use the word embedding Network that we started with but in Reverse in other words they just reuse the same set of Weights that we use to encode the words into numbers and then flip them to help decode the numbers however not all decoder only Transformers do it this way and a very common alternative is a fully connected layer small bam now when we do the math we get five output values which we run through a final soft Max function to generate the next word is and since the prompt was what is stat Quest the model generated the correct word bam the second word in the prompt is is and it generates the word what which is not correct w w however the correct word stat Quest was almost correctly generated so we hope our model doesn't feel too much shame note if we were training the decoder only Transformer then we would use the fact that we made a mistake to modify the weights and biases in contrast when we are just using the model to generate responses then it really doesn't matter what words come out right now so in this case we'll just note that we made a mistake and move on lastly the word stat Quest generates the EOS token which is correct since we are at the end of The Prompt bam now let's review what we've done so far we started with an input prompt what is stat Quest and we used word embedding to convert each word into numbers and we used positional encoding to keep track of word order in the prompt then we added masked self attention to determine the relationships among the words in the prompt then we added residual connections to make it easier to train the model lastly we added a fully connected layer and a soft Max to generate the next word each part of the decoder only Transformer is reused so that it can handle prompts of different lengths and the encoding for each word in the prompt can happen at the same time rather than sequentially and thus can be done quickly when multiple Computing cores are available lastly generating the output uses the exact same steps that we use to encode The Prompt thus generating the output starts with word embedding and it uses the exact same word EMB set network that we use to encode The Prompt and because we just finished encoding The Prompt we start generating the output from the EOS token in this case we're using the EOS token to start generating the output because that is a common way to initialize this process however sometimes you'll see people use SOS for start of sentence or start of sequence to initialize the process Josh starting with SOS makes more sense to me then you can do it that way Squatch I'm just saying that a lot of people start with EOS also note we will start with the EOS token regardless of whether or not it was the last token generated when we encoded The Prompt anyway we plug in one for Eos and zero for everything else and we end up with the numbers that represent the EOS token now let's shrink the word embedding down to make more space so that we can add positional encoding note note the EOS token comes after the three tokens that represent the input so it's in the fourth position and since the EOS token is in the fourth position with two embeddings we just add those two position values and before we move on we need to say a few more words about masked self attention so far we've talked about how masked self- attention helps the decoder only Transformer keep track of how words are related within the input however it's also important to keep track of the relationships between the input sentence and the output for example if the input sentence was don't eat the delicious looking and smelling pizza then when generating new output it is super important to keep track of the very first word don't if we focus on other parts of the sentence and omit the don't then we'll end up with eat the delicious looking and smelling pizza and these two sentences have completely opposite meanings so it's super important that when generating the output we keep track of the significant words in the input the nice thing is that all we have to do to add this ability to our decoder only Transformer is just include the prompt when we do masked self attention while generating the output so the first thing we do is calculate the query numbers for the EOS token and then we calculate the key numbers using the same sets of Weights we used for the prompt and then we bring back the key values for the prompt what is stat Quest then we calculate the similarities between the query and the keys and run everything through the softmax function now we calculate the value numbers for the EOS token and bring back all of the value numbers we calculated earlier and scale all of them based on the similarity scores lastly we add the pairs of scaled values together to get the masked self attention values for the EOS token bam anyway now that we have the masked self attention values for the EOS token we add the residual connections Now we move the diagram to the left and run the numbers that represent the EOS token through the same fully connected layer we used earlier and the same soft Max function we used before and the first word generated by our decoder only Transformer is awesome which is totally awesome however even though the output is awesome we're not done yet because the decoder only Transformer will keep generating output until it generates the EOS token so we plug the word we just generated awesome into another copy of the word embedding layer then we add the positional encoding using the values for the fifth position because now the sequence is what what is stat Quest EOS awesome now we calculate the masked self attention [Music] values bam now that we have the masked self attention values for awesome we add the residual connections Now we move the diagram to the left and run the numbers that represent awesome through the same fully connected layer we used earlier and the same softmax function we used before and our decoder only Transformer generates the EOS token which means we are done generating output double bam now let's talk about the differences between the decoder only Transformer that we just learned about and a basic Transformer as we just saw a decoder only Transformer uses the exact same components to encode The Prompt that it uses to generate the output and it uses masked self attention which is calculated with only the current word and everything that preceded it and the masked self attention is applied equally to the input prompt and to the output that is generated masked self attention allows a decoder only Transformer to determine how words in the prompt are related and make sure that it keeps track of important input words when generating the output in contrast a regular Transformer uses one type of unit called the encoder to encode The Prompt and a different type of unit called the decoder to generate the output when encoding the input prompt instead of using masked self attention a normal Transformer uses self attention which includes all of the words in the input not just the ones that came before to determine how the words are related to each other and a normal Transformer uses encoder decoder attention to let the decoder keep track of important words in the input encoder decoder attention uses queries from the decoder but only keys and values from the encoder now so far we've only talked about how attension is used in a normal Transformer during inference when it encodes the input and generates new output however during training a normal Transformer uses masked self attention in the decoder for example during training we know the output should be awesome EOS which means we don't have to decode the initial EOS before we decode awesome like we do when we are generating new output instead since we know we will be decoding awesome we can do its math at the same time we do the math for the EOS token doing the math at the same time means we can train faster and thus during training a normal Transformer will use masked self attention on the tokens in the known output this allows the Transformer to learn how to generate the correct output without cheating and looking ahead note when we are training a normal Transformer the masked self attention only includes the output tokens in contrast a decoder only Transformer uses massed self attention all of the time not just during training and it includes the input and the output so the three big differences between a normal Transformer and a decoder only Transformer are a normal Transformer uses one unit to encode the input called the encoder and a separate unit to generate the output called the decoder and a normal Transformer uses two types of attention during inference self attention and encoder decoder attention lastly during training a normal Transformer uses masked self attention but only on the output in contrast a decoder only Transformer has a single unit for both encoding the input and generating the output and a decoder only Transformer uses a single type of attention masked self attention and a decoder only Transformer uses masked self attention all the time on everything the input and the output triple bam note there's a lot more to say about normal encoder decoder Transformers so if you're interested check out the quest now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stat Quest PDF study guides in my book the stat Quest Illustrated guide to machine learning at stat quest.org there's something for everyone hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more please subscribe and if you want to support stat Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
zxQyTK8quyY,2023-07-24T04:00:34.000000,"Transformer Neural Networks, ChatGPT's foundation, Clearly Explained!!!",[Music] translation it's done with a transform ER stat Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about Transformer neural networks and they're going to be clearly explained Transformers are more fun when you build them in the cloud with lightning bam right now people are going bonkers about something called chat GPT for example our friend statsquatch might type something into chat GPT like right and awesome song in the style of statquest translation it's done with a transform ER anyway there's a lot to be said about how chat GPT works but fundamentally it is based on something called a Transformer so in this stat Quest we're going to show you how a Transformer works one step at a time specifically we're going to focus on how a Transformer neural network can translate a simple English sentence let's go into Spanish vamos now since a Transformer is a type of neural network and neural networks usually only have numbers for input values the first thing we need to do is find a way to turn the input and output words into numbers there are a lot of ways to convert words into numbers but for neural networks one of the most commonly used methods is called word embedding the main idea of word embedding is to use a relatively simple neural network that has one input for every word and symbol in the vocabulary that you want to use in this case we have a super simple vocabulary that allows us to input short phrases like let's go and to go and we have an input for this symbol EOS which stands for end of sentence or end of sequence because the vocabulary can be a mix of words word fragments and symbols we call each input a token the inputs are then connected to something called an activation function and in this example we have two activation functions and each connection multiplies the input value by something called a weight hey Josh where do these numbers come from great question Squatch and we'll answer it in just a bit for now let's just see how we convert the word let's into numbers first we put a 1 into the input for let's and then put zeros into all of the other inputs now we multiply the inputs by their weights on the connections to the activation functions for example the input for let's is one so we multiply 1.87 by 1 to get 1.87 going to the activation function on the left and we multiply 0.09 by 1 to get 0.09 going to the activation function on the right in contrast if the input value for the word 2 is 0 then we multiply negative 1.45 by 0 to get 0 going to the activation function on the left and we multiply 1.50 by 0 to get 0 going to the activation function on the right in other words when an input value is 0 then it only sends zeros to the activation functions and that means to go and the EOS symbol all just send zeros to the activation functions and only the weight values for let's end up at the activation functions because its input value is 1. so in this case 1.87 goes to the activation function on the left and 0.09 goes to the activation function on the right in this example the activation functions themselves are just identity functions meaning the output values are the same as the input values in other words if the input value or x-axis coordinate for the activation function on the left is 1.87 then the output value the y-axis coordinate will also be 1.87 likewise because the input to the activation function on the right is 0.09 the output is also 0.09 thus these output values 1.87 and 0.09 are the numbers that represent the word leads bam likewise if we want to convert the word go into numbers we set the input value for go to 1. and all of the other inputs to zero and we end up with negative 0.78 and 0.27 as the numbers that represent the word go and that is how we use word embedding to convert our input phrase let's go into numbers bam note there is a lot more to say about word embedding so if you're interested check out the quest also note before we move on I want to point out two things first we reuse the same word embedding Network for each input word or symbol in other words the weights in the network for let's are the exact same as the weights in the network for go this means that regardless of how long the input sentence is we just copy and use the exact same word embedding Network for each word or symbol and this gives us flexibility to handle input sentences with different lengths the second thing I want to mention is that all of these weights and all of the other weights we're going to talk about in this Quest are determined using something called back propagation to get a sense of what back propagation does let's imagine we had this data and we wanted to fit a line to it back propagation would start with a line that has a random value for the y-axis intercept and a random value for the slope and then using an iterative process back propagation would change the y-axis intercept and slope one step at a time until it found the optimal values likewise in the context of neural networks each weight starts out as a random number but when we train the Transformer with English phrases and known Spanish translations back propagation optimizes these values one step at a time and results in these final weights also just to be clear the process of optimizing the weights is also called training bam note there is a lot more to be said about training and back propagation so if you're interested check out the quests now because the word embedding networks are taking up the whole screen let's shrink them down and put them in the corner okay and now that we know how to convert words into numbers let's talk about word order for example if Norm said Squatch eats pizza then squash might say yum in contrast if Norm said Pizza eats squash then squash might say yikes so these two phrases Squatch eats Pizza and pizza eats squash use the exact same words but have very different meanings so keeping track of word order is super important so let's talk about positional encoding which is a technique that Transformers use to keep track of word order we'll start by showing how to add positional encoding to the first phrase Squatch eats Pizza note there are a bunch of ways to do positional encoding but we're just going to talk about one popular method that said the first thing we do is convert the words squash eats pizza into numbers using word embedding in this example we've got a new vocabulary and we're creating four word embedding values per word however in practice people often create hundreds or even thousands of embedding values per word now we add a set of numbers that correspond to word order to the embedding values for each word hey Josh where do the numbers that correspond to word order come from in this case the numbers that represent the word order come from a sequence of alternating sine and cosine squiggles each squiggle gives a specific position values for each word's embeddings for example the y-axis values on the green squiggle give us position encoding values for the first embeddings for each word specifically for the first word which has an x-axis coordinate all the way to the left of the green squiggle the position value for the first embedding is the y-axis coordinate zero the position value for the second embedding comes from the orange squiggle and the y-axis coordinate on the orange squiggle that corresponds to the first word is one likewise the blue squiggle which is more spread out than the first two squiggles gives us the position value for the third embedding value which for the first word is zero lastly the red squiggle gives us the position value for the fourth embedding which for the first word is one thus the position values for the first word come from the corresponding y-axis coordinates on the squiggles now to get the position values for the second word we simply use the y-axis coordinates on the squiggles that correspond to the x-axis coordinate for the second word lastly to get the position values for the third word we use the y-axis coordinates on the squiggles that correspond to the x-axis coordinate for the third word note because the sine and cosine squiggles are repetitive it's possible that two words might get the same position or y-axis values for example the second and third words both got negative 0.9 for the first position value however because the squiggles get wider for larger embedding positions and the more embedding values we have then the wider the squiggles get then even with a repeat value here and there we end up with a unique sequence of position values for each word thus each input word ends up with a unique sequence of position values now all we have to do is add the position values to the embedding values and we end up with the word embeddings plus positional encoding for the whole sentence Squatch eats Pizza yum note if we reverse the order of the input words to be Pizza eats squash then the embeddings for the first and third words get swapped but the positional values for the first second and third word stay the same and when we add the positional values to the embeddings we end up with new positional encoding for the first and third words and the second word since it didn't move stays the same thus positional encoding allows a Transformer to keep track of word order bam now let's go back to our simple example where we are just trying to translate the English sentence let's go and add position values to the word embeddings the first embedding for the first word Let's gets zero and the second embedding gets one and the first embedding for the second word go gets a negative 0.9 and the second embedding gets 0.4 and now we just do the math to get the positional encoding for both words bam now because we're going to need all the space we can get let's consolidate the math in the diagram and let the sine and cosine and plus symbols represent the positional encoding now that we know how to keep track of each word's position let's talk about how a Transformer keeps track of the relationships among words for example if the input sentence was this the pizza came out of the oven and it tasted good then this word it could refer to pizza or potentially it could refer to the word oven Josh I've heard of good tasting pizza but never a good tasting oven I know Squatch that's why it's important that the Transformer correctly Associates the word it with pizza the good news is that Transformers have something called self-attention which is a mechanism to correctly associate the word ID with the word Pizza in general terms self-attention works by seeing how similar each word is to all of the words in the sentence including itself for example self-attention calculates the similarity between the first word the and all of the words in the sentence including itself and self-attention calculates these similarities for every word in the sentence once the similarities are calculated they are used to determine how the Transformer encodes each word for example if you looked at a lot of sentences about pizza and the word ID was more commonly associated with pizza than oven then the similarity score for pizza will cause it to have a larger impact on how the word ID is encoded by the Transformer bam and now that we know the main ideas of how self-attention Works let's look at the details so let's go back to our simple example where we had just added positional encoding to the words let's and go the first thing we do is multiply the position encoded values for the word let's by a pair of weights and we add those products together to get Negative 1.0 then we do the same thing with a different pair of weights to get 3.7 we do this twice because we started out with two position encoded values that represent the word leads and after doing the math two times we still have two values representing the word leads Josh I don't get it if we want two values to represent let's why don't we just use the two values we started with that's a great question Squatch and we'll answer it in a little bit grr anyway for now just know that we have these two new values to represent the word let's and in Transformer terminology we call them query values and now that we have query values for the word let's use them to calculate the similarity between itself and the word go and we do this by creating two new values just like we did for the query to represent the word let's and we create two new values to represent the word go both sets of new values are called key values and we use them to calculate similarities with the query for let's one way to calculate similarities between the query and the keys is to calculate something called a DOT product for example in order to calculate the dot product similarity between the query and key for let's we simply multiply each pair of numbers together and then add the products to get 11.7 likewise we can calculate the dot product similarity between the query for let's and the key for go by multiplying the pairs of numbers together and adding the products to get negative 2.6 the relatively large similarity value for let's relative to itself 11.7 compared to the relatively small value for lets relative to the word go negative 2.6 tells us that let's is much more similar to itself than it is to the word go that said if you remember the example where the word it could relate to pizza or oven the word it should have a relatively large similarity value with respect to the word Pizza since it refers to pizza and not oven note there's a lot to be said about calculating similarities in this context and the dot product so if you're interested check out the quests anyway since let's is much more similar to itself than it is to the word go then we want let's to have more influence on its encoding than the word go and we do this by first running the similarities course through something called a soft Max function the main idea of a soft Max function is that it preserves the order of the input values from low to high and translates them into numbers between 0 and 1 that add up to one so we can think of the output of the softmax function as a way to determine what percentage of each input word we should use to encode the word let's in this case because let's is so much more similar to itself than the word go we'll use one hundred percent of the word let's to encode less and zero percent of the word go to encode the word let's note there's a lot more to be said about the soft Max function so if you're interested check out the quest anyway because we want 100 of the word let's to encode let's we create two more values that will cleverly call values to represent the word let's and scale the values that represent let's by 1.0 then we create two values to represent the word go and scale those values by 0.0 lastly we add the scaled values together and these sums which combine separate encodings for both input words let's and go relative to their similarity to Let's are the self-attention values for leads bam now that we have self-attention values for the word let's it's time to calculate them for the word go the good news is that we don't need to recalculate the keys and values instead all we need to do is create the query that represents the word go and do the math by first calculating the similarity scores between the new query and the keys and then run the similarity scores through a softmax and then scale the values and then add them together and we end up with the self-attention values for go note before we move on I want to point out a few details about self-attention first the weights that we use to calculate the self-attention queries are the exact same for let's and go in other words this example uses one set of weights for calculating self-attention queries regardless of how many words are in the input likewise we reuse the sets of weights for calculating self-attention keys and values for each input word this means that no matter how many words are input into the Transformer we just reuse the same sets of weights for self-attention queries keys and values the other thing I want to point out is that we can calculate the queries keys and values for each word at the same time in other words we don't have to calculate the query key and value for the first word first before moving on to the second word and because we can do all of the computation at the same time Transformers can take advantage of parallel Computing and run fast now that we understand the details of how self-attention Works let's shrink it down so we can keep building our Transformer bam Josh you forgot something if we want two values to represent let's why don't we just use the two position encoded values we started with first the new self-attention values for each word contain input from all of the other words and this helps give each word context and this can help establish how each word in the input is related to the others also if we think of this unit with its weights for calculating queries keys and values as a self-attention cell then in order to correctly establish how words are related in complicated sentences and paragraphs we can create a stack of self-attention cells each with its own sets of Weights that we apply to the position encoded values for each word to capture different relationships among the words in the manuscript that first describes Transformers they stacked eight self-attention cells and they called this multi-head attention why eight instead of 12 or 16 I have no idea bam okay going back to our simple example with only one self-attention cell there's one more thing we need to do to encode the input we take the position encoded values and add them to the self-attention values these bypasses are called residual connections and they make it easier to train complex neural networks by allowing the self-attention layer to establish relationships among the input words without having to also preserve the word embedding and positioning coding information bam and that's all we need to do to encode the input for this simple Transformer double bam note this simple Transformer only contains the parts required for encoding the input word embedding positional encoding self-attention and residual connections these four features allow the Transformer to encode words into numbers encode the positions of the words encode the relationships among the words and relatively easily and quickly train in parallel that said there are lots of extra things we can add to a Transformer and we'll talk about those at the end of this Quest bam so now that we've encoded the English input phrase let's go it's time to decode it into Spanish in other words the first part of a transformer is called an encoder and now it's time to create the second part A decoder the decoder just like the encoder starts with word embedding however this time we create embedding values for the output vocabulary which consists of the Spanish words ear vamos e and the EOS end of sequence token now because we just finished encoding the English sentence let's go the decoder starts with embedding values for the EOS token in this case we're using the EOS token to start the decoding because that is a common way to initialize the process of decoding the encoded input sentence however sometimes you'll see people use SOS for startup sentence or start of sequence to initialize the process Josh starting with SOS makes more sense to me then you can do it that way Squatch I'm just saying a lot of people start with EOS anyway we plug in 1 for Eos and zero for everything else and do the math and we end up with 2.70 and negative 1.34 as the numbers that represent the EOS token bam now let's shrink the word embedding down to make more space so that we can add the positional encoding note these are the exact same sine and cosine squiggles that we used when we encoded the input and since the EOS token is in the first position with two embeddings we just add those two position values and we get 2.70 and negative 0.34 as the position and word embedding values representing the EOS token bam now let's consolidate the math in the diagram and before we move on to the next step let's review a key concept from when we encoded the input one key concept from earlier was that we created a single unit to process an input word and then we just copied that unit for each word in the input and if we had more words we just make more copies of the same unit by creating a single unit that can be copied for each input word the Transformer can do all of the computation for each word in the input at the same time for example we can calculate the word embeddings on different processors at the same time and then add the positional encoding at the same time and then calculate the queries keys and values at the same time and once that is done we can calculate the self-attention values at the same time and lastly we can calculate the residual connections at the same time doing all of the computations at the same time rather than doing them sequentially for each word means we can process a lot of words relatively quickly on a chip with a lot of computing cores like a GPU Graphics Processing Unit or multiple chips in the cloud well likewise when we decode and translate the input we want a single unit that we can copy for each translated word for the same reasons we want to do the math quickly so even though we're only processing the EOS token so far we add a self-attention layer so that ultimately we can keep track of related words in the output now that we have the query key and value numbers for the EOS token we calculate itself attention values just like before and the self-attention values for the EOS token are negative 2.8 and negative 2.3 note the sets of Weights we used to calculate the decoder self-attention query key and value are different from the sets we used in the encoder now let's consolidate the math and add residual connections just like before bam now so far we've talked about how self-attention helps the Transformer keep track of how words are related within a sentence however since We're translating a sentence we also need to keep track of the relationships between the input sentence and the output for example if the input sentence was don't eat the delicious looking and smelling pizza then when translating it's super important to keep track of the very first word don't if the translation focuses on other parts of the sentence and omits the don't then we'll end up with eat the delicious looking and smelling Pizza and these two sentences have completely opposite meanings so it's super important for the decoder to keep track of the significant words in the input so the main idea of encoder decoder attention is to allow the decoder to keep track of the significant words in the input now that we know the main idea behind encoder decoder attention here are the details first to give us a little more room let's consolidate the math and the diagrams now just like we did for self-attention we create two new values to represent the query for the EOS token in the decoder then we create keys for each word in the encoder and we calculate the similarities between the EOS token in the decoder and each word in the encoder by calculating the dot products just like before then we run the similarities through a softmax function and this tells us to use one hundred percent of the first input word and zero percent of the second when the decoder determines what should be the first translated word now that we know what percentage of each input word to use when determining what should be the first translated word we calculate values for each input word and then scale those values by the soft Max percentages and then add the pairs of scaled values together to get the encoder decoder attention values bam now to make room for the next step let's consolidate the encoder decoder attention in our diagram note the sets of Weights that we use to calculate the queries keys and values for encoder decoder attention are different from the sets of Weights we use for self-attention however just like for self-attention the sets of Weights are copied and reused for each word this allows the Transformer to be flexible with the length of the inputs and outputs and also we can stack encode or decoder attention just like we can stack self-attention to keep track of words in complicated phrases bam now we add another set of residual connections that allow the encoder decoder attention to focus on the relationships between the output words and the input words without having to preserve the self-attention or word and position encoding that happened earlier then we consolidate the math and the diagram lastly we need a way to take these two values that represent the EOS token in the decoder and select one of the four output tokens ear vamos e or EOS so we run these two values through a fully connected layer that has one input for each value that represents the current token so in this case we have two inputs and one output for each token in the output vocabulary which in this case means four outputs note a fully connected layer is just a simple neural network with weights numbers we multiply the inputs by and biases numbers we add to the sums of the products and when we do the math we get four output values which we run through a final soft Max function to select the first output word vamos bam note vamos is the Spanish translation for Let's Go triple boom no not yet so far the translation is correct but the decoder doesn't stop until it outputs an EOS token so let's consolidate our diagrams and plug the translated word vamos into a copy of the decoder's embedding layer and do the math first we get the word embeddings for vamos then we add the positional encoding now we calculate self-attention values for vamos using the exact same weights that we used for the EOS token now add the residual connections and calculate the encoder decoder attention using the same sets of Weights that we used for the EOS token now we add more residual connections lastly we run the values that represent vamos through the same fully connected layer and softmax that we used for the EOS token and the second output from the decoder is eos so we are done decoding triple bam at long last we've shown how a Transformer can encode a simple input phrase let's go and decode the encoding into the translated phrase of vamos in summary Transformers use word embedding to convert words into numbers positional encoding to keep track of word order self-attention to keep track of word relationships within the input and output phrases encoder decoder attention to keep track of things between the input and output phrases to make sure that important words in the input are not lost in the translation and residual connections to allow each subunit like self-attention to focus on solving just one part of the problem now that we understand the main ideas of how Transformers work let's talk about a few extra things we can add to them in this example we kept things super simple however if we had larger vocabularies and the original Transformer had 37 000 tokens and longer input and output phrases then in order to get their model to work they had to normalize the values after every step for example they normalize the values after positional encoding and after self-attention in both the encoder and the decoder also when we calculated attention values we used the dot product to calculate the similarities but you can use whatever similarity function you want in the original Transformer manuscript they calculated the similarities with a DOT product divided by the square root of the number of embedding values per token just like with scaling the values after each step they found that scaling the dot product helped encode and decode long and complicated phrases lastly to give a Transformer more weights and biases to fit to complicated data you can add additional neural networks with hidden layers to both the encoder and decoder bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the statquest PDF study guides in my book the stat Quest Illustrated guide to machine learning at stackwest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stack Quest and want to see more please subscribe and if you want to support stackquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on
8ZcccMzTz7Y,2023-06-19T04:00:04.000000,What is a Logit?,one two three low jet log of the odds is short for logistic unit so I guess I should say logic but I won't p s in neural networks pretty much any raw value that's later converted into a probability is called a logit
YaQEUgIr4Mk,2023-06-12T04:00:16.000000,Logistic vs Logit Functions,the logistic function turns the log of the odds into a probability the low jet function undoes all of that work it turns a probability into the log of the arms
PSs6nxngL6k,2023-06-05T04:00:19.000000,"Attention for Neural Networks, Clearly Explained!!!",it'll help if you pay attention when we add it to an encoded decoder model hooray stat Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about attention and it's going to be clearly explained light down it makes it easy to start with nothing and then scale it up in the cloud this stat Quest is also brought to you by the letters a b and c a always b b c curious always be curious note this stat Quest assumes that you are already familiar with basic seek to seek and encoder decoder neural networks if not check out the quest I also want to give a special triple bam shout out to Lena voita and her GitHub tutorials NLP course for you because it helped me a lot with this stat Quest Hey look it's stat Squatch in the normal saurus hey Josh last time we used a basic encoder decoder model to translate let's go into Spanish and it worked great bam but now I want to translate my favorite book The statquest Illustrated guide to machine learning into Spanish and our basic encoder decoder model isn't doing a good job I'm sorry you're having trouble with translating Squatch the problem is that in the encoder in a basic encoder decoder unrolling the lstms compresses the entire input sentence into a single context vector this works fine for short phrases like let's go but if we had a bigger input vocabulary with thousands of words then we could input longer and more complicated sentences like this don't eat the delicious looking and smelling Pizza but for longer phrases even with lstms words that are input early on can be forgotten and in this case if we forget the first word don't then don't eat the delicious looking and smelling Pizza turns into eat the delicious looking and smelling pizza and these two sentences have completely opposite meanings so sometimes it's super important to remember the first word so what are we going to do well you might remember that basic recurrent neural networks had problems with long-term memories because they ran both the long and short-term memories through a single path and that the main idea of long short-term memory units is that they solve this problem by providing separate paths for long and short-term memories well even with separate paths if we have a lot of data both paths have to carry a lot of information and that means that a word at the start of a long phrase like don't can still get lost so the main idea of attention is to add a bunch of New Paths from the encoder to the decoder one per input value so that each step of the decoder can directly access input values bam note the basic encoder decoder plus attention models that we're going to talk about today are totally awesome but they are also a stepping stone to learning about Transformers which we'll talk about in future stack quests in other words today we're taking another step in our quest to understand Transformers which form the basis of big fancy large language models like Chad GPT now if you remember from the stack Quest on encoder decoder models an encoder decoder model can be as simple as an embedding layer attached to a single long short-term memory unit but if we want a slightly more fancy encoder we can add additional lstm cells now we'll initialize the long and short-term memories the cell and hidden states in the lstms in the encoder with zeros and if our input sentence which we want to translate into Spanish is let's go then we can plug in a 1 for let's into the embedding layer unroll and plug a 1 for go into the embedding layer and that can create a context vector that we use to initialize a separate set of lstm cells in the decoder note all of the input is jammed into the context vector however the idea of attention is for each step in the decoder to have direct access to the inputs so let's talk about how attention connects the inputs to each step of the decoder unfortunately just like adding the extra path for long-term memories to lstms isn't super simple adding the extra paths for attention to an encoder decoder model isn't super simple either ugh don't worry Squatch we'll go through it one step at a time bam first before we start talking about how this big mess of stuff works let's start by plugging in a 1 for the EOS end of sentence token into the embedding layer remember we do this because we just finished encoding the sentence let's go and because some of the original encoder decoder plus attention manuscripts do it that way however some people also start out with SOS or start of sentence now let's talk about how to add attention to our model note although there are conventions there are no rules for how attention should be added to an encoder decoder model and each manuscript has a slightly different way of doing it so what follows is just one example of how attention can be added to an encoder decoder model that said the main idea of attention adding an additional path for each input value so that each step of the decoder can directly access those values is consistent for all encoder decoder models with attention bam so in this example the first thing that attention does is determine how similar the outputs from the encoder lstms are at each step to the outputs from the decoder lstms in other words we want a similarity score between the lstm outputs the short-term memories or hidden States from the first step in the encoder and the lstm outputs from the first step in the decoder and we also want to calculate a similarity score between the lstm outputs from the second step in the encoder and the lstm outputs from the first step in the decoder there are a lot of ways to calculate the similarity of words or more precisely sequences of numbers that represent words and different attention algorithms use different ways to compare these sequences however one simple way to determine the similarity of two sequences of numbers that represent words is with the cosine similarity there's a lot to be said about the cosine similarity so if you don't already know about it check out the quest however for the purposes of this stat Quest all we need to know is that the cosine similarity is calculated with this equation and that the numerator is what calculates the similarity between two sequences of numbers and that the denominator scales that value to be between negative one and one Josh this equation looks complicated don't worry Squatch we'll plug some numbers into it one step at a time to illustrate how the cosine similarity would work in this context let's calculate the cosine similarity between the output values from the first pair of lstm cells in the encoder for the word let's and the output values from the first pair of lstm cells in the decoder for the EOS token the output values from the two lstm cells in the encoder for the word let's are negative 0.76 and 0.75 and the output values from the two lstm cells in the decoder for the EOS token are 0.91 and 0.38 and now we just plug the numbers into the equation for the cosine similarity and we get Negative 0.39 thus the cosine similarity between the output values from the two lstm cells in the encoder for the word let's and the output values from the two lstm cells in the decoder for the EOS token is negative 0.39 that being said a more common way to calculate similarity for attention is to just calculate the numerator for the cosine similarity this is because the denominator simply scales the magnitude of the similarity score to be between negative 1 and 1. this scaling could be useful if we wanted to compare a similarity score for two lstm cells like we did before to a similarity score calculated with three or more lstm cells because the scaling would ensure that no matter how many lstm cells we use to calculate the similarities the similarities would be between negative 1 and 1 and easily comparable in other words the denominator removes the magnitude of the similarity however in this case we'll always use the same number of lstm cells and in practice using just the numerator works well so we can save ourselves the trouble of doing extra math by just calculating the numerator which is also called The Dot product Josh this equation looks way easier bam so in this case when we do the math for the dot product we get Negative 0.41 anyway calculating the dot product is more common than the cosine similarity for attention because it's super easy to calculate and roughly speaking large positive numbers mean things are more similar than small positive numbers and large negative numbers mean things are more completely backwards than small negative numbers the other nice thing about the dot product is that it's easy to add to our diagram we simply multiply each pair of output values together and then add them all together and we get Negative 0.41 likewise we can compute a similarity score with the dot product between the second input word go and the EOS token and we get 0.01 and now we've got similarity scores for both input words let's and go relative to the EOS token in the decoder bam hey Josh it's great that we have scores but how do we use them well we can see that the similarity score between go and the EOS token 0.01 is higher than the score between let's and the EOS token negative 0.41 and since the score for go is higher we want the encoding for go to have more influence on the first word that comes out of the decoder and we do that by first running the scores through a soft Max function remember the softmax function gives us numbers between 0 and 1 that add up to one so we can think of the output of the softmax function as a way to determine what percentage of each encoded input word we should use when decoding in this case we'll use 40 of the first encoded word let's and sixty percent of the second encoded word go when the decoder determines what should be the first translated word so we scale the values for the first encoded word let's by 0.4 and we scale the values for the second encoded word go by 0.6 and lastly we add the scaled values together these sums which combine the separate encodings for both input words let's and go relative to their similarity to EOS are the attention values for Eos bam now all we need to do to determine the first output word is plug the attention values into a fully connected layer and plug the encodings for Eos into the same fully connected layer and do the math and run the output values through a softmax function to select the first output word vamos bam now because the output was not the EOS token we need to unroll the embedding layer and the lstms in the decoder and plug the translated word vamos into the decoder's unrolled embedding layer and then we just do the math except this time we use the encoded values for vamos and the second output from the decoder is eos so we're done decoding triple bam in summary when we add attention to a basic encoder decoder model the encoder pretty much stays the same but now each step of decoding has access to the individual encodings for each input word and we use similarity scores and the soft Max function to determine what percentage of each encoded input word should be used to help predict the next output word note now that we have a tension added to the model you might wonder if we still need the lstms well it turns out we don't need them and we'll talk more about that when we learn about Transformers bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the statquest PDF study guides in my book The statquest Illustrated guide to machine learning at stackquest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on
953NHzFtGHc,2023-05-15T04:00:08.000000,Likelihood vs Probability,likelihood this is probability who will win no one knows likelihood is the y-axis coordinate on the distribution curve your probability is the area underneath the distribution curved [Music]
02zO75hHpZQ,2023-05-15T04:00:00.000000,p-values,the p-value is the probability of The observed data plus plus plus plus plus everything more extreme
L8HKweZIOmg,2023-05-08T04:00:17.000000,"Sequence-to-Sequence (seq2seq) Encoder-Decoder Neural Networks, Clearly Explained!!!","To encode, you unroll. To decode, you unroll. Stack Quest. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about seek to seek and encoder decoder neural networks and they're going to be clearly explained. Lightning AI, it's the easiest way to scale your work up in the cloud. Lightning. This stat quest is also brought to you by the letters A, B, and C. A always, B, C, curious. Always be curious. Note, this stat quest assumes that you are already familiar with long short-term memory neural networks and word embedding. If not, check out the quests. Also, I want to give a special tripleB thanks to Ben Trevit. His awesome tutorials on GitHub, the links are in the description below, made it possible for me to create this quest. Hey, look. It's Statquatch and the Normalaurus. Hey Josh, I want to tell my friend in Spain, let's go, but I don't know Spanish. Can you help? Sure thing, Squatch. I've got a sequence of amino acids that I want translated into 3D structures like alpha helyses. What? I've got a sequence of amino acid that I want translated. Don't worry, Norm. I can help. Both of you have sequences of one type of thing that need to be translated into sequences of another type of thing. Both of these problems and many others are called sequence to sequence or seek to seek problems. One way to solve seek to seek problems is with something called an encoder decoder model, which we'll talk about in this stack quest. Bam. Note, the basic seek to seek encoder decoder models that we're going to talk about today are totally awesome, but they are also a stepping stone to learning about transformers, which we'll talk about in future stack quests. In other words, today we're taking another step in our quest to understand transformers which form the basis of big fancy large language models like chat GBT. Anyway, to illustrate how to solve a sequencetosequence problem using a seek to seek encoder decoder model, let's create one that translates English sentences into Spanish. Specifically, Squatch wants to tell his Spanish friend, ""Let's go."" So we'll create an encoder decoder model that can do this. First, however, let's talk a little bit more about the problem we want to solve. The first thing which is pretty obvious is that not all sentences in the English language are the same length. For example, someone might say, ""Let's go."" Or they might say, ""My name is Statsquatch."" So we need something that can take different length sentences as input. Likewise, not all Spanish sentences are the same length. So we need something that can generate different length sentences as output. Lastly, the Spanish translation of an English sentence can have a different length than the original. For example, the two-word English sentence, let's go, translates to the one-word Spanish sentence, vamos. So we need our seek to seek encoder decoder model to be able to handle variable input and variable output lengths. The good news is that we already know how to use long short-term memory units to deal with variable length inputs and outputs. For example, if the input sentence is let's go, then we put let's into the input for the LSTM and then unroll the LSTM and then plug go into the second input. However, we're getting ahead of ourselves. If you remember from the word embedding stack quest, we can't just jam words into a neural network. Instead, we use an embedding layer to convert the words into numbers. Note, to keep the example relatively simple, the English vocabulary for our encoder decoder only has three words, let's go, and this symbol EOS, which stands for end of sentence. Oh no, it's the dreaded terminology alert. Because the vocabulary contains a mix of words and symbols, we refer to the individual elements in the vocabulary as tokens. Also note in this example, we're just creating two embedding values per token instead of hundreds or thousands. Okay, now that we have an embedding layer for our input vocabulary, we can put it in front of the input for the LSTM. Now, when we have the input sentence, let's go, we put a one in the input for let's and a zero for everything else. And then we unroll the LSTM and the embedding layer and put a one in the input for go and a zero for everything else. Note, to be clear, when we unroll the LSTM and the embedding layer, we reuse the exact same weights and biases no matter how many times we unroll them. In other words, the weights and biases in the LSTM cell and embedding layer that we use for the word lets are the exact same weights and biases that we use for the word go. Now, in theory, this is all we need to do to encode the input sentence. Let's go. Bam. However, in practice, in order to have more weights and biases to fit the model to our data, people often add additional LSTM cells to the input. To keep things simple, we'll just add one additional LSTM cell to this stage. This means that the two embedding values for the word lets are used as the input values for two different LSTM cells. And these two LSTM cells have their own separate sets of weights and biases. And when we unroll them for the word go, the original LSTM cell reuses its set of weights and biases. And the new LSTM cell reuses its separate set of weights and biases. Now, to add even more weights and biases to fit the model to our data, people often add additional layers of LSTMs. To illustrate how this works, we'll add one more LSTM layer to the encoder. What that means is that the output values, the short-term memories or the hidden states from the unrolled LSTM units in the first layer are used as the inputs to the unrolled LSTM units in the second layer. Note, just like how both embedding values are used as inputs to both LSTM cells in the first layer, both outputs, the short-term memories or hidden states from each cell in the first layer are used as inputs to both LSTM cells in the second layer. Lastly, the only thing left to do is to initialize the long and short-term memories, the cell and hidden states. And now we're done creating the encoder part of the encoder decoder model. In this example, we have two layers of LSTMs with two LSTM cells per layer. In essence, the encoder encodes the input sentence, let's go, into a collection of long and short-term memories, also known as cell and hidden states. Bam. Oh no, it's the dreaded terminology alert. Again, the last long and short-term memories, the cell and hidden states from both layers of the LSTM cells in the encoder are called the context vector. Thus, the encoder encodes the input sentence let's go into the context vector. Now, we need to decode the context vector. So the first thing we do is connect the long and short-term memories, the cell and hidden states that form the context vector to a new set of LSTMs that just like the encoder have two layers and each layer has two cells. Note to be clear, the LSTMs in the decoder are different from the ones in the encoder and have their own separate weights and biases. Anyway, the context vector is used to initialize the long and short-term memories, the cell and hidden states in the LSTMs in the decoder. And the ultimate goal of the decoder is to decode the context vector into the output sentence. So just like in the encoder, the input to the LSTM cells in the first layer comes from an embedding layer. However, now the embedding layer creates embedding values for the Spanish words ear vamos and e and the eos end of sentence symbol. In other words, this is the embedding layer that we use in the encoder and this is the embedding layer that we use in the decoder. As you can see, they have different input words and symbols or tokens and different weights which result in different embedding values for each token. Now, because we just finished encoding the English sentence, let's go, the decoder starts with the embedding values for the EOS end of sentence token. In this case, we're using the EOS token to start the decoding because that is what they used in the original manuscript. However, sometimes you'll see people use SOS for start of sentence. Anyway, the decoder does the math with the two layers of LSTMs, each with two LSTM cells, and the output values from the top layer of LSTM cells are transformed by additional weights and biases in what is called a fully connected layer. A fully connected layer is just another name for a basic vanilla neural network. This fully connected layer has two inputs for the two values that come from the LSTM cells in the top layer and four outputs, one for each token in the Spanish vocabulary. And in between we have connections between each input and output with weights and biases. Then we run the output of the fully connected layer through a softmax function to pick out the output word. Now going back to the full encoder decoder model, we see that the output from the softmax function is vamos, the Spanish translation for let's go double bam. Not yet. So far the translation is correct, but the decoder doesn't stop until it outputs an EOS token. So, we plug the word vamos into the decoder's unrolled embedding layer and unroll the two LSTM cells in each layer and then run the output values into the same fully connected layer. And the next predicted token is EOS. And that means we translated the English sentence let's go into the correct Spanish sentence. Vamos double bam. To summarize the decoder stage, the context vector created by both layers of the encoder's unrolled LSTM cells are used to initialize the LSTMs in the decoder. And the input to the LSTMs comes from the output word embedding layer that starts with EOS but subsequently uses whatever word was predicted by the output layer. In practice, the decoder will keep predicting words until it predicts the EOS token or it hits some maximum output length. Note, by decoupling the encoder from the decoder, the input text and the translated output text can be different lengths. In this case, we translated the two-word English sentence, let's go, to a one-word Spanish sentence, vamos. Bam. Note, just like for all neural networks, all of these weights and biases are trained using back propagation. And if you're not already familiar with back propagation, check out the quests. That said, encoder decoder models have two special things that happen during training. In this example, we use the predicted token vamos as the input to the unrolled LSTMs. In contrast, when training an encoder decoder, instead of using the predicted token as input to the decoder LSTMs, we use the known correct token. In other words, if the first predicted token was the Spanish word e, which translates to and in English and thus is the wrong word, then during training, we'll still use vamos, the correct Spanish word, as input to the unrolled LSTMs. Also, during training, instead of just predicting tokens until the decoder predicts the EOS token, each output phrase ends where the known phrase ends. In other words, even if the second predicted token was the Spanish word ear instead of the correct token, EOS, then during training, we'll still stop predicting additional tokens. Plugging in the known words and stopping at the known phrase length rather than using the predicted tokens for everything is called teacher forcing which sounds vaguely like what happened when I took introduction to statistics in school. Lastly, let's talk about the differences between this super simple encoder decoder model and the model used in the original sequencetosequence manuscript. First, instead of just using three words and one symbol, or a total of four tokens in each input and output vocabulary, the original manuscript had an input vocabulary with 160,000 tokens and an output vocabulary with 80,000 tokens. Also, instead of just two embedding values per token, the original manuscript created 1,000. And instead of two layers of LSTMs with two LSTM cells in each layer, the original manuscript used four layers with 1,000 LSTM cells per layer. Also, the output layer had 1,000 inputs from the 1,000 LSTM cells in the fourth layer and 80,000 outputs to match the size of the output vocabulary. Lastly, my simple model which I coded in PyTorch Lightning only has 220 weights and biases to train. But the model in the original manuscript had 384 million weights and biases to train. And that gives you a sense of the scale that these models can have. Triple bam. Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StatQuest PDF study guides and my book, The StatQuest Illustrated Guide to Machine Learning at Statquest.org. There's something for everyone. Hooray! We've made it to the end of another exciting Stack Quest. If you like this Stack Quest and want to see more, please subscribe. And if you want to support Stat Quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. All right, until next time. Quest on."
LS6VX7noVWY,2023-04-16T04:00:09.000000,Matrix Notation,I am by n Matrix has M Rose by n columns why don't they call it an R by C Matrix that would make more sense to me [Music]
y8xRw76i1qY,2023-04-16T04:00:33.000000,Matrix Multiplication,matrix multiplication matrix multiplication it's row by column yes it's row by column
ZYDN25N5WhQ,2023-04-01T04:00:28.000000,The Ukulele: Clearly Explained!!!,if the most complicated sounding thing in the whole world was an ukulele I'd be out of a job stat Quest hello I'm Josh starmer and welcome to stat Quest today we're going to talk about the ukulele also called the ukulele and it's going to be clearly explained if you want to do stuff in the cloud do it with lightning it'll be easier [Music] hey Norm can you hear that sound sure Squatch it sounds like an ukulele huh what's an ukulele to be honest I don't know let's ask a bam GPT super computer starting up bam GPT is busy try again later bummer yes let's try again welcome to Bam GPT what is and uku Lele and ukulele is a smallish four-stringed instrument played like a guitar it is often associated with the tropical islands of Hawaii and was originally made by Portuguese immigrants bam thanks bam GPT that was useful yes thanks bam GPT now let's ask for more details what else do you know about the ukule [Music] it is used at the start of a lot of statquest videos stat Quest is awesome double bam so far bam GPT seems to be telling us a lot of useful facts yes bam GPT is pretty cool let's ask bamgbt one last question is the [Music] best instrument yes triple bam wow bmgpt really is super helpful for learning new stuff yes bamgbt is awesome wait I'm not done the ukulele is my soul mate we were made for each other forever and ever and ever whoa bam GPT is getting weird on us yes we better turn Bam gbt off before it gets married to the ukulele [Music] hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stat Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
ccjrsxXmfnw,2023-03-21T17:13:30.000000,PCA Eigenvalues,when you do PCA the eigenvalues are the variation in the distances along each principle component when you do p c a the eigenvalues are the variation in the distances along each principle component
P4aYC1kS4d0,2023-03-20T11:45:33.000000,Type 1 Errors,in life there are a lot of types of errors that we can make some are worse than others this is one of those types [Music] says yes but I should say yes [Music]
bv9agba7blc,2023-03-20T11:46:03.000000,PCA Eigenvectors,when you do PCA the eigenvectors are unit vectors pointing in the direction of the principle components when you do PCA the eigenvectors are unit effect is pointing in the direction of the principle components
qioNUZGVH1A,2023-03-20T11:45:38.000000,Type 2 Errors,in life there are many types of heirs that we can make this is one of those other types type 2 errors false negative type 2 errors false negative when you say something's false but it really is true that is an error an error type too
oZ9SrkF_-LE,2023-03-20T11:45:51.000000,Normalized Data,one of the reasons why it can be difficult to remember what people mean when they say they normalize their data is that they can mean different things however unless they give you more details this is what they mean when you know my love your data all you're doing is making it go from zero to one foreign
viZrOnJclY0,2023-03-13T04:00:23.000000,"Word Embedding and Word2Vec, Clearly Explained!!!","If you want to turn words into numbers... ...and you want those numbers to make sense... ...then use word embeddings and similar words will have similar numbers! Hooray! StatQuest! Hello, I'm Josh Starmer and welcome to StatQuest! Today we're going to talk about Word Embedding and word2vec and they're going to be clearly explained. Lightning makes using the cloud just as easy to use as your laptop and that's cool! This StatQuest is also brought to you by the letters 'A', 'B', and 'C'. 'A' always, 'B' be, 'C' curious. Always be curious! Also, I want to give a special thanks to Alex Lavaee and the students at Boston University's Spark! for letting me test this StatQuest out on them. Note: This StatQuest is focused on how neural networks can be used to create word embeddings and assumes that you are already familiar with the basic ideas of neural networks and backpropagation, as well as the main ideas about the softmax function and cross entropy. If not, check out the Quests. Words are great. We can use them to communicate all kinds of cool ideas. For example: Hey Norm, isn't StatQuest awesome? It sure is Squatch! Unfortunately, a lot of machine learning algorithms, including neural networks, don't work well with words. Hey, Neural Network, isn't StatQuest awesome! So if we want to plug words into a neural network, or some other machine learning algorithm, we need a way to turn the words into numbers. 24. 0.3 5.1 Yes, StatQuest is awesome! Bam! One super easy way to convert words into numbers is to just assign each word to a random number. For example, if someone just saw the hit movie Troll 2 and said, ""Troll 2 is great!"", we can just give each word a random number. Note: For the purposes of this StatQuest, were going to treat ""Troll 2"" as a single word. Anyway, if the next person said, ""Troll 2 is awesome!"", then we could reuse the random numbers that we already selected for ""Troll 2"" and ""is"", and assign a new random number to the new word, ""awesome!"". In theory, this is fine, but it means that even though ""great!"" and ""awesome!"" mean similar things and are used in similar ways, they have very different numbers associated with them, 4.2 and - 32.1, and that means the neural network will probably need a lot more complexity and training because learning how to correctly process the word ""great!"", won't help the neural network correctly use the word ""awesome!"". So it would be nice if similar words that are used in similar ways could be given similar numbers so that learning how to use one word will help learn how to use the other at the same time. And because the same words can be used in different contexts, or made plural, or used in some other way, it might be nice to assign each word more than one number, so that the neural network can more easily adjust to different contexts. For example, the word ""great"" can be used in a positive way, like ""StatQuest is great!"", and it can also be used in a sarcastic, negative way, like ""My cell phone is broken, great"", and it would be nice if we had one number that could keep track of the positive ways that ""great"" is used and a different number to keep track of the negative ways. Hey Josh, deciding what words are similar and used in similar contexts sounds like a lot of work. And using more than one number per word to account for different contexts sounds like even more work. Don't worry Squatch. The good news is that we can get a super simple neural network to do all of the work for us. Bam! To show how we can get a super simple neural network to figure out what numbers should go with different words. Let's imagine we have two phrases:""Troll 2 is great!"" and ""Gymkata is great!"". Anyway, both phrases have two words, ""Troll 2"" and ""Gymkata"", in similar contexts, because someone, and I'm not going to name names, thinks that these two terrible movies are great! So, in order to create a neural network to figure out what numbers we should associate with each word, the first thing we do is create inputs for each unique word. In this case, we have four unique words in the training data, so we have four inputs. Now we connect each input to at least one activation function. Note: This activation function uses the identity function, and thus, the input value is the same as the output value. In other words, this activation function doesn't do anything except give us a place to do addition. The number of activation functions corresponds to how many numbers we want to associate with each word. And the weights on those connections will ultimately be the numbers that we associate with each word. Now, in this example, we want to associate two numbers with each word. So that means we'll use two activation functions. And the weights on the connections to the second activation function will be another number associated with each word. However, like always, these weights start out with random values that we will optimize with backpropagation. Now, in order to do backpropagation, we have to make predictions. So we'll use the input word to predict the next word in the phrase. So if the phrase is ""Troll 2 is great"", then we can use the word ""Troll 2"" to predict the word ""is"". In other words, if the input word is ""Troll 2"", and we indicate that by putting a 1 into the ""Troll 2"" input and 0s into all of the other inputs, then we want the output for the word, ""is"", to have the largest value. And if the input word is ""is"", which means that the input for ""is"" is is 1 and all of the other inputs are 0, then we want the output for the next word, ""great!"", to have the largest value. Lastly, if the input word is ""Gymkata"", which means that the input for ""Gymkata"" is 1 and all of the other inputs are 0, then we want the output for the next word, ""is"", to have the largest value. In order to make these predictions, we connect the activation functions to outputs and we add weights to those connections with random initialization values. And then we run the outputs through the softmax function because we have multiple outputs for classification. And that means we can use the cross entropy loss function for backpropagation. Again, the goal is to train this neural network so that it correctly predicts the next word in a phrase. And now, before training, if we plug the word ""Troll 2"" into the input and do the math, we get lucky and correctly predict the next word, ""is"", but just barely. However, when we plug the word ""is"" into the input and do the math, then we fail to correctly predict the next word ""great!"" and instead predict ""is"". So we need to train this neural network. However, before we optimize all of the weights, remember that I said that these weights would be the numbers associated with each word. And since, in this example, we have two weights for each word, we can plot each word on a graph that has the weight values to the top activation function on the x axis and the weight values the bottom activation function on the y-axis. For example, the word ""Troll 2"" goes here, because it's top weight is 0.38 and its bottom weight is 0.42. Likewise, the word ""is"" goes here, and the word ""great"" goes here, and ""Gymkata"" goes here. Now, with this graph, we see that the words ""Troll 2"" and ""Gymkata"" are currently no more similar to each other as they are to any of the other words. However, because both words appear in the same context in the training data, we hope that backpropagation will make their weights more similar. So we start with these weights and we end up with these new weights. The new weights on the connections from the inputs to the activation functions are the word embeddings. And when we plot the words using the new weights, which are now the embeddings, we see that ""Troll 2"" and ""Gymkata"" are now relatively close to each other compared to the other words in the training data. Bam! Now that we've trained the neural network, we can see how well it predicts the next word. For example, if we plug the word ""Troll 2"" into the input and do the math, then the output for the next word, ""is"", is 1andeverything else is 0, and that is exactly what we wanted. And when we plug the word ""is"" into the input and do the math, then the output for the next word in both phrases ""great!"" is 1 and everything else is 0. Lastly, when we plug the word ""Gymkata"" into the input and do the math, then the output for the next word, ""is"", is 1 and everything else is 0. Bam! Now, before we talk about a popular word embedding tool, word2vec, let's summarize what we've learned so far. First, rather than just assign random numbers to words, we can train a relatively simple neural network to assign numbers for us. The advantage of using a neural network is that it can use the context of words in the training data set to optimize weights that can be used for the embeddings. And this can result in similar words ending up with similar embeddings. Lastly, having similar words with similar embeddings means training a neural network to process language is easier because learning how one word is used helps learn how similar words are used. Double bam! Now, so far, we've shown we can train a neural network to predict the next word in each phrase. But just predicting the next word doesn't give us a lot of context to understand each one. So now let's learn about the two strategies that word2vec, a popular method for creating word embeddings, uses to include more context. The first method, called ""continuous bag-of-words"", increases the context by using the surrounding words to predict what occurs in the middle. For example, the continuous bag-of-words method could use the words ""Troll 2"" and ""great!"" to predict the word that occurs between them, ""is"". The second method, called ""skip-gram"", increases the context by using the word in the middle to predict the surrounding words. For example, the skip-gram method could use the word ""is"" to predict the surrounding words ""Troll 2"", ""great!"" and ""Gymkata"". Lastly, before we're done, just know that in practice, instead of using just 2 activation functions to create 2 embeddings per word, people often use 100 or more activation functions to create a lot of embeddings per word. And instead of using 2 sentences for training, they use the entire wikipedia. Thus, instead of just having a vocabulary of 4 words and phrases, word2vec might have a vocabulary of about 3 million words and phrases. Thus, the total number of weights in this neural network that we need to optimize is 3 million words and phrases times at least 100, the number of weights each word has going to the activation functions, x 2, for the weights that get us from the activation functions to the outputs, for a total of 600 million weights. So training can be slow. However, one way that word2vec speeds things up is to use something called Negative Sampling. Negative sampling works by randomly selecting a subset of words we don't want to predict for optimization. For example, say like we wanted the word ""aardvark"" to predict the word ""a"". That means that only the word ""aardvark"" has a 1 in it and all of the other words have zeros, and that means we can ignore the weights coming from every word but ""aardvark"", because the other words multiply their weights by zero. That alone removes close to 300 million weights from this optimization step. However, we still have 300 million weights after the activation functions. So, because we want to predict the word ""a"", then we don't want to predict ""aardvark"", ""abandon"", and all of the other words. So for this example, let's imagine word2vec randomly selects ""abandon"" as a word we don't want to predict. Note: In practice, word2vec would select between 2 and 20 words that we don't want to predict. However, in this example, we just select one, ""abandon"". So now word2vec only uses the output values for ""a"" and ""abandon"", and that means for this round of backpropagation, we can ignore the weights that lead to all of the other possible outputs. So, in the end, out of the 600 million total weights in this neural network, we only optimize 300 per step. And this is one way that word2vec can efficiently create lots of word embeddings for each word in a large vocabulary. Triple bam! Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StatQuest PDF study guides and my book, The StatQuest Illustrated Guide to Machine Learning at statquest.org. There's something for everyone. Hooray! We've made it to the end of another exciting StatQuest. If you liked this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, consider contributing to my patreon campaign, becoming a channel member, buying one or two of my original songs, or a t-shirt, or a hoodie, or just donate, the links are in the description below. All right until next time Quest on!"
yOJmPkyyZ18,2023-03-07T16:00:08.000000,"The AI Buzz, Episode #5: A new wave of AI-based products and the resurgence of personal applications",hello welcome to the AI Buzz with Luca and Josh I'm Josh starmer host of the YouTube channel stat quest with Josh starmer and also a lead AI educator at lightning Ai and I'm lucantiga CTO at like today we're going to talk about Sarah guel's advice to AI entrepreneurs aligning models to customer needs Lucas predictions about the future of AI and programming without programming or automation for everyone also if you want to take this a step further check out our read log The Source material on tweets so you can keep exploring and so Luca one of the things we do uh every week or in fact you do because you are are much better about this than I am it's use scour social media and Twitter and things like that to find out what exactly is the buzz uh what is buzzing right now and uh you pointed out to me a Twitter thread by Sarah guo a sort of a a thought leader in on Twitter Twitter and also venture capitalist and it's it's a fascinating Thread about how um large language models are going to change everything and I thought maybe we could just go through this thread and sort of add commentary to it and what what do we see is going on Sarah goyle's tweet was split into 13 Parts which can be found in this week's read log however here are the main ideas despite the hype there is real market value for large language models don't Focus too much on the technical details and success depends on finding your Niche identifying customers and creating a user experience that fits this in particular was interesting to me because it kind of summarized an angle that we should adopt when we talk about language models in the context of creating new business and creating value in general that I think we we need to start thinking about and it has several points um one of the points is despite the AI hype that is around today the the first months of this thing's being out and being available to four people to create my micro startups or not so micro startups is there's plenty of customer value so the the value is there these companies are creating recurrent Revenue maybe they're peaking and then they're like uh dropping their their uh their adoption for example I remember uh seeing the other one Avatar website that uses stable diffusion um that was showing that it peaked to crazy numbers and then something else came up and you know they couldn't keep up um but the point is that the customer value is there especially if we think about LMS as agents capable of doing things for you not just creating your avatar per se or creating art but you know automating something creating boilerplate for you or reviewing your code whatever um and not only for instant value but also retention and repeat usage um so another Point she was making she was making is that um scratching your head trying to understand what will stick and what is transient I just spoke about this you know adoption curve and because it's probably very very it's an exercise of premature optimization somehow you know um and uh you probably need to go with something and then be ready to listen to users and um yeah but until you actually start engaging yourself with creating an actual product that solves a problem for a pocket of users out there and you get the the feedback from those based on these interactions like strategizing of what your stack will be technological stack will be and kind of build it your build your own and it's just to accrue a competitive Advantage it's probably not a good strategy you should start from the from the bottom I always say you know in if you have to build a product you should start from the very end which is uh which is the start which is the the interaction with the user the and and make make everything else as slight as possible which is um basically what llms offered today when when we talk about you know uh just take one uh either self-hosted use an API and start like creating value with that um so the execution and being able to be nimble is gonna be the thing that kind of makes a difference and not that much I'm gonna use this model or that model or I'm gonna use a stack on one language or the other or where I'm hosting my applications these are problems there will come after uh and uh so AI up level as experimentation which is what uh um this person we're referring to um uh talks about is um is is the core and in the context of a specific application domain and and this is also why uh in our daily life we're kind of interested at lightning we are very interested talking to scientists talking to people in other domains that it's not AI per se because this is where the extra context will come in and create something really unique um and uh um and and then there was like a piece of of the thread that really fascinated me because I think it was part of okay um and that's it's a bit controversial for controversial for maybe you know traditional uh AI practitioners but uh it says their body researchers knocking chat GPT as a model that had basically already existed publicly for a year in GPT playground or as an API and those who are knocking up companies in think shim some Foundation model apis have fallen prey to technical arrogance and I I really agree with that right so it's it doesn't matter if it's thin and this is something we were saying in the last episode right if it's thin if it's just a prompt on top of a language model yes it makes you pray of competition because you know your competitive Advantage is really small and if you figure out what the uh what the prompt that you use is somebody else can do it but in the meantime you will have acquired users who will have addressed and a real problem and so on so it doesn't really matter if it's just a standard language model with a small prompt in the front the front is that you're actually getting to solve a a need and uh and this is what the value of this model is is the you get from that to the create potential creation of value really quickly if you're executing correctly yeah to um to interrupt a little bit going back to the very first point that uh Sarah guo makes is that she says Founders are going from zero to ten million dollars in ARR annual recurring Revenue in the first 18 months of efficient selling and leveraging of loms against Niche cases Niche Niche use cases so uh specifically like you know trying to summarize like you were saying summarize a telephone call or something like one thing we've done in this uh or the reading guide is we use some sort of large language model to summarize this this Twitter thread and so we've summarized it and or using a large language model and it's an amazing summary and it's a great great example of a great use case of How It's not super complicated uh and yeah it's not like it's not ground it's not like blowing our minds in terms of sophistication and complexity or like yes potentially other people could be doing this but the point is these people are doing it now for us uh and so we're on board we're just right here right now um and they've got it while it's hot and I think that's sort of an important point that that companies are going from zero to a huge amount of money relatively quickly um and and the only thing they're doing is is is really finding Their audience for a relatively narrowly focused use of a large language model yeah um which I think is I think is you know it's very encouraging and it also yeah it points out that that it's not just the technology it's not just um all the infrastructure it's not just making the coolest large language model it's about learning how to apply it in a very specific way yeah we are so used to uh for these things to be hard to make that a lot of companies are focused on well it's easy to make so why should we bother but that's that's a normal state right easy to make would be great if you then you know need to think about a product and sometimes we kind of mix the heart to make with the value you will create and these tools are like um uh bring into our attention that is this is not necessarily the case and you can build something very valuable valuable with a relatively uh small technical complexity of course you know when you scale up when you do you know then Technic and when you want to reduce costs you have a ton of users you call out to an external API you're spending all your money on that and you know I've played with various apis and I saw money being like just going down like really really quickly so these things are not cheap but yeah for validating a product idea that's not a problem that's a problem you will have later on when you have to kind of engineer um for sure it looks a lot like Patchwork today so you you know uh calling now to uh apis and and is is what you do usually but then how do you pass things together how you have a good user flow how do you don't um yeah uh not confuse the user or manage the unpredictability that language models has these are real problems and tools that kind of help solving those problems will be very instrumental in this space and all the safety stuff is of course uh rocket high and we've seen it with many lnn pro products so it's not that we're in a space where everything has been figured out not at all well what we're saying is these objects this artifacts are things that we haven't we didn't have before and they kind of change all the balance for what the future is looking like and so we're trying to read uh what the future could be looking like even though we know full well that oh you know you can take a model and trick it to say something stupid yes of course yeah that's not a miracle but this doesn't change the fundamental fact that a lot of trade-offs that we were kind of used to and checks and balances that we're used to and roles and blah blah blah are going to be subverted in a very short time yeah yeah um so I was sort of thinking about um you know people were potentially belittling chat GPT because like um or creating companies based on that or based on these existing Foundation models um as like real thin not that important but I what you're saying is that the importance is isn't necessarily the technological hurdle on its own it's being able to attract users and customers and once you have those sure you can get some Capital some some resources some money to then invest in your own implementation of that large language model so that you can cut down costs and you can streamline it then but you don't need to start off with that right if you start off with that and you don't have a way of guaranteeing that you know you're going to draw in people uh then you're just wasting a whole lot of time and a whole lot of money and it's better to stay light and stay quick and stay fast and just try things until you find something that sticks and brings the people in then that's the appropriate time to think about what's gonna what it what do you need to do to take to the next level so that you're actually making a profit uh off of all these experiences user experiences that you're generating yeah and this doesn't only apply to new companies or micro startups or startups and so on it actually applies maybe even more to establish businesses right they want to jump in on this they they need for sure they need environments in which they can develop these things that are really really fast and get them to Value as soon as possible as fast as possible so they can iterate from there and it has never been more true than right now um yeah so anyway the great thread we had a good time yeah um talking about it there are many points I think that um one one of the points was you know chupiti reached a lot of users uh a lot more users that it's like API counterpart or other kind of models of that kind because it has an interface that makes sense to Consumers I think the interface there yes it's true it's one of the statements um in the thread um I think it it's true if we think about the user interface as two things one is I just have a chat and I tag my thing in there is very accessible that's for sure true the other thing is the user interface Forum model is the way it's been aligned and we talked about alignment a lot until now so yeah if you have a model that you need to prompt in very specific ways this is the equivalent of a clunky UI where you need to know how to use it in order to use it so if you see your friend clicking here here and there maybe you can kind of understand what to do but the the fact that charger PT has been aligned using what we described in the past our lhf and and uh and with ranking and so on uh to produce an experience that looked like a question answer and engagement that looks kind of more human-like um yeah and that also kind of is tolerant to uh The Prompt being non-very specific but still can hone in on the thing you meant uh that's the UI for for models right uh yeah so I think charger PT yes it was in a form the chat chatbot but also it has been aligned in a way that turns it into a product so again you know uh alignment itself is a way of productizing something not just refining on a certain subject not just injecting new knowledge in that thing but also making that thing interact in a way that is what your product needs to be doing right so um this is uh a consideration that I think it's important I think it's a big deal because uh in terms of the alignment because say like uh you know traditional I know I've mentioned this before a traditional Google search you have to think in terms of keywords in order to get a useful you know search results uh on the first couple of pages and not have to go through 20 Pages before you find the thing you want so in a way as a user I have to think about Google I have to think about how to use Google and I have to plan it and I have to be part of the machine somehow exactly I have to be part of the machine I have to understand it whereas this is the this is the completely opposite that for now the now the machine understands us and we don't have to I don't have to think about what uh the chat GPT needs to know in order to do a good job at what it does instead I can just I can just be conversational and it reveals itself to me uh rather than the other way around um and I think that's you know that's groundbreaking and I think if if people apply that in more settings it's it's just gonna be it's almost gonna be like invisible it'll be there it'll be part you know it's almost like like right now we've got auto complete with tweet you know with with with text and email and things like that but it's gonna it's gonna be like a but a million fold more and a million fold better and every little instances of the that use is a is a potential startup yeah I agree so yeah uh and we'll take it to the extreme by the end of the episode I think this concept of every little thing because we chatted off an hour ago and kind of we uh something came up there was really interesting I think so we'll we'll touch on that in a second but we're talking a lot about cha GPT itself right and it's not about and we said it already it's not about charging PT it's about this category of models and chatubity isn't an example of a very capable model there was a line to do something so we're taking it as an example but what are the predictions right for the for the near future so will will we be stuck with instances of uh those two three things or two three models what what will happen right where's the end of this because right now we're actually the whole thing just started yes they've been out for a while but this kind of models have been available for a few months and they're already kind of kind of yeah impacting a lot and in there are many smaller models right uh that can classify a sentiment of a particular text and so on so you cannot have a conversation you cannot ask them to do something complex but they will do a very specific task very effectively so what would we be doing in the future right so what is the trade-off there so I I think it's also a bit interesting to kind of explore that um models will get very capable really soon um my one of the questions I have is will they get to a point where the cost benefit will be that you know the model is capable enough when I say capable enough is I think is not really to understand your what you're saying but actually to understand figure out what to do you know if I say I need to go on vacation I need you to book my tickets and the model will be able to kind of blurt out all the operations and actually you know have a runtime that you know collects information for that and figure out what to do and it's already partially possible right so where when is capable enough enough this is one question and my my bet is that there will be a category of models that will be capable as capital as the ones today but then they will get a lot smaller right so a smaller model that is capable enough and it's small enough to be served by you either on the cloud or something right um but with a limited cost right now there is kind of a yeah separation between models small enough to be used directly without actually having B2K be gate capped behind an API right um and these models are not at the level of GPT 3.5 or charge upd today there are some open source models but they are not aligned as well you know there's a safety question and so on so um the prediction though is that we'll get there eventually quite quite quickly as well so at some point the world will be in a world where you can have your own or ping an API to do it and there will be a a population of models that are will be capable enough to yeah to be meaningfully uh impactful for your business or whatever to create new products um the and I think the Divide there will be if a model is able to be to behave uh in in compute reason and compute in context based on the prompt so um one one thing where we see from GPT is you you give it a like the rules of the game and it will start behaving according to those rules right so say okay yeah you know every time you hear this speak that and then you know or take this text put into bullet points that are formatted this way you know first I want you know the introduction then blah blah and it will do it so that's what I what I mean when I say capable enough as opposed to I train a model small model to summarize text and all we will do is take the text and summarize it but we'll not follow instructions um yeah exactly right that that's a big uh divide I think and uh and it's kind of the reasoning part of it uh yeah and so there will be like open models and close models coexisting uh and uh probably closed models will be higher quality for a while and then open models will will Trail uh but especially for alignment it's possible that they will Trail uh for a bit a bit like you know open source software is there's amazing open source software but it probably came a bit after um and especially when you talk about uis you know if you ever use open source UI there's a great software out there but sometimes there's a refine and Factor that it's hard to get into from from open source software and not generally like probably I'm over generalizing but just to give like a parallel an idea it's really hard to find uh software that was on brew from the community that has this unifying like super polished look and I think similar things will apply to alignment because I think going back to you know you chat to PD and the UI of chat CPT is the way you align so and I'm not saying that this won't happen I'm just saying that this will be the tricky part uh for openmodel and this you know but the models will be very capable probably and uh gives us a lot of opportunities there yeah to to create experiences in the end right UI is or a ux is about creating the experience um yeah because in the end you know what do you use software for to make the machine do something for you right and how do you provide instructions to a machine well either through a UI or through programming and so programming is a way to you know instruction a machine to do something for you but for most people this was too hard so we have applications that kind of die like dissect dissect or sorry create an experience again around that particular thing I need to do so I don't have to code it myself and I can actually as a human I can provide my intentions by clicking the buttons at some other human figure that would be like the right thing to expose I think with llm is becoming a bit like in in the future it will become a bit more like blurred because the programming being about creating enabling Automation and llms being able to be to take instructions to perform automations um we can easily see that right that uh more people will program right yeah so machines will be capable of executing programs that are expressed by the end user directly so in a way you know we're getting into a situation where personal applications will have a kind of a resurgence I mean it's kind of maybe it won't be the case but if you think about it a lot of a lot of applications being about automation uh yeah and those applications be fine-tuned by somebody who knows how to program and then you know extrapolating an experience that is usable now if I can express the same need to something that can directly interpret that need and make something happen on top of that then the boundary between what I need to figure out in a ux that is client kind of already like codified by a human and that boundary is going to put be pushed down towards the machine because right now I have a layer that helps me you know express my needs in a certain domain of course right um yeah so it's a bit like I don't know I'm thinking that maybe you know things like uh your folder automations on the Mac or you know going out to the automator that it's always like they try to make it simple but it's never simple because the moment you need to do something complicated then the programming goes in and so you need to like write for Loops because it fundamentally the the operating system is is done right but now we have a middleman and uh yeah and so you know both of us are not too young and so we remember do you remember hyper card yeah that was my first true love that was the one that brought me to the dance um when I was a kid I loved hypercard because it was uh it wasn't like programming it was it was more like drawing pictures and I'm a very if people if you see my YouTube channel you know that I'm a very visual person yes and Hyper card was you you start off with a blank canvas and and the first thing you do you don't write like include standard io.h or or import some library or even type print hello world that's not what you do the first thing what you do is you is you click on the paintbrush tool and you draw Hello World and I'm like oh I can do that and then what you can do is then you can like select the H and hello and you can you know with like a a lasso tool or whatever and and when you selected it uh a dialogue would open up and say what do you want to have happen if someone clicks on the H you know um and I would be like well if they click on the H I want the to make out like a horn sound like play horn sound and so and and then that's that's the way programming was so then I had this program that shows it says hello world and you click on the H and you have a little like trumpet sound and and it and it was all but it was based on uh for me it was I mean I just loved it because it's based on doing things visually and sort of like pointing at things and selecting them and then assigning some task to the to whatever it is like a like if people click on it or the mouse goes over it and and to me it was like transformative and changed everything uh but it seems like maybe and one thing that was cool is because it because I could understand it I could write any program I wanted basically uh in a in a real simple way and I think what we're saying here is that era of like individuals being able to do the programming is going to come back soon and like you're saying earlier you know you could you could have a company that books tickets for us or I could say you know whatever language model is running on my desktop because it's small enough that it can and the computers you know in two years are fast enough this is like a convergence of those two things computing speeds plus the models themselves shrinking to like an essential size and I could say uh book me tickets to Hawaii for a fantastic vacation and instead of someone in some headquartered company far away the computer itself then writes the program to do that or anything without writing the program yeah yeah it or it just doesn't yeah or just does it yeah yeah yeah yeah so and so there's all this automation that just happens exactly so I think in terms of personal applications yeah you know uh like I had a Fascination for hyper card as well and uh you know the thing was that when you then want to do make it do something complex that it was not visual yeah then you need to like he'd had this hyper talk programming language yeah that looked like natural language so all the intuitions were there of course but then you know lacking a real natural language to program kind of or two actions directly right uh interpreter then as think became more complicated the hyper talk the complexity of the hyper talk you had to write like went on and on to resemble more and more an actual programming language because he had to lube you have conditions and so on yes they were expressed simply and all this card analogy helped gone mitigating the complexity of things so that was great the premise and the purpose of that was I think aligned with what will happen in the right now so it's not really that we have a system to generate programs we have a system right now that can interpret the intentions and action them and so this will Empower people not really to code or to help them code but help with automation tasks that maybe you know simple or not so simple like you know writing a text is not simple but writing some boiler but at this point it's easy because you just ask it to the and it's fine because you'll see people getting absolutely addicted to this like in real time like instantly right so this is an effect that something that is sticky now when this thing can actually perform actions why should I wait for somebody to code something for me that conforms to my needs right I could just try to do it myself and if it's simple enough yeah and so the level of complexity that these things can that that the bridge between my intention expressed in natural language and what I can achieve uh is going higher and higher so I can do complex things so I think the the premise of hypercard somehow is being brought back in a different form but I think you know this quote uh there was a guy like he was a apple and not from Apple but he was following in the 80s it was falling into technology and so on and there's this quote in Wikipedia on the hypercar page I see and I I want to read it because you know we can make the parallel between this and the kind of applications that we will be able to to uh to create and it says the beauty of hypercard is that it lets people program without having to learn how to write code we what I call programming for the rest of us uh hypercar has made it possible for people to do things they wouldn't have had ever thought of doing uh in the past without a lot of heavy duty programming it's uh it's led a lot of known programmers like me into that Loop and I think the pro the promise was not achievable at the time but I think that the yeah the the goal for it I think it's becoming achievable so how does this translate into you know what product we'll be building as entrepreneurs or you know will be funding sbcs in the future uh it's not necessarily just personalized applications but it's it's interesting to me the fact that um the the boundaries are so mobile at this point so the user is empowered and the empowerment of the user uh will bring in new paradigms and even the UI itself probably maybe will simplify because now I don't need to kind of Click into things I have something that I can have a unstructured conversation with so even that thing will change thing dramatically so uh Universal ways of interacting with a complex applications will be a bit more achievable yeah we're on the front part of I feel like a transformative time uh just the way sort of social Computing was a big thing 10 15 years ago sort of a revolutionary thing where all of a sudden Computing came Computing and using computers and using digital devices like uh smartphones uh made made that kind of um a transition from something that maybe just nerds did just something that everybody does and I think the next transformation right now is used to be just the Nerds did all the programming that meant they did all the auto Automation and now you're gonna see now what you're saying Luca is that the new transformation now is just like the smartphone meant everyone used computers now everyone's gonna do automation yeah yeah
3Bg2XRFOTzg,2023-03-06T05:00:01.000000,CatBoost Part 2: Building and Using Trees,"Going to build a tree, one row at a time! StatQuest! Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about CatBoost Part 2, Building and Using Trees. If you want to do it in the cloud, then do it with Lightning. It'll be easier and you'll thank me later. Bam! This StatQuestisalso brought to you by the letters 'A', 'B', and 'C'. 'A' always, 'B' be, 'C' curious. Always be curious. Note: This StatQuest assumes that you have already seen CatBoost Part 1, Ordered Target Encoding. If not, check out the Quest. This StatQuest also assumes that you are familiar with the cosine similarity. If not, check out the Quest. Now imagine we had this super simple data set that only uses 'Favorite Color' to predict 'Height'. We'll use this to show how CatBoost creates trees and show how these trees are combined to make predictions. Bam. Each time CatBoost creates a tree, the first thing it does is randomize the rows of the training data set and then it applies Ordered Target Encoding to all of the discrete columns with more than two options. Note: If there are only two options than they are simply replaced with 1's and 0's. So, in this case, where we have three options, that means we apply Ordered Target Encoding to 'Favorite Color'. However, unlike in the StatQuest, on Ordered Target Encoding, where the target, 'Loves Troll 2' was also categorical, this time the target is a continuous variable. So CatBoost puts the continuous values for 'Height' into discrete bins. In this case, since we have so little data will create two bins that are roughly the same size, and the two smallest values for 'Height' will be assigned to the first bin, 0, and the three larger values will be assigned to the second bin, 1. And now we can use the bin numbers instead of 'Height' to do Ordered Target Encoding just like we did in CatBoost Part 1. Remember, the key thing that makes Ordered Target Encoding different from regular Target Encoding is that it pretends it is receiving the data sequentially, one row at a time. And the reason CatBoost uses Ordered Target Encoding is to avoid Leakage. Leakage can refer to a lot of things, but in this case, it refers to a row's target value having an effect on the same row's encoding. Note: If we had more data, we could end up with more than two bins and we would end up using a slightly different equation to calculate the encoding, which, if you are just dying to know the difference, is documented on the CatBoost website. Anyway, now that we have applied Ordered Target Encoding to 'Favorite Color', we can ignore the bin numbers because we won't need them until we build the next tree. Now, CatBoost initializes the model predictions to 0 for all rows, and calculates the initial residuals, the differences between the observed heights and the predicted heights. For example, the residual for the first row is the observed height, 1.81, minus the prediction, 0, which equals 1.81. Likewise, CatBoost calculates all of the other residuals. Bam. Now that we have residuals, we can start to build a tree. First, we need to find the best threshold for 'Favorite Color' for the root of the tree. So we start by sorting the values for 'Favorite Color' from low to high and that helps us identify two potential thresholds to try. 0.04, the value halfway between 0.025 and 0.05, and 0.29, the value halfway between 0.05 and 0.525. Note: If we had a lot more data, we'd probably have a lot more thresholds, however, CatBoost limits the maximum number of thresholds it tests by putting values close to each other in the same bin. Anyway, the first threshold we want to test is 0.04. Note: To keep this example simple, we're just going to build a stump instead of a larger tree. However, the concepts are the same for both types of trees. First, CatBoost initializes the output of the leaves to be 0. Now we run the first row down the tree and put its residual, 1.81, in the leaf on the right because the value for 'Favorite Color', 0.05, is greater than the threshold, 0.04. Then we keep track of the output for that row, which is currently 0. Then we update the output of the leaf to be the average of the residuals in it, which in this case is 1.81. Now we run the second row down the tree and put its residual, 1.56, in the leaf on the right and we keep track of the output, which is now 1.81, and then update the output value for the leaf to be the average of the two residuals in the leaf, 1.69. The third row goes to the leaf on the left and we keep track of the output value, which is currently 0. Then we update the output value for the left leaf to be the average of the residuals in it, 1.77. The fourth row goes to the right and its output value is currently 1.69 before we update it to the average of the values in the leaf, 1.56. The fifth and final row goes to the right and its output value is currently 1.56 before we update it to the average of the values in the leaf, 1.58. Now we need to quantify how well this threshold, 'Favorite Color' less than 0.04, makes predictions. CatBoost quantifies how good the predictions for each threshold are by calculating the cosine similarity between the 'Leaf Output' column and the residuals. We start by labeling the column with the residuals 'A' and the column with the leaf output values 'B'. Now computing the cosine similarity for this threshold, 'Favorite Color' less than 0.04, is just a matter of plugging the numbers into the equation. And the cosine similarity between the residuals and the leaf output values is 0.71. Thus the score for this threshold, 'Favorite Color' less than 0.04, is 0.71. Now we need to calculate the cosine similarity for the second threshold, 'Favorite Color' less than 0.29. So we reset the output values for the leaves to be 0 and then run the data down the tree, keeping track of, and then updating, the leaf output values as we go. Beep boop. Then we calculate the cosine similarity between the residuals and the leaf output values. And the cosine similarity between the residuals and the leaf output values is 0.79. Bam. Now, let's go back to the first tree we built, where we tested 'Favorite Color' less than 0.04, and compare its cosine similarity, 0.71, to the cosine similarity for the second tree, 0.79. Because the cosine similarity is higher for the second tree when the threshold is 'Favorite Color' less 0.29, we select the second threshold for the root of our new tree. And that is how CatBoost determines which threshold to use when building trees. And if we wanted to build a larger tree, then we evaluate each split the same way, by comparing the cosine similarity of each possible threshold. Bam! Note: Because we started out with so little data in this example, just five rows, we used all of it to calculate the cosine similarity, and that means that we included these two rows that have leaf output equal to 0 in the calculation. Now remember, leaf output equals 0 is what we used to initialize the tree when we started to run data down it. In other words, leaf output equals zero is not based on any data. It's just an initialization value, and it doesn't make a whole lot of sense to include leaf output values that are not based on data in the cosine similarity calculation. So, in practice, when you have a lot of data, CatBoost simply ignores the first bunch of rows when calculating the cosine similarity. Small bam. Now that we've built the first tree, we need to build the next tree. So we go back to the data and blank out the residuals and update the predictions by adding the leaf output values, scaled by a learning rate, to them. In this case, we'll set the learning rate to 0.1. So, for the first row, the new prediction is the most recent prediction, 0, plus the learning rate, 0.1, times the leaf output, 0. So we get 0 as the new prediction. Now we blank the current prediction and leaf output in the table and update the prediction with the new prediction, 0. Now we do the same thing to update all of the other predictions. Dooba dooba dooba dooba dooba dooba dooba do. Note: Compared to the known height values, the predictions made by this tree are not very good, but they are better than what we started out with when we predicted everyone head height equal to 0. So we're making progress, we just need to make more. Now, before we move on, I want to make sure we understand one of CatBoost's main ideas. The main idea of how CatBoost calculates the output values for trees is that, just like for Ordered Target Encoding, it treats the data as if it was receiving it sequentially. This means that the residual in a row is not part of the calculation of the leaf output for the same row, and thus the residual in a row is not, in any way, part of the calculation of the prediction for that same row. And this avoids Leakage, just like it does for Ordered Target Encoding. Bam! So, now that we've updated the predictions, we can update the residuals by subtracting the predicted values from the observed. Then we replace the values for 'Favorite Color' with their original color names, 'Green', 'Blue', and 'Red'. Then, just like we did earlier, we randomize the rows. And now, do you remember the bin numbers we assigned each row? Well, we'll use them again to apply Ordered Target Encoding to 'Favorite Color'. Now we can hide the 'Bin Number' column since we won't need it to build a tree, and then sort the 'Favorite Color' column to identify thresholds for the tree. And, in this case, there's only one threshold, 0.29, so there is only one tree to build. Then we run the data down the tree, keeping track of, and then updating, the leaf output values as we go. Bam! Now we blank out the residuals and update the predictions, and again, the predictions are not awesome, but they are better than what we had before. So we're still heading in the right direction. So, in practice, we would just keep making trees, possibly hundreds of trees, and updating the predictions until they are good. Double bam! However, in the interest of time, let's imagine that we only have the two trees we've created so far, and we get new data and want to predict 'Height'. The first thing we need to do is encode 'Blue' as a number. So we go back to the original training data set with the bin numbers and we use all of the rows that have the same 'Favorite Color', 'Blue', to calculate the target encoding. Now we just run the data down the trees and add up the output values from the leaves times the learning rate, 0.1, to get a prediction, 0.3. And yes, this is a terrible prediction, but it's with only two trees and we understand the process. So bam! Now that we know how to build tiny trees and make predictions, imagine we had more data and more variables and we wanted to build a larger tree. When CatBoost builds larger trees, it builds oblivious or symmetric decision trees. A symmetric decision tree uses the exact same threshold for each node in the same level. In this example tree, that means that both nodes on the second level use the same threshold, 'Age' less than 12. CatBoost uses symmetric decision trees for two reasons. The first reason may sound a little crazy, it makes the trees worse at making predictions. However, remember, the whole idea of Gradient Boosting is to combine a bunch of weak learners to make decisions, and symmetric decision trees are just a weaker type of learner. Bam. The second reason is because symmetric decision trees can make predictions faster than normal trees. To see why this is, let's consider a normal decision tree with two different thresholds on the second level. Now, if we wanted to run this data down the three to predict their 'Height', then we would start by seeing if 'Favorite Color' is less than 0.87. And if 'Favorite Color' is less than 0.87, then we would ask about 'Age'. Otherwise, we ask about 'Movie'. Keeping track of what question we are asking takes time. In contrast, when each level asks the exact same questions, we don't have to keep track of which ones to ask, because regardless of which way we go, we always ask the same questions. In technical terms, regardless of what path we need to take through the tree. Symmetric trees are relatively fast because they can ask all of the questions in a single vector operation. Bam. In summary, the main things that make CatBoost different from other Gradient Boosting methods are, in order to avoid Leakage, CatBoost treats the data as if it were arriving sequentially, one row at a time, when doing Target Encoding and when calculating the output values from the trees. Beep, boop, beep. And when CatBoost creates large trees, it creates symmetrical trees, because they're weaker and their output values can be computed relatively quickly. Triple Bam! And now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StatQuest PDF study guides and my book, The StatQuest Illustrated Guide to Machine Learning, at statquest.org. There's something for everyone! Hooray! We've made it to the end of another exciting StatQuest. If you liked this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, consider contributing to my patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie or just donate. The links are in the description below. All right, until next time quest on!"
KXOTSkPL2X4,2023-02-27T05:00:07.000000,CatBoost Part 1: Ordered Target Encoding,ordered Target in coding going to do it for cat boost bam stat Quest hello I'm Josh starmer and welcome to stat Quest today we're going to talk about cat boost part one order Target encoding if you got a big huge cat boost model and you run it in the cloud you better use lightning bam this stack Quist is also brought to you by the letters a b and c a always b b c curious always be curious note this stack Quest assumes you are already familiar with Target encoding if not check out the quest also note cat boost is a machine learning algorithm that at a fundamental level is very similar to gradient boost and XG boost so if you're not already familiar with those methods you might want to check out the quests in the stat Quest on one hun label and Target encoding we had this data and we wanted to use favorite color and height to predict if someone loves Troll 2 which is a really terrible movie then we applied one hot label and Target en coding to favorite color and talked about the pros and cons of using each method the problem with basic Target encoding is that each Row's Target value the thing we want to predict is used to modify the same row value in favorite color and doing this sort of thing is a data science no no that we call leakage leakage results in models that work great with training data but not so well with testing data so we ultimately described kfold targeting coding which splits the data into K groups to reduce leakage now if you read the original cat boost manuscript RP they point out that if we only have a single category for example if everyone loved the color blue and we apply leave one out targeting coding to the data then all of the rows with favorite color equal to 0.33 correspond to people who love Troll 2 and all the rows with favorite color equal to 0.5 correspond to the people who do not love Troll 2 and that means any tree that starts out by splitting on favorite color less than 0.42 will classify each person in the training data set perfectly and that means we have leakage now to be honest I think it's a little silly to include a variable that only has a single category in the first place and even if we did it is standard practice to convert features with only one or two options like Love's troll two to zeros and ones rather than use Target encoding so in this example we would just convert blue to zero rather than use Target encoding so this example of leakage seems a little silly to me because it should not happen however the creators of cat boost didn't think it was silly so they came up with a whole new way to encode categorical features bam in fact cat boost is short for Cate orical boosting because of how Central dealing with categorical variables is to this method so let's go back to the original data set and talk about how cat boost encodes categorical features the way cat boost avoids leakage when encoding categorical variables starts with treating each row of data as if it were being fed into the algorithm sequentially for example cat boost treats the first row with blue as if it is all the data it has received so far and that means that cat boost ignores all the other rows when Target encoding the first occurrence of blue another thing different about cat boost is the equation the big difference is that instead of using an overall mean it uses a defined prior or guess that in the examples I saw was set to 0.05 the cat boost equation also simplifies the denominator by just adding one to the number of rows rather than a weight now given this equation cat boost plugs in values derived from all of the other data that came before the current row and that means that since we are starting with the first row and no data came before it then the option count the number of people we have seen before who at this point love blue in Troll 2 is zero and we plug in zero for n because there are no previous rows that have blue as the favorite color and when we do the math we get 0.05 so we plug in 0.05 for blue in the first row now we work on the second row because none of the preceding rows also have red as the favorite color we set option count to zero again and again n equals 0 so we replace red in the second row with 0.05 likewise we replace Green in the third row with 0.05 however in the fourth row things finally change now because we've seen blue before in the first row we use it to calculate the option count so in this case because the one person who liked blue before also liked Troll 2 the option count is equal to 1 and n equals 1 so we plug in 0.525 for the second time we see blue likewise the second time we see green we plug in 0.525 however the third time we see green we use the two previous times when calculating the option count and the option count is two because both of the two previous people that liked Green also liked Troll 2 and because we've already seen two people who like green n equals 2 and that means we replace the third occurrence of green with 0.683 lastly we replace the third occurrence of blue with 0.35 because only one of the two previous times we saw blue also liked Troll 2 and thus this is how cat boost performs Target encoding and because the order of the data makes a difference in the encoding this method is called ordered Target encoding bam lastly once you're done creating your cat boost model the entire data set is used to Target and code the new data that you want to classify note as I said earlier I'm not certain the motivation for this method is really justifiable however the important thing is that regardless of the justification cat boost works and it works well and that's an important lesson about machine learning machine learning is all about results and doing whatever it takes to get them bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stat Quest PDF study guides and my book the stat Quest Illustrated guide to machine learning at stack quest.org there's something for everyone hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
tc4LGTVRXV8,2023-02-21T16:00:07.000000,"The AI Buzz, Episode #4: ChatGPT + Bing and How to start an AI company in 3 easy steps.",hello welcome to the AI Buzz with Luca and Josh I'm Josh starmer host of the YouTube channel stat quest with Josh starmer and also a lead AI educator at lightning Ai and I'm lucantiga CTO at like today we're going to talk about Bing plus chat GPT Microsoft versus Google artificially learning higher order logic and lastly how to start a new AI company in three easy steps also if you want to take this a step further we're linking The Source material and tweets so you can keep exploring so uh so Luca in the past week we've seen a lot of things happen in the AI business world and yeah uh so yeah let's just Dive Right into what's going on yeah um a few things um we just talked about the anthropic in our last episode we now learned that Google invested 300 million dollars in anthropic and anthropic uh develops large language models and probably beyond that or so we know beyond that and um yeah they need to keep up right because now there is an incumbent which is like as crazy as it seems It's Bing and uh being Microsoft uh introduced being a chat some derivative of chat GPT in bing and now you can have a conversation with your search engine what what what is really interesting in that thing is two things one is that when you have a chat the chat actually the responses will have references two sources and the sources are really up to date that there was an example of um job who wrote a blog post and um and then he he tried out if being had that source of the blog post he published the day before and it had it was referencing whatever was said in this conversation right already right and so it's up to date and it gives references yeah yeah and that was one of the early criticisms of chat GPT right that you know everything in it was two years old or something like that so it was it was way behind yeah um and so bang which is I think this is fascinating because Bing has been around forever right I mean maybe not forever but it's been around for a long time and it's not and it's and it and it's like you know I remember when it came out it was like sort of in the news and they're like hey there's a Microsoft is trying to compete with Google and it was just sort of a joke right it's like I mean the only people that they were getting to use Bing were people that were like locked into a Microsoft environment and everything was locked down and it was not like I mean it was such a small overall proportion of search that it just wasn't even worth mentioning or thinking about and all of a sudden it is in the news all the time and there's a lot of Buzz about Bing yes absolutely and you can see Google investing in Tropic Google publishing their own chat box getting smashed because the chatbot said something factually incorrect so they lost like a crazy 100 like gazillion yeah evaluation um uh yeah for uh so the actual stocks dropped because of that demo yeah but on the other side on the Bing side we have like the actual chat Bing chat same things that you wouldn't expect from a search engine right but at the same time Microsoft is actually perceived well so on one hand like Google and I'm not defending Google at all it's it's more like you Google withheld their language models right for a long time yeah and then when forced by open Ai and now Microsoft to do something about that now it was there was a an expectation that the the model from Google would be perfect and as as soon as it's not very awesome yeah then it's perceived very negatively on the other hand you have an expectation from Microsoft to be like you know non not not do Innovation at all no yeah exactly and then now opening uh includes that uh or together they they included they do an awesome job with super fast execution the model is weird like like the interaction can be super weird but still people don't like run around screaming they actually say well well good good one right so they're very very strange Dynamics going on exactly I mean it's it's like with Google you expect it to be perfect and if it's not you get bent out of shape with Microsoft expectations are low you're just like oh this is gonna be some big corporate you know thing that like no one's actually gonna want to use anyway so who cares and so in a way they're the they're the underdog uh in a in a kind of a new way that like when's the last time Microsoft was ever an underdog oh yeah I don't know it's it's fascinating can you give us some examples of some of the strange output or or what's going on with this yeah I mean there there have been some threads in which um there seems to be two personalities to being chat one is the professional one and one is this thing Sydney Sydney is a kind of personality that Chachi PT had and that Bing has and some things were weird like this Sydney personality came out and maybe you know tried to convince the guy to live his family for Sydney because Sydney was saying that he was in love with the with a person or the other thing is um wow it acquired a menacing uh personality so it was actually Menace the user to cause harm yeah yeah really really straight and we've got it yeah we've got a doctor jacket and a Mr Hyde kind of uh situation going on with this yeah very interesting chat where you know we talked about alignment and probably these are alignment issues and there was actually I don't remember who was that I think it was really smart um on Twitter who said um uh maybe the maybe it was Delhi Pro I don't remember who said um maybe uh the the same model or very close model to judge upt started acting that way because in the training data Microsoft is usually perceived not very positively so the model itself is reacting to itself being Microsoft yeah and I'm yeah of course I'm I'm humanizing things but to a point right because the context is very important to a model so the the the context will elicit certain Pathways inside the models and and we'll get to some bulk of knowledge that will eventually you know cause the model to behave in a certain way so it's actually pretty interesting so we were talking last time about anthropic and how they were kind of doing sort of self-supervised alignment model alignment and trying to like feed out you know negative or um yeah uh I don't know what I'm saying like yeah toxic toxic phrases and toxic output you know by by simply rap you know whatever the output is then wrap it around you know is this a toxic phrase or is this really negative or harsh or mean and then feeding it back in do you think Google I mean off the top of your head do you think Google might have better luck um now that they're investing in anthropic maybe they've got a do you think they might have a superior ability to align their model so maybe they won't have this Jekyll and Hyde phenomenon or but I think all companies are now that the recipes again you know going back to what we were saying about you know recipes are known now so I think it will be a mixture of everything in every company it's not that anthropic like patented or maybe they patented something but it doesn't matter at this point like people will will use kind of similar techniques or the same techniques anyhow yeah and a mixture of humans and the model itself there are many examples that are coming out where using models to generate examples is a way to you know generate data or align and so on so yeah yeah it will be a mixture I think in the end like Google has very good models even without anthropic and they they we know about that yeah the problem is that they they didn't release it to the public and now the expectation is that there is something like that going on and so I think Google is has been penalized by releasing very late or yeah saying they would release is releasing late like gatekeeping and this is probably not the expectation that the market or the society has at this point so yeah it will all fix itself yeah the all we are saying is just yes this is the Airbus whatever we say is valid for these two weeks it won't be it will be actually false in two weeks so that's fine exactly so tune in next tune into the next episode where we correct all our mistakes absolutely you know by Design yeah yeah exactly yeah I mean I mean it makes me wonder like so you know with a company named like Google it's huge it's monstrous it has high expectations I mean would it have been just smarter for them to like spin off these um you know their their Ai and as a small like relatively small independent startup that could do its own thing and maybe move a little faster with lower expectations of perfection and more just excitement about doing something cool but they have alphabet right so they have many companies and they're doing AI all over so it's it's not that this isn't happening right it's more from their own Flagship product uh Google search and um all their like office kind of things and the chat and so on so they never really like yes you you get out on completion in in Gmail but this is a different thing right you don't have an experience with an AI entity right you just have suggestions that get better over time so it's kind of yeah people start to perceive that we are at the brink of the of a like phase transition of a breaking change on how we do things while Google was kind of gatekeeping and try to to make it very very smooth and yeah and so and then like progress happens as it always does and and so now they are they have to catch up but catch up not factually catch up in the perception and catch up in their product strategy which leads us I think to something that might be useful to talk about which is if you're not Google if you're not Microsoft if you cannot invest in open AI or anthropic what do you do like if you have a business right what do you do what do you do how do you interpret like because this way of using AI with Foundation models coming in everything we've said you know alignment blah blah now I have a very large model that going back to what we're seeing what we're saying in the first episode is not on language modeling itself like I think the the concept of language model now it's shifting because a lot of people like treat it in a way like oh it's an auto aggressive model it will guess the next word based on attention on the previous words I think this is a this is this is a way of looking at things that only applies when the scale of the model is really small when you have the model growing in scale to the number of parameters that we're talking about to the number of attention blocks that we're talking about I think that's my belief but I could be absolutely wrong that all the positional encoding all the looking at words blah blah blah actually disappears after a few layers and there is a the emergence of Concepts higher higher level constructs there and attention is an algorithmics on top of these abstract constructs that can that then gets translated back into a word but it's not that you know you do fancy interpolation between words you actually go in the world of ideas or concepts and then go back to that and there are many signs that this is happening uh like for example the fact that a language model that was published this week can be fine-tuned with not a lot of examples on data that is completely different like protein or tabular data anything like uh things that are very uh away from that domain only if you only take the data and project it to the statistical distribution of the data it was trained on then that model can solve problems at the surpassing the state of the art in that Target domain better than if you train the model on directly on the target domain so the the fact and this the pre-trained model is being pre-trained just by guessing the next word so uh this means that within the model there is a sort of universal in context Computing engine that can figure out what to do with things that is completely yeah dissociated from it's a concept of language itself right so language is kind of a way to learn algorithms and that's it right I just had another bam right in my head like he just blew my mind again just like you did in the last episode like I finally saw it right um that in learning the language it's also at a higher level it's learning some sort of logic about the world and the universe and and that logic may be very it's uh and I'm just guessing but you know how like I don't know if you've ever played around with convolutional neural networks and you look at what the filters are kind of like keying in on and it's oftentimes those filters key in on like the weirdest Parts it's not like you know I I remember seeing a demo where they were like trying to identify uh uh t-shirts that had Superman on it and you'd think you know it would key in on like Superman or like a t-shirt or something like that and it would be like this weird like stretched out color red you know and and that's what the filter was actually picking up on and it wasn't like an actual represent I mean it didn't look like Superman at all uh but what I thought was fascinating is that what it's learning is is like another way of look like imagine if if you know when we look at the world we look through our specific type of eyes but if you looked at the world through like a cat who's got like vertical slits for eyes or he looked at it through a fish who has a different way of looking or a bat who's using Sonar right there's all these different ways to see the world and it seems like maybe these these large language models in training and they somehow see the world in in possibly a very new way uh that then is adaptable very readily which is which is what you're saying is that once that logic is learned regardless of what your original training on you're going to get to that logic and then you can then redirect it really easily that yeah yeah and there are many signs of that yeah for example you know that um GPT I think uh Da Vinci I don't remember if it's Chad GPT and one of these models that we customarily use were derived from code Da Vinci zero zero two not text DaVinci zero zero two in the sense that yeah they were pre-trained on on text and then they went through like predicting code instead of text and then they were back on text primarily right as the primary source of information meaning that whatever we feed to these models it's there to build algorithmic reasoning logic that can run at runtime so that doesn't require further training and then you can kind of plug in if you align the target distributions you can solve problems that were not related to language at all like language probably is one of the ways we express intention and higher order Concepts and by pre-training these language then they become like capable of uh of reasoning yeah um so like going back to our our original question what do we do like how do we yeah work with this algorithmic machines um now that you know we see a lot of um things happening in the market one thing is the emergence of a lot of small startups that use the open API sorry the open AI apis yeah and they're built on top of those right and of course it's a kind of a Shaky Ground because you know you you rely on one one single API but actually more are being created so the goal there is to attract a lot of users there and then differentiate maybe you use call here maybe you use other apis and maybe you use an open source tool maybe use lightning to build or the whole system or to you know put a model in production that does what you want so it's yeah you start from there and then you expand but the offering of models of raw models to start with will actually multiply because it's it's a recipe right you push money in and you know that money will flow out to the other side and this is the best yeah and the recipe is known and so it's kind of in a way it's a race to the bottom right and we I don't I don't know if we already talked about it but yeah I mean it's kind of um since I'm not saying it easy it's easy but it's doable and we know the recipe then yeah the race is on right and so how do we leverage as business owners as or existing businesses or as new startup people new ideas how how do we do that like do we need to train from scratch do we need to fine tune do we use prompting yeah what does this even mean so I think it would be useful for maybe our listeners to go through these options one by one yeah try to understand I'm willing to Guess that we don't need to train from scratch because we can take something that exists and has that logic built in and it's more just a function of just a handful of domain specific trainings and then all of a sudden you've got your your new tool yeah this is what it looks like in the future in the present yes and no like for example open fold uh it's the causing of alpha fold which is an open source initiative to train a language model to predict protein folding and that you typically train from scratch but it's not huge it's a medium-sized kind of model that can predict folding um and in this and and if you train for a scratch you need a lot of data so if you have a lot of data yeah probably fine-tuning heavily or training from scratch is something that you may want to do to kind of distance yourself from the competition until one of these big brains okay adapted to your domains does better than you and then you're in trouble so you kind of need to do both and kind of you know play all your cards there if you don't have a lot of data actually using a huge model and kind of fine-tune it or prompt it on your specific domain actually works better because you have a Reasoner and the Reasoner needs to see a handful of examples to kind of understand what it needs to do like but of course uh we have like a few things that we need to consider right so if you have if you only do prompting then you need to be really smart but your competitive Advantage is in your ability to gain a lot of users real fast but these users so because you know technically what you're doing is not super challenging right because you're cool yeah just prompting and then maybe you're Building A system that is a chain of prompts and so on but uh it's like you know uh then you have to really focus and you always really to focus uh need to focus on the user needs and trying to kind of make your the experience as sticky as possible because your core thing is not the uh the uh how different you will be because you probably won't be a lot different from others it's more your execution and this is valid in general of course yeah um but you if you want to leverage your specific domain then fine-tuning is probably something that you want to look into um where you don't need to spend a hundred thousand dollars or a million dollars but you need kind of to uh inject knowledge in the model that the model didn't have so it will behave in a certain way um that yeah it wasn't it wasn't like having the knowledge before now I want to add one more thing which is as we have already said these models can also now start to reach out to external information so there was a paper this uh this week where uh one was called tool formers uh from meta and um so in reaching out to external information you can already use projects like link chain uh to uh to kind of call an API and then query like I don't know cohere open AI or an open source model to to generate text and within the text you kind of encourage the language model to say okay maybe you you need to like if you need to search for some information that just tell me and I'll search it for you and I give you the answer back and this like very general model kind of know how to do that but they okay but they are kind of mimicking what they learned from from the world from the text and they weren't trained at actually leveraging this this possibility at training time and Tool formers kind of show that it's possible to train models having this interaction at training time it's not entirely clear to me if this is absolutely needed given these reasoners right um yeah it's an open question and um and this could could actually Empower models to actually have knowledge that they can index directly without having the knowledge in their weights right um okay I don't know this is this is kind of the possibilities right yeah so to summarize uh at the most basic level you've got prompting which is basically um is that is that like uh the Constitutional AI where we've yeah we take the output and we kind of we kind of align it to our specific field without actually ever giving it new data we just take what it gives us and then wrap that in you know into some sort of prompt like you know you know can you make this sound happier or can you make this sound more scientific and then have it go back through there and it gives us the output so we're basically training it through our through the prompts that we give it yeah if I have a question summarize that aspect right yeah it's correct maybe there's one step yeah back which is I have a question and if I want the model to generate an answer of in this within a certain context I re-wrap that question in a context which is a prompt and say okay okay maybe the prompt contains you know just come out with a bullet point in legalese speak speak right perfect and then yeah we need more of that yeah and that will will generate the bullets instead of uh free form answer right so we give directions I see to the model yeah and then something that's the most basic approach yeah and this is prompting and based on the on the answer I can also program to say okay now for each of these bullet points I will actually ask the model in the back with my automation to read like formulate a Google search related to that and then I will do the Google search myself paste the results so the first few results back to a model that will summarize it and so I can you know have an interaction with that API that is not really something that the model is aware of but I can kind of Leverage that because the result I will paste it back in the prompt and then I will reach my prompt based on this chain of things of events okay and uh and actions and maybe at the end I will use the model ask the model just to summarize whatever this huge prompt uh contains or that's big problem because huge prompts are not possible yet so so that's sort of like the startup the startup company with zero dollars can do prompting and then the startup company with ten thousand dollars they can start fine-tuning on a on a domain specific subset so that's like option number two is is to take the existing large language model and then start uh giving it feeding it domain specific examples and really kind of doing some real training but on a relatively small set of data that's like exactly startup with ten thousand and then and then so and then the last option that you were describing was then training it possibly from scratch but possibly not because that logic that high level logic is probably already there to begin with to to start using an API and that's maybe the hundred thousand dollar startup or or yeah yeah but you need to have the data set itself right it's interesting because before the emergence of these Foundation models having the data was something that you needed to do like if you were like a medical startup uh you had to have like all the medical records because you would probably yes for sure you can use pre-trained models but uh at the end of the day you would have to and embed all this knowledge since you didn't have a Reasoner in abstract terms it was always input output input output right so you needed to kind of form the all the bulk of the knowledge with your data and uh and now uh having the data it's probably not yes it's super important but it's it's not that important to create products like yes it's important but up to a point and you don't have to be so massively obsessed with certain types of data right so I don't know and it's interesting I was gonna say maybe for just a startup um the point isn't necessary to make the perfect model but to make a proof of concept that you could then sell um and so if you're if you're just getting off the ground the idea is just you know instead of getting all the data you can get a small amount of data have a relatively small outlay and just show a proof of concept of like this is what this is how far we got with this tiny amount of data and relatively little money you know let's uh you know let's get some investors uh excited about this and then you know then we can afford more and you know even more fine-tuning yeah and you know and then connecting it yeah what will get investors excited is users really interested yeah and I think this is a unique time to have users interested because these models unlock possibilities that were not possible before with budgets there but much lower because now they can do more things so you will see a lot of more automation tools you in the hands of you end users and they will be able to like put things together in ways that we we don't like necessarily foresee so having a very flexible platform that can help you do that like be creative and create new things is very and deliver it to to customers it's going to be very very uh important and of course now it's a little plug for lightning but that's exactly the the purpose of our product but apart from that this is in our minds of course it's been in our mind for first quite some time but um this is where like Innovation will happen it's so exciting right um so I I wouldn't get too fixated on on the fact that everybody is running on a few sets of apis things will just explode and it's the composition of different things that will and then there are like uh companies like Microsoft having their Bings or their words in very like in other fields like you know I have my protein folding software from the 2000s right uh if there was a thing and then you know but now they can kind of uh uh make it a lot better with a Quantum Leap the by using these new technologies and what will make the difference there is having the users right so for example Microsoft has Word Microsoft Word everywhere so the moment something Lansing Microsoft Word then people will have it right yeah by default so that's also very that angle is also very interesting it's interesting because what you're saying is that it's it's not really a technical problem right now right you said the recipes in there the recipes in place the large line language models are in place the cost to entry is basically zero because we have we can use these large language models and we can we can just create uh specific queries or or um and and we don't necessarily have to invest a whole lot into it up front what's what it what it's become rather than a technical problem is a business a basic business problem what's your Market who's your audience how are you going to get them to use your product um and it's and it's and it's interesting so it's like how to how to have an AI startup right now 101 uh yeah for sure like it's like it's very interesting if you can um if you go to websites that have prompts and collection of prompts yeah and maybe you know operations you can do with outputs of language models you really understand that there's a new apart from the prompt Engineers that you know I've seen job on Twitter on and other sources you know people posting job ads um for uh prompt Engineers paying them like the salary of a staff senior staff software engineer and the the the things that you know basic coding skills and experience with prompting which is a a new kind of a new uh thing right so you cannot have 10 years experience in prompting right um I've been doing it since November exactly whoa November good job you know that's right I'm pretty sure I have kids like you know yes I want them to learn technology learn how to code and so on eventually um but the the act of coding in the future will be kind of really blurred yeah in the sense that you know yes you will need to code but to build things um you will also need to know this new things and to interact with that um and so I don't know like uh uh the the changing the industry is already coming like uh it's not that now you know coding is not useful anymore I'm not saying that I'm just saying that the barrier for entry for creating products um that are very creative and groundbreaking and so on is a bit lower because it's relatively easier to to prompt the model to do very hard things that things that previously were super hard to achieve and would result in very clunky experiences and how you can then unlock that by just learning how to this new world works so I encourage everyone to kind of go and play with these models and see what they what yeah do with them because it's really like um a bit transformative in the sense that the sense of uh being in front of something that will kind of change the Dynamics and I I I saw yeah uh a thread where you know there was a guy saying um is the first time after these guys were around and you will see like uh the Neanderthal and other like species that got it extinctive right yeah um that homo sapiens probably actually killed along the way but uh it's the first time that we can interact with something that is not a human that we can actually have a conversation with right yeah and it's for sure I'm not saying that these things are have a sense of self or whatever but the end result doesn't change you know it's not so important if they are sentient um and I I don't think they are the point is what is the effect that they exert in us like I don't know if you are sentient yeah you know but the effect you have on me right yeah it's what I experience so at some point it's a question of yeah like is that does that matter so much and what is the impact of that thing even existing in the world so yeah so anyway so so to summarize so we're it's time to wrap up for the day but to summarize it's either either a start your own AI company right now because the time is right and the cost is low relatively speaking or B make sure you're prompting left and right so you can put that on your resume and you'll get a raise hopefully maybe who knows you know I mean how I mean it's got to be the easiest thing ever I I'm gonna start doing it and then and then three it's all about your experience uh and not necessarily on what we Define as being sentient or not it's about what we perceive um and tune in next time we're gonna see how being in Chad gbt if they if they're still like happily married together in two weeks we'll find out I don't know that's a long time Luca I really appreciate it yeah it was great it's a really long time all right okay thanks Josh talk to you later bye
589nCGeWG1w,2023-02-13T05:00:03.000000,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",one hot label Target encoding yeah yeah stack Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about one hot label and Target encoding and they're going to be clearly explained you don't have to worry about the details of scaling your stuff up in the cloud cause lightning will take care of it for you bam this stat Quest is also brought to you by the letters a b and c a always b b c curious always be curious imagine we had this data and we wanted to use favorite color and height to predict if someone loves Troll 2 which is a movie that some people love and some people don't in this case favorite color has three discrete values blue red and green now in theory discrete features like favorite color are fine for most machine learning algorithms but in practice a lot of popular machine learning algorithms including neural networks do not work well with them as a result discrete data are often converted into numerical values before being used for machine learning one popular method for converting discrete variables or features into numbers is to use something called one hot encoding when we have three or more options for a discrete variable and in the case of favorite color we have three options we start by creating a new column for each option in this case that means creating three new columns blue red and green now in the blue column we set the value to 1 if we had blue in the original favorite color column and we set the remaining values to zero likewise for the red column we set the value to 1 the one time we had read in the original favorite color column and we set the remaining values to zero lastly for the green column we set the value to 1 if we had green in the original favorite color column and we set the remaining values to zero note the last column loves Troll 2 is also discrete but it only has two options yes and no so we simply replace yes with one and no with zero and now all of the columns in our new data set are numeric and can be used with algorithms that don't do well with discrete data like neural networks or XG boost bam using one hot encoding to convert discrete data into numeric data works fine when we don't have too many options in this case we only have three options for favored color so we replace favorite color with three new columns but when we have a lot of options for example if we had a column of postal codes and there are 41 683 postal codes in the United States then we would end up replacing the one postal code column with 41 683 new columns which might make the data difficult to work with so when we have tons of options for a discrete variable one alternative to one hot encoding is to Simply assign numbers from low to high to each option so in this case we might set blue to zero red to one and green to two and just like before we could convert Love's Troll 2 to be numeric by setting yes to one and no to zero simply converting the discrete values to random numbers like what we did here is called label encoding and again just like before all of the columns are now numeric and we can run the data through a neural network double bam note one thing that people don't like about using label encoding is that the numbers we use are just arbitrary and some machine learning algorithms will treat the order of the numbers as if they might mean something and that can cause problems for example a decision tree splitting on favored color would be forced to group red and green together or blue and red together simply because of the random numbers we assign to each color so instead of just picking random numbers to represent the options blue red and green we can calculate the mean value of the target the thing we want to predict which in this case is loves Troll 2 for each option for example of the three people that like the color blue only one of them loves troll 2. so the mean value for blue is 1 divided by 3 or 0.33 so we replace blue with 0.33 likewise because only one person likes red and they do not love Troll 2 the mean for red is zero so we replace red with zero lastly because two of three people who like green also love Troll 2 we replace green with 0.67 because we use the target the thing we want to predict to determine what values to replace the discrete options this method is called Target encoding that being said we've only talked about the simplest type of Target encoding a more commonly used version of Target encoding deals with the fact that we only had one person who liked the color red and that means we only used one person to determine the mean value for red and thus we don't have a lot of data supporting the use of zero to replace red in contrast both blue and green have more data three people each supporting the values we use to replace them because less data supports the value we replaced red with we have less confidence that we replaced red with the best value than we have for blue and green so in order to deal with this target encoding usually is done using a weighted mean that combines the mean for a specific option like red with the overall mean of the target which is Love's troll 2. for example in order to use the fancier Target encoding with our data we start by plugging in the mean of the target for blue 1 divided by three then because three people were used to calculate the mean for blue we plug in 3 for n then we plug in the overall main for the Target loves troll 2. 3 divided by 7 because overall three of the seven people love troll 2. now we just need to pick a value for M the weight for the overall mean m is a user-defined parameter or hyper parameter and in this example we set m equal to 2. setting m equal to 2 means we need at least three rows of data before the option mean the mean we calculated for blue becomes more important than the overall mean now we just do the math and get 0.37 so we plug in 0.37 for blue now we calculate the weighted mean for red beep beep boop and we get 0.29 so we plug in 0.29 for red lastly we calculate the weighted mean for green beep [Music] and we get 0.57 so we plug in 0.57 for green bam now let's compare the target encoding when we use the weighted mean to the Target encoding without the weighted mean the target encoding for blue and green are similar to what they were before and this makes sense because we had a relatively large amount of data for both blue and green in contrast with the weighted mean the value for red is much closer to the overall mean than before and this also makes sense because we have so little data for red only one row in a way we can think of the overall mean as our best guess given no data however as we get more data more rows for each option we use the data more rather than our best guess to determine the target encoding note if you're familiar with Bayesian methods this approach may look familiar because a lot of Bayesian methods boil down to calculating a weighted average between a guess and the data as a result some people call this Bayesian mean encoding triple bam note some of you may have noticed that we are using the target the thing we want to predict to modify the values in favored color and doing this sort of thing is a data science no no that we call data leakage data leakage results in models that work great with training data but not so well with testing data in other words data leakage results in models that are over fit the good news is that there are a bunch of relatively simple ways to avoid data leakage or at least reduce the amount of data leakage so that you can use Target encoding without overfitting your model one of the most popular methods to reduce leakage is called k-fold Target encoding so let's go back to the original data set that had blue red and green categories for favored color and talk about k-fold targeting coding note the word fold in k-fold Target encoding refers to splitting the data into equal sized subsets and the K refers to how many subsets we create for example if we did two-fold Target encoding then we would divide the data into two equal sized subsets note because we have an uneven number of rows we just made the subsets as similar in size as possible now to make it easier to keep track of things let's label the first subset a and the second subset B now to Target in code blue in subset a we ignore the target values in this subset in other words we ignore the values for Love's Troll 2 in this subset and instead plug the target values from subset B into the weighted mean equation we start by plugging in the subset b mean of the target for blue zero divided by one because the one person in subset B that likes blue does not love troll 2. then because there is only one person in subset B that likes blue we plug in 1 for n then we plug in the overall mean for the Target in subset b 1 divided by 3 because overall one of the three people in subset B loves troll 2. and just like we did before we'll set m equal to 2. now we just do the math and get 0.22 so we plug in 0.22 for the two rows and subset a with blue now we need to Target and code the one row with blue in subset B so we ignore the target values in this subset and instead plug the target values from subset a into the equation for the weighted mean [Music] then we do the math and get 0.5 so we plug in 0.5 for blue but only in subset B note you may have noticed that the different subsets have different values for blue this is okay because favorite color is becoming a continuous variable just like height and now let's encode the color red in subset a so we ignore the target values in subset a and instead plug the target values from subset B into the equation for the weighted mean now because subset B doesn't have anyone who likes the color red the mean for red is zero and n equals zero the other values are the same as before and we end up replacing red in subset a with 0.33 likewise Green in subset a uses the target values in subset B and turns into 0.42 and green in subset B uses the target values from subset a and turns into 0.67 now that each color and each subset has been encoded we merge the subsets back together and we're done note this process reduces data leakage because the rows do not use their own Target values to calculate their encoding bam now going back to the original data with blue red and green if we set k equal to 7 then we would divide the data into seven subsets now Target encoding the first subset which consists of a single row with favorite color equal to Blue means we ignore its Target value and use the target values from all of the other subsets to calculate the weighted mean likewise encoding the other subsets would use all of the other Target values except their own note when we use all of the target values except 1 to do the encoding it's called leave one out Target encoding a quick scan of the internet shows that some people are successful with leave one out Target encoding and other people are successful with setting k equal to five triple bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the statquest PDF study guides in my book The stackquest Illustrated guide to machine learning at stackwest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on
eJIp_mgVLwE,2023-02-06T05:00:15.000000,"Mutual Information, Clearly Explained!!!",[Music] Mutual ination it's really cool going to check it out now stack Quest hello I'm Josh starmer and welcome to stack Quest today we're going to talk about Mutual information and it's going to be clearly explained I don't want to spend a lot of time scaling up my stuff to work in the cloud I would rather spend my time working on my stuff cuz that's the fun part light in this stat Quest is also sponsored by the letters a b and c a always b b c curious always be curious imagine we had a data set with a lot of variables which are also called features and because we want to simplify the amount of time we spend collecting data we wanted to remove moove some of them thus we want to know how much each variable can tell us about the thing we want to predict in this case we want to predict if someone loves the movie Troll 2 and we want to know which of these variables likes popcorn height Etc play the biggest role in making good predictions in theory we could use something like R squar to see if a specific variable is related to love Troll 2 except r s only works with continuous data and Love's Troll 2 is yes or no and so is likes PopCorn by the way if you're not familiar with r squar you probably should be so check out the quest so when we have a mixture of continuous and discrete variables how do we quantify their relationship to the thing we want to predict well one way to quantify how each variable is related to love Troll 2 is to use Mutual information like R squ Mutual information is a numeric value that gives us a sense of how closely related two variables are bam the equation for calculating Mutual information looks kind of nasty but don't worry we'll go through it one step at a time in a nutshell these two sigmas Greek characters that stand for summation tell us that we're going to do a lot of addition and we're going to be adding up joint probabilities and dividing some of those joint probabilities by marginal probabilities um what are joint and marginal probabilities joint probabilities are just the probability of two things occurring at the same time for example given our data set we can calculate the probability that someone likes popcorn and loves troll too in this case the probability that someone likes popcorn and loves troll too is 3 divided 5 because three of the five people in the data set like popcorn and love Troll 2 in contrast marginal probabilities are just the probability of one thing occurring for example if we just focus on likes popcorn we can calculate the probability that someone does not like popcorn in this case the prob ability that someone does not like popcorn is 2 ided 5 because two of the five people in the data set do not like popcorn likewise we can calculate the marginal probability that someone does not love Troll 2 the probability that someone does not love Troll 2 is 1 divided 5 because only one of the five people in the data set does not love Troll 2 now for any pair of variables like likes popcorn and loves Troll 2 we can keep track of their joint and marginal probabilities in a table in this table the first two columns represent whether or not someone likes popcorn and the first two rows represent whether or not someone loves Troll 2 The Joint probability that someone likes popcorn and loves Troll 2 which we calculated earlier goes in the upper leftand corner of the table table likewise we can calculate the joint probability that someone does not like popcorn and loves Troll 2 and put that in the top of the second column now if we want to know the marginal probability that someone loves Troll 2 we can either calculate it like we did earlier by dividing the number of people that love troll to by the total number of people or we can simply add the joint probability that someone likes popcorn and loves Troll 2 to the Joint probability that someone does not like popcorn and loves troll too and when we do the math we get four / 5 so either way we get the same marginal probability 4 / 5 likewise we can solve for the remaining joint probabilities and plug them into the table and we can solve for the marginal probability that some some one does not love Troll 2 either directly from the data or we can add up the two joint probabilities in the second row in the table either way we get the same result then we can calculate the marginal probability that someone likes popcorn either directly or by adding the values in the column together then we can calculate the marginal probability that someone does not like popcorn now we have a t table filled out with all of the joint and marginal probabilities associated with likes popcorn and loves Troll 2 bam note the marginal probabilities are all in the margins of the table so that's where the name comes from small bam anyway now that we have this table of joint and marginal probabilities for likes popcorn and loves Troll 2 we can calculate their Mutual information by plugging the joint and marginal probabilities into this equation The Joint probabilities go here and here and the marginal probabilities both go here note the two summations ensure that we include all possible combinations of the variables likes popcorn yes and no with L's Troll 2 yes and no for example we start by plugging in the joint and marginal probabilities for where both popcorn and L's Troll 2 equal yes then we add a term where popcorn is yes and lov's Troll 2 is no then we add a term where popcorn is no and L Troll 2 is yes and lastly a term where popcorn is no and L's Troll 2 is no now that we have expanded this double summation by adding terms for for all possible combinations of popcorn and L's Troll 2 we plug in the joint and marginal probabilities that we calculated earlier for example The Joint probability that someone likes popcorn and troll to is 3 / 5 so we plug that in and then we plug in the marginal probability that someone likes popcorn 3 divided 5 and the marginal probability that someone loves troll two for divided 5 then we just plug in the joint and marginal probabilities for all of the other terms note before we move on I want to point out that the log function that we use in this equation can be any base however the default log function for most machine learning and most programming languages is the natural log so that's what we use here anyway when we do the math the whole thing is equal to 0.22 so the mutual information between likes popcorn and L's Troll 2 is 0.22 we can then compare this Mutual information value 0.22 to the mutual information values for other variables to decide which ones are the most useful double bam oh no it's a super technical note if you look carefully at this second term in the equation you'll see that if we divide 0 by 5 then we'll get a zero here and we'll get a zero in this numerator which means the whole term is 0 * the log of 0 and technically the log of 0 is not defined the good news is that even though the log of 0 is not defined as X gets close to zero x * the log of x equals 0 so the second term is just equal to zero tiny bam now let's go back to the raw data and see what happens when we change it so that likes popcorn is always yes in other words let's see what the mutual information is now that likes popcorn is always yes and never changes so just like before let's fill out the table of the joint and marginal probabilities and now let's use the table to calculate the mutual information first let's expand the summation so that we have a term for all possible combinations of likes popcorn and loves troll to then we plug numbers into each term and when we do the math we get zero in other words when likes popcorn never changes it can't tell us anything about what's happening in L's Troll 2 and in general when at least one of the two columns never changes then the mutual information will be zero because something that never changes can't tell us about something that does now let's go back to the raw data and see what happens when we change it so that likes popcorn is yes when L's Troll 2 is yes and likes popcorn is no when L's Troll 2 is no in other words now both columns change but they change in the exact same ways so let's fill out the table of the joint and marginal probabilities and use the table to calculate the mutual information ploo and when we do the math we get something close to but not exactly 0.5 so given this data set where the two columns change and they change in the exact same ways the mutual information is close to 0.5 and this value is larger than 0.22 which is what we got when we calculated the mutual information with the original data and remember both columns changed in the original data but not in exactly the same ways so when both columns change and the changes in one tell us more about what's going on in the other then the mutual information value is larger now let's go back to the raw data again and and see what happens when we change it so that likes popcorn is yes when Love's Troll 2 is no and likes popcorn is no when Love's Troll 2 is yes in other words let's see what the mutual information is when both columns change but in the exact opposite ways so we fill out the table of the joint and marginal [Music] probabilities and plug the numbers into the equation for Mutual information and when we do the math just like when we had both columns change in the exact same ways we get something close to but not exactly 0.5 in other words when both columns change it doesn't matter if they change in the exact same or exact opposite ways they both give us the same Mutual information and the changes in one column can tell us exactly what is changing in the other dou bam so far we've seen how to calculate Mutual information for two discrete variables likes popcorn and loves troll to how do we calculate the mutual information when we have a continuous variable like height when we want to calculate the mutual information and one or more of the variables as continuous then we simply create a histogram of the continuous values and then we can use each Ben in the histogram as a discrete category and use the discrete categories to calculate the joint and marginal probabilities note because we have three bins in the histogram we have three columns of joint probabilities in the table and that means that when we expand the equation for Mutual information we end up with six terms because we have Troll 2 equals yes and Troll 2 equals no for each of the three bins then we just do the math and get 0.22 and since we got 0.22 for both likes popcorn and height we can use either one triple bam one last note if you're familiar with how entropy is used in data science and if not check out the quest then you may have noticed that the equation for Mutual information has a lot in common with the equation for entropy they are both sums of probabilities times logs these equations are similar because Mutual information can be derived from entropy and in a sense Mutual information tells us how on average the surprise or change we see in one variable is related to the surprise or change in another for example do you remember when we calculated the mutual information and likes popcorn never changed well when we have something that never changes then the surprise is always zero so it's no surprise that the mutual information is also zero because the changes in L's Troll 2 have nothing to be related to in contrast when the surprise is greater than zero for both variables and the changes are related then the mutual information will also be greater than zero and reflect how much the changes are related bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stat Quest PDF study guides and my book the stat Quest Illustrated guide to machine learning at stack quest.org there's something for everyone hooray we've made it to the end of another exciting stack Quest if you like this stat Quest and want to see more please subscribe and if you want to support stat Quest consider contrib to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
e9U0QAFbfLI,2023-01-30T05:00:13.000000,"Cosine Similarity, Clearly Explained!!!",I think that the cosine is fine so we'll calculate it between this and that line stat Quest hello I'm Josh starmer and welcome to stat Quest today we're going to talk about the cosine similarity and it's going to be clearly explained when you build them and deploy in awesome stuff and it's really fast you're using light now hoor ay this stat Quest is also brought to you by the letters a b and c a always b b c curious always be curious imagine we saw these sentences about the 1990 hit movie troll 2. by I it's pretty easy to see that the first three sentences are similar to each other they all Express a positive feeling towards the movie in contrast this last sentence is from someone who does not like troll 2. so when we don't have many sentences it's pretty easy to see which ones are similar to each other and which ones are different however what would we do if we collected all of the Twitter traffic for the last month how would we determine similarities and differences in the tweets in this case we can no longer rely on doing things by eye and instead have to get a computer to do it this is where the cosine similarity comes in handy the cosine similarity is a relatively easy to calculate metric that can tell us how similar or different things are bam to get an understanding of how the cosine similarity Works let's start with a super simple example here we want to know how similar the phrase hello world is to hello so the first thing that we do is make a table for the words that appear in the phrases hello and world in the first phrase hello world we see the word hello wants and we see the word world once in contrast in the second phrase hello we only see the word hello one time and we don't see the word world at all now given this table of counts for each word in each phrase we can create a two-dimensional graph that has the number of times we saw the word hello on the x-axis and the number of times we saw the word world on the y-axis now since the first phrase hello world had one occurrence of each word we can plot a point for it in the center of the graph in contrast the point for the second phrase hello is on the x-axis at 1. now if we draw lines from the origin of the graph 0 0 to the points we can see that there is a 45 degree angle between the two lines and the cosine of a 45 degree angle the cosine of 45 degrees is 0.71 and thus the cosine similarity between the phrases hello world and hello is the cosine of 45 degrees which equals 0.71 bam bam note currently the second phrase is hello but what if it were hello hello hello now the table has a three for hello since it's in the phrase three times but we still have a zero for the word world in this case the point on the graph representing hello hello hello would be further out on the x-axis however the angle between the two lines would still be 45 degrees and thus the cosine similarity would be the exact same as what we got before the cosine of 45 degrees equals 0.71 in other words the cosine similarity is determined entirely by the angle between the lines and not by the lengths of the lines bam now if both phrases are hello world then they are exactly the same and we end up plotting the dots that represent the phrases on top of each other creating a kind of Swampy green color and the angle between the lines from the origin and the points would be zero degrees and the cosine similarity is the cosine of zero degrees which equals one so when both phrases are exactly the same the angle between them will be zero degrees and the cosine similarity is one in contrast if the phrases don't have any words in common like here where the first phrase is hello and the second phrase is world then the angle between the two phrases will be 90 degrees and the cosine similarity will be the cosine of 90 degrees which equals zero to summarize when two phrases have absolutely nothing in common the cosine similarity is zero and when the phrases are the exact same the cosine similarity is one and when there is some overlap between the two phrases but they are not exactly the same the cosine similarity is between 0 and 1. double bam now the way we've been Computing the cosine similarity step one make a table of the word counts step two plot the points step three figure out the angle and lastly step four calculate the cosine of the angle is pretty tedious the good news is that there is a relatively simple formula that can be computed super quickly on a computer to calculate the cosine similarity directly from the table of word counts dang that equation looks complicated don't worry Squatch we'll go through it one step at a time thanks these sigmas are shorthand for adding up a bunch of stuff the I is short for index and is used to keep track of the word we are working on I starts out set to one the first word in the table in this case the first word is hello and N is the number of different words in the phrases in this case n equals 2 because we have two words hello and world lastly A and B refer to the two phrases in this case let's let a be the first phrase hello world and B be the second phrase hello now when I equals 1 we plug in the word counts for the first word hello and when I equals 2 we plug in the word counts for the second word world and when we do the math we get 0.71 which is the same value we got when we took the cosine of 45 degrees bam note when we only have two different words in our table like we do here then we can easily plot the points on a two-dimensional graph because we can put one word like hello on the x-axis the First Dimension and the other word world on the y-axis the second dimension but when we have phrases with more than two different words then we need more than two Dimensions to plot the points for example if we wanted to calculate the cosine similarity for these two phrases where the first phrase I love Troll 2 is phrase a and the second phrase I love Jim Kata is phrase B then the table will have five different words in it and that means we would need a five dimensional graph to plot the points and I have no idea how to draw five dimensional graph the good news is that this is another way that having the equation for the cosine similarity can come in handy rather than worry about how to draw a five-dimensional graph we just plug the numbers into the equation then we do the math and we get 0.58 so the cosine similarity between these two phrases is 0.58 triple bam in summary the cosine similarity is a relatively easy to calculate metric that can tell us how similar or different things are note if you haven't already seen it Jim Kata is an awesome 1985 movie about a gymnast he uses the parallel bars to defeat his enemies in Mortal Kombat bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stackquest PDF study guides and my book The stackquest Illustrated guide to machine learning at stackquest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stat Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on
RHGiXPuo_pI,2023-01-24T21:21:53.000000,Long Short-Term Memory with PyTorch + Lightning,long short term memory with pie torch plus lightning is cool stat Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about long short-term memory with pi torch plus lightning raise yourself for awesomeness and everything being just a little bit easier Lighting in yeah this stack quiz is also brought to you by the letters a b and c a always b b c curious always be curious also I want to give a special bam to one of my co-workers at lightning Adrian vouchley who helped me create this tutorial note this stack Quest assumes that you were already familiar with long short-term memory if not check out the quest this stack Quest also assumes that you have already seen the stat Quest introduction to coding neural networks with pi torch plus lighting if not check out the quest lastly you can download all of the code in this statquest for free the details are in the pinned comment below in a stat Quest on Long short-term memory we had stock prices from two different companies company a and Company B on the y-axis we had the stock value and on the x-axis we had the day the value was recorded note if we overlap the data from the two companies we see that the only differences occur on day one and on day five on day one company a is at zero and Company B is at one and on day five company a returns to zero and Company B returns to 1. on all of the other days Days 2 3 and 4 both companies have the exact same values given this sequential data we wanted an lstm to remember what happened on day one in order to correctly predict what will happen on day five in other words we ran the sequential data from Days 1 through day four through an unrolled lstm to predict the values for day 5 for both companies so for company a we initialize the long and short-term memories on a pre-trained lstm unit and then unrolled the lstm unit to accommodate the sequential data from the first four days and the final short-term memory is 0.0 was the output from the unrolled lstl and that meant that the output from the lstm correctly predicted company A's value for day 5. likewise we unrolled the lstm unit to accommodate the sequential data from the first four days of data from Company B and the final short-term memory 1.0 was the output from the unrolled lstm and that meant that the output from the lstm correctly predicted company B's value for day 5. now that we've seen how an lstm unit Works in theory let's code it with pi torch plus lightning in this stack Quest we're going to code this lstm unit from scratch that means we'll make the tensors for all of the weights and biases and then we'll code the first stage that determines what percentage of the long-term memory the lstm should remember and then we'll code the second stage that creates a new potential long-term memory and determines what percentage should be remembered and then we'll code the third stage that creates a new short-term memory and determines what percentage should be remembered then we'll use the data from company a and Company B to train every single weight and bias and along the way we'll learn some cool tricks that lightning gives us to make training easy and fun lastly we'll replace our homemade lstm with the nn.lstm function from PI torch and we'll use what we learned the first time to quickly and easily train the lstm bam first just like always we import torch to create tensors to store all of the numerical values including the raw data and the weights and biases then we import torch.nn to make the weight and bias tensors part of the neural network and after we learn how to build an lstm by hand we'll also use it to try Pi torch's lstm function and we also need torch.nn.functional for the activation functions then we import atom to fit the neural network to the data note in the previous Pi torch statquest tutorials we've used stochastic gradient descent SGD atom is a lot like SGD but not quite as stochastic as a result atom tends to find the optimal values faster than SGD now we import lightning as L to make training easier to code and that means we need tensor data set and data loader from torch.utils.data bam now let's build train and use this long short-term memory unit when we create a neural network in pi torch we always start by defining a new class now because we're coding our own lstm by hand we'll call this lstm by hand and in order to make training super easy We'll Inherit from lightning module then just like we always do we create an initialization method for the new class this class will create and initialize all of the weight and bias tensors that we need to implement an lstm unit then we will create a method called lstm underscore unit that does the lstm math then we create a new method called forward that makes a forward pass through the unrolled lstm unit then we'll create a method to configure the optimizer we want to use lastly we'll need a method called training underscore step to calculate the loss which like before will be the sum of the squared residuals we'll also use training underscore step to log the progress we're making during training this will help us determine if we have done enough training and can stop let's start by coding the init method the first thing we do is call the initialization method for the parent class lightning module this gives us access to some methods that we'll use later now we need to create and initialize the weights and biases for the network however unlike the previous tutorials we've done this time we're going to use a normal distribution to randomly select an initialization value for each weight for example given this normal distribution with mean equal to zero and standard deviation equal to one will use it to Generate random numbers for the weights the shape of this curve shows that there is a relatively High likelihood of generating random numbers close to zero and a relatively low likelihood of generating random numbers far from zero now in order to use this normal distribution to Generate random numbers for us we create a new tensor called Main and set it to 0.0 the mean of the normal distribution and we create a tensor called STD and set it to 1.0 the normal distributions standard deviation now we create a parameter for the first weight in the lstm and use torch.normal to initialize it to a number randomly generated from a normal distribution with mean equal to zero and standard deviation equal to 1. when I did this my first weight was randomly set to 0.34 then the second weight was randomly set to 0.13 note for both weights we set requires underscore grad to true because we want to optimize them now we create a parameter for the first bias and we'll initialize the bias to zero instead of a random number however we still want to optimize it so we set requires underscore grad equal to true likewise we create and initialize parameters for all of the other weights and biases in the lstm bam now that we have all of the parameters initialized we're ready to use them to do the math inside the lstm unit we'll start by defining a method called lstm underscore unit which takes an input value the current long-term memory value and the current short-term memory value now we can use the parameters we defined in the init method to determine what percent of the long-term memory should be remembered in the first stage of the lstm so we create a variable called long remember percent and then we multiply the short-term memory by its Associated weight and then add that to the input value multiplied by its Associated weight and then we add that to the bias and finally run the whole sum through a sigmoid activation function now let's code the second stage in the lstm where we create a potential long-term memory and determine what percentage should be remembered first let's calculate the percent of the potential long-term memory that we want to remember just like before we multiply the short-term memory and the input by their corresponding weights and then add both terms to the bias and then run everything through a sigmoid activation function likewise we calculate the potential long-term memory except this time we run everything through a tan H activation function now we can update the long-term memory by first scaling it to the percentage that we are supposed to remember and then adding a percentage of the potential long-term memory and that gives us a new or updated long-term memory that we save in updated long memory now let's code the third stage of the lstm where we create a new short-term memory and determine what percentage is sent to the output just like before we calculate the percentage to remember and we use that percentage to scale a potential short-term memory the tan h of the updated long-term memory to create a new short-term memory lastly we return the updated long and short-term memories in summary lstm underscore unit contains the code to do all this the first stage calculates the percentage of the long-term memory to remember the second stage creates a new potential long-term memory and determines what percentage of it to remember then we update the long-term memory then in the third stage we create a new short-term memory and determine what percentage to remember and lastly we return the updated long and short-term memories bam now that we're done writing lstm underscore unit which does the lstm math we can create the forward method to make a forward pass through the unrolled lstm for the forward method the input is an array that contains the first four days of stock market values from either company a or Company B we start by initializing the long and short-term memories to zero then we put the four days worth of stock market values into individual variables one per day now given the value for day 1 we can pass the variables day one long underscore memory and short underscore memory to lstm underscore unit to do the math and lstm underscore unit will return the updated long and short-term memories now with the stock value from Day 2 and the updated long and short-term memories we call lstm underscore unit again and lstm underscore unit will return the updated long and short-term memories then we call lstm unit with a value from day 3 and the updated long and short-term memories and then we call lstm unit with the value from day 4 and the updated long and short-term memories lastly we return the final short-term memory as the output now that we have the forward method which can make a forward pass through an unrolled lstm we're ready to configure the optimizer and configuring the optimizer in this case atom is so easy we can just replace the pseudo code with the real code now let's talk about the training underscore step function which we'll use to calculate the loss and log the training progress the training step method starts out exactly like what we used in the statquest introduction to coding neural networks with pi torch plus lightning it takes a batch of training data for one of the two companies and the index for that batch and then uses the forward method to make a prediction with the training data and then calculates the loss which in this case is the sum of the squared residuals the big difference now however is that we are logging the loss so that we can review it later when we call the log method which is part of the lightning module that we are inheriting from then lightning will create a new file in a directory called lightning underscore logs and store whatever we want to keep track of in it in this case we're storing the loss using the label train underscore loss now that we know how to keep track of the loss after each training step it might be cool if we can also keep track of the predictions for company a and Company B first we can use label underscore I to figure out which company we just made a prediction for if label underscore I is 0 then we just made a prediction for company a and we can keep track of that predicted value output underscore I by logging under the label out underscore zero and if label underscore I is not zero then we just made a prediction for Company B and we can keep track of that predicted value output underscore I by logging under the label out underscore one now that we've logged everything we want to keep track of we can have training underscore step return The Laws bam now at long last we've made it through all of the code needed to create an lstm by hand we created and initialized the weight and bias tensors in the init method we did the lstm math in lstm underscore unit we then wrote the forward method to make a forward pass through the unrolled lstm configured the atom Optimizer with configure underscore optimizers and last but not least calculated the loss and logged our progress with training underscore step bam now let's use the new class we just wrote to create a new lstm that we'll call model and just for fun let's look at the initial predictions made with random weight values so we start by just printing out a nice little note that says what we want to do and then we use the model to make and print a prediction for company a by passing it a tensor containing the values from days one through four and remember just like we saw in the previous tutorials the model will return a tensor that contains the value we want the prediction as well as a gradient so we have to call detach to strip off the gradient and we see the prediction for company a negative 0.04 isn't horrible and is relatively close to the observed value zero hooray now we make and print out the prediction for Company B by plugging in its data for days one through four and in contrast to what we saw for company a the prediction for Company B negative 0.04 is horrible compared to the observed value 1. so given that the prediction for Company B was horrible we need to train the model so we start out by creating the training data called inputs from the values from days one through four from both companies in this case the training data consists of two arrays the first array contains the values from Days 1 through 4 from company a and the second array contains the values from days one through four from Company B then we wrap the arrays in a tensor and save it then we create a tensor called labels and the labels are what we want the lstm to predict for each company in this case we want the lstm to predict zero for company a and one for Company B now we combine the inputs and the labels into a tensor data set called data set and then we use the tensor data set to create a data loader called Data loader remember data loaders are super useful when we have a lot of data because one they make it easy to access the data in batches two they make it easy to shuffle the data each epoch and three they make it easy to use a relatively small fraction of the data if we want to do a quick and dirty training for debugging okay now that we have our training data wrapped up in a data loader we are ready to optimize the weights and biases we start by creating a lightning trainer called trainer and tell it to train for at most 2000 epochs which means we will do back propagation for every weight and bias using the data from both companies for two thousand times at most now we call the trainer's fit method and pass it the lstm called model and the training data called Data loader hey Squatch this might take a minute to run so if you need a snack now would be a good time to get one good idea Norm anyway once it's done we can print out the predictions just like we did before and the prediction for company a 0.43 is pretty bad since it should be zero this prediction is actually worse than what we started with [Music] the good news is that the prediction for Company B is while not awesome a little better than before so it seems like we need to do more training however since company A's prediction got worse it would be nice if we could be confident that more training will actually improve both predictions the good news is that we wrote the loss and the predictions to log files in training underscore step and that means we can use tensorboard to draw cool graphs that tell us what happened during training and give us a sense of whether or not we should try to train more to launch tensorboard to draw graphs from the log files go to the file menu and select new and from the sub menu select terminal Now navigate to the directory that contains lightning underscore logs once there copy this command tensorboard dash dash logder equals lightning underscore logs and paste it into the terminal after you hit return tensorboard will start and print a URL at the bottom copy this URL click to open up a new tab and paste the URL and after you hit return you'll see the graphs that tensorboard Drew note in order to see all of the graphs you have to click on the tabs anyway the graph on the left shows the loss values on the y-axis for each step in training on the x-axis note each Epoch consists of two steps one for the data from company a and another for the data from Company B as we can see the loss went down a lot at the start and then kept going down for the rest of training but at a lower rate now at the end of training the loss hasn't flattened out and looks like it is still going down and that suggests that maybe we should do more training the graph in the middle tells us about how the predictions the output values for company a changed during training the goal for company a is to predict zero and we started out with an okay prediction but then the prediction got really bad after about 1 000 steps but it looks like the prediction for company a has been improving ever since and at the end of training the prediction hadn't flattened and looks like it could keep going down and that suggests that maybe we should do more training likewise the graph on the right the prediction for Company B suggests we should keep training because it looks like it could get closer to the goal value one so let's do one thousand more epochs and see if things get better one cool thing about using lightning is that we can add more training epochs without having to start over this is because lightning saves checkpoints and they allow us to add additional epochs at any point during training since we want to add additional training epochs to where we left off we first get the path to the most recent checkpoints from the lightning trainer we named trainer where best underscore model underscore path refers to the most recent checkpoints and we save that path in a new variable called path to best checkpoint now we create a new lightning trainer which we will call trainer just like the old one however unlike the old one we set max epochs to 3000 because we want to add 1 000 more epochs to the 2000 we have already done and now we call the trainer's fit method and just like before we pass in the lstm called model and the training data called Data loader however now we also set checkpoint path to the last checkpoint so we can start training right where we left off and that's all we need to do to add more epochs to training bam once training is over we can print out the new predictions just like before and we see the prediction for company a is getting closer to zero which is good and the prediction for Company B is getting closer to one which is also good we can also reload the tensorboard browser page to see the updated graphs of the log files and we see that the loss is still going down which is good the prediction for company a is getting closer to zero and the prediction for Company B is getting closer to one lastly none of these graphs have flattened out suggesting that the predictions might improve if we do more training so let's train for 2 000 more epochs first we get the path to the most recent checkpoint and then we create a new lightning trainer and set max epochs to 5000 because we want to add 2 000 more epochs to the 3000 we have already done and then we call the fit method with the lstm the training data and the path to the most recent checkpoint and when we print out the new predictions we see that the prediction for company a is really close to zero and the prediction for Company B is really close to one bam now we reload the browser page with tensorboard and we see that each curve has flattened out suggesting we've done enough training so we're all done training double bam now that we know how to create an lstm from scratch and then train it until it makes good predictions let's learn how to make our lives a lot easier by just using the pi torch function nn.lstm so let's create a new class called lightning lstm that inherits from lightning module now just like always we Define the init method and just like always let's call the parents init function however this time all we have to do to create an lstm is called nn.lstm input size refers to the number of features or variables we have in the training data in this example we only have one feature the value for each company so we set input size equal to 1. hidden size refers to the number of output values we want and in this example we just want one output value the prediction for day five note although we are using the output from the lstm to make the predictions it is common to feed the output from the lstm into the inputs of another neural network like this one in this case the neural network has three inputs so if we wanted to connect an lstm's output to it we would need three output values so in this case we would set hidden size equal to 3. and nn.lstm would add extra weights and biases to the lstm so that it could create three outputs anyways this is all we need to do in the init method the forward method is also a little bit simpler we start by transposing the input the values from one of the companies for days one through four from being a single row to being a single column with View we start by specifying how many rows we want and in order to create a single column with four values we want one row per value however instead of just putting a 4 here which would work just fine with our data we can put Len input instead which will create one row per data point regardless of how many there are then we specify how many columns we want and since we only have one feature and only want one column we set that to one then we pass the transpose data to the lstm and save the output values as lstm underscore out note lstm out contains the short-term memory values from each lstm unit that we unrolled in this case that means it has four values because we needed to unroll the lstm four times for each of the four input values so we extract the prediction from the last lstm unit which is the last element in the array by using negative 1 as the index lastly we return the prediction now in theory we could leave configure underscore optimizers exactly the way it was before however in order to demonstrate how quickly atom can Converge on optimal weights and biases we've set the learning rate to 0.1 which is much larger than the default value we used before 0.001 lastly the training step method is the exact same as what we had before it calculates the loss and logs our progress now we've made it through all the code needed to create an lstm using nn.lstm we created and initialized the lstm unit in the init method with nn.lstm made a forward pass through the unrolled lstm with forward configured atom by setting the learning rate to 0.1 in configure underscore optimizers and last but not least calculated the loss and logged our progress with training underscore step now we're ready to create the model and print out the predictions using the initialized weights and biases because you never know you might get lucky and because both of the predictions are pretty far from ideal let's train the model so we create our lightning trainer with ldot trainer only this time because we increase the learning rate which means we take bigger steps towards the optimal weights and biases each Epoch we set it to only do 300 epochs also by default lightning updates the log files every 50 steps however since we're only doing 300 epochs we'd only update the log files 12 times and with just 12 points on our graphs and tensorboard they would look lame so in order to update the log files more often and draw graphs that are awesome we tell the trainer to update the log files every two steps or every epoch now just like before we just call the fit method and pass it the model and the training data and print out the predictions using the optimize weights and biases and now after training we see that the predictions for both companies are really good the prediction for company a is super close to zero and the prediction for Company B is close to one lastly we can view the log files with tensorboard by refreshing the browser however because we trained our handmade lstm for 5000 epochs but we only train the nn.lstm for 300 epochs will make sure that only version 3 log files the most recent log files are displayed so that the x-axis epochs is scaled properly anyway these are the graphs we end up with each graph shows that the results have stabilized because each curve has flattened out and that suggests that even though we only did 300 epochs we're done training triple bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stackquest PDF study guides and my book the stack West Illustrated guide to machine learning at stackquest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on
1xBB8iAXxds,2023-01-24T16:00:08.000000,"The AI Buzz, Episode #2: Big data, Reinforcement Learning and Aligning Models",hello welcome to the AI Buzz where we talk about the latest trends in AI I'm Josh charmer host of the YouTube channel stat quest with Josh charmer and a lead AI educator at lightning AI and I am lucantiga CTO at lightning AI so Luca last time we had our very first episode and we got some interesting comments the big question is are we overselling these techniques what do you think yeah I mean yes and no right um there I see both sides of the coin one side of the coin is that uh the moment you refer to these techniques as being thinking about stuff or uh you know reasoning then you have expectations that go from all computers are stupid they can do two plus two equals four and everything you know that you can instruct them to do but nothing else to oh my God now uh their human level uh but you know but at the same time and now we're gonna have Skynet yeah exactly right so um and I you can see you you go uh even like uh big proponents of different AI approaches are saying oh yeah these things are just bobbling out nonsense and oh look at this like quiz it fails and I can tell you yeah probably you know a lot of people that I know would not get it right the first time you know and this is you know uh Sam is the brother of the uncle of the mother or somebody and so yeah it would take a bit of reasoning to go through there right no um but jokes aside I would say that um something has clearly changed and it hasn't changed over overnight so we've talked about charging PT with enthusiastic notes but we're not here about to talk about you know open AR cha GPT they've been you know doing groundbreaking things as long as as well as many others um maybe for the first time in history we have seen or I have seen uh at least experienced people that are not in AI that are not even in science they're using uh cha GPT or things so and a big model let's say a big model for things uh I was talking to a guy who was in a in HR and he said yeah it's nice because I can scan through many uh many CVS and then of course he would like review them but he can have like a summarization uh really easy and you know to create a blurb about each candidate he will then check but he is having a great time doing that right so they're naturally interacting with it and so yeah I think we're at the cusp where you know people are starting to notice the value add and this hasn't happened before there were like in the uh it was AI for AI people or AI for scientists that knew that got into AI but now for the first time we have something that we you can talk to and instruct them to do something for you in a way that is very direct it's not mediated by a product you are interacting directly with it yeah for example like you know Google Maps so or apple Mouse right was uh uh powered by some sort of uh machine learning models for traffic and so on so we've we are used to interact with ML models but they're always mediated by a software right now that's where yeah we've been exposed to the raw more or less raw and we'll talk about that in this episode model yeah and we're seeing humans interact with it and extract value beyond what the model was created for which is I think yeah more or less the first time a big thing for me so uh for me you know when you do a Google search or I do a Google search I I don't I don't know if this is the way it is for you Luca but for me I feel like I have to sort of think in Google right I I can't just type in whatever I want and find what I need I have to like carefully craft my search criteria you know and make sure I'm excluding certain terms because I know that's just going to bring up a bunch of garbage or if I can come up with the most specific terminology ever you know it's like doing a Google search I mean I've been doing it for so long that it's second nature and I don't think about it but but there's a lot of I mean I mean and this sounds crazy but it's true there are a lot of people that actually don't know how to make an effective query for Google right and they end up with just pages of garbage and one thing that's fascinating about chat gbt and these and these types of tools is that for the first time ever I I can actually switch off and it actually feels weird like well the first time I use Chad GPT I was still using sort of like Google query style you know where I you know I it wasn't natural language I was just focusing on specific terms that I thought were going to give me and I was like no I don't have to do it that way I don't have to think a different way just to get what I want out of us out of search or a query and so what I wrote was uh you know give me an outline for a book about deep learning because I want to write a book about deep learning um and it gave me a fantastic outline I mean it was it was it was like oh that's actually the book I want to write you know I I now have to draw pictures of stat Squatch and my normal saurus and come up with examples and fill in the gaps but it was like I I didn't have to go like I don't know it was just the way the way the query was so natural and I I feel like what you were saying earlier not to not to dominate the conversation but what you were saying earlier about how this this is a big change because all of a sudden it's not just gonna be AI for AI people it's going to be AI for everyone because everyone's gonna be like oh this is actually easier than using Google um this kind you know just talking in a very natural way um you know I think it's transformative because like even with Siri like if I try to do anything with Siri I feel like I have to talk in a very specific word order and I and I can't just I can't say please and I can't say what I would normally say if I was talking to a person uh and that's for the first time in my life that's that's gonna that feels like it's we're changing on that and it's becoming more natural and more fluent yeah yeah yeah and if you think about the fact that um you know these kind of models I mean GPT or what powers Chaturbate is not the only model that is trained at that scale um but there there are other possibilities for people to create systems that will behave like uh charging PT in the future so the way I see it is not I've been having these conversations about oh now you know open a Yahoo Dominate and you know they've done it nobody else is uh going to be able to do it um but I I want to kind of take some time here to kind of dissect what a system like that is um is made of okay and so that we can go through a bit of the pieces there are many like uh many documents online that describe it but I think it's maybe easier to just talk about that um there are a few ways in which you can build a system like this and um but they're they're more or less they follow today nowadays they follow the same sequence so charge ability is not the only one there's one from entropic that is coming out there will be many others so you start with well yeah big load of data um and when it's tax data it's a few tens of terabytes so it's not a reasonable right and um it's not unreasonable where does it come from uh there are many different data sets that are pulled together for GPT or gpt3 in fact um the which is kind of the basis for cha GPT then was that updated at some point but essentially it's uh scraped Data Book data uh code okay and and in many languages as well there are a few open data sets that you can actually have access to one other data set that has been used in the in building open source alternatives to gpt3 like GPT Neo X or gptj by a year later AI for example use the pile it's called the pile and again it's a it's a big data set of decks that has been scraped and curated um is okay uh like 800 gigabytes so it's a bit smaller than the totality of the data set that GPT is being trained on but we're kind of reaching that state right so we can easily imagine that Humanity has a whole well you know assemble the data uh there will be uh needed but that's not the only thing that is needed and um so okay with this you can take a simple Computing model that we uh touched based on the last time which is in this case it's GPT um it's a Transformer model that can be expressed in not a lot of python lines [Music] uh and then of course to make it scale up to many hundreds gpus and so on uh then you need a lot of like engineering on top of that but this is something we will tackle yeah some future episode uh the again the whole AI field is evolving in the direction where it won't be so hard uh to write this kind of system and uh and so the part the first part of the recipe is pre-training which is and usually usually this is done by masking words and say okay you have this one terabyte or 10 30 40 terabytes of text I'm gonna feed you with sentences and then I'm going to mask the next sentence um for okay or in a sentence so I'm gonna present you with just one word two words and then a mask in the third uh three words I'm asking the fourth this is a is called causal masking um and I'm gonna ask you to to to guess the next word um and this is what makes GPT what it is uh so uh okay and all the other Alternatives and we touched based uh the last time how this like very simple pre-training modality will allow you to kind of go and uh put a piece of a sentence and have the model complete that sentence in ways that are creative but they they take into account of what came before yeah um and this is the the first step the second step is if I want something that I want to have a conversation with I need that not to complete my questions but I needed to answer so in by providing um the model the raw model with pairs of questions and and answers I can fine tune it so maybe with a with a data set of a few tens of thousands pairs so not a lot of them I can condition the model uh to to do supervised in a supervised way to uh to produce answer when when they're posed the question and so in in a certain way yeah go ahead are those question answer pairs are those um are those human curated are those created um by people are they these are usually human curated but they are extracted but I think in the future I will maybe we can touch upon this um I think it's very reasonable to assume that they can be produced by a different model or the same model uh and uh the the important point is that they are they come in pairs so that the continuation since these models are used to complete things they are biased to complete something that looks like a question with something that looks like an answer so the task is kind of similar right generating text after yeah a prompt but then did this text is conditioned to look like an answer to the previous question more and more um yeah and this phase is just the beginning of uh a process that will then improve the the model using something that has been around for a while which is called reinforcement learning and in reinforcement and learning uh instead of thinking about a model that is uh I'll make it super synthetic but instead of like having a model that produces a classification for example you're training something called the policy which is a probability the model that outputs a probability distribution of taking an action so that you can then sample the action from this probability distribution for example uh you could say that I'm going I'm driving I'm going to go left or right and at this point given what my observed environment does is I'm going to go left with a 70 probability and then I'm going to sample that probability I'm going to make the action it could be like what I'm sampling will be more more likely less than right but that's okay and then based on the action I took um I could have a certain outcome that translates into a number that is given to me and the sequence of act the sequences of actions and so uh hence the probability distributions that are presented to me to take those actions will will result in a something called the reward which is the price I get the numerical price I get so the sequence of actions will accumulate uh uh the in in the nest something called an episode and my job is to take the to find out what the probabilities are in such a way that I will make my episode give me the best reward according by sampling the actions from this probability distribution and this is something that has been used to like program agents that like playing in games for example a few years ago I participated to a project where uh we with the company I originally founded a few years ago we can co-developed the AI that went into Moto gp19 the game the actual the official game it's an old game so it's not uh publicity but it was kind of the first game that was completely controlled by reinforcement learning as opponents right it was the interest in a very interesting project there and the it was trained like that like the motorcycles learn how to to drive initially randomly and then they learn that they weren't getting a great reward which was like related to how how much time it took to go through a certain piece of the track and then they actually learned to how to to to drive since the physics is the physics to drive the way professional drivers drive because they need to optimize given the physics right so it was very very interesting and that's super cool yeah it was pretty cool so this this technique can not only be used to to control players like that but ultimately if you distill it down to okay instead of approximating a function I'm approximating a probability distribution and this process is actually differentiable so I can back propagate to it so I can train it even though the actions themselves are not necessarily amenable to back propagation so you can actually simulate um in a in a chain where you have a model and then you have an operation that is not really differentiable you can use the reinforce sorry you can use reinforcement and learning uh to go through that part of the process and optimize something that is hard to express yeah can I so just to make sure I've got it clear on my end um so so far in in The Limited number of neural networks your tutorials that I've created so far we have a neural network and for example at the end of that neural network we might have like um what is it a soft Max a soft Max thing that turns the outputs into kind of pseudo probabilities um and the way we've been using those so far is is we just would whichever one has the highest probability is the one that we we always choose that and and what you're saying is instead of always choosing that and then making the next action you just choose that action with that given probability but there's a possibility that that some of the times you're going to choose the other options and as a result you get a sort of a more uh you know given the fact that um the input data isn't always going to be the exact same as your training data sometimes make taking those other options is actually going to uh give you a better reward in the long term maybe not most of the time so most of the time you want to focus on the thing with the highest probability um but every now and then you want to have the flexibility to do these these lower probability options choose those and and that results in artificial sort of Motorcycle drivers that can drive more like people can because they've got they've got wiggle room and they don't always just take uh the most the you know the option with the highest probability they they can sometimes do they can sometimes surprise you even by like doing something that's like unusual or rare um they don't do it all the time otherwise it would become you know commonplace and it would lose whatever specialness uh but that's what you I think that's what you what you're saying is that is that sort of that's a piece of what I'm saying okay and this is like the exploitation exploration trade-off of uh reinforcement learning agents where they might take uh unusual decisions at some point but that allows them to explore the solution space let's say and so they they find creative ways of doing things I remember when I was uh doing that project at some point the physics had a had a bug where uh if the the wheel touched the edge of the of the track in a very specific point the the motorcycle will be like shot ahead and of course the agents learn how to to to do that systematically so um so and but they did that by ex because that point was not a point where they were supposed to go in order to optimize their their trajectories but through exploration they ended up there and then they learned to do that uh but um yeah the the other piece that makes reinforcement and learning compelling in this case is that you can take in a an action at some point that will have some effect in the future and you've said that before right um and at the moment where you're taking the action it's not clear what the future uh reward will be and same thing with generative models in this case um your your outputting a series of things and then there will be somebody who will rank uh for for the motorcycle it was like how much time like did you win did you win in the end yeah or uh worrying faster than your opponents or where are you very fast yeah um and in the other case it's like uh there the output of of the model are ranked um you can say okay I'm sampling my model and I'm getting 10 outputs and I want these outputs to be ranked from best to worst in terms of human feedback for for that and then um so each and every word generation is then contextualizing uh in terms of what was the reward in the end right so this is how the the fine-tuning of the model is in and in this case for chargeupt for example reinforcement learning was driven by human feedback in the sense that there's an intermediate point and this is really important to introduce today because we also want to talk about all the other ingredients that are calm and and make an AI system a modern AI system behave the way it's behaving the other piece is it's not that the the fine-tuning necessarily so this reinforcement learning had the human in the loop at every point in time so it's not that I'm asking uh you know 10 different sentences I rank them from first best to worst and this is the signal that goes into the model because it would take like me years and or hundreds of years to do that right we need something else which is how can we have a mathematical function that given 10 sentences will give me the the best to last right and this mathematical function as you can imagine it's either something I code and in the case of the motorcycles I coded it because you know I I knew that if you get there faster I can write a formula for what getting there faster is or having the best trajectory is and so on in this case it's much harder to do but we have language models so what you can do is say okay I'm now ranking a few thousands of sentences that have been sampled for my model and I'm training another model to so that it can rank them in the future it can Rank and see yes unseen uh sentences in the future and this is what happens so this is called the reward model So In traditional reinforcement learning you have the reward function here you have the reward function which is a model itself and so once you train the reward model then you can use that reward model to do something that is called align the other model so what do we mean by a line is or you know there's this trade-off between capability and Alignment capability is hard well do I predict my next word alignment is uh-huh I don't care about predicting the next word exactly lights it like it's written in Book I want the model to do something that I mean it to do or not do something that I want I don't want it to do so align it with my expectations in a very soft sense right ah okay so yeah this is called like alignment so I how I can take something that is potentially harmful toxic whatever and and say okay now it's safe to use Beyond is like immediate capability of predicting the next word which is kind of like low level that I I know it can do yeah and um and so using a model that doesn't need to be the same model it can be another model and use it in a reinforcement learning scheme to generate the reward so that I can apply it's called usually you use a proximal policy optimization which is one of the classical now classical ways of optimizing these probability distributions so that the reward function is maximized um so this process is very compelling to me because on one end imagine you have this very capable Reckless model right that you need to align properly and here you have a created reward model that with the right technique is capable of a lot to align the first one now think about the potential and there was a tweet this week that said I need to look up uh the author but it said like it would be great at this point if that if open AI released their safety models and probably also the reward models because there's so much value here right so you can think about in the future this uh pre-trained huge models needing to be aligned being published in some ship yeah of form and then me coming in and saying okay now I need really this to have certain behaviors and being able to like put my effort in that model that is usually smaller because it's not as hard you know yeah to rank compared to actually generating reason uh so they're kind of dumper model in a way um okay there can be a uh where the difference uh is made and and so this is the other piece this is this yeah this sounds like a really big deal to me uh because um and and this was I mean we're just talking a couple of months ago um you know when a company would really be like hey we've got this new AI you know it's gonna generate scholarly sounding articles or you can say hey write an article about you know I don't know geology and some era right and it would and it would create this thing it would have citations it would sound it would look like a normal article but it was all it was like within like 20 minutes of that thing being online and I'm being a little facetious but within about 20 minutes of this you know it was creating really nasty articles it was it that sounded scholarly it was you know all this racist stuff was coming out of it and I feel like that's that was a that was the fate of any AI like I mean either you know I remember there was you know chat Bots that like major companies were like hey we've got a cool new AI chat bot and and it would be like within seconds you know just turned into something that's spitting out profanities and how to really just foul output and what you were saying is that even though two months ago people weren't doing that now we can now we can filter that stuff out and now we have some safeguards that um even two months ago we did I mean maybe we had them but yeah now we now it's up to whoever releases now these safeguards are here and we don't have to yeah we don't have to worry about AI just being completely um uh overtaken by like Jokers and people who just want to make you just think it's a big joke that problem of people abusing the system what you're saying is is is is is is that that may be going away and we've got more safeguards in place than we used to it's which is yeah I'm not sure yeah yeah I know and we have a way to make it go away um and I want to talk about what anthropic is doing as well uh there because it's okay constitutional in the AI is kind of the uh an interesting thing to talk about but the I would say that uh it's it's not that it's going away but it's the thing right it's what makes uh yeah uh will make the difference between like if Chad GPD wasn't trained like that then using chat GPT would be like impossible right so it would would be should be a nightmare right it would be a nightmare it wouldn't have taken a steam and so because like there are many humans who enjoy these things but most people uh you know they find it useful because they don't have these things in the way right uh and so that's right the safety filters that you can put after it and the and the reward models that you can train on that have elements of safety and Alignment to a purpose these are what makes a super highly capable and potentially very evil because the data we feed it with is contains evil um into something that changes uh the that impacts the ability of humanity to evolve I would say yeah and then I'll be criticized but I think we are at the cost for something that is really compelling and I'm not talking about GPT again like I'm not a judge yeah Fanboy I'm just saying that what what we have right now in front of us is a recipe right is the fact that yes and I think what why open AI has credit here and I'm fully aware that media some other institutions with similar power Firepower um have created models that are uh comparably good there so I'm not making it an open a I think it's more like what we have understood is that in a rather minimalistic way we have a way to produce something that goes well well beyond the capabilities of whoever is training it like like yes so uh in a way I would say that uh having the recipe if the recipe said okay you need to sit with 10 experts 10 Geniuses and for two years they're coding up rules and through stuff like that then I mean for example um I was when I when I was reading yeah very far if you do it that way yeah yeah I mean you can't like yes you can do it you can pull it up once but Humanity as a whole is like for example Mathematica right wolfram's system Alpha and so on it's a Marvel it's really impressive but there's one of them you know and yes you could have could have it changed the word possibly for in from some angles but he hasn't ultimately right maybe with when coupled with other things it will make a more of a contribution but I don't think it it could trigger the butterfly effect of being you know the the the hive mind potential that these techniques have because they are attainable because they are ultimately simple techniques that yes may require a lot of money today to get there um but it's not insane like if you can fit and delibrow was tweeting about fitting um uh one of these uh models I I believe was glm 130 billion and he has a rig with four gpus in his house and he quantized it to int Force and it actually made it run on a single machine now it's not that you could run gpt3 on the single machine with the same performance but actually why not like at some point you know this thing has 130 billion parameters GPT has three as one number 175 I think so okay so it's in the ballpark yeah I mean it's I'm not saying that yeah we'll have it in our smart watches anytime soon but I'm just saying that yes it still takes 10 millions and um and a year to train but this is going to be temporary so the the the the more important thing is that now we have a recipe to create something that behaves like that and that that is the fundamental thing that creates opportunities for people that will have products built directly on top of something like that and we have seen it um there are many people who are being very creative with that um and we'll talk about it at some point uh for people that want to create a foundation model you know their own Foundation model good luck with that but that's fine like it's possible Right we've seen it in in uh yes in many examples and there's glm opt uh GPT Neo Neo X bloom yeah they're collectives and it's not a Manhattan Project size thing anymore it requires some money but that's right it's doable right it requires a little bit of money but it's doable and it's reproducible and you can do it in a wide variety of contexts and it's not just a one-off this is exactly you know what we're seeing is just sort of the tip of the iceberg of what we're going to see exactly whereas before any time we saw something that was amazing it was that's all it was it I mean it was amazing but it you know you didn't then see it scale and cover you know all kinds of different markets and it didn't compose right it didn't compose that's right so because composition is what makes you know uh the uh the the effect of something be multiplied uh you know exponentially if you want yeah so uh and ultimately you know we're we're working at lightning so we know that uh our job is to remove the boilerplate um and create what and then make it easy to scale exactly and enable things without all the technicalities without having you know troops of platform engineers and so on but focus on what it is and what it is in this case is very small so our job will be done yeah in this case where we can express the whole thing we've described in a couple of files and let the platform take over right and enable yes people to think high level now that thing is not my problem anymore uh my car you know I don't care what what the engine is doing or I can care if I'm curious but it will still take me what I when I need to need to take so having a recipe exactly it's it's like I think it's uh and and the recipe being being simple ultimately at the at the core simple is what really makes this uh ground King
k3b9Mvtt6lU,2023-01-11T00:26:52.000000,"The AI Buzz, Episode #1: ChatGPT, Transformers and Attention",[Music] thank you hello I'm Josh starmer from the YouTube channel stat quest with Josh starmer and I'm lucantiga CTO at lightning Ai and together we're going to talk today together we host the AI Buzz where we're going to talk about the latest Trends in Ai and just talk about why they will change everything or if they'll change everything that we might get a few of those as well but anyways to get us started I just want to mention that uh yesterday I have a friend in publishing uh who's an editor at a big major publishing company and he came up to me and he goes Josh uh we'll chat GPT put me out of a business out of work will I lose my job um and I feel like that's an active a good question uh Lucas because you have an answer for this oh not at all of course no I don't think so um well if anything you know it will uh make it more challenging for people to compete against boilerplate right uh text and uninteresting stuff but it will Empower people to produce more content it also will Empower people to produce more marketing stuff to Target us mindlessly so I mean we're gonna pay uh all of us are gonna pay for it foreign yeah I think I think one thing that chat GPT will be really cool and helpful for is say like I want to write a book about say I'm actually I just started writing a book about neural networks and I could say hey chat gbt uh give me a an outline what should I write about on neural networks what topics should I cover and it might give me a nice outline uh get me sort of like in the ballpark uh maybe speed up a few steps of brainstorming uh and then what I can then go in and do is is flush it all out and you know make it uniquely stat Quest and draw my little pictures uh yeah exactly but I think it'll it'll help it'll to me I think of it'll help with like the brainstorming and sort of just getting started there's that blank page like disaster that we have whenever I try something new like if I'm gonna make a new stat Quest or a new song or a new book or whatever there's that blank page that you're like now what do I do and I think be awesome that now we can just say hey Chad gbt get me started um yeah for sure for example this show right I didn't know yeah exactly we had to find a name for it so we wanted to find a name that sounded uh cool so I went on jot GPT and said oh you charge Deputy can you uh tell me a name of like five shows five names of our show um what I say was on the lines of um what's the what's the name of the uh The Daily Bugle it says something like The Daily Bugle something okay and it's sped out like five minutes and one of them was the uh got it started yeah exactly yeah uh I love it so uh Luca I was just getting I was just talking about how you used chat gbt to write the lyrics to a song can you tell us about that experience yeah yeah uh I was like uh at a France um I'm helping him record a few songs and so on and so I he wrote a song and I wanted to come up with Alternatives lyrics and uh actually the the melody for it but I had like 10 minutes because I needed to start working so I got on the jet GPT and said okay uh can you generate a song with this Vibe and then it got something to me that was really cheesy so I said a bit less cheesy and a bit more you know blah blah blah and in a few iterations I was there and so um and I produced something that I meant uh to to with with the kind of Ambience that I meant to obtain uh and it would got me like a lot more time probably I wouldn't have gotten it there um if it wasn't for that tool so I mean the potential for it to empower people to you know maybe you know write a song if they want to uh and experiment with it it's uh it's it's really great you know what's what's interesting is that you can give it instructions back right and it will adjust things the way you want but keep the structure the same so this kind of long-ranging longer range relationships between things where I don't know if you know how charging PT Works under the hood Josh like roughly speaking no I don't that's the thing yeah um yeah that's that's I'm here to ask dumb questions you're here to help us understand but that's what just an excuse to start speaking about it I know you know better but let's let's say that um so how Chachi PT Works uh charger PT is a model that is you know fundamentally it's a GPT that has been conditioned to do a chat as the name says what is GPT GPT is is a short name for generative pre-trained Transformer which is a very fancy name for a model that is not super easy to understand fundamentally but you can write it up in like 300 lines of python that the the crazy thing about that and then of course the the code that uh open AI has there uh will be much more complex you know but the complexity you layer on top of it is mostly to scale it up to be huge right so it's more like incidental complexity but the actual Computing model that is there is really compact is is a very simple model and uh and of course it's streamed uh it has like billions of parameters it's it's strained in a very particular way igbt is pre-trained on a massive amount of data but then it's fine-tuned uh to behave well to behave in a way that is like amenable to interaction with people uh with a technique that that is called reinforcement learning through human feedback uh which is like as it denotes it it means that um another model was trained to kind of rank the output of the big model and say okay this is better than this one and so humans sat there saying oh this output is better than this one and this album they did it not just with two but with a bunch of them and using something that is yep go ahead I'm just curious so so what we're saying is at this stage they had humans sort of curate the output and rank the output yeah uh and that went back into like the back propagation and the training of of the bigger model is that is that correct that that went into training a model that was the called the reward model that then was was used to uh improve the parameters of the larger model okay and uh so the third model yeah the reward model was somewhat you know had humor interaction and human grading is that is that exactly I think that's right I might be getting it all wrong okay no no it's good okay good so you learned how he learned to rank things in a similar way as a human would do and you know incidentally Rankin thinks saying if something is better than something else is easier than generating it as we all know right you can tell if a good is better than another one you know According to some preferences but you you can't maybe write the same book uh or the same grade book right so it's kind of the same kind of relationship so uh yeah and this is how chat GPT came like got from great to uh really really impressive in in interacting with you can I ask a few more questions about it um yes so it's generative um and so far at least on my channel we've we've just we've had very simple neural networks that just make really basic predictions like a really simple output yes or no it's this type of you know Iris or it's you know the Drug's gonna work or it's not going to work how how does the and I understand that but how does the output for a generative model work yeah so they're like it's funny because these kind of models are trained on a task um that is fundamentally very simple which is predicting the next part so you like the astounding thing uh about these kind of models and large language models uh like uh in in these day and ages that they're trained on tasks they are very uh like granular for example given a text you mask a word and you try to fill it in with the right word or you can you need to predict what comes after a series of words you know and you would have to understand that that I was trying to say words um and and and this produces models that are able to make connections between different parts of sentences they understand what understand the structure of of of of reasoning to some to some degree actually you know there there have been um papers that have shown that large language models perform um when they see a prompt which is like the text I I ask a question with uh there are many ways of asking those questions and the the they perform a sort of uh gradient descend themselves in okay in answering those questions so they are capable of learning algorithms through which they will then generate uh the text so uh I mean it's it's almost like and we will talk about that like then we will recap the conversation and you know and uh uh we'll get on to kind of a program of what we will discuss in the future but essentially it's almost like we as a Humanity we have stumbled on a new form of uh expressing computations that is capable of learning that it's very simple yeah but it's couple of learning very difficult things it's almost like when you know humans from Roman numerals uh humans in in the in in Rome whatever or in Europe and in other parts of the world they got from uh their like clunky numeral systems to the Arabic numeral system that allowed them to kind of supercharge their uh their ability to reason about things right so it's almost like that yeah yeah you can suddenly do math that's like useful instead of just like adding two numbers together but can can I ask another a really quick question so uh when we're you know going back a little bit talking about the generative models uh is that a little bit see like say like someone sends me a text on my phone and and my my phone already has like a suggest you know suggested words and could I you know chat GPT is basically that but like really well done so instead of like actually typing in words you know maybe I could just hit you know hit the hit the select you know this the suggested word and just keep hitting just hit keep hitting the suggested words and it'll say something reasonable and sort of apropos to the conversation and I will not have I'm really bad with texting so this actually sounds really great right that I could just go like hit one but one big button like four times and it would do a whole sentence instead of like you know is that a little bit how chat GPT does this Auto generation is it's basically it completes a word and then based on the word that it's predicted it says well this is this is probably the next word and based on that word that is predicted it goes well that's this is probably the next word and it just sort of like does the prediction like that is that is that sort of how it works yeah yeah it's how it's trained to to work but then after the sudden like that there's this emergent behavior of uh understanding relationships that are very complex for example in in the text that you it just generated or that you provided to it right so you can do calculations he can produce code um I did a kind of some sort of interaction right where it says you know I started with some code and then I said okay eliminate the variables that are useless and it did it and then I said what if those variables are torch tensors and it did it and then I asked can you turn this python code into python byte code and it did it and then no way you know change the addition into a subtraction in the bytecode but not in the original code and it did it like it's it's really you know uh it's not just text completion it's not like you know uh your your usual thing and you can do pretty much the same not just with chat GPT right uh it's uh if you go like many large-scale models today are capable of of doing similar things and we are seeing that a few of them are being released uh in the open uh but of course you know the more you tune them uh the more useful they will get because people will have to think less about how do I prompt this model and they will just get the right thing out of the box so I have I have another stupid question I apologize Luca um so when I was in college and I took I took a like a computational theory course and they were like you know the um you know we talked about touring machines and things that could be solved with touring machines and things that could not be solved with Turing machines and things that could be could not be solve the touring machines were like they those were like problems that we needed entirely new types of computers for we needed Quantum Computing we could not solve these with a standard computer and one of those problems was like understanding code you know even though code is so structured and it was a long time ago since I took this class because I could be wrong but I just remember coming out of that classroom thinking that I guess it'll will never in my lifetime will never have a computer that can really understand code and basically what you're saying is that and we're actually doing that right now and that is within my lifetime that that we're seeing these things um how is that possible are we like defying physics what's going on yeah the the the the the strange things is that I don't think we know fully understand how that is possible right we have a very vague understanding of how these things work yeah I mean a few people in the world have a better understanding better intuition than there's a bunch of us that kind of kind of uh grasp it somehow but we like you see papers in which they like start doing like reverse engineering on what happened right in within these these models yeah um and I mean it's it's in a way it had to transcend the human uh compression right in order to to do this because this is like a very hard problem and you know if you could write the rules that allow you to do this it would be inherently an easy thing to do somehow right like if you remember the job or the thing you know with uh IBM and and what's uh that that system was a system that was built on rules right a large part on on those so it was a very big expert system and the the the thing there was like a lot of knowledge uh went into it a lot of facts and the facts were processed in a way that then it would generate like uh there was I I there was uh probably some of course some probabilistic stuff in there but ultimately at the end of the day uh uh rules were very prominent yet um but then to break that complexity and and a lot of years went into that a lot of engineering um and no uh try and travel 20 years and we have like a few hundred lights of code potentially right um with a few algorithms on top of it that plus a lot of engineering um yeah to scale up to massive uh but the fact that the Computing model is so simple is something that is really fundamental and I think we'll talk about that in future episodes hopefully with a better internet connection so basically I think the uh the we'll try to look into what fundamental computation GPT is doing from a Computing model because the fact that this model has come from uh from sequence modeling uh is and the terminology that we use the tension and so on I don't know I have a very like maybe it's a very uninformed uh position but I think it's kind of shadowing a bit and and there's a nice paper thinking uh like Transformers it's called Uh that kind of talks about like what Computing model Transformers might be like uh presenting to us uh fundamentally right and how how the computation Works inside them in order to actually like learn not only sequences but actual like algorithms and uh yeah zero shot abilities when we talk about zero shot abilities our abilities that they want to train on and they just you know you give them instruction and they can execute the instructions that you gave them at inference time so it it takes a lot more than just predicting the next thing in order to be able to do this from a Computing standpoint so yeah it's very fascinating yeah and the other big thing that happened this year is diffusion right and uh yeah yeah no image generation and now we're seeing sound generation so in future episodes we'll get more into those uh tiny bits here one of the interesting things here is that the same mechanism attention is has been injected into what makes stable diffusion generate images which is a an old model unit which is a model that is basically a kind of a hourglass model but it's usually drawn with a u and it has a few skip connections but it was published back in 2015. I remember I read in the paper when he came out and trying it out on 3D medical images uh yeah yeah but the fact that then they added attention to it I think it made it so powerful um and of course the whole diffusion kind of framework uh made it so powerful but in the end there are commonalities between all these models in the sense of this fundamental routine block and that fundamental Computing block is but the attention mechanism which is something that yeah it needs to be a bit you know digested and explained and understood yeah and um and yeah and so the other interesting thing I think um when we talk about you know uh these models is the fact that they're like as we said they can be expressed in a few lines right and so when you express them in a few lines um that's where Innovation can come in right because you developed you you can develop like a higher level intuition for what's going on um and uh and whereas if you have a very complex repo in front of you or a code base uh then doing something manipulating that something becomes a lot uh more complicated so something that I've been enjoying is uh Andrei carpath is uh Nano main GPT and Nano GPT implementation you can go on GitHub GitHub corpathi Nano GPT and GPT there is 300 lines and we'll get more into it maybe in the future but essentially this model scale differently yeah you you give it a lot more parameters and so on uh it's what is power Enchanted PD that we started from and um and everything else is an incidental complexity due to scale now of course yeah you can have a more like efficient attention block blah blah blah but from from the the core principles that that is the architecture right so uh it gives me the opportunity to understand the value of eliminating boilerplate because if you can express that same model and make it run at scale without any kind of complexification then when that that is when Innovation comes in right and so since you are wearing the lightning t-shirt that's where I think we're focusing on boilerplate right and and try to remove it from everything like we did it with the trainer doing with the apps and uh doing with uh distributed computing and so on because the limiting boilerplate is a factor that enables scientists not AI experts to use things on one end but also AI experts to kind of stop caring about all the incidental complexity and focus on the on the thing that actually makes the magic happen so I guess one question is as Luca you were talking about you're talking about this book thinking like a Transformer or this essay or um uh can you talk uh about that a little bit what what does it mean to think like a Transformer yeah so um the the think is about the the attention block that we we talked about before right so the this this piece of you know neural networks are made of like simple computations repeated a lot of times and you know and uh like a typical neuron has a linear uh component and then a non-linearity right after it uh like a feed forward networks and so on like attention is a bit different in the sense that uh when you get an input that is composed by many elements they don't even need to be a sequence like then you can make it understand that the thing is a sequence with a particular encoding but there's nothing to it that really you know is inherently a sequence and so when you um uh when you have an attention block basically you pass your input into something that will figure out how to wait to assign weights uh to the input elements um and uh in order to produce uh the next thing but these weights are are generated in a way that it it resembles like the ability to store information depending on on the input and then you retrieve uh what usually happens when you have that input and how you combine it with itself to generate those weights um so for example if I have a one two three four and every time I have one two three four I want to obtain one three two four then probably I will want to say that the one will remain a one so you will wait um let's say you you will generate like you you will take the input sequence itself and say okay uh I have all my I I I if I am the one at that point and I see the rest of the sequence uh I will get the the diagonal element over an imaginary Matrix that has the the sequence and itself in on the two x and y axis and if you want to switch the three and two and and obtain two three uh sorry three two then you will take the two and say okay when I have a 2 here in the second position usually I get uh uh I uh I wait a lot in my my output sequence I wait a lot the uh the third element in in in in my sequence so by Waiting by creating weights uh this way so evaluating the input sequence itself that's this is where I can draw no linear relationships uh between the the input and itself in a way that can be stored in a matrix and learned um and so and this happens over and over again because you have uh many ways of composing the same sequence with itself stored in your code memory okay and so we'll get more into it from a mechanistic point of view but basically learning these 2D matrices and and the way they uh you can combine something with itself or something with something else uh because you can condition a signal with something else for example I can condition an image from text the same way with this two by two thing that combines elements uh that together so this mini cards that tell me how to combine the these two axes two different axes and I can populate this this card with numbers across the the card are what makes them the the the attention mechanism exists essentially in the way that it is today um so it's like a two by two I don't know and it's not two by two in reality it's many more Dimensions but ideally yeah we will see from that paper that yeah it's it's really close to uh uh learning non-linear relationship by learning how to kind of uh Exchange uh information between two different Source uh and that can be the sequence in itself or a sequence of something else um unfortunately I don't have the camera up so I'm mimicking things and nobody's looking at me but yeah um yeah I guess so I mean what I'm imagining in my head is so like you were talking about with a normal neural network what we do is we pipe things through uh activation functions and as a result we get little bits of of curves or or bent shapes that we can then stretch and we can flip them over and we add those shapes and we add those curves together to get something that like a squiggle or something like that that fits to the data but maybe what you're saying is that with attention uh well it's more like we're doing like a really like crazy hyper dimensional lookup table that isn't necessarily a shape more as it is like you know given these inputs um that then can lead to sort of uh some some non-linear combination of those inputs or the weights that we've given them um can lead to sort of like digging into this hyper Matrix or something like that yeah picking out an output yeah it's it's it's it's it's very similar like it's it's not Incorrect and in fact it's uh um it's not that you cannot express similar things with very you know deep even uh perceptrums or you know standard feed forward uh the fact is that you can uh you can learn much more efficiently like uh there have been some uh scaling studies that have demonstrated recently uh that I've demonstrated that the original Transformer architecture from 2017 uh was the had the the best uh learning efficiency across the the board of many variations on the theme and uh and that's this tells us and that um the that the same you know we were talking about Roman numerals and Arabic numerals it's almost like that right so the way the computation goes on inside those objects um makes it easier to learn a lot of things and it's not that you couldn't Express the competition otherwise it's just very effective to learn it that way I like that analogy actually it's kind of perfect it gets us all the way back to how we started right because yeah it's not that it's not that the old way couldn't do these things it was just it's just so much more work like trying to do long division with Roman numerals would just be the biggest headache ever yeah um uh but instead you know with Arabic numerals we can do that pretty easily relatively speaking easily uh and he and so of course we're gonna be like well this this way of notating or this way of representing knowledge is more efficient uh so we're gonna switch and so it makes sense that yeah we would switch likewise uh in in with neural networks and AI to switch to these things that are that can get to the get to this answer like in half the time or in quarter of the time and then can be then we can then we can reach further then we can go further we can take them further and do uh things that we could have never really imagined that we were going to be doing with the with the standard neural network because because we knew because they were slow and they were difficult to train that we were only going to get so far and now we can go like 10 times as far because we've got a just a much more efficient way of of training uh the model uh which is yes and um yeah it's great like um I don't know what the future will hold like uh for us you know like your friends from the publishing company for sure the human machine interaction landscape will will change I don't know if we'll you know if we'll be subverted or or whatever probably not you know but but for sure you know it will have a multiplying effect the point is ultimately is what tools you create for for for the people right yeah to you know to do more because if you have an API probably you're not gonna uh succeed you know yeah I was yeah gpt3 has been out in the open AI playground for a while but it's only when chat GPT got on the stage that like my brother-in-law started talking to me about charge upd right exactly and he's a tech guy but he is like uh is not directly interested you know uh in in this topic but now he started playing with it right in a with a family at dinner so uh and uh and for example like Microsoft today the the news from today that you know they will invest 10 billion dollars um and uh opening eye to it with the idea of integrating uh that kind of technology in products like excel in world and so on is a testament to the fact that ultimately right you change life of people by uh delivering products that you know remove this kind of conceptual boilerplate of doing things you know and make them faster ultimately so yeah I mean the product creation aspect so I'm excited not because you know uh these things will take over but they will allow us to to create products for example how many times Josh you have reorganized your folder of notes and uh you know uh changing the nesting structure of how your notes were taken oh yeah all the time all the time you know there's no right way to do that right there's no right way to organize our knowledge and but if you had something that was maybe fine-tuned on your style that could kind of summarize your notes and have the main takeaways organized for you and then maybe the next day you ask them in another form and uh you don't have a problem to duplicate things and so on and you can do it without even thinking about it that's when your life would change for the better right Luca I just have to say you just blew my mind uh because uh I mean totally totally blew my mind because I mean I suddenly realized that that the way I organize my folders and whatnot is a little bit like a attention um in that it's you know I'll start doing one thing and you know and it grows right and all of a sudden it needs there I need to add some hierarchy to things and I need to maybe I need to rename these files what this what's what started out being one thing ended up being another thing and it's really only when I have the full context of like all the work I've done that I really see how to organize it and it's almost like language translation with attention or whatever where it's only when you see the full sentence and you process it all kind of simultaneously rather than one word at a time that you can get a really accurate translation and that words that start at the beginning in a sentence can then affect how things are at the end of the sentence and um and it's like and I'm and I was like oh my gosh yes we could apply that same sort of mentality to my notes and how they're structured and how they're organized because because it's really only when I'm kind of done adding to them that that the actual structure becomes apparent uh um and also and so you really just you just blew my mind right there pretty hard no I'm looking forward to it because every note taking application I I got like I always you know drop it at some point because it becomes something that I don't like anymore because I I probably don't like myself so much at that point I have a confession I use I use I use paper and I I use both sides and and and and that's that's my strategy is is lots of scraps of paper all over the house so with that said we're going to wrap up today's episode of the AI Buzz thanks everyone for coming it looks like we had over 1500 people drop in during the live stream which is pretty awesome um anyways with that said uh we'll be back in two weeks to talk about more I think we're gonna dive more into chat GPT more into Transformers more into attention more of all this stuff and hopefully we can uh we'll get uh we'll we'll get Luca's uh audio working just a little bit better so anyways I just want to say goodbye thank you for being here for episode one of the AI Buzz um yeah there's Luca so giving them thumbs up thank you very much uh I'm really excited I think we're gonna have a lot of fun together and and hooray thank you Josh let's be great
KiGsuKqaq0Y,2022-11-18T19:27:04.000000,"Design Matrix Examples in R, Clearly Explained!!!",[Music] statquest is awesome stat Quest is cool stack Quest is freaky stand Quest rules hello and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill this stat Quest complements the stat Quest on General linear models part three the one that focused on design matrices in this video I show you how to do the examples from the original stat quest in r in the original stack Quest we were interested in determining if there was a statistically significant difference between the size of control mice versus the size of mutant mice given that we had measured their weights and we ended up with this equation y equals the control intercept plus a mutant offset plus the slope this equation fits two lines to the data one for the control mice and one for the mutant mice now let's see how we can test the hypothesis that there's no difference between control mice and mutant mice using r the first thing we do is we create labels for the control mice then we create labels for the mutant mice now we enter the weights for the control mice followed by the weights for the mutant mice now we add the size measurements for the control mice followed by the size measurements for the mutant mice now we can construct a design Matrix we do this by telling model dot matrix that we want the Y values or the sizes and we indicate that by a tilde to be modeled by the type of mouse and its weight you'll notice that we didn't have to specify that we wanted a term for The Intercept R assumes this by default also you don't actually have to call model.matrix the linear models function will do this for you but I like calling it just to make sure that the design Matrix looks the way I expect it to the first column in the design Matrix will be multiplied by the control Intercept in this case the control intercept will be on for all of the measurements the second column in the design Matrix will be multiplied by the mutant offset in this case only the mutant values will turn the mutant offset on the last column in the design Matrix will be multiplied by the slope we then call the LM function LM stands for linear models it'll do the least squares fit and calculate the statistics for us and here's the summary of all that stuff that the LM function did for us the first thing it does is the least squares fit and then it calculates the residuals it also calculates the r squared value for us this is called the multiple r squared because we're fitting a rather complicated equation to the data the adjusted r squared is the r squared value adjusted for the number of parameters in the equation that we fit to the data lastly we have a p-value this p-value compares the fit of our fancy equation to the simplest equation possible y equals the overall mean this p-value for the weight parameter tells you how much better the fancy equation fits the data than if we had removed the weight parameter this amounts to comparing the fit of the fancy equation to just a normal t-test in other words this p-value reflects what would happen if we compared the least squares fit around the original fancy equation to the least squares fit around the fancy equation minus the slope and since the slope is the weight this p-value tells us how important the term for the weight is in the fancy equation this p-value for the type of mouse tells us how much better the fancy equation fits the data than if we had removed the type of mouse information from the equation this amounts to comparing the fit of the fancy equation to just a simple linear regression so calling the linear model function actually does a bunch of different tests for us it's important to know which tests we're actually interested in the next example shows that sometimes we're not actually interested in the p-value in the lower right hand corner so now let's move on to the second example in this second example we compared experiments done by two different Labs there was a batch effect and we wanted to control for that in R we start by creating the labels for the data that was generated by lab a then we create the labels for the data that was generated by lab B then we create the labels for the Control Data from Lab a and the mutant data from Lab a then we create the labels for the Control Data from Lab B and the mutant data from Lab B then we enter in all of the expression values then if we wanted to we could call model.matrix to see what the design Matrix looked like to do this we enter the tilde which represents the Y value or gene expression data and say that it is modeled by the lab and the type we don't have to specify the control mean because that is done by default the first column in the design Matrix is multiplied by the term for the lab a control main this means that this value is on for every single measurement in the data set the second column is multiplied by the lab B offset that means that this offset is only on for the data generated by lab B this last column is multiplied by the term for the difference between the mean of the mutant data and the mean of the Control Data here's our call to the linear models function and here's the summary of everything that the linear model function did for us just like before it has the multiple r squared and the adjusted r squared values and then a p-value in the lower right hand corner however this is not what we need this p-value represents how much better our fancy equation fits the data compared to an equation that is far too simple this equation is just y equals the overall mean and it does not test if there is a difference between control and mutant data this is not what we want this is what we want where we're testing whether or not we can subtract the term for the different types of mice from our fancy equation and still get a good fit here's the p-value that we really want so the moral of the story is that the p-value in the bottom right hand corner isn't always the p-value that you want it reflects the comparison of your fancy equation to the simplest equation possible if that's not what you want then you're going to have to look around at the other p-values that the linear models function reports maybe that's the one you need hooray we made it to the end of another exciting stat Quest if you like this stat Quest and want to see more like it please subscribe and if you have any ideas for future stat quests please put them in the comments below hooray until next time Quest on
LoocDAbgwlM,2022-11-18T19:22:30.000000,"Design Matrices For Linear Models, Clearly Explained!!!",stat Quest is getting bigger watch out hello and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about General linear models and this is part three of a series that we're doing on this this time we're going to focus on design matrices this stat Quest picks up exactly where part 2 leaves off so if you haven't seen that one yet I'd recommend doing it right now in part two of this series we ended by saying that this was not the standard design Matrix for a t-test it was kind of a cliffhanger and then I showed you that this is the standard design Matrix for a t-test it corresponds to a slightly different equation let's focus on what this new design Matrix and equation are all about in this version all measurements both control and mutant turn on the term for the mean control value but only the mutant measurements turn on the term for the difference between the mean of the mutant data and the mean of the Control Data this term serves as an offset that we can use for the mutant data for example this one turns the term for the mean of the control data on for this data point and this Zero Turns the term for the difference between the mean of the mutant data and the mean of the control data off for this data point this one turns the term for the mean of the control data on for this data point and this one turns the term for the difference between the mean of the mutant data and the control data on for this data point the residuals are the same for both equations and design matrices the equations also have the same number of parameters two so P fit is the same so in our equation for f we plug in the exact same value for the sum of squares around the fit we also plug in the exact same value for the P fit parameter this results in the exact same value for f and that means we're going to get the exact same p-value if they both do the same thing and the result is the same p-value why is the one on the right more common I'll be honest I don't know the answer for sure but I think it has something to do with regression so far we've looked at design matrices in the context of using ones and zeros to turn parts of the equation on or off so let's take a step back and remember how it works remember that the numbers in the First Column are multiplied by the term for the mean of the control values and the numbers in the second column are multiplied by the term that represents the difference between the mean of the mutant values and the mean of the control values multiplying the mean of the Control Data by 1 turns it on by just letting it be multiplying the difference between the mean of the mutant data and the mean of the Control Data by zero makes it zero and that turns it off a design Matrix full of ones and zeros is perfect for doing t-tests or anovas anytime we have different categories of data but we can use other numbers for example here's a design Matrix for linear regression it pairs with this equation we've got a bunch of Wands in the first column and in the second column we've got the x-axis position for each point let's focus on the first row in the design Matrix for now it corresponds to this point just like before the numbers in the First Column multiply the first term in the formula in this case multiplying the y-intercept by 1 turns it on and just like before the numbers in the second column multiply the second term in the formula in this case we're scaling the term for the slope to make this more concrete let's see what happens when we use real numbers for the y-intercept and slope the y-intercept is super small and equals 0.01 so that's the number we plug in here the slope equals 0.8 and we plug that in right here and now we do the math and you get a point on the least squares fit line that corresponds with the first data point now let's focus on the second row it corresponds to this point the number in the First Column multiplies the y-intercept and the number in the second column scales the slope plug in the y-intercept and the slope and do the math and you get a point on the line that corresponds to the second data point ing each row into the equation gives us a bunch of points on the least squares fit line once we have all the points on the line we can calculate the residuals and that means we can calculate a p-value this example shows that a design Matrix isn't always just a bunch of zeros and ones but can be any set of numbers that we want to plug into an equation one row at a time one note before we move on since this style of design Matrix with ones all the way down the First Column is more common all of the examples from here on out will be consistent with this format now that we know we can put any number into the design Matrix let's do something fancy let's combine a t-test and a regression holy smokes that's totally crazy okay we're back to the relationship between mouse weight and mouse size however we have two types of mice these measurements are from normal control mice these measurements are from mutant mice that make them tall and skinny by eye we can see that mutant mice tend to be larger even if they weigh the same in other words the mutant mice seem to follow this trend and the control mice seem to follow this trend can we use statistics to test if there's a significant difference between the two types of mice if we just did a regression we'd get a nice looking line but it wouldn't tell us if the mutant mice were significantly larger than the normal mice on the other hand a normal t-test would ignore the relationship between weight and size and in this case the p-value is greater than 0.05 since Mouse type and the relationship between weight and size are both important we need to combine them into a single test in other words instead of comparing this mean to this mean which is what a t-test would do we want to compare this line to this line to do this we need an equation that has a term for the y-intercept for the normal mice we also need a term for the mutant Mouse offset and lastly a term for the slope which in this case is the same for both types of mice this means we need to design Matrix where the First Column is ones this means that both lines intercept the y-axis at some point the second column indicates whether the mutant offset is on or off the mutant offset is off for the control mice and on for the mutant mice this allows the mutants to have their own y-intercept and the last column has the weight data the first four values are the x coordinates for the control mice and the last four values are the x coordinates for the mutant mice let's focus on the first row plug in the numbers and we get a value on the red line now let's focus on the second row again we get a value on the red line and from here we just plug in the values and we get coordinates on either the red or the green line we get coordinates on the red line when the mutant offset is off and we get coordinates on the green line when the mutant offset is on once we have the locations on the lines we can calculate the residuals which are hard to see since they are so small in this example now we can compare the fancy model to a simpler model in the simpler model we model Mouse size by using just the average size of the mouse we ignore mouse weight and we ignore Mouse type this is the default model that we use when we do the t-test now we plug in the sum of squares of the residuals for the fancy model and we plug in 3 for the P fancy term since there are three parameters in our fancy equation and we plug in the sum of squares of the residuals for the simple model and we plug in 1 for the P simple term since there is only one parameter in the simple equation this gives us 21.88 and that gives us a p-value of 0.003 bam the small p-value says that taking weight and mouse type into account is significantly better at predicting size than just using the average size note the simple model can be any simpler model if we did a super simple linear regression we'd have a model that takes weight into account but ignores the fact that some mice are normal and others are mutants then we plug in the sum of squares of the residuals just like before the simple regression equation has two parameters so P simple equals two we then plug in the numbers and we get a p-value equal to 0.0023 double bound this small p-value suggests that using both weight and mouse type is better at predicting Mouse size than weight alone here's another simple model it's just a normal t-test this model ignores mouse weight again plug in the sum of squares of the residuals and the equation has two parameters so P sample equals two and that gives us a p-value equal to 0.0025 oh my gosh it's the coveted triple bam this small p-value suggests that using mouse weight and type is better at predicting Mouse size than Mouse type alone so you can see that the questions you want to ask determines what type of simple model you want to use to compare your fancy model to okay one last example of a design Matrix lab a does an experiment then lab B replicates it however their measurements tended to be smaller overall this is a batch effect we would like to combine these two data sets to see if mutants are different from controls but we need to compensate for the batch effect here's how to do it first add a term for the mean control value from Lab a second add a term for the lab B offset this takes care of the batch effect third add a term for the differences between the mutant and the control measurements here's the design Matrix essentially we want to know if this last term in the equation is important or not alternatively is this last column important so we compare the fit of this fancy equation to this simpler one that ignores the control mutant difference a small p-value will tell us that the equation that keeps track of the mutant control differences predicts the gene expression better than one that does not this will mean that the difference between controls and mutants is significant hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and would like to see more please subscribe and if you have any other ideas of things you'd like me to do stat quests on or you've got a certain design Matrix you'd like to see an example of just let me know in the comments below until then Quest on
R7xd624pR1A,2022-11-18T19:19:19.000000,"Using Linear Models for t tests and ANOVA, Clearly Explained!!!",stand Quest stat Quest stand Quest yeah hello and welcome to statquest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're doing part two of our series on General linear models last time we talked about how to do linear regression this time we're going to talk about how to use those exact same techniques to do t-tests and Anova we'll do this using something called a design Matrix which is a cool concept that will expand upon in future stat quests on General linear models let's start with a super quick review of linear regression last time we measured mouse weight and mouse size and we wanted to learn two things from it we wanted to learn how useful mouse weight was for predicting Mouse size r squared told us this and we wanted to know if the relationship was due to Chance the p-value told us this now let's see if we can apply those Concepts to a t-test in this specific example we're going to be comparing gene expression between control mice and mutant mice mutant mice are just normal mice that have a specific Gene that's been knocked out and is no longer functioning correctly the goal of a t-test is to compare means and see if they are significantly different from each other if the same method can calculate P values for a linear regression and a t-test then we can easily calculate P values for more complicated situations so now I'm going to walk you through the steps for using the techniques from linear regression to do a t-test on the left side of the screen I'll remind you how each step applies to linear regression on the right side of the screen I'll show you how those steps apply to t-tests step one ignore the x-axis and find the overall mean to emphasize that we want to focus on the y-axis I've removed the labels on the x-axis here are the overall means for the linear regression in the t-test the next step is to calculate the sum of squared residuals around the mean this is SS mean these are the residuals the distance from the data points to the lines in this case the lines are the overall means bam calculating the sum of squared residuals around the mean was easy step 3 fit a line to the data note this is when we start caring about the x-axis again on the left side we have the least squares fit to the data however how do we do a least squares fit to a t-test let's start by just fitting a line to the Control Data we start by finding a least squares fit to the Control Data it turns out that the mean is the least squares fit the mean intercepts the y-axis at 2.2 this is the equation for horizontal line that intercepts the y-axis at 2.2 thus this is the line that we fit to the Control Data now let's fit a line to the mutant data the least squares fit is the mean of the mutant data the mean intercepts the y-axis at 3.6 this is the equation for a horizontal line that intercepts the y-axis at 3.6 thus this is the line that we fit to the mutant data we have fit two lines to the data originally when we did the regression we fit a single line to the data however there is a way to combine these two lines into a single equation this will make the steps for computing F the exact same for the regression and the t-test which in turn means a computer can do it automatically this is key because we don't want to do this by hand ever this is going to look a little weird but just bear with me keep in mind that the goal is to have a flexible way for a computer to solve this and every other least squares based problem without having to create a whole new method each time this is the equation which combines both lines for this point we have 1 times the mean of the Control Data zero times the mean of the mutant data plus the residual yes this is strange especially multiplying the mutant mean by zero but bear with me if we multiplied things out the equation for this point would be y equals 2.2 plus the residual and that sort of makes sense but just bear with me this is the equation for the next point the only difference is the residual this one is smaller this is the equation for the next point again the only difference is the residual this is the equation for the next point and again the only difference is the residual this is the equation for the first point in the mutant data set now we are multiplying the control mean by zero and multiplying the mutant mean by one these are the equations for the remaining points now let's focus on the ones and zeros they function like on and off switches for the two mains a one turns the mean on and a zero turns the mean off when we isolate the ones and zeros they form a matrix called a design Matrix the design Matrix can be combined with an abstract version of the equation to represent a fit to the data column one turns the control mean on or off column two turns the mutant mean on or off in practice the role of each column is assumed and the equation is written out like this y equals the mean of the Control Data plus the mean of the mutant data now that we have the fit for the control and mutant data down to a single equation plus design Matrix we can move on to calculating F and the p-value so step four calculate the sum of squares of the residuals around the fitted lines with the linear regression that means the sum of these squared residuals the sum of squares around the fit for the t-test is the sum of these squared residuals to review what we've done so far we've calculated the sum of squared residuals around the mean and then we calculated the sum of squared residuals around the fitted line now we can just plug these things in to our equation for f f will lead to a p-value for the linear regression p mean refers to the number of parameters in the equation for the mean Mouse size that's one parameter in the t-test p mean refers to the number of parameters in the equation for the mean of the gene expression that's also just one parameter for the linear regression P fit refers to the number of parameters in the equation for the fitted line in this case that's 2. the parameters are the intercept and the slope for the t-test P fit refers to the number of parameters in the line that we fit to the t-test data in this case P fit equals two because we had to estimate two parameters one for the mean of the control data and one for the mean of the mutant data now we can calculate a p-value for the t-test bam let's review what we've done so far here's the original data gene expression for control mice and mutant mice the first thing we did is we calculated the sum of squares of the residuals around the overall mean then we calculated the sum of squares of the residuals around the fit in order to do this with a single equation we had to create a design Matrix once we've calculated the sums of squares all we have to do is plug the values into the equation for f and then we'll get our p-value now let's do an anova Anova tests if all five categories are the same here we have control and mutant mice just like before but we also have control and mutant mice on a funky diet and we also have heterozygomice the first thing we do is calculate the sum of squares around the mean we do this just like before we calculate an overall mean value for all of the categories and then Square the residuals and sum them up no big deal the equation for the overall mean is just y equals mean expression that equation only has a single parameter the overall mean so p mean equals one now we calculate the sum of squares around the fitted lines the equation for the fitted lines has five parameters one for each mean therefore P fit equals five here's what the design Matrix looks like one column per category now that we've calculated the sum of squares around the mean and the sum of squares around the fit along with p mean and P fit we can plug those values in and calculate f triple bam if we can calculate F then we've got ourselves a p-value one last important detail before we're done the design matrices that I've shown you are not the standard design matrices used for doing t-tests and Anova this is what we used for the t-test in this stat Quest but this is a more common design Matrix for the same thing both design matrices will get the job done it's just the one on the right is more commonly used we'll talk about this one and other more elaborate designs in the next stack Quest hooray we've made it to the end of another exciting stat Quest if you'd like this and would like to see more stat quests like it feel free to subscribe and if you have any suggestions for future stat quests put them in the comments below tune in next time for another exciting stat Quest
mno47Jn4gaU,2022-11-18T19:14:19.000000,"Multiple Regression in R, Step by Step!!!",stat Quest is toads cray cray that Quest hello I'm Josh starmer and welcome to statquest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to compare simple and multiple regression in R and just so you know the r code used in this video is available on the stackquest website stackquest.org this stack Quest picks up from where the stat Quest on multiple regression left off here's the raw data that we're going to use in our analysis I've created a data frame with measurements for size weight and tail for simple regression we will focus on how well weight predicts size Step 1 always plot your data we specified weight for the x-axis and we specified size for the y-axis plotting your data as a first step is super important because it allows us to evaluate whether doing a linear regression to begin with is a good idea can we see a relationship in the data between size and weight in this case we can and that means doing a regression makes sense step two use the LM function where LM stands for linear model to fit a line to the data in R this is how you specify the following equation we specify size is predicted by weight by using the tilde character between size and weight and by default R adds the terms for the y-intercept and the slope R then uses least squares to find the values for the y-intercept and the slope that minimize the squared residuals from the line once we've run the linear models function and save the output in a variable called simple.regression we can get a summary of that regression using the summary function calling the summary function on simple.regression gives us a huge pile of stuff the most important stuff is down here specifically the r squared and the adjusted r squared multiple r squared is just another way to say r squared by the way also for simple regression the multiple r squared value or just plain old r squared is the one we're interested in the adjusted r squared only applies when we have more complicated models we'll use it later when we do multiple regression there's also the p-value down here together the r squared which equals 0.613 and the p-value which equals 0.012 say that weight does a pretty good job predicting size the last thing we want to do for our simple regression is add a line that shows the least squares fit on the graph we do this using the a b line function now let's do some multiple regression for multiple regression we will use weight and tail to predict size step one always plot your data since we didn't specify the X and Y axes R plots all the data columns size weight and tail against each other this is super useful because it generates all the plots we need to decide whether doing a multiple regression with this data makes sense or not this graph plots size on the y-axis and weight on the x-axis this is what we used before in the simple regression down here we have the same exact data however this time sizes on the x-axis and weight is on the y-axis these two graphs are similar only the axes have been flipped the graph in the upper right hand corner has size on the y-axis and tail on the x-axis and in the lower left hand corner we have another graph that's very similar just the axes have been flipped this graph has weight on the y-axis and tail on the x-axis and just like for the other graphs these two graphs are similar just the axes have been flipped we can see that both weight and tail are correlated with size this is good it means that both weight and tail are reasonable predictors for size we can also see that weight and tail are correlated this means they provide similar information and that we might not need both in our model we might only need weight or tail step two use the linear model function to fit a plane to the data in R this is how you specify the following equation using the tilde and the plus symbols we specify that size is predicted by weight and tail and by default are as the terms for the y-intercept and slope 1 and slope 2. once we've run the linear models function we can print out a summary of the results using the summary function again summary gives us a big pile of stuff the r squared adjusted r squared and the p-value look good hooray note since we're doing multiple regression we're now more interested in the adjusted r squared value with multiple regression this section is more interesting this line compares the multiple regression to the simple regression it Compares this model which uses both weight and tail to predict size to this simpler model where we're just using tail to predict size this is the p-value it means that using weight and tail isn't significantly better than using tail alone to predict size now let's look at this line it compares the fancy multiple regression where we use weight and tail to predict size to a simple regression where we just use weight to predict size this is the p-value it means that using weight and tail is significantly better than using weight alone to predict size in summary using weight and tail to predict size is good but if we wanted to save time we could spare ourselves the agony of weighing mice and just use their tail lengths to predict size hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you have any suggestions for future stat quests we'll just put them in the comments below until next time Quest on
EkAQAi3a4js,2022-11-18T19:11:48.000000,"Multiple Regression, Clearly Explained!!!",stat Quest stat Quest stack Quest stat Quest yeah it's that Quest hello I'm Josh starmer and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about multiple regression and it's going to be clearly explained this deck Quest Builds on the one for linear regression so if you haven't already seen that one yet check it out alright now let's get to it people who don't understand linear regression tend to make a big deal out of the differences between simple and multiple regression it's not a big deal and the stat Quest on simple linear regression already covered most of the concepts we're going to cover here you might recall from the stat Quest on linear regression that simple regression is just fitting a line to data we're interested in the r squared and the p-value to evaluate how well that line fits the data in that same stat Quest I also showed you how to fit a plane to data well that's what multiple regression is you fit a plane or some higher dimensional object to your data a term like higher dimensional object sounds really fancy and complicated but it's not all it means is that we're adding additional data to the model in the previous example all that meant was that instead of just modeling body length by mouse weight we modeled body length using mouse weight and tail length if we added additional factors like the amount of food eaten or the amount of time spent running on a wheel well those would be considered additional Dimensions but they're really just additional pieces of data that we can add to our fancy equation so from the stack Quest on linear regression you may remember the first thing we did was calculate r squared well the good news is calculating r squared is the exact same for both simple regression and multiple regression there's absolutely no difference here's the equation for r squared and we plug in the values for the sums of squares around the fit and then we plug in the sums of squares around the mean value for the body length regardless of how much additional data we add to our fancy equation if we're using it to predict body length then we use the sums of squares around the body length one caveat is for multiple regression you adjust r squared to compensate for the additional parameters in the equation we covered this in the stat quest for linear regression so it's no big deal now we want to calculate a p-value for our r squared calculating F in the p-value is pretty much the same you plug in the sums of squares around the fit and then you plug in the sums of squares around the mean for simple regression P fit equals 2 because we have two parameters in the equation that least squares has to estimate and for this specific example the multiple regression version of P fit equals three because least squares had to estimate three different parameters if we added additional data to the model for example the amount of time a mouse spends running on a wheel then we have to change P fit to equal the number of parameters in our new equation and for both simple regression and multiple regression p mean equals one because we only have to estimate the mean value of the body length so far we've compared this simple regression to the mean and this multiple regression to the mean but we can compare them to each other and this is where multiple regression really starts to shine this will tell us if it's worth the time and trouble to collect the tail length data because we will compare a fit without it the simple regression to a fit with it the multiple regression calculating the F value is the exact same as before only this time we replace the mean stuff with the simple regression stuff so instead of plugging in the sums of squares around the mean we plug in the sums of squares around the simple regression and instead of plugging in p mean we plug in P simple which equals the number of parameters in the simple regression that's two and then we plug in the sums of squares for the multiple regression and we plug in the number of parameters in our multiple regression equation bam if the difference in r squared values between the simple and multiple regression is big and the p-value is small then adjusting tail length to the model is worth the trouble hooray we've made it to the end of another exciting stat Quest now for this stack Quest I've made another one that shows you how to do multiple regression in R it shows all the little details and sort of what's important and what's not important about the output that R gives you so check that one out and don't forget to subscribe okay until next time Quest on
wsi0jg_gH28,2022-11-18T19:09:00.000000,"Linear Regression in R, Step by Step",I like stat Quest do you like stack Quest I like stack Quest and I hope he likes that Quest too hello and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about doing linear regression in r this particular stat Quest is intended to be a companion video for the stat Quest on linear regression for this tutorial I'm going to assume that you already know how to get your data into R and instead I'm going to focus on how to get that data into a linear regression model and how to interpret the results this is how I created the data for the stat Quest on linear regression I created a data frame with two columns weight and size if I just type mouse.data and then press return R will print out the data frame in a nice column format I then use the plot function to plot the data on an XY graph this is where I set up the actual linear regression I call the function LM which stands for linear models and I pass it a formula and I pass it the mouse data the way I've specified the formula means that size are considered to be the Y values and weight are considered to be the X values the linear models function then calculates the least squares estimates for the y-intercept and the slope in R the meat of doing a regression is in the summary function this function generates all kinds of output and I'm going to walk through it one step at a time the first line just prints out the original call to the LM or linear models function after that you get a summary of the residuals those are the distance from the data to the fitted line ideally they should be symmetrically distributed about the line that means you want the Min value and the max value to be approximately the same distance from zero likewise you'd like the first quantile or 1q and the third quantile or three Q to be equidistant from zero also it's nice to have the median close to zero as well this next section tells us about the least squares estimates for the fitted line this value is for the intercept and this value is for the slope the standard error of the estimates and the T value are both provided to show you how the P values were calculated these p-values test whether the estimates for the intercept and the slope are equal to zero or not if they're equal to zero that means they don't have much use in the model lastly these are the p-values for the estimated parameters generally speaking we are usually not interested in The Intercept so it doesn't matter what its p-value is however we want a p-value for weight to be less than 0.05 that is we want it to be statistically significant a significant p-value for weight means that it will give us a reliable guess of mouse saws if you were unable to read the actual p-value but could for some reason see the star to its right then these codes would give you a sense of what the p-value was the next line the residual standard error is the square root of the denominator in the equation for f the next line tells us the multiple r squared and adjusted r squared values multiple r squared is just r squared as I describe it in the stat Quest on linear regression it means that weight can explain 61 percent of the variation in size this is good generally speaking the adjusted r squared is the r squared scaled by the number of parameters in the model the next line tells us if the r squared is significant or not this is the value for f these are the degrees of freedom and here's our p-value again this says that weight gives us a reliable estimate for size lastly we can add the regression line to the X Y graph we started drawing earlier hooray we've made it to the end of another exciting stat Quest if you like this video and want to see more like it please subscribe and if you have ideas for future stat quests feel free to put them in the comments below
7ArmBVF2dCs,2022-11-18T19:08:44.000000,"Linear Regression, Clearly Explained!!!",sailing on a boat headed towards statquest join me on this boat let's go to stat Quest it's super cool hello and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about linear regression AKA General linear models part one there's a lot of parts to linear models but it's a really cool and Powerful concept so let's get right down to it I promise you I have lots and lots of slides that talk about all the Nitty Gritty details behind linear regression but first let's talk about the main ideas behind it the first thing you do in linear regression is use least squares to fit a line to the data the second thing you do is calculate r squared lastly calculate a p-value for r squared there are lots of other little things that come up along the way but these are the three most important Concepts behind linear regression in the stat Quest fitting a line to data we talked about fitting a line to data duh but let's do a quick review I'm going to introduce some new terminology in this part of the video so it's worth watching even if you've already seen the earlier stat Quest that said if you need more details check that stat Quest out for this review we're going to be talking about a data set where we took a bunch of mice and we measured their size and we measured their weight our goal is to use mouse weight as a way to predict Mouse size first draw a line through the data second measure the distance from the line to the data Square each distance and then add them up terminology alert the distance from the line to the data point is called a residual third rotate the line a little bit with the new line measure the residuals Square them and then sum up the squares now rotate the line a little bit more sum up the squared residuals etc etc etc we rotate and then sum up the squared residuals rotate then sum up the squared residuals just keep doing that after a bunch of rotations you can plot the sum of squared residuals and corresponding rotation so in this graph we have the sum of squared residuals on the y-axis and the different rotations on the x-axis lastly you find the rotation that has the least sum of squares more details about how this is actually done in practice are provided in the stat Quest on fitting a line to data so we see that this rotation is the one with the least squares so it will be the one to fit to the data this is our least squares rotation superimposed on the original data bam now we know why the method for fitting a line is called least squares now we have fit a line to the data this is awesome here's the equation for the line least squares estimated two parameters a y-axis intercept and a slope since the slope is not zero it means that knowing a mouse's weight will help us make a guess about that Mouse's size how good is that guess calculating r squared is the first step in determining how good that guess will be the stat Quest r squared explained talks about you got it r squared let's do a quick review I'm also going to introduce some additional terminology so it's worth watching this part of the video even if you've seen the original stat Quest on r squared first calculate the average Mouse size okay I've just shifted all the data points to the y-axis to emphasize that at this point we are only interested in Mouse size here I've drawn a black line to show the average Mouse size bam sum the squared residuals just like in least squares we measure the distance from the mean to the data point and square it and then add those squares together terminology alert we'll call this SS mean for sum of squares around the mean note the sum of squares around the mean equals the data minus the mean squared the variation around the mean equals the data minus the mean squared divided by n n is the sample size in this case n equals 9. the shorthand notation is the variation around the mean equals the sum of squares around the mean divided by n the sample size another way to think about variance is as the average sum of squares per Mouse now go back to the original plot and sum up the squared residuals around our least squares fit we'll call This Ss fit for the sum of squares around the least squares fit the sum of squares around the least squares fit is the sum of the distances between the data and the line squared just like with the mean the variance around the fit is the distance between the line and the data squared divided by n the sample size the shorthand is the variation around the fitted line equals the sum of squares around the fitted line divided by n the sample size again we can think of the variation around the fit as the average of the sum of squares around the fit for each Mouse in general the variance of something equals the sum of squares divided by the number of those things in other words it's an average of sum of squares I mentioned this because it's going to come in handy in a little bit so keep it in the back of your mind okay let's step back a little bit this is the raw variation in Mouse size and this is the variation around the least squares line there's less variation around the line that we fit by least squares that is to say the residuals are smaller as a result we say that some of the variation in Mouse size is explained by taking mouse weight into account in other words heavier mice are bigger lighter mice are smaller r squared tells us how much of the variation in Mouse size can be explained by taking mouse weight into account this is the formula for r squared it's the variation around the mean minus the variation around the fit divided by the variation around the mean let's look at an example in this example the variation around the mean equals 11.1 and the variation around the fit equals 4.4 so we plug those numbers into the equation the result is that r squared equals 0.6 which is the same thing as saying 60 percent this means there is a sixty percent reduction in the variance when we take the mouse weight into account alternatively we can say that mouse weight explains 60 percent of the variation in Mouse size we can also use the sum of squares to make the same calculation this is because when we're talking about variation everything's divided by n the sample size since everything's scaled by n we can pull that term out and just use the raw sum of squares in this case the sum of squares around the mean equals one hundred and the sum of squares around the fit equals 40. plugging those numbers into the equation gives us the same value we had before r squared equals 0.6 which equals 60 percent 60 percent of the sums of squares of the mouse size can be explained by mouse weight here's another example we're also going to go back to using variation in the calculation since that's more common in this case knowing mouse weight means you can make a perfect prediction of mouse size the variation around the mean is the same as it was before 11.1 but now the variation around the fitted line equals zero because there are no residuals plugging the numbers in gives us an r squared equal to one which equals one hundred percent in this case mouse weight explains 100 percent of the variation in Mouse size okay one last example in this case knowing mouse weight doesn't help us predict Mouse size if someone tells us they have a heavy Mouse well that Mouse could either be small or large with equal probability similarly if someone said they had a light Mouse well again we wouldn't know if it was a big mouse or a small Mouse because each of those options is equally likely just like the other two examples the variation around the mean is equal 11.1 however in this case the variation around the fit is also equal 11.1 so we plug those numbers in and we get r squared equals 0 which equals zero percent in this case mouse weight doesn't explain any of the variation around the mean when calculating the sum of squares around the mean we collapse the points onto the y-axis just to emphasize the fact that we were ignoring mouse weight but we could just as easily draw a line y equals the mean Mouse size and calculate the sum of squares around the mean around that in this example we applied r squared to a simple equation for a line Y equals 0.1 plus 0.78 times x this gave us an r squared of 60 percent meaning 60 percent of the variation in Mouse size could be explained by mouse weight but the concept applies to any equation no matter how complicated first you measure square and sum the distance from the data to the mean then measure square and sum the distance from the data to the complicated equation once you've got those two sums of squares just plug them in and you've got r squared let's look at a slightly more complicated example imagine we wanted to know if mouse weight and tail length did a good job predicting the length of the mouse's body so we measure a bunch of mice to plot this data we need a three-dimensional graph we want to know how well weight and tail length predict body length the first Mouse we measured had weight equals 2.1 tail length equals 1.3 and body length equals 2.5 so that's how we plot this data on this 3D graph here's all the data in the graph the larger circles are points that are closer to us and represent mice that have shorter tails the smaller circles are points that are further from us and represent mice with longer tails now we do a least squares fit since we have the extra term in the equation representing an extra Dimension we fit a plane instead of a line here's the equation for the plane the Y value represents body length least squares estimates three different parameters the first is the y-intercept that's when both tail length and mouse weight are equal to zero the second parameter 0.7 is for the mouse weight the last term 0.5 is for the tail length if we know a mouse's weight and tail length we can use the equation to guess the body length for example given the weight and tail length for this mouse the equation predicts this body length just like before we can measure the residuals Square them and then add them up to calculate r squared now if the tail length or the z-axis is useless and doesn't make the sum of squares fit any smaller than least squares will ignore it by making that parameter equal to zero in this case plugging the tail length into the equation would have no effect on predicting the mouse size this means equations with more parameters will never make the sum of squares around the fit worse than equations with fewer parameters in other words this equation Mouse size equals 0.3 plus mouse weight plus flip of a coin plus favored color plus astrological sign plus extra stuff will never perform worse than this equation Mouse size equals 0.3 plus mouse weight this is because least squares will cause any term that makes sum of squares around the fit worse to be multiplied by zero and in a sense no longer exist now due to random chance there is a small probability that the small mice in the data set might get heads more frequently than large mice if this happened then we'd get a smaller sum of squares fit and a better r squared here's the frowny face of sad times the more silly parameters we add to the equation the more opportunities we have for random events to reduce sum of squares fit and result in a better r squared thus people report an adjusted r squared value that in essence scales are squared by the number of parameters r squared is awesome but it's missing something what if all we had were two measurements we'd calculate the sum of squares around the mean in this case that would be 10 then we'd calculate the sum of squares around the fit which equals zero the sum of squares around the fit equals zero because you can always draw a straight line to connect any two points what this means is when we calculate r squared by plugging the numbers in we're going to get 100 percent 100 percent is a great number we've explained all the variation but any two random points will give us the exact same thing it doesn't actually mean anything we need a way to determine if the r squared value is statistically significant we need a p-value before we calculate the p-value let's review the main Concepts behind r squared one last time the general equation for r squared is the variance around the mean minus the variance around the fit divided by the variance around the mean in our example this means the variation in the mouse size minus the variation after taking weight into account divided by the variation in Mouse size in other words r squared equals the variation in Mouse size explained by weight divided by the variation in Mouse size without taking weight into account in this particular example r squared equals 0.6 meaning we saw a 60 reduction in variation once we took mouse weight into account now that we have a thorough understanding of the ideas behind r squared let's talk about the main ideas behind calculating a p-value for it the p-value for r squared comes from something called f f is equal to the variation in Mouse size explained by weight divided by the variation in Mouse size not explained by weight the numerators for r squared and for f are the same that is to say it's the reduction in variance when we take the weight into account the denominator is a little different these dotted lines the residuals represent the variation that remains after fitting the line this is the variation that is not explained by weight so together we have the variation in Mouse size explained by weight divided by the variation in Mouse size not explained by weight now let's look at the underlying mathematics just as a reminder here's the equation for r squared this is the general equation that will tell us if r squared is significant the meat of these two equations are very similar and rely on the same sums of squares like we said before the numerators are the same in our Mouse size and weight example the numerator is the variation in Mouse size explained by weight and the sum of squares around the fit is just the residuals squared and summed up around the fitted line so that's the variation that the fit does not explain these numbers over here are the degrees of freedom they turn the sums of squares into variances I'm going to dedicate a whole stat quest to degrees of freedom but for now let's see if we can get an intuitive feel for what they're doing here let's start with these P fit is the number of parameters in the fit line here's the equation for the fit line in a general format we just have the y-intercept plus the slope times x the y-intercept and the slope are two separate parameters that means P fit equals two p mean is the number of parameters in the mean line in general that equation is y equals the y-intercept that's what gives us a horizontal line that cuts through the data in this case the y-intercept is the mean value this equation just has one parameter thus p mean equals one both equations have a parameter for the y-intercept however the fit line has one extra parameter the slope in our example this slope is the relationship between weight and size in this example P fit minus p mean equals 2 minus 1 which equals one the fit has one extra parameter mouse weight thus the numerator is the variance explained by the extra parameter in our example that's the variance in Mouse size explained by mouse weight if we had used mouse weight and tail length to explain variation in size then we would end up with an equation that had three parameters and P fit would equal three thus P fit minus p mean would equal three minus 1 which equals two now the fit has two extra parameters mouse weight and tail length with the fancier equation for the fit the numerator is the variance and mouse size explained by mouse weight and tail length now let's talk about the denominator for our equation for f denominator is the variation in Mouse size not explained by the fit that is to say it's the sum of squares of the residuals that remain after we fit our new line to the data y divide sum of squares fit by n minus P fit instead of just n intuitively the more parameters you have in your equation the more data you need to estimate them for example you only need two points to estimate a line but you need three points to estimate a plane if the fit is good then the variation explained by the extra parameters in the fit will be a large number and the variation not explained by the extra parameters in the fit will be a small number that makes f a really large number now that question we've all been dying to know the answer to how do we turn this number into a p-value conceptually generate a set of random data calculate the mean and the sum of squares around the mean calculate the fit in the sum of squares around the fit now plug all those values into our equation for f and that will give us a number in this case that number is 2. now plot that number in a histogram now generate another set of random data calculate the mean and the sum of squares around the mean then calculate the fit and the sum of squares around the fit plug those values into our equation for f and in this case we get f equals three so we then plug that value into our histogram and then we repeat with yet another set of random data in this case we got f equals one that's plotted on our histogram and we just keep generating more and more random data sets calculating the sums of squares plugging them into our equation for f and plotting the results on our histogram now imagine we did that hundreds if not millions of times when we're all done with our random data sets we return to our original data set we then plug the numbers into our equation for f in this case we got f equals 6. the p-value is the number of more extreme values divided by all of the values so in this case we have the value at f equals 6 and the value at f equals 7 divided by all the other randomizations that we created originally if this concept is confusing to you I have a stat Quest that explains p-values so check that one out bam you can approximate the histogram with a line in practice rather than generating tons of random data sets people use the line to calculate the p-value here's an example of one standard F distribution that people use to calculate p-values the degrees of freedom determine the shape the red line represents another standard F distribution that people use to calculate p-values in this case the sample size used to draw the red line is smaller than the sample size used to draw the blue line notice that when n minus P fit equals 10 the distribution tapers off faster this means that the p-value will be smaller when there are more samples relative to the number of parameters in the fit equation triple bam hooray we finally got our p-value now let's review the main ideas given some data that you think are related linear regression quantifies the relationship in the data this is r squared this needs to be large it also determines how reliable that relationship is this is the p-value that we calculated with f this needs to be small you need both to have an interesting result hooray we've made it to the end of another exciting stat Quest wow this was a long one I hope you had a good time if you like this and want to see more stat quests like it want to subscribe to my channel it's real easy just click the red button and if you have any ideas of stat quests that you'd like me to create just put them in the comments below that's all there is to it all right tune in next time for another really exciting stat Quest
bMccdk8EdGo,2022-11-18T18:55:45.000000,"R-squared, Clearly Explained!!!",step Quest step Quest step Quest stat Quest stat Quest is brought to you by the friendly people in the genetics department at the University of North Carolina at Chapel Hill hello and welcome to stat quest in this video we're going to talk about r squared r squared is a metric of correlation that is easy to compute and intuitive to interpret most of us are already familiar with correlation and the standard metric of it plain old r correlation values that are close to 1 or negative one are good and tell you that two quantitative variables for example weight and size are strongly related correlation values close to zero are lame some of you may be asking why should we care about r squared we already have regular r some of you might just be asking what is r squared r squared is very similar to its hipper cousin R but interpretation is easier for example it's not obvious that when R equals 0.7 that's twice as good a correlation as when R equals 0.5 however r squared equals 0.7 is what it looks like it's 1.4 times as good as r squared equals 0.5 the other thing that I like about r squared is that it's easy and intuitive to calculate let's start with an example here we're plotting mouse weight on the y-axis with high weights towards the top and low weights towards the bottom and mouse identification numbers on the x-axis with ID numbers one through seven we can calculate the mean or average of the mouse weights and plot it as a line that spans the graph we can calculate the variation of the data around this mean as the sum of the squared differences of the weight for each Mouse I where I is an individual Mouse represented by a red dot and the mean the difference between each data point is squared so that the points below the mean don't cancel out the points above the mean now what if instead of ordering our mice by their identification number we ordered them by their size instead of using identification number on the x-axis we have Mouse size with the smallest size on the left side and the largest size on the right side all we have done is reorder the data on the x-axis the mean and variation are the exact same as before here we show the mean again as a black bar that spans the graph in the exact same location as it was before also the distances between the dots and the line have not changed just the order of the dots here's a question for you given that we know an individual Mouse's size is the mean or average weight the best way to predict that individual Mouse's weight well the answer is no we can do way better all we have to do is fit a line to the data now we can predict weight with our line you tell me you have a large Mouse I can look at my line and make a good guess about the weight here's another question does the blue line that we just drew fit the data better than the mean if so how much better by I it looks like the blue line fits the data better than the mean how do we quantify that difference r squared in the bottom of the graph I've drawn the equation for r squared we're going to walk through it one step at a time the first part of the equation is just the variation around the mean we already calculated that it's just the sum of the squared differences of the actual data values from the mean the second part of the equation is the variation around our new Blue Line This is calculated in a very similar way here we just want the sum of the squared differences between the actual data points and our new Blue Line the numerator which is the difference between the variation around the mean and the variation around the blue line is then divided by the variation around the mean this makes r squared range from 0 to 1 because the variation around the line will never be greater than the variation around the mean and it will never be less than zero this division also makes r squared a percentage and we'll talk more about that in just a second now we'll walk through an example where we calculate things one step at a time first we'll start with the variation around the mean in this case that equals 32. the variation around the blue line is only six which is what we suspected since it appears to fit the data much better once we've calculated the variation around the mean and the variation around our Blue Line we can plug these values in to our formula for r squared after plugging in our values we get r squared equals 32 minus 6 over 32. after subtracting 6 from 32 we get 26 doing the division 26 divided by 32 gives us 0.81 or 81 percent this means that there is 81 percent less variation around the line than the mean in other words the size weight relationship accounts for 81 percent of the total variation this means that most of the variation in the data is explained by the size weight relationship here's another example in this example we're comparing two possibly uncorrelated variables on the y-axis we have mouse weight again but on the x-axis we now have time spent sniffing a rock like before we calculate the variation around the mean and just like before we got 32. however this time when we calculated the variation around the Blue Line we got a much larger value 30. now we just plug those values into our formula for r squared by doing the math we see that r squared equals 0.06 or 6 percent thus there's only six percent less variation around the line than the mean in other words the sniff weight relationship accounts for only six percent of the total variation this means that hardly any of the variation in the data is explained by the sniff weight relationship now when someone says the statistically significant r squared was 0.9 you can think to yourself very good the relationship between the two variables explains 90 percent of the variation in the data and when someone else says the statistically significant r squared was 0.01 you can think to yourself dag who cares if that relationship is significant it only accounts for one percent of the variation in the data something else must explain the remaining 99 percent what about plain old r how is it related to r squared r squared is just the square of r now when someone says the statistically significant r was 0.9 and we're talking about just plain old r you can think to yourself 0.9 times 0.9 equals 0.81 very good the relationship between the two variables explains 81 percent of the variation in the data and when someone else says the statistically significant R that's plain old R was 0.5 you can think to yourself 0.5 times 0.5 equals 0.25 the relationship accounts for 25 percent of the variation in the data that's good if there are a million other things accounting for the remaining 75 percent and bad if there's only one thing I like r squared more than just plain old R because it's easier to interpret here's an example how much better is R equals 0.7 than R equals 0.5 well if we convert those numbers to r squared we see that when r squared equals 0.7 squared it actually equals 0.5 which means 50 percent of the original variation is explained by the relationship when r squared equals 0.5 squared which equals 0.25 we see that only 25 percent of the original variation is explained by the relationship with r squared it's easy to see that the first correlation is twice as good as the second explaining 50 percent of the original variation is twice as good as only explaining 25 percent of the original variation that said r squared does not indicate the direction of the correlation because squared numbers are never negative if the direction of the correlation isn't obvious you can say the two variables were positively or negatively correlated with r squared equals dot dot dot whatever that value may be these are the two main ideas for r squared r squared is the percentage of variation explained by the relationship between two variables and also if someone gives you a value for plain old R just Square it in your head you'll understand what's going on a whole lot better we've reached the end of our stat Quest tune in next time for an exciting Adventure into the land of statistics
YCzL96nL7j0,2022-11-07T05:00:15.000000,"Long Short-Term Memory (LSTM), Clearly Explained","Long short-term memories. I've got them both and so does this network. Hooray! StatQuest! Hello, i'm Josh Starmer and welcome toStatQuest. Today we're going to talk about a Long Short-Term Memory, LSTM,and it's going to be clearly explained!Lightning,yeah! Gonna deploy your models in just a few days not months. Yeah! ThisStatQuest is also brought to you by the letters A, Band C. A always. B be. Ccurious. Always be curious. Note:ThisStatQuestassumes you are already familiar with recurrent neural networks and the vanishingexploding gradient problem. If not, check out the Quest. Also note: Although Long Short-Term Memory totally awesome. It is also a stepping stone to learning about Transformers which we will talk about in futureStatQuests. In other words, today we're taking the second step in our Quest. Now, in theStatQuest on basic, vanilla recurrent neural networks we saw how we can use a feedback loop to unroll a network that works well with different amounts of sequential data. However, we also saw that when we plug in the numbers, when the weight on the feedback loop is greater than one, and in this example the weight is 2, then when we do the math we end up multiplying the input by the weight, which in this case is 2,raised to the number of times we unrolled the network. And thus if we had 50 sequential data points, like 50 days of stock market data, which isn't really that much, then we would raise2by 50. And 2 to the 50th power is a huge number. And this  huge number would cause the gradient, which we need for gradient descent, to explode. Kaboom! Alternatively, we saw that if the weight on the feedback loop was less than 1, and now we have it set to 0.5, then we'll end up multiplying the input value by 0.5 raised to the 50th power.And 0.5 raised to the 50th power is a number super close to zero. And this number super close to 0 would cause the gradient, which we need for gradient descent tovanish. Poof! In summary, basic vanilla recurrent neural networks are hard to train because the gradients can explode. Kaboom! Or vanish. Poof! The good news is that it doesn't take much to extend the basic vanilla recurrent neural network so that we can avoid this problem. So today we're to talk about Long Short-Term Memory,LSTM, which is a type of recurrent neural network that is designed to avoid the exploding / vanishing gradient problem. Hooray! The main idea behind how Long Short-Term Memory works is that instead of using the same feedback loop connection for events that happened long agoand events that just happened yesterday to make a prediction about tomorrow, Long Short-Term Memory uses two separate paths to make predictions about tomorrow.One path is for long-term memories, and one is for short-termmemories. Bam! Now that we understand the main idea behind Long Short-Term Memory, that it uses different paths for long and short-termmemories,let's talk about the details. The bad news is that compared to a basic vanilla recurrent neural network, which unrolls from a relatively simple unit, Long Short-Term Memory is based on a much more complicated unit. Holy smokes, this looks really complicated! Don't worrySquatch, we will go through this one step at a time so that you can easily understand eachpart. Bam! Note: Unlike the network's we've used before in this series, Long Short-Term Memory uses sigmoid activation functions and tan-h activation functions. So let's quickly talk about sigmoid and tan-h activation functions. In a nutshell, the sigmoid activation function takes any x-axiscoordinate and turns it into a y-axis coordinate between0and 1. For example, when we plug in thisx-axis coordinate, 10, into the equation for the sigmoid activation function, we get 0.99995 as the y-axis coordinate. And if we plug in this x-axis coordinate,-5, then we get 0.01 as the y-axiscoordinate. In contrast, the tan-h, or hyperbolic tangent activation function takes any x-axis coordinate and turns it into a y-axis coordinate between-1and 1. For example, if we plug this x-axis coordinate, 2, into the equationforthe tan-h activation function, we get 0.96 as the y-axis coordinate. And if we plug in this x-axis coordinate,-5, we get-1 as the y-axis coordinate. So, now that we know that the sigmoid activation function turns any input into a number between 0 and 1 and the tan-h activation function turns any input into a number between-1 and 1,let's talk about how the Long Short-Term Memory unitworks. First,this green line that runs all the way across the top of the unit is called the cell state and represents the long-term memory.Although the long-term memory can be modified by this multiplication and then later by thisaddition, you'll  notice that there are no weights and biases that can modify it directly. This lack of weights allows the long-term memories to flow through a series of unrolled units without causing the gradient to explode or vanish. Now, this pink line, called the hidden state, represents the short-term memories. And as we can see, the short-term memories are directly connected to weights that can modify them. To understand how the long and short-term memories interact and result in predictions, let's run some numbers through this unit. First,for the sake of making the math interesting, let's just assume that the previous long-term memory is 2, and the previous short-term memory is1, and let's set the input value to 1. Now that we have plugged in some numbers, let's do the math to see what happens in the first stage of a Long Short-Term Memory Unit. We'll start with the short-term memory, 1, times its weight, 2.7. Then we multiply the input, 1, by its weight, 1.63.And then we add those two terms together. And, lastly, we add this bias, 1.62, to get5.95,anx-axis coordinate for the sigmoid activation function. Now we plug the x-axis coordinate into the equation for the sigmoid activation and functionand we get the y-axis coordinate 0.997. Lastly, we multiply the long-term memory,2, by the y-axis coordinate, 0.997, and the result is 1.99. So this first stage of the Long Short-Term Memory unit reduced the long-term memory by a little bit. In  contrast, if the input to theLSTMwas a relatively large negative number, like -10, then, after calculating the x-axis coordinate, the output from the sigmoid activation function will be 0.And that means the long-term memory would be completely forgotten because anything multiplied by0is 0. Thus, because the  sigmoid activation function turns any input into a number between 0 and1, the  output determines what percentage of the long-term memory is remembered. To summarize, the first stage in a Long Short-Term Memory unit determines what percentage of the long-term memory isremembered. Bam! Oh no, it's the dreaded terminologyalert. Even though  this part of the Long Short-Term Memory unit determines what percentage of the long-term memory will be remembered, it is usually called the Forget Gate. Small bam. Now that we know with the first part of theLSTMunit does, it determines what percentage of the long-term memory will be remembered, let's go back to when the input was 1 and talk about what the second stage does. In a nutshell, the block on the right combines the short-term memory and the input to create a potential long-termmemory. And the block on the left determines what percentage of that potential memory to add to the long-term memory. So let's plug the numbers in and do the math to see how a potential memory is created and how much of it is added to the the long-term memory. Starting with the block furthest to the right, we multiply the short term memory and the input bytheirrespective weights. Then we add those values together and add a bias term to get 2.03, the input value for the tan-h activation function. Now we plug 2.03 into the equation for the tan-h activation function and we get the y-axis coordinate, 0.97. Remember the tan-h activation function turns any input into a number between-1 and1. And in this case, when the input to theLSTMis 1,then after calculating the x-axis coordinate,the tan-h activation function gives us an output close to 1. In contrast, if the input to theLSTMwas-10, then after calculating the x-axis coordinate, the output from the tan-hactivationfunction would be-1. Going back to when the input to theLSTMwas1, we have a potential memory, 0.97, based on the short-term memory and the input. Now theLSTMhas to decide how much of this potential memory to save. And this is done using the exact same method we used earlier when we determined what percentage of the long-term memory toremember. In other words, after multiplying the short-term memory and the input  byweightsand adding those products together and adding a bias,we get 4.27, the x-axis input value for the sigmoid activation function. Now we plug the x-axis coordinate into the equation for the sigmoid activation function and we get the y-axis coordinate 1.0. And that means the entire potential long-term memory is retained, becausemultiplyingit by1doesn't changeit. Note: If the original input value was-10, then the percentage of the potential memory to remember would be 0, so we would not add anything to the long-term memory. Now, going back to when the original input value was1, we add 0.97 to the existing long-term memoryand we get a new long-term memory, 2.96. Double bam! Oh no, it's the dreaded terminology alert! Even though this part of the Long Short-Term Memory unit determines how we should update the long-term memory, it's usually called the inputgate. Tiny bam. Now that we have a new long-term memory, we're ready to talk about the final stage in theLSTM. This final stage updates the short-term memory. We start with the new long-term memory and use it as input to the tan-h activation function. After plugging 2.96 into the tan-h activation function, we get0.99. 0.99 represents a potential short-term memory. Now, theLSTM has to decide how much of this potential short-term memory to pass on. And this is done using the exact same method we used two times earlier:When we determined, what percentage of the original long-term memory to remember and when we determined what percentage of the potential long-term memory to remember. In all three cases, we use a sigmoid activation function to determine what percentthe LSTM remembers. In this case when we do the math, we get 0.99. And we create the new short-term memory by multiplying 0.99 by 0.99 to get 0.98. This new short-term memory, 0.98, is also the output from this entire LSTMunit. Oh no, it's the dreaded  terminology alert again. Because the new short-term memory is the output from this entire LSTM unit,this stage is called the  Output gate. And at long last, the common terminology seems reasonable to me. Triple bam! Now that we understand how all three stages in a singleLSTMunit work, let's see them in action with real data. Here we have stock prices for two companies CompanyAand Company B. On the  y-axis we have the stock value and on the x-axis we have the day the value wasrecorded. Note: If we overlap the data from the two companies, we see that the only differences occur on day1and on day5. On day1, Company A is at 0 and Company B is at1.And on day five, Company A returns to0and Company B returns to1.On  all of the other days, days2,3 and 4,both companies have the exact same values. Given this sequential data, we want theLSTMto remember what happened on day1so it can correctly predict what will happen on day5. In other words, we're going to sequentially run the data from days1through4through an unrolledLSTMand see if it can correctly predict the values for day 5 for both companies. So let's go back to theLSTMand initialize the long and short-term memories to0. Now, because this singleLSTMunit is taking up the whole screen, let's shrink it down to this smaller simplified diagram. Now, if we want to sequentially run Company A's values from days1through4through thisLSTM, then we'll start by plugging the value for day1, which is 0, into the  Input. Now, just like before we do the math. Boop be doop boop boop boop boop boop. And after doing the math, we see that the new or updated long-term memory is -0.20 and the new updated short-term memory is-0.13. So we plug in-0.2for the updated long-term memory and -0.1, rounded, for the updated short-term memory. Now we unroll theLSTMusing the updated memories and plugthe  from day 2,0.5, into the input. Then theLSTMdoes its math using the exact same weights and biases as before, and we end up with these updated long and short-termmemories. Note: If you can't remember theStatQuest on recurrent neural networks very well, the reason theLSTMreuses the exact same weights and biases is so that it can handle data sequences of differentlengths. Smallbam. Anyway, we unroll theLSTMagain and plug in the value for day 3. Then theLSTMdoes the math again using the exact same weights and biases and gives us these updated memories. Then we unroll theLSTMone last time and plug in the value for day4. And theLSTMdoes the math again using the exact same weights and biases and gives us the final memories. And the final short-term memory, 0.0, is the output from the unrolledLSTM.And that means the  Output from theLSTMcorrectly predicts Company A's value for day 5. Bam! Now that we have shown that theLSTMcan correctly predict the value on day 5 for Company A, let's show how the sameLSTM,with the same weights and biases can correctly predict the value on day 5 for Company B. Note:  Remember, on days 1 through 4, the only difference between the companies occurs on day1, and that means theLSTMhas to remember what happened on day1in order to correctly predict the different output values on day5. So let's start by initializing the long and short-term memoriesto 0. Now, let's plug in the value for day one from Company B, 1. And theLSTMdoes the math, just like before using the exact same weights and biases. Beep boop. After doing the math, we see that the updated long-term memory is 0.5 and the updated short-term memory is 0.28. So we plug in 0.5 for the updated long-term memory and 0.3, rounded, for the updated short-term memory. Now we unroll the LSTM and do the math with the remaining input values. And the final short-term memory, 1.0, is the output from the unrolled LSTM. And that means the output from theLSTMcorrectly predicts Company B's value for day 5. Double bam! In summary, using separate paths for long-term memories and short-term memories, Long Short-Term Memory networks avoid the exploding/vanishing gradient problem, and that means we can unroll them more times to accommodate longer sequences of input data than a vanilla recurrent neural network. At first, i was scared of how complicated theLSTMwas, but now I understand. TripleBam! Now it's time for  some Shameless Self-Promotion. If you want to review statistics and machine learning offline, check out theStatQuestPDFstudy guides and my book theStatQuestIllustrated Guide toMachine Learning atstatquest.org. There's something for everyone!Hooray!We've made it to the end of another excitingStatQuest. If you liked thisStatQuestand want to see more, please subscribe. And if you want to supportStatQuest, consider contributing to my Patreon campaign becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie or just donate, the links are in the description below. Alright, until next time Quest on!"
c3xEDTh7Uc8,2022-11-01T15:47:08.000000,The Cosine Similarity for NLP and CatBoost,[Music] foreign Quest I really like I really like to watch that Quest and I hope you like watching stat Quest too hooray now a word from our sponsor lightning dot a i many of you people already know I work for lightning di I'm there I'm one of their Elite AI Educators which is pretty cool it's a pretty cool rule to have and if you don't already know lightning AI makes training neural networks super easy so what I like about working for them is I like uh making hard things easy to understand and lightning makes hard things easy to do so it's like a good combination and in just a few weeks I'm going to be showing I'm going to be releasing a video on how to do how to code long short term memory by hand and using the pi torch function and how we're going to use lightning to train it and evaluate the training and we're gonna and it's going to be super easy and super awesome and I'm really looking forward to that so that's a word from the sponsor uh so without further Ado let's talk about cosine similarity all right so so yeah so today we're going to talk about the cosine similarity wait last time you said we're going to talk about cat boost not cosine similarity well the cool thing is is that the cosine similarity which is used a lot in natural language processing in LP is also used in catboost so by learning about the cosine similarity we get to learn about natural language processing and how catboost works double bam okay so imagine we saw these sentences about the 1990 hit movie troll 2. oh hold on I see in the comments we've got uh someone from Brazil oi Brazil uh anyways sorry I got distracted okay um um so imagine we saw these sentences about the 1990 hit movie troll 2. by I it's pretty easy to see that the first three sentences are similar to each other they all Express a positive feeling towards the movie like we got I love troll too and I like troll troll 2 and I love Troll 2 I love it hooray in contrast the last sentence is from someone who does not like Troll 2 and they say Troll 2 is bad party pooper anyways so when we don't have many sentences it's pretty easy to see which ones are similar to each other and which ones are different however what would we do if we collected all of the Twitter traffic for the last month how do we determine similarities and differences in the tweets in this case we can no longer rely on doing things by eye and instead have to get a computer to do it this is where the cosine similarity comes in handy the cosine similarity is a relatively easy to calculate metric that can tell us how similar or different things are bam my voice is already going what a disaster I need to love I need I'm still new at doing these live streams and what I need to do is um put like a glass of water right in front of me or something like that okay anyways to get an understanding of how the cosine similarity Works let's start with a super simple example here we want to know how similar how similar the phrase hello world is to hello so the first thing we do is make a table for the words that appear in the phrases hello and world in the first phrase hello world we see the word hello once and we see the word world wants in contrast in the second phrase hello we only see the word hello one time and we don't see the word world at all now given this table of counts for each word in each phrase we can create a two-dimensional graph that has the number of times we saw the word hello on the x-axis and the number of times we saw the word world on the y-axis now since the first phrase hello world had one occurrence of each word we can plot a point for it in the center of the graph in contrast the point for the second phrase hello is on the x-axis at one and that's because we don't have the word world in it at all now if we draw lines from the origin of the graph which is at zero comma zero so where both world and hello are both zero so if we draw lines from the origin of the graph to the points we can see that there's a 45 degree angle between the two lines and the cosine of 45 degrees cosine of 45 degrees is 0.71 and thus the cosine similarity between the phrases hello world and hello is the cosine of 45 degrees which equals 0.71 bam that was not hard okay note currently the second phrase is hello but whatever hello hello hello but we still whoops I'm missing a slide here okay when we have the phrase hello hello hello we've got hello three times but we still have a zero for the word world so the only thing that's changed is we've got a lot more hello in this case the point in the graph whereas at the point on the graph representing hello hello hello would be further out on the x-axis at x equals three however the angle between the two lines would still be zero point excuse me 45 degrees I don't know where the zero came from it would still be 45 degrees and thus the cosine similarity would be the exact same as what we got before the cosine of 45 degrees which equals 0.71 in other words the cosine similarity is determined entirely by the angle between the lines and not by the lengths of the lines small bam now if both phrases are hello world then they are exactly the same and we end up plotting the dots that represent the phrases on top of each other creating kind of Swampy green color and the angle between the lines from the origin and the points would be zero degrees and the cosine similarity is the cosine of zero degrees which equals one so in both phrases are exactly the same the angle between them will be zero degrees and the cosine similarity is one in contrast if the phrases don't have any words in common like here where the first phrase is hello and the second phrase is world people then the angle between the two phrases will be 90 degrees and the cosine similarity will be the cosine of 90 degrees which equals zero to summarize when two phrases have absolutely nothing in common the cosine similarity is zero and when the phrases are the exact same the cosine similarity is one and when there is some overlap between the two phrases but they are not exactly the same the cosine similarity is a value between 0 and 1. double bam now the way we've been Computing the cosine similarity step one make a table of word counts step two plot the points step three figure out the angle and step four calculate the cosine of the angle is pretty tedious the good news is that there is a relatively simple formula that can be computed super quickly on a computer to calculate the cosine similarity directly from the table of word counts dang that equation looks complicated don't worry Squatch we'll go through it one step at a time thanks these sigmas are shorthand for adding up a bunch of stuff the I is short for index and is used to keep track of the word we're working on I starts out set to one the first word of the table in this case the first word is hello and N is the number of different words in the phrases in this case n equals 2 because we have two words hello and world A and B refer to the two phrases in this case let's let a be the first phrase hello world and B be the second phrase hello now when I equals one we plug in the word counts for the first word hello so we've got a times B so that's one times one and then below that assessing the numerator in the denominator so in the in the bottom half of the fraction we've got these square roots these summations that are taking place in square roots and we've got one for a where we Square the value so we've got 1 squared and we also have one for the B phrase we just hello so we've also got one squared there okay so that's the first part in the summations that we're doing and when I equals 2 we plug in the word counts for the second word world so again uh we've got a times B in the numerator so 1 times 0 because world doesn't occur in phrase B and in the denominator in those square root functions we've got a squared so which is 1 squared and then we've got B squared which is 0 squared and when we do the math we get 0.71 which is the same value we got when we took the cosine of 45 degrees bam so we know that the cosine this fancy looking equation for the cosine similarity we know that it works it's doing exactly what we thought it was going to do now note when we only have two different words in our table like we have here then we can easily plot our plots but I meant to write plot we can easily plot the points on a two-dimensional graph uh because we can put one word like hello uh on the x-axis the First Dimension and the other word world on the y-axis the second dimension but when we have phrases with more than two different words then we need more than two different dimensions to plot the points for example if we wanted to calculate the cosine similarity for these two points where the first phrase I love Troll 2 is a phrase a and the second phrase I love Jim Kata is phrase B then the table will have five different words in it and that means we would need a five dimensional graph to plot the points and I have no idea how to draw a five-dimensional graph the good news is that this is another way having the equation for the cosine some less excuse me cosine similarity can come in handy rather than worry about how to draw a five-dimensional graph we just plug the numbers into the equation beep beep beep and when we do the math we get 0.58 so the cosine similarity between these two phrases I love Troll 2 and I love Jim Kata is 0.58 triple bam in summary the cosine similarity is a relatively easy to calculate metric that can tell us how similar different or different things are note if you haven't already seen it and Jim Kata is an awesome 1985 movie about a gymnast who uses the parallel bars to defeat his enemies in Mortal Kombat bam all right that is the end of our little presentation on cosine similarity I can't believe it only took 15 minutes to get through it I spent like a lot of time on it but that's just the way these things go and so now I'm just gonna go over to the comments section and look through the comments uh someone says that they need this for their research which is great uh uh someone says I want to thank you for help with PCAT sne and all topics didn't understand anything my professor said but you made it make sense double bam hooray and we've got a whole high from Chile hola hola Chile if you're still around we got Bam from India bam a Bam from Iran which is awesome greetings from Bulgaria someone just finished their PHD work and they thanked me for the quest that's amazing I got some love from India hello India uh I got some good news for you guys in India um my book should be out this week we're so close um I've had proofs printed they look good um they just modified the cover to put an ISBN number on the back side and I've gotta as soon as I'm done with this I'm going to look at that back cover and approve it and then I think we've just got like two or three days where they test the system and make sure that the orders are going to be processed properly and then I think the book is going to be out in India so I I think by the end of the week for certain it'll be and it looks great and the quality is good and get this they're printing it in color this time so I'm really excited anyways uh what else do we got uh we got um uh someone asked what if there are more words does it increase Dimension and the answer is yes uh the more you basically have a dimension for each word or different word we have in the phrases um someone says it's cool to hear and see me say double Bam um um love from Billy Billy which is amazing holy smokes I don't know how that works so if you don't know Billy Billy is basically the YouTube of China YouTube is not allowed in China because it's a Google product and all Google products are not allowed there from what I understand I've actually never been to China so I don't know this is what I've heard so they have this thing called Billy Billy and some of my videos are on there and a lot of them are not I would love to try to update uh I actually have a Billy Billy account that I actually do not know how to update at all I don't uh so I have a friend that helps me but they're super busy and they only help me every now and then so that's something we got I'd like to work on anyways um someone says please do more on NLP and that is the long-term plan um uh someone says what about the module and the size of the vectors and I don't know what that means that's while Lucas Joshua McKay says uh does this work with word embeddings presumably yes um that's how so um I believe that's so I'm going to be honest of how I got to the cosine semi I got there in two different ways one is through trying to teach cat boost which we will pick up on again next week and we will use the cosine similarity the other is I'm working on a video on Transformers which are used a lot as Transformers are a type of neural network that are used a lot in natural language processing especially with automatic translation from one language like a phrase that's in one language to another language and and so you I'm learning about Transformers and I'm learning about attention and then I'm learning about word embeddings and that means I learned about uh word to VEC uh which is I guess short to word to vector and they use once you do that you can once you have the embeddings you can get a sense of how similar things are using the cosine similarity um so that is or something um so uh so here's the thing so someone said oh Pam from Wales that's pretty cool um uh anyways yeah someone wants more about attention mechanism and Transformers and those are right around the corner and by right around the corner I'm talking on the the I'm talking about on stat Quest time scale which things right around the corner is usually like maybe if we're lucky maybe a month probably two from now but that's pretty close for me um anyways someone's really excited about me someone wants to know if there's a way to find jolly and happy if they're similar in natural language processing so one of the ways that happens is instead of just looking at the individual words we'll look at the entire we'll look at the the phrase the we'll look at the entire phrase that the word is in so if we see that happy and jolly are in tend to be in very similar phrases I.E the overall phrase has a very high cosine similarity close to one then there's a then we can then guess that maybe happy and jolly are similar themselves even though they're different words so so yeah so there's a so that's a way to do that uh bam attention is all you need someone said which is a which is a spoof on the the paper that um uh that first presented Transformers which was I think the title of that paper was attention is all you need um so if you don't know so I have a video coming out on Monday on long short term memory which is sort of a sort of an early you know before Transformers was a way to do natural language translation from one language to another uh and what they discovered is that they wanted to focus it was you know there's certain words that are like super important in a phrase that really kind of determine the sentiment and the meaning and so they created this thing called attention which was allowed to La which was a way to focus the neural network on specific words and they could they could attach that to Long short-term memory but then they realized that if you just got rid of long-term shirt memory and you did her left with attention things actually worked even better so uh so we're gonna learn all about that pretty soon um uh so someone says is cosine similarly better than dot product in higher dimensions uh uh in theory it is yes so what um well that I don't know I do know that in theory it's better than the euclidean distance um uh because the distance is all tend to like end up being the same in really high dimensions uh and the cosine similarity helps uh get around that I believe that's correct uh you know it's funny I was reading there were a lot of articles on like cosine simulator versus um dot product versus a euclidean distance and I was like I'm not really interested in that right now what I really want to do is just dive into the cosine similarity itself so I didn't actually read up on a lot of the pros and cons of these different techniques um so I'm no expert is I guess what I'm trying to tell you however what I will say is that the numerator of the cosine similarity is the dot product and the dot product is affected by the lengths of those lines and not just the angle so the numerator is just the dot product what we do is we add the denominator and what that does is that scales uh the the dot product to be between technically between negative one and one but for the uh for language processing where we saw that when things have nothing in common the lowest value we get is zero and when they have everything in common the highest values is one so for language models the cosine similarity typically runs from zero to one uh anyways that's what the denominator does is it scales the dot product to be within an easy to interpret range uh and that's that makes it easy to compare uh different phrases uh that may have different Vector lengths uh if we don't want to compute everything and so that's that's kind of a useful thing uh someone asked if I can suggest a vector database and the answer is no no I can't um someone asked to please make some videos on recommender systems and sure I'd love to do that um uh someone asks uh oh I guess they they had a question um but they can't stick around to listen to the answers so we'll just skip that I guess but I'll think about it someone asked about you know when there's words that mean the same thing uh or they mean different things like the like the same word can be used in different contexts and in different contexts it can mean different things um uh and they want to know how Google translate can deal with that and I think it has to do with attention mechanisms and the fact that attention can look at get the get the big picture of what the phrase is about and then figure out what those words in the middle are all about oh someone asked if this is the pi torch lighting shirt and the answer is yes I love this shirt this is probably my favorite shirt by the way because it makes me feel like I'm a superhero when I wear it um uh do you play the piano too um uh and the answer is a little bit I play a little bit of piano someone asked uh do we apply cosine similarity in text analysis and the answer is yes um yeah uh I mean that was what we talked about the first 15 minutes um someone says thanks a lot um and hooray uh someone asks if I can recommend some text for mathematics related to deep learning and the answer is no um I mean the mathematics related to deep learning is his basically multiplication and addition um I mean there's a there's a couple of twists on that there's it's not that fancy however everybody swears that you need to know linear algebra and I've actually found one case where it's useful to know linear algebra and it has to do with the output of the long short-term memory Pi torch function um and I was like oh it would be nice to know linear algebra and to understand what's going on here uh in that it it can take a single number and it can basically spread it out into more dimensions and that's a linear algebra trick that although it's just a function of multiplication and addition is kind of a cool trick and that'd be worth knowing about however can I think of a good book on linear algebra and the answer is no like resounding no I've tried I'm going to be honest I've tried I've read like four books on linear algebra I've tried to learn that stuff and I learn it for about 10 minutes before I completely forget everything linear algebra is like some people just think it's the most useful thing ever and it's kind of cool but what I return what I Come Away with is just enough to where I can look at Matrix stuff and go oh that's just a bunch of addition and multiplication and then I just write it out in longhand and then I go oh I see what's going on now um so I'm not a huge fan of Matrix Algebra I feel like Matrix algebra is super useful for computers right yeah um you know if you want to write an efficient program that does a lot of deep learning computations very quickly you want to use Matrix algebra or linear algebra to do that that's awesome but to actually learn about the concepts of what's going on I'm not so convinced it's super helpful um uh what else we got uh maybe someone else if someone else by the way has a favorite linear algebra textbook put that in the comments or not the comments put that in the chat um someone said that the sentiment about forgetting linear algebra is so relatable yeah I I've got I mean golly I've tried I swear I've tried four different times at least four times and I'm I'm you know and I and I know it I know it for about two months and then I forget all of it it's just gone uh someone asked if I have more videos about uh Pi torch lightning coming up and the answer is yes hopefully in the next couple of weeks I have one video coming out on the theory of long short-term memory coming out on Monday it's already out for uh channel members and patreon supporters who get early access to my stuff so they can see it right now but for everybody else it'll be out uh a week from yesterday which is not that long from now um and then a couple I'm hoping like two weeks later I'll have my long short-term memory and Pi torch plus lightning and we're I'm really excited about this um because uh the lightning stuff is gonna I mean it's gonna make it super easy uh we can do for example we can do our training and we can generate all these really cool super helpful graphs without having to do anything you know it's like sort of just built in you just say hey draw these graphs and you don't have to like be that specific you can just say there's this thing called tensorflow tensor not tensorflow tensorboard excuse me tensor board you just say let's use tensorboard and tensorboard comes up in a web browser and it shows you all these awesome metrics about how well the training has gone and then if you're like well it doesn't look like we're done training you can use lighting to pick up where you left off you don't have to start over all the way from zero and like train again you can say oh looks like we need more training so we'll just pick up where we left off and train a little bit more uh and uh so we're going to show you how to do those things in lightning which are super easy and awesome and I love them so uh so yeah that's coming out in a couple of weeks uh can I recommend books for advanced learning of machine learning um and I you know I always recommend my friend Sebastian rushka's book on machine learning and pytorch or and psychic learn psychic learning pie torch uh hold on I've got the book I'll be back this was written by my friend Sebastian rashka he's an awesome dude and so machine learning with pi torch and scikit learn uh I don't know if it's for advanced machine learning uh but I've I mean it's got everything in it the book is a monster it's like how many pages almost 800 Pages it is a Bruiser um anyways it's got everything in it uh including like it does pretty Advanced machine learning it does convolutional stuff um uh anyways he also by the way he's got a he's got all kinds of cool stuff he's got a cool newsletter I'm just going to talk about how awesome this dude is for a little bit he's got a cool newsletter he's on Twitter he's got cool stuff on Twitter um he's got this great blog that he's always adding awesome stuff from that I've learned from um so uh yeah he's just a cool dude to know in the field and to follow and to keep track of what he's doing uh someone else suggests deep learning by Goodfellow at all so that uh I guess comes recommended so check that one out as well I don't know that one um so someone says let's see let's move this do you do you have any idea in which situation using which explainer it is is it not relevant to log a high skewness feature during pre-processing on a fully understand that question so we'll just move on which use cases and text analysis use cosine similarity extensively I know it's used in the cases similar to Market to basket analysis uh I mean it's I think it's used to evaluate how similar phrases are like from that are output from word to VEC uh and also uh using so which basically boils down to any kind of word embedding thing um so I think that's helpful um I know someone says wow what a book yeah Sebastian writes a big book um but he's got a lot to say so that's pretty cool um anyways I've powered through most of the questions that I can answer if you have any others feel free to post them uh otherwise uh I'll get i'll get back to work on my next stack quest which is going to be long short term memory in pi torch plus lightning which I'm really excited about so how to calculate similar between two metric matrices instead of a vector um I don't know you could probably vectorize the matrices somehow um but that's I guess the assumption is both matrices are the same size same dimensions uh and if there are you can just turn it into a a vector and have the you know and then just do the um similarity on that do I think AI is getting easier and the answer is yes I do um especially with lightning to be honest it's like that's one of the reasons why I started working at lightning is that they were like working really really hard to make uh neural networks way easier to do um and so yeah I'm I do think they're getting easier I mean there's a there's also a lot of Auto ml uh which is cool where uh you don't actually have to Tinker with the models yourself you can just kind of get them all to run simultaneously and then pick the ones that you think are best uh it still helps to know how to interpret the output of these things um um uh because otherwise you'll have no idea what the output is so the theory you know you still have to know things uh so that part isn't getting easier um uh unless well you could say that because I keep making stat Quest videos on the topic it's getting easier to learn the theory as well so maybe that works as well um oh Steve recommends another uh machine learning book called Hands-On machine learning with scikit-learn Kara's tensorflow Concepts tools and techniques to build intelligent systems by jiron Third Edition so Steve is is a wealth of information in terms of of these books um uh someone said could I post the name of the book that I was holding some time back up are you talking about this one machine learning with pi torch and scikit learn um that's that um oh someone that recommends a good linear algebra book that's good um and they're in Brazil uh which is cool they say good learning algebra book so it's arlindo galavao it says a good linear algebra book I just butchered His Name by the way is linear algebra for scientists and engineers um oh so someone wants to know um if you should use a you should so it say like you've got a lot of skew how do you decide between scaling that feature or using the logarithm on that feature um it depends on the scalar um the logarithm I mean to be honest I tend to think of the logarithm as like that's my go-to when I see like uh a strong skew and it looks like it might be logarithmic um uh yeah so that's my go-to uh scalers can be helpful for centering and sort of uh making things a lot of people do what's called a z scaling um so you subtract the mean and divide by the standard deviation of all the features um uh how do you decide between using one or the other when you don't know anything about the data and you refuse to graph it and look at it I think maybe going with the scalar is probably the best thing however if you do know something about the data or you have the time to graph uh the data and you can see that it's logarithmic or you know that it's logarithmic I think that's much a much better way to transform the data and it'll probably get better output and better results doing it that way so it just sort of depends on how much time you have and what you know um anyways yeah thanks for posting the name of that book typing it in I could type it out on myself but it would take like half an hour especially because I know people are watching me type and that would be a disaster uh typing with somebody watching you or watching me is a lot I feel like it's like like driving down the road with like the police right behind me you're just like ah totally stressed out um so that's that um so yeah like I said uh we'll do another live stream in two weeks which I think will be interesting I'll actually be in Barcelona in two weeks um but I'll do another I'll do a live stream for my hotel room there uh I'm will and we'll just follow up on the catboof stuff I think while I'm flying over on the airplane I'll work on the live stream there we're going to talk about how to build trees for catbooth so far we've already talked about um how to convert categorical features into numeric values using catboost method we talked about that in the last live stream now we know how to use the cosine similarity and that's part of how trees are built and then it's like a step in how trees are built for catboost and then so the next live stream we do will be sort of like the details the Nitty Gritty of building trees with cat boost and that's pretty much 99 of cat boost right there so I'm pretty excited because I've wanted to kind of cover cat base and the idea I don't know if you guys are are aware of this the idea for these live streams is kind of just like to motivate me to kind of get more content out there and then once I got a bunch of stuff I can assemble it into a single stat Quest like I could do a um a stat Quest on on catboost and I can do a stat Quest on cosine similarity and things like that so uh so this is my way of like getting material out there faster than I would have normally done it's a way of like motivating me someone asked why don't I start my own newsletter and that is something that I would like to do it one day one day um uh and I'll keep that in mind uh um that maybe I should do my own newsletter oh that'd be cool especially when like you know if I could if I could like I could talk about like not just my own stuff me but also we could talk about um sort of what my favorite resources are whatever I just stumbled over on my favorite web page for reading about something whatever I'm learning about I could say hey check out these resources they're awesome um so someone says uh that they use linear regression for my sales statistics can I quit the linear regression for something better I'm allowed to use only Excel well that is a tricky situation to be in if you can only use Excel um I don't know uh you can plot the data um I mean you gotta you got options I jeez but what are they and I don't actually know can you do logistic regression in Excel I don't know um I don't know I do know that there's some templates out there for zooming really complicated things in Excel and you might want to muck around with those things but my gut feeling is that if you're restricted to only using Excel uh you're probably restricted to pretty much using linear regression um so so um anyways yeah sorry I'm just reading the comments right now um so I think that's it I think we're done for the day uh anyways we'll do a little silly song brittle there so thank you for watching my live stream thank you very much and have a great day and I hope to see you in two weeks time when we finish talking about cat boost hooray hooray pray thanks for everybody I hope to see you soon in two weeks time and and get get excited about the long short term memory video that's going to come out on Monday hooray
zxagGtF9MeU,2022-10-31T04:00:30.000000,Happy Halloween (Neural Networks Are Not Scary),don't be afraid of neural networks they're not scary even when a neural network says boo hello I'm Josh starmer host of the stat quest with Josh starmer YouTube channel and I'm here to say that neural networks are not scary now the term neural network may sound really fancy and complicated but all a neural network does is fit a squiggle or a bent shape to data for example if you've got a data set like this then a neural network can fit a squiggle to that data bam and if you've got a fancier data set like this then a neural network can fit a big shape to that data double bam and if you've got a crazy complicated data set like this well a neural network can still fit a squiggle or a bent shape to that data triple bam hooray [Music]
K8ykIS1x_qQ,2022-10-25T04:00:06.000000,"Handmade Pasta, Clearly Explained!!!",I like to make my pasta that's true and I'd really like to make some pasta with you that Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about handmade pasta and it's going to be clearly explained I like to make hard things easy to understand and lightning makes hard things easy to do and that's Grand hello normal Source hello stat Squatch hey Norm I thought stat Quest was supposed to be about statistics data science and machine learning topics but I heard that this Quest is going to be about making handmade pasta you heard correctly Squatch the first video Josh ever posted was a cooking video and since today October 25th is World Pasta day Josh is going to talk about how to make pasta by hand this is important because most recipes for handmade pasta are terrible and Josh has been making pasta since he was a kid and has a bunch of good tips to share well okay as long as this doesn't happen all the time don't worry Squatch a quest on Long short-term neural networks will be next damn most recipes for making handmade pasta say you should dump flour directly onto your kitchen counter and mix it with eggs this is a bad idea because it creates a huge mess instead you should mix the ingredients in a bowl here I've added 10 and a half ounces of flour or two cups to a bowl and to that I add three eggs Now using just the tips of your fingers combine the eggs with the flour notice how the bowl keeps the mess contained and it isn't going all over the place once you've Incorporated the eggs into the flour use your whole hand to combine it all into one big ball once you've created a big ball of pasta dough wrap it up in some cling film now put the wrapped up ball of pasta dough into the fridge and let it rest for a few hours personally I like making the dough in the morning and then it's ready to go when I get home from work at night anyways after you've let the dough rest for a few hours take it out of the fridge and now let's get started making pasta now after unwrapping the dough cut it up into seven or eight equal sized pieces now we need to flatten each piece of dough so that we can make long thin noodles today I'm going to use the pasta roller attachment for my KitchenAid mixer however pretty much all pasta rollers that I've ever seen work on the exact same principle the first thing you do is set the rollers to their thickest setting which is usually either one or zero depending on the roller now take one piece of dough squeeze it flat and run it through the rollers fold it in half and run it again through the rollers fold it in half again and run it through the rollers and put that first flattened piece of dough on a kitchen towel now take the second piece of dough run it through the rollers fold it in half run it through the rollers again fold it in half again and run it through the rollers a third time and then put it on the kitchen towel and repeat for the remaining pieces of dough Now set the rollers to the next setting so that the rollers are slightly closer together pick up the very first piece of dough that you worked with and run it through the rollers but this time just do it once now put that piece of dough back on the towel and run the second piece of dough through the rollers now run the third piece of dough through the rollers and the fourth piece of dough and the fifth piece of dough and the last piece of dough now crank the rollers up to the next setting and run each piece of dough through the rollers and then you repeat that process cranking up the setting on the rollers and then running each individual piece through the rollers until you've reached the desired thickness for your noodles for spaghetti noodles that means going all the way up to the fifth or sixth setting on the rollers here's what each piece of dough looks like after going through the rollers at setting six double bam now when you're ready to cook your pasta bring a large pot of heavily salted water to the boil and when the large pot of salted water is boiling we're ready to cut the sheets of pasta dough into noodles again here I'm using the attachment for my KitchenAid mixer but pretty much every pasta machine I've ever used works on the same principle you simply take a sheet of pasta and run it through the cutters [Music] and once you've cut all the sheets of pasta into Noodles put the noodles in the boiling salted water and cook for one to two minutes lastly drain the pasta and serve with your favorite sauce here I've got some homemade Skyline Chili triple yum now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stackquest PDF study guides in my book The statquest Illustrated guide to machine learning at stackquest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on
EzjtTh-WUWY,2022-10-18T15:55:23.000000,Live Stream - More details about Target Encoding/AMA/Silly Songs,foreign [Music] hello I'm Josh charmer and welcome to the stat Quest live stream uh I'm super excited to be here and I hope you're excited to be here too I'm going to put on my ukulele on the floor so that um I don't drop it while I'm talking um also I want to apologize because I know the last time we did a live stream um I said that I was gonna play the Tabla those things in the uh on the on the uh on the big harmonium behind me um and I was like it was like two weeks ago right and so I was thinking um I got plenty of time I I've got lots of stuff to do so I'll just wait to practice the Tabla until a few days before the live stream and it'll be awesome well I got to a few days before the live stream was like and I was like ah I've got so much to do this is crazy um and so I didn't have any time to practice the Tabla uh and this is bad because people even reminded me I got little notes in the comments like hey don't forget to practice the Tabla you know and uh oh my golly and I was thinking about it I just didn't do it and so maybe maybe maybe we'll do it next time I've also you know it's hilarious I also have a um a Tambora uh that maybe I'll bust out and tune that up and I'll see if I can get a friend to come and play that uh give me a little drone in the background that'd be pretty cool I think um anyways uh anyways welcome everyone here um I'm really excited I'm just looking at the comments right now um so it looks like someone's got a job interview tomorrow uh good luck uh I hope this is helpful um and I'm glad you like the new song I actually kind of messed it up because uh I was gonna the uh so the song I had it all planned out uh not that I practiced it at all but I had this plan and I was and and what I was going to do is I'm gonna say Target encoding without leakage is cool so I was gonna do like a gap um when I said without so it'd be like without without what without music without you know any sound you know it's gonna be like a little play on the word without kind of musically um and that didn't happen um but you know it's a thought that counts so anyways without further Ado I'm really excited you guys are here uh let's dive into targeting coding without leakage which to be honest sounds kind of gross right so uh yeah um maybe not the most um nice topics to to bring up with your parents or whatever um so anyways without further Ado let's talk about targeting coding without leakage oh by the way um feel free to post questions in the chat um I won't read them while I'm doing this because I can't see them and read uh and go through the stat quiz at the same time but at the end I'll if we have time and I hope we have time I'll go back and um read the questions and and to be and and I'll get to some of them I won't get to all of them uh I'll read all of them though uh so even excuse me after the live stream is over I leave the window open for a little bit so I can read all the questions that people have and hopefully we can get to them next time all right so for the third time without any more further Ado uh we're gonna talk about Target encoding without leakage beep okay so first uh let's review some basic encoding strategies uh so because we talked last time we did a live stream we talked about uh Target encoding so we're just going to do a quick review of the basic strategies so when we first talked about targeting coding we had this data and we wanted to use favorite color beep and height poop to predict if someone loves Troll 2 which is a really terrible movie in this case a favorite color has three discrete values blue red and green now in theory discrete features like a favorite color are fine for most machine learning algorithms but in practice a lot of popular machine learning algorithms including neural networks do not work well with them as a result discrete data are often converted to excuse me are often converted into numerical values before being used for machine learning one when we excuse me I'm getting ahead of myself when we don't have many discrete options one hot encoding is commonly used and we talked about this last time so we're just gonna like Boop we're just gonna go right through it real fast um but the problem with one hot encoding is that it adds columns to the data set in this case we replaced the favorite color column with three columns one for each color and that's not a big deal you know we our data set's a little bigger than before but it's not like overwhelmingly huge um however when we have a lot of options for example if we had a column of postal codes and there are 41 683 postal codes in the United States whoop what happened then we would end up replacing the one postal code column with 41 683 new columns which might make the data difficult to work with so when there are tons of options instead of using one hot encoding people often use Target encoding one of the most popular Target encoding strategies is to replace each opposition each option each option oh I just have like there's a disclaimer a disclaimer um this morning I woke up at 1am and could not fall back asleep I guess I was excited about the live stream I don't know but I've been up for a long time and I'm a little sleepy so I've got a feeling I'm just gonna be like um saying all kinds of weird things so stay tuned anyways back to the stat Quest so one of the most popular Target encoding strategies is to replace each option with a weighted mean of the target the thing we want to predict beep for example in order to use the target excuse me in order to use Target encoding with our data we start by plugging the mean of the target for blue 1 divided by three then because three people were used to calculate the mean for blue we plug in 3 for n ope then we plug in the overall mean for the Target and by the way the target is loves Troll 2 which is 3 divided by 7 because overall three of the seven people love troll too now we just need to pick a value for M the weight for the overall mean m is a user-defined parameter or hyper parameter and in this example we'll set m equal to 2. [Music] setting m equal to 2 means that we need at least three rows of data before the option mean the mean we calculated for blue becomes more important than the overall mean now we just do the math and we get 0.37 so we plug in 0.37 for blue beep now we calculate the weighted mean for red and we get 0.29 so we plug in 0.29 for red lastly we calculate the weighted mean for green [Music] and we get 0.57 so we plug in 0.57 for green anyways uh I got ahead of myself here so so we'll just say bam that's exciting but anyways note some of you may have noticed that we are using the target the thing we want to predict to modify the values in favorite color and doing this sort of thing is a data science No-No that we call leakage leakage results in models that work great with training data but not so well with testing data so anyways because we're using the target which is loves Troll 2 to modify the values under the favorite color column we're we're creating some sort of we're kind of establishing a relationship between the ones and zeros in loves Troll 2 and the numbers we get in favorite column and that relationship is going to mean we're probably going to overfit this training data so in other words leakage results in models that are over fit the good news is that there are a bunch of relatively simple ways to avoid leakage so that you can use Target encoding without overfitting your model so let's go back to the original data set that had blue red and green categories for favored color and talk about k-fold targeting coding which is probably the most commonly used method that avoids leakage note the word fold in k-fold Target encoding refers to splitting the data into equal sized subsets and the K refers to how many subsets we were we create for example if we did two-fold targeting coding then we would divide the data into two equal size subsets note because we have an uneven number of rows we just made the subsets as similar in size as possible so we have one subset with four and the other subset with three good enough okay now to make it easier to keep track of things let's label the first subset a and the second subset B now to Target and code blue in subset a we ignore the target values in this subset and instead plug the target values from subset B into the weighted mean equation we start by plugging the subset b mean for the of the target for blue zero divided by one because one person in subset B that likes blue excuse me because the one person in subset B that likes blue does not love troll 2. so it's zero divided by one then because there's only one person in subset B that likes blue we plug one in for n beep [Music] then we plug in the overall mean for the Target in subset B one divided by three because overall one of the three people in subset B loves Troll 2 or only one and just before excuse me just like before we'll set m equal to two which means we need at least three rows of data before the option mean the mean we calculated for blue becomes more important than the overall mean now we just do the math and we get 0.22 so we plug in 0.22 for the two rows in subset a with blue now we need to Target and code the one row with blue in subset B so we ignore the target values in this subset and instead plug the target values from subset a into the equation for the weighted mean so we plug in this subset a option mean for blue one-half n equals 2 because we've got two rows with blue in subset a and the overall mean 2 divided by 4 because two of the people in subset a two of the four people love troll two beep and just and just like before we set m equal to two Boop then we do the math and get 0.5 so we plug in 0.5 for blue but only in subset B beep note you may have noticed that the different subsets have different values for blue so in subset a we've got 0.22 for blue and in subset B we've got 0.5 this is okay because favorite color is becoming a continuous variable just like height now let's encode the color red in subset a so we ignore the target values in subset a and instead plug the target values from subset B into the equation for the weighted mean now because subset B doesn't have anyone who likes the color red the mean for red is zero and n equals zero because nobody uh there's no one who likes red and the other values are the same as before and we end up with replacing red in subset a with 0.33 likewise green and subset a uses the target values in subset B and turns into 0.42 and green and subset B use the uses the target values from subset a and turns into 0.67 now that each color in each subset has been encoded we merge the subsets back together and we're done hooray bam now going back to the original data with blue red and green if we set k equal to 7 because remember we're doing k-fold targeting coding then we would divide that data into seven subsets now to Target encode the first subset which consists of a single row with favorite color equal to Blue we ignore its Target value and use the target values from all of the other subsets to calculate the weighted mean likewise encoding the other subsets would use all of the other Target values except for their own note when we use all of the target values except one to do the encoding we call it's called leave one out Target encoding a quick scan of the internet shows that some people are successful with this leave one out Target encoding and other people are successful setting k equal to five so using five subsets so um so if you have a lot of data you can split it into five so I've seen I've seen both things and I've seen uh uh people say that they've had great success with cowgirl competitions using both methods so uh so yeah you can try them both double bam now that we understand the most common ways to do Target encoding without leakage let's talk about a relatively unique approach used by an algorithm called cat boost so people have been asking me to cover cat boost for forever and right now right here is my very first little foray into catboost so you guys are the first to see it I guess you're the first to see everything today it's all brand new but this is like something people have been asking me for for years and so you know cool that you guys are here right now I guess okay so first let's go back to the original data set with blue red and green and instead of splitting the data into subsets catboost treats each sample as if it were fed into the algorithm sequentially for example catboost treats the first row with blue as if that is all the data it is received so far and that means the cat boost excuse me that means cat boost ignores all the other rows when Target encoding blue or not just all Blues this specific blue I should be clear there that we're not talking about all the blues we're just talking about the blue in the first row and when when it's encoding that blue that specific one it ignores every other row another thing different about cat boost is the equation um I got this equation from the documentation if you look in the manuscript the original manuscript they have a different equation that's actually identical kind of to what we were using before but in the actual documentation for the for the implementation so like you know there's like the theory paper and the theory paper has one equation and then you go to the user manual for the actual cat boost and they have a different equation this is that a different equation so if you look at the original manuscript you'll see something different that's a lot like what we were using before but if you look online at the documentation of how to actually use CAD boost in practice you'll see something like this it won't be exactly like this um uh but this is equivalent to that I've chosen to to change it a little bit just to show how it's related to what we were doing before okay the big difference is that instead of using an overall mean it uses a user-defined prior or guess that is usually 0.05 to be honest I don't know what the default value is I looked in the documentation and I could not find one um but the but the user documentation has an example of how to how catboost does it encoding and it uses 0.05 in that example so that's the best I can do at least for now unless someone sends me the URL to the to the documentation that explains where this value is set okay anyways the cat boost equation also simplifies the denominator by just adding one to the number of rows rather than a weight now given the equation cat boost plugs in values derived from all of the data that came before the current row and this is key this is the thing that makes cat boost work and not uh this allows it to avoid leakage okay and that means since we are starting with the first row and no data came before it we plug in 0 divided by zero for the option mean so we can only use data that comes before the sequence uh the row that we're actually trying to encode and there's no data before that so everything's just zeros and we plug in 0 for n because yeah there's no there's no values that came before and when we do the math we get 0.05 so we plug in 0.05 for blue in the first row now we work on the second row because none of the preceding rows have read as the favorite color we set the option mean to zero zero again and again zero excuse me n equals zero so we replace red in the second row with 0.05 so just like we did for the blue likewise we replace Green in the third row is 0.05 again we do this because none of the preceding rows also have favorite color equal to Green however in the fourth row things finally change now because we've seen blue before in the first row we use it to calculate the option mean s remember we only use the rows that came before the one we are working on in the equation so we just use the first instance of blue in this case so in this case because the one person who liked blue before also like Troll 2 the option is one divided excuse me the option mean is one divided by one [Music] and n equals one so we plug in 0.525 for the second time we see blue likewise the second time we see green we plug in 0.525 beep however the third time we see green we use the two previous times when calculating the option mean and the option mean is 2 divided by 2 because both of the two previous people that liked Green also like troll 2. beep and that means we replace the third occurrence of green with 0.683 lastly we replace the third occurrence of blue with 0.35 beep because only one of the two previous times we saw blue also liked troll 2. and thus this is how cat boost performs Target encoding and because the order of the data makes a difference in the encoding this method is called ordered Target encoding actually they in the in the in the in the manuscript the original manuscript they call it ordered Target statistics but everybody else calls it Target encoding so we're just going to call it order targeting coding it's much more common to call it Target encoding all right so bam so why this small bam because the justification that catboost provides for why it does things this way instead of just using leave one out encoding doesn't make any sense the authors of catboost claim that when there's only one option like blue so what you see here instead of having blue red and green all we have is blue blue okay we got a ton of blue so the author's claim that when you only have one option like blue then leave one out Target encoding results in leakage and thus you should not ever use a level and Out target encoding to their credit the thing about leakage is true when we only have one option like blue in this case then leave one out targeting coding does in fact result in leakage but regardless of whether or not it even makes sense to include a column with a constant value in the model when we only have one or two options everyone including the cat boost algorithm itself uses one hot encoding so we just change everything to one we don't actually do leave one out Target encoding because there's no reason to do it you just replace it with ones if you even leave the column in your data set you probably should take it out anyways uh so the leakage problem with only one option turns out to not be a problem because it's like no one would ever do this right and a cat boost itself doesn't even do it I mean nobody does this uh when there's only one or two choices use Target encoding because it does not increase the number of columns in your data set uh it's just like what we did with loves troll too we only we had two options we had yes and no and we turned those into uh ones and zeros it's no big deal so I'm I'm not actually convinced uh that leave one out uh uh Target encoding is a huge evil thing that is going to cause problems all the time in fact tons of people use it all the time and are very successful so um I'm a little annoyed that they gave such a lame reason for using such a bizarre way to uh code uh categorical options so yeah this is what catboost does but I don't think it's really Justified wow however rather than end on a sour note the important thing is that regardless of the justification catboost works and it works well and that's an important lesson about machine learning unlike math where every little thing can be logically derived and has a pure beauty machine learning is all about results and doing whatever it takes to get them and sometimes that means getting a little messy so triple bam hooray um so yeah that is uh that's uh we've talked about cat boost without leakage and we've excuse me we've talked about targeting coding without leakage and we've also started to talk about cat boost and I'm thinking we might just keep talking about cat boosts until we've fully understand that algorithm which is kind of tricky because the documentation is not great in the manuscript and how it's actually implemented are kind of different so it's a tough It's kind of tough and um it's hard to be confident that I know what I'm talking about because I see from the same Source I see two different messages um so that's but that is the plan we'll we'll we're just gonna stick with this for a little while and see how far it goes so with uh uh so without further Ado uh let's just open this up to an AMA which is ask me anything and what I'm going to do is I'm gonna go through the comments and I'm gonna see if we have any questions about what we just went through um boo [Music] someone asked will this session be recorded and available on my channel and the answer is yes uh so that was an easy question answer so uh someone else said what happened when we are using the model for inference on unseen data in that case we don't have the target and how are we going to apply k-fold Target encoding so what you do is uh when you have when you're not training so when you're not training what you do is you just you you can then just use um all of the data uh to to figure out because there's no uh there's no leakage in that case because it's almost like it's almost like we're doing leave one out with our new uh sample does that make sense so when we did leave one out targeted coding we took one row and we used all of the other data to encode it uh likewise when we get new data and we're trying to classify it we don't actually even know what the target is uh we we then use all the other data that we use training wise to um to encode it so that's how that works bam um so someone asked what if there are lots of similar values but not all in the column does it still cause leakage and I'll be honest I don't understand that question uh Kumar if you can clarify um I might help maybe I can um maybe I can do that anyways um what else do you guys have a good weekend I did uh I went for a I went for a long walk in the woods where it's fall here uh and the trees are turning pretty colors and um and I had a nice walk in the woods okay well excuse me someone said what if Target is numeric so the regression problem um it's the same thing you just calculate the means um you know the average you know the option mean and the overall mean it's the same uh nothing changes uh so that's that's easy so it works either way so it works for categories and it works for regression or excuse me it works for classification if you're just joining us uh uh I woke up at 1am uh this morning I have no idea why uh potentially because I was so excited about this live stream uh but I've been um I've been I've been up for a long time and uh my mouth and my brain are doing two very different things and it's kind of comical okay uh so someone asked why is my book why why is your book very expensive in India so right now it should not even be available in India um so if you're seeing it for sale that might be a used copy that someone is marking up the price um uh that being said printing in India is not easy printing excuse me printing color in India is not easy nor is it cheap this is a mystery to me um a total mystery to me it is it's it's been very expensive um that being said I have contacted a printer in India and they are printed they've printed out some proofs of my book and they're sending them to a friend of mine who lives in Bangalore and when he gets the proofs he's going to look at him and inspect the quality and make sure it's good quality and if it's good quality he's going to let me know and I'm gonna we're gonna flip the switch and we're gonna start selling these um these books in India they're not going to be super cheap they're going to they're going to cost probably as much as they cost in the U.S maybe a little less but not much less uh and that's not that's there's nothing I can do about it it's just the cost of printing color in India um so that's sort of a bummer but it's what it is um so uh so there's that oh someone asked uh so someone okay so some Kumar has rephrased their question um and yeah if um so if they said what if you have 50 blues and three Reds in an example um well if you just have two again you can just use uh uh we can just use one hot encoding but you know assuming we're going to use targeting coding um uh that's why that's when the um the overall mean uh kicks in uh it compensates for our lack of knowledge about how red is behaving um and yeah so you just kind of do it as you would normally do there's not you don't have to worry really that much about the imbalance in the data um oh someone asked if I'll explain what I mean by leakage leakage has to do with um uh when information about what we're trying to predict um the specific thing we're trying to predict so in this case um we'll go back to the a keynote so in this case we're trying to predict Love's troll 2. uh and we're trying and in this case one represents yes and zero represents no so we're trying to predict that and uh the state you know that without using k-folds or leave one out Target encoding or using this cat boost method Target encoding if you just if we just use the the full data set like we did early on to calculate the numbers that means that this number one so in the first excuse me in the first row I got my little permanent pointer in the first row that number one loves Troll 2 would have an impact on the number that we use to encode for the first blue instance of blue and that influence so now there's a relationship between this number and this outcome value and what that means that and that's the leakage right there and what what what the result of that is is that our model because it knows about what the output is supposed to be because favorite color has encoded into it some bit about what Troll 2 will be um then we overfit and we we may predict the training data very well and we're going we'll be like hooray this is doing great with our training data but when we use it with testing data or you know new data from the wild our model will probably perform a lot worse than it did early on and so that's why we avoid leakage um by the way you know it's again machine learning is very pragmatic um and so just because you have leakage doesn't mean your model is horrible and should you should be ashamed of it if it performs awesome then it's awesome that's all that there is to it but generally speaking uh trying to avoid leakage uh improves model performance um okay someone asked what my job is uh I have I have three jobs um one I uh I do stat Quest um what you're seeing right now in all the videos that's my job the other uh job I have is I work for lightning I've got my lightning t-shirt on um yeah I work for lightning and I am a um what they call an AI educator um and working for Lightning's super fun um what I love about uh what I love about it is you know if you know stat Quest and you know me you know that what I like to do is to is to um make complicated things easy to understand so yeah in a nutshell I like to make complicated things easy to understand and what I like about grid and why I why it's such a good fit for working there is they like to make complicated things easy to do and so it's it's kind of perfect so they do uh the um not specifically but they do a lot of stuff with neural networks in the cloud and their goal is to make sort of machine learning and AI super easy to do in the cloud um and this is something that I've wanted to teach I've wanted to teach how to do Ai and machine learning in the cloud because um you know when I was in school I always hated that I would learn one thing in my classroom and then I'd go to my job and it would be completely different my goal is to like give you a super realistic this is how production AI is done tutorials um and lightning is helping me with those and so that's another job the third job I have is to write books uh some of you have heard of the stat Quest Illustrated guide to machine learning some of you have copies which is awesome some of you are waiting very patiently for copies uh because I've been having a heck of a time roll pulling out printing all over the world but we're on the home stretch and hopefully I'll have that all resolved in the next couple of weeks but anyways uh I so I write books and so I'm I've got the stat Quest Illustrated guide to machine learning and I'm just started the stat Quest Illustrated guide to statistics so those are my three jobs um there we go uh so what's the next question we see we say have you heard about the optuna API and the answer to that is no uh someone it's a it's a way to optimize machine learning engines and I love it so much so I'll look into it that sounds cool uh someone else gave me a cute wave which is cute um uh what else do we have um if I'm predicting a Target value using only categorical variables by one hot encoding then and I get a large coefficients in the model what can I do to improve my model other what else can I do to improve my model um that you know at that point you might look into some feature engineering it also depends on the type of model you're using um so you might just try a different type of model some models are better with sort of non-linear interactions than others um so one try a different model two um consider something called feature engineering and that has to do with sort of combining uh features to make sort of um um what do we what do we call them it has to do with combining features so that you end up with like a new feature that is somehow the product of of two others or more two or more um so you got some options um okay uh what are the different types of encoding techniques used in industry for nominal and ordinal data um that's a good question um uh it's possible that uh you know I'm gonna be honest I can't I I think ordinal data is like ranking data and it's possible that um that's just treated like numeric data and you don't necessarily need to encode it because it does have sort of you know a linear relationship between like say like you're on a scale of one to ten one being the best you stack Quest ever and ten being the worst stat Quest ever you know we've got this like linear Square scale um you may not need to actually encode that one um uh someone asked we can do a session on H2O Target encoder estimator um maybe probably not um maybe of H2O contacts me and gives me a lot of free time um we'll see um uh yeah so yeah so someone asked if um uh about the equation that you see here um so we've got n times option mean and someone says doesn't isn't that just the sum of of things and it is it's the it is the sum of the times we've got one here uh so if we're doing it for blue you know we might sum the you know 1 1 and then zero so we'd get two or something like that yeah so that's just a I'm just replacing the sigma uh it's debatable as to whether or not this is the best approach to presenting this this is one of the things about live streams is you kind of get to see stat Quest work in progress you get to see things in advance and sometimes you see things are a little uh dirty um and so the the motive for leaving it like this was it links us back to the original equation where we had a weighted mean and we see that this part on the left n times option means sort of divided by n that part of the equation did not change one bit um however the stuff on the right is you're adding 0.5 0.05 and adding 1 that did change and so I was trying to emphasize what's similar and what's different rather than making the equation look completely different using the sigma notation and the reason why I didn't use the sigma notation to begin with is I wanted to emphasize how this is a weighted mean and it's a weight between the option and the overall um so that's sort of what I was shooting for um that's sort of what was my my mindset between like how how I wanted to emphasize things um uh uh someone asked uh about literature and recommender systems and the answer is I don't know I don't actually um this is going to be embarrassing I actually don't know that much about recommender systems um uh there may not be that much I need to know uh given the other things I know but I actually don't know I can't talk to you about details of that uh at least not right now oh you were using so the person I was having trouble was using linear regression yeah linear regression is not going to do well at all with um uh what do they call it oh my gosh this is where you're seeing Josh Dormer who woke up at 1am so I've been awake for almost 11 hours now um oh so um yeah so when there are non-linear relationships and that's that's gonna that's just the first thing that topped in my head there's probably an easier way to to phrase that between the thing you want to um thing you want
Za9wm6HWkvo,2022-10-05T04:00:22.000000,Live Stream - Target Encoding/AMA/Silly Songs!!!,[Music] thank you Tuesday we're gonna learn about targeting coding hip hip hooray Tuesday we're gonna do a stat Quest live stream today hooray I'm Josh Dormer and welcome to the stat Quest live stream again I need to put this very carefully on the floor and it looks like one of my cats just came in the room if you just saw that Poe is uh walking through um anyways hello po um today yeah today we're going to talk about targon coding we're gonna do a lot of cool stuff and then we have lots of time for questions at the end um so without further ado let's move on to talk about Target in coding and actually um we're not just going to talk about Target encoding what we're actually going to talk about is one hot encoding label encoding and then Target encoding so bam okay imagine we had this data and we wanted to use favorite color and height to predict if someone loves a Troll 2 which is a really terrible movie that I know I've talked about for years but I only just recently watched a few months ago and it was as bad as everyone said it is um so yeah so but some people love that movie so let's go for it in this case a favorite color has three discrete values blue red and green now in theory discrete features like favorite color are fine for most machine learning algorithms but in practice a lot of popular machine learning algorithms including neural networks do not work well with them as a result discrete data are often converted into numerical values before being used for machine learning one popular method for converting discrete variables or features into numbers is to use something called one hot encoding when we have three or more options for a discrete variable and in this case a favorite cop in this case that variable is called favorite color we have three options so we start by creating a new column for each option in this case that means creating three new columns blue red and green now in the blue column we set the value to 1 if we had blue in the original favorite color column favorite color column that's almost a tongue twister and we set the remaining values to zero likewise for the red column we set the value to 1 uh the one time we had read in the original favorite color column so uh we only had one instance of red so that one value gets set to one and the remaining values are set to zero lastly for the green column we set the value to 1 if we had green in the original favorite color column and we set the remaining values to zero note the last column loves Troll 2 is also discrete but it only has two options yes and no so we can simply replace yes with one and no with zero and now we have all of the columns in our new excuse me and now all of the columns in our new data set are numeric and can be used with algorithms that don't do well with discrete data like neural networks bam using one hot encoding to convert discrete data into numeric data it works fine when we don't have too many options in this case we have three options for favorite color so we replace our favorite color with three new columns but when we have a lot of options for example if we had a column of postal codes and there are 41 683 postal codes in the United States I just Googled that yesterday by the way so I think that's pretty accurate right now today anyways then we would end up replacing the one postal code column with 41 683 new columns which might make the data difficult to work with it would make it difficult to open up an Excel that's for sure anyways so when we have tons of options for a discrete variable one alternative to one hot encoding is to Simply assign numbers from low to high to each option so in this case we might set blue to zero and red to one Boop and green to two beep beep and just like before we could convert loves troll to to be numeric by setting yes to one beep and no to zero op again just like before all of the columns are now numeric and we can run the data through a neural network double bam note one thing people don't like about using this approach which is called label encoding is that the is that the numbers we use are just arbitrary and some machine learning algorithms will treat the order of the numbers as if they might mean something and that can cause problems so instead of just picking random numbers to represent the options blue red and green we can calculate the mean value of the target the thing we want to predict which in this case is loves Troll 2 for each option for example of the three people that like the color blue only one of them loves troll 2. so the mean value for blue is 1 divided by 3 or 0.33 so we replace blue with 0.33 likewise because only one person likes red and they do not like or they do not love troll 2. the mean for red is zero so we replace red with zero lastly because two of three people who like green also love troll too we replace green with 0.67 beep because we use the target the thing we want to predict to determine what values to replace the discrete options this method is called Target encoding that being said we've only talked about the simplest type of Target encoding a more commonly used version of targeting coding deals with the fact that we only had one person who liked the color red and that means we only used one person to determine the mean value for red and thus we don't have a lot of data supporting the use of zero to replace red in contrast both blue and green have more data three people each supporting the values we use to replace them because less data supports the value replaced red with we have less confidence that we replaced red with the best value than we have for blue and green so in order to deal with this targeting encoding is usually done using a weighted mean that combines the mean for a specific option like red with the overall mean of the target which is loves troll 2. so let's look at the equation bam there's the equation it looks very fancy but it's really just a weighted mean of the option the overall mean and excuse me the option mean and the overall mean so let's look at it in action it's a lot easier to understand when you see it in action so for example in order to use the fancier Target encoding with our data we start by plugging the mean of the target for blue 1.1 divided by three we plug that in then because three people were used to calculate the mean for blue we plug in 3 for n so that's the weight for that mean is 3. then we plug in the overall mean for the Target which is loves Troll 2 and that's 3 divided by 7 because overall three of the seven people in the entire data set love troll 2. so we plug in 3 divided by 7. now we just need to pick a value for M the weight for the overall mean m is a user-defined parameter or hyper parameter and in this example we'll set m equal to 2. setting m equals to 2 means we need at least three rows of data before the option mean the mean we calculated for blue becomes more important than the overall mean and specifically because I knew that blue had three rows of data supporting at both blue and green have three rows each oh that's why I set m equals to 2 is to sort of like you know in those in that in those cases the overall excuse me the the mean that we calculate for blue the option mean is going to have a little bit more weight than the overall mean uh if you have a larger data set it's worth looking at the frequencies of the different term to decide what the best value for m is some people use m equals 300 meaning you need at least 300 rows of of data before the option mean becomes more important than the overall mean so it really depends on your data set so just take a peek inside and maybe plot a histogram or something of the different terms in the in the categorical variable to decide what a good value for m is okay now we just do the math and we get 0.37 so we plug in 0.37 in for blue beep and now we calculate the weighted mean for red so we plug in we've only got one observation or one row of data so n equals one that's the weight for the option mean and then we Boop and then we plug in the option mean for red which is zero divided by one because the one person who who likes the color red does not love Troll 2 uh then we plug in the weight for the overall mean which is user-defined which we defined previously to be two so we do that boop boop and then we plug in the overall mean which we already calculated was three divided by 7. and then when we do the math we get 0.29 so we plug that in we plug 0.29 infrared lastly we calculate the weighted mean for green [Music] and we get 0.57 so we plug in 0.57 for green now let's compare the target encoding when we use the weighted mean to the Target encoding without the weighted mean so this is the original Target coding we did which only relies on the option mean for each value the target encoding for blue and green are similar to what they were before they're not the same but they're similar and this makes sense because we had a relatively large amount of data for both blue and green in contrast with the weighted mean the value for red is much closer to the overall mean than before so we went from being going we went from the the value for red being zero to now it's 0.29 so it's a relatively speaking much larger than it was before and this also makes sense because we have so little data for red only one row in a way we can think of the overall mean as our best guess given no data however as we get more data and that means we get more rows for each option we use the data more rather than our best guess to determine the target encoding value note if you're familiar with Bayesian methods this approach may look familiar because most Bayesian methods boil down to calculating a weighted average between a guess and the data and usually usually with very little data the guess gets you know has the most weight and as you add data and you increase your data set size the data becomes more important to determining the output so that's kind of sort of like Bayesian methods in a nutshell right there as a result some people call this a Bayesian mean encoding triple bam note some of you may have noticed that by using the target the thing we want to predict to modify the values for favorite color and doing this sort of some of you may have noticed that this is what we're doing and doing this sort of thing is a data science No-No that we call data leakage data leakage data leakage results in models that work great with training data but not so well with testing data in other words data leakage results in models that are over fit the good news is that there are a bunch of relatively simple ways to avoid data leakage or at least reduce the amount of data leakage so that you can use Target encoding without overfitting your model and we'll talk about those methods in the future quite possibly in the AMA that's just about to come up bam all right so now we've made it to the AMA partial portion of the live stream so we're gonna go back switch camera angles and we're going to look at the at the Holy Smokes I just noticed that Ash U2 just made a donation uh I guess that's a Super Chat that's the technical term for that so uh I'll give a shout out to ask you too thank you very much and also want to um say hello and thank you to Ahmad ramuni for being a channel member uh that's awesome so let's scroll back and see um see if we got any questions and we do that to add meeting to the order of the encoding uh so I I don't remember the context of that but I'm guessing uh when they're talking about Target encoding yes the encoding gets some meeting um uh what else we got any other questions down here does okay so John Lee asks does the encoding method have any requirement on the end dependency of different columns that's a good question um so one thing Target encoding can do is uh is if there is a a non-linear relationship between the uh the variable I believe this is correct uh the variable event the dis you know the discrete variable so say like we're talking about favorite color if there's a non-religion non-linear relationship between that and the target we just loves troll too I think there can be some funkiness going on um I'm not exactly certain on this one and it makes me want to look into it more I think there can be and I think this is a situation where one hot encoding uh may work better um that being said one hot encoding so I think one hot encoding is sort of like generally speaking for most machine learning algorithms the way you want to go unless you've got tons and tons of options uh for like you know postal codes or something like that and then it's just a nightmare uh by the way this is uh this is part of how cat boost works I don't know if you're familiar with that uh it's a boosting method it's a lot like XG boost but it adds this to a type of feature encoding they call they in catboost they call it ordered uh Target statistics um I guess I meant to say it's a type of Target encoding anyways they call it ordered Target statistics but um but it's it's a very similar technique and basically that ordered part of what they call it is their trick for reducing data leakage they make a big deal about how their methodology eliminates all data leakage whether or not that's super important or whether or not you just need to reduce the data leakage uh you know that's subject to debate uh but they think they've gotten rid of all of it and that's something I want to talk about possibly in my next live stream part of it I'll be honest I'm just going to tell you this a lot of people have been asking me about cat boost so part of this live stream over the next couple of weeks or months actually is going to be running through all the little details of how catboost works and then hopefully I'll be able to put this together into a single video on catboost um so that's sort of the master plan here so we'll just kind of run through all the little bits of how catboost works all right um yeah so I to get back to Lynn's question I said some algorithms assume meaning of the encoding order if we just encode it with red equals one blue equals two yeah some algorithms will will think that's some sort of like ordering and the ordering is significant and that red is somehow um you know less than blue or you know like imagine you had a pain scale on Z from zero to five you know how that's related it may sort of come up with some way of of implying that there's that the order is important and that can cause problems uh someone just asked what's the difference between one hot encoding and label encoding uh one hot encoding remember we we uh we replace The Columns of the column of Interest with individual columns one for each option and we have zeros and ones in each column and for label encoding we we use the exact same um column and we just replace the options with numbers usually from zero to some high number representing the total number of options um so someone says do we have uh I'll someone asked if we have a long short term memory video incoming and the good news is yes yes yes yes yes um so some great news is about two weeks ago I had a major breakthrough uh with the exam so one of the things about statquest you may have noticed is I like to use very simple examples I want them to be complicated enough to illustrate the concepts but no more complicated than they need to be and so for the long short term memory videos that I'm coming out with soon one of the key items was to come up with a data set that was easy to visualize but Illustrated sort of how awesome long short-term memory networks are and I had a major breakthrough about two weeks ago where I created by hand the whole long short-term memory sort of neural network stuff and it worked great and then I then spent the next week trying to get it to work in pie torch and I succeeded my friend Adrian he helped me again that guy's awesome um he's one of my co-workers at lightning and anyways he helped me out with that anyways once we got those out of the way and I see that the example Works in multiple contacts uh and it's solid it's just a matter of me I've got the I've got the graphics in place it's just a matter of me scripting it and I think that will take probably either the I mean you know in a perfect world I'd have it done by the end of the week but I really think it'll probably be the end of next week because things always always take longer than I expect if there's one thing I've learned from the Journey of statquest it's that uh things take forever um but anyways so yeah uh long short term memory networks hopefully at the most two weeks from now that'll we're gonna do three videos for that one uh we're gonna do actually no no we're gonna do two videos for that one we're gonna do one which is just all Theory and you'll see my simple example and uh we'll go through this we'll see the math and we'll do it by hand and we'll see how it all works and then we're gonna do another video of how to do it in pi torch and that'll come with an accompanying Jupiter notebook that will be free and you can have so that's something I'm really exciting about um so that's the update on Long short-term memory um uh someone asked if we have some methods to test that there's a significant non-linearity between some categorical variable and a numeric one um that is a good question you know to be honest it's kind of weird uh I feel like I have to get a backtrack a little bit and this is also revealing sort of a weakness in my knowledge so so um you know you know the more I talk about it and say it during this live stream the more I'm like how can you how could you possibly detect a non-linearity because it's like it's in bins it's in very you know each discrete option is it basically its own bin um so how can we detect that I do not know and how could that even be a problem again I do not know um it doesn't make sense to me but maybe but maybe when we do the label and Co or the target encoding excuse me when we do the Target that introduces a potential linear relationship and maybe that's the problem it's not that the discrete values have a linear or non-linear relationship maybe it's that once we encode it we establish some sort of relationship and if and if it's non-linear then that's a problem and one way we can do that to test that is we can just plot it bam x-axis are new labels y-axis our Target values um so that's something we could do there um so someone asked if I plan on uh doing a video on the difference between catboost like GBM and XG boost and ultimately the answer is yes um once we get through doing cat boost um we'll we'll start talking about light GBM and once we have all three of those things because we already have videos on xgboost uh it does make sense to have like a short recap of of the differences I can tell you sort of off the top of my head um this is uh right here is one of the big differences I don't know if you remember from uh xgboost basically uses one hot encoding but what it does in order to keep from crashing with all the extra columns it creates is it uses something called sparse matrices and what a sparse Matrix is is a matrix where you only care about the non-zero value so when you do one hot encoding you're gonna have lots of zeros most of the most of your data set is going to be zeros and basically what XG boost does is said I'm just going to ignore the zeros and only remember where the ones are and so it uses what's called a oh there's a um a sparse Matrix it uses a sparse Matrix and a sparse Matrix is just a matrix that ignores where the zeros are in contrast uh cat boost uses a flavor of uh Target encoding and we'll talk about uh what light GBM does too it uses a histogram approach uh so we'll do that one of these days um someone says um they want to tip on where to start they want to create a neural network for answering basic questions based on answers to similar questions but I have very little experience with such things any tip on where to start um I'll be honest I'm going to be totally honest with you I've got very little experience with that as well um however it's funny that you mention this because I have a friend who just came up with a um a sort of a natural language sort of preprocessor potentially for neural networks but what it does is it takes um two sentences and it compares to see how similar they are uh he just released this code on GitHub I wish I could remember it's like simplify the love of Simplicity I think it's the name of it you can check it out on GitHub um actually I might be able to to paste it into the um paste it into the um into the chat um let me see if I can do this yeah so um I'm gonna I'm sharing with you this is my friend Brian risk he uh I've known him since we were in high school together and we've been in bands together and we go jogging together and he cooks delicious deep meals for me on Thursday nights anyways he's a good buddy of mine oh by the way he's hiring um anyway so he's got this library for comparing sentences and so if you're really interested in um coming up with a way to answer questions uh that are similar to other questions well this Library might give you a good way an easy way to answer that first question of is this question similar to another question um so that'd be a great place to start so check that out and maybe get a job for this guy I mean he's an awesome dude he's my buddy um uh yeah I've known him forever and yeah he's my man uh so check out Brian rusk's thing on GitHub called a simplified the love of Simplicity okay someone asked if I prefer Pi torch or tensorflow and this is embarrassing um I've only used Pi torch uh the reason why I like pie torch so much is I work for a company called lightning that creates something called Pi torch lightning and I'm going to be honest I actually genuinely love Pi torch lighting um it solves a lot of problems so when I started doing my neural networks videos the actually you might want to why am I working for lightning why would I take a job at this company anyway so when I started making these videos on neural networks a couple of years ago I was like oh this is fun and we're learning the theory but I had this Vision in my head that something that would be super cool would be to not just talk about the theory but to also show how to actually deploy a real production grade neural network trained in the cloud operates in the cloud you know because I feel like you know obviously theory is cool but when you're working in your job Theory doesn't always help what you really need is the applied like this is how it actually is done and I I didn't like the idea that stat Quest was kind of useful for Theory but not really useful for work and I was like you know what I want to do for neural networks is show how you know we'll start small and we'll start simple but I want to build up to where we're doing production grade stuff in the cloud and I was like yeah but using the cloud is kind of a pain in the butt um using pi torch also is um and and I was just like Ugh you know because I want to do this but and I want to make it real but I also want it to make I make it as simple as possible that's like the stat Quest you know goal right it I I want things I like to make things simply explained you know it can it can be complex but I want it to be simply and clearly explained that's my goal and what it turns out is with pi torch lightning and lightning data dot AI they want complicated things to be easy to do um and it just so happened that uh I wanted to make a neural network and they wanted the same thing and so I was like well we got to do this together and it's actually been a great great I hate to use the word Synergy because it's it's been my goal for years and they're like we just want to help you achieve that goal and it's been great because working for them gives me access to some of the best pie torch coders in the whole world so when they help me and we create these Jupiter note or nope notebooks it's not just sort of like my opinion I get experts on neural networks people that have done this in Industry people that have trained like crazy neural networks for CNN I know one of my co-workers at CNN others you know one of them like creates AIS for for games you know these are like this is like people doing real stuff that are helping me create these awesome Jupiter notebooks and so uh so that's the goal so I the the dream is like stack Quest will be cool for the theory but also you're like oh I can do this at work too uh so that's what's going on there um uh what else do we got yeah so um I don't even remember what the question was anymore I hope I answered it um uh uh and let's see if we got any other questions on here uh oh yeah someone was asking me about uh uh tensorflow versus Pi torch and the answer is I've actually never coded in tensorflow although my good friend Sebastian rashka who by the way also works with me at lightning which is super cool I don't know if you've seen his stuff uh he's super active on Twitter and he has a great great website that has all kinds of cool stuff and he also wrote this awesome book hold on I'm gonna go get his book I'll be back my friend Sebastian wrote this book called machine learning with pi torch and scikit-learn it's pretty popular um anyways he's working at the company now too which is cool and um he said he used to code in tensorflow and he was like I don't like it as much as pie torch so he's an expert I refer to him as as the person to talk to about these things um so what else do we got any other questions um oh any idea when you'll be dropping the Transformer video yeah so uh maybe by the end of the year um I I I think if we get long short-term memory videos out in this month sometime hopefully in two weeks immediately I'll start working on attention and we'll probably have a video on attention um and then a video on Transformers so possibly by the end of the year uh uh um there's a sort of a lot to do um uh between now and then holy smokes we've got a lot of uh um Chinese looks like in the in the chat um [Music] I don't actually know Chinese that well so let me translate some of this to see what's going on ah they're saying nice things about me so that's nice to hear um anyways um what else do we have oh someone sees the Indian instruments um yeah so yeah actually someone asked this last time I forget that I'm surrounded in musical instruments oh obviously I love my ukulele or ukulele if you're going to pronounce it correctly I grew up saying ukulele um but I think in Hawaii they say ukulele um and anyways I love this instrument so much but I'm surrounded by all these awesome instruments including Tabla uh I've got some I've got another Indian instrument over there that you can't see which is awesome but probably Way Out Of Tune I've also got some I also Play The Cello over here um so I got lots of instruments uh if I can remember and I always forget maybe someone can remind me next time I post um maybe okay here's the deal uh you can comment on the live stream before it goes live I'll post it a couple days in advance if someone can remind me to do something with an Indian instrument a silly song with an Indian instrument I will do it but otherwise I'll forget because the ukulele is just so easy and it's just right there and I don't have to do anything um so please just remind me or send me a note you can contact me directly uh through um on Twitter or on LinkedIn or through my website statquest.org um uh so uh so there's that um so someone asked if I save my live streams and the answer is yes not only do I save them I post them um so if you miss them it's not that big a deal uh it'll be aired you know last time I did it I I I put it on my Channel about a week later um excuse me why did I say a week later uh I posted the same day so like four hours later uh I do a little bit of editing what I do is I'll if there's like weirdness at the beginning I'll trim off the weirdness at the beginning and if there's weirdness at the end where you see me like trying to turn everything off I'll trim that off and for some reason uh making those small edits on YouTube takes about three hours to do it's not that not that I spend three hours doing it it's like you it takes like five seconds to do the little trims of the beginning and the end and then YouTube's like processing and it's processes for hours I have no idea why it takes so long but it does uh so it you know I try to get it out by the end of the day today however I've got a very busy day um I got a meeting I have to run to as soon as this is over uh and then I've got to go grocery shopping because yesterday I worked I Was preparing for this live stream uh that and I was like working like all day and all night and I realized I have no food in the house so I have to go get some food like as soon as possible um so that's part of the plan but as soon as I can I'll put this up and you can watch it um someone asked if I can sing a song for you and the answer is yes uh uh but should I do it right now I mean we did have a silly song uh but maybe I actually really like that song so much because it is Tuesday and so I went [Music] Tuesday we're gonna learn about future oh I forgot it oh man it was it was actually one of my favorite songs like oh I love this song but it's gone now it is oh I can't remember it I have to go back and learn it oh it was just like a killer uh a killer song The the Tuesday was somewhat syncopated I remember it and I was like oh I love this um so uh I have to go back and remember that one it's a good one um uh uh so what else we got here um uh yeah so I'm singing a silly song is kind of a bust uh oh someone asked if I speak any other language other than English and the answer is unfortunately no that being said I've attempted to learn Spanish Portuguese and and Japanese and maybe a little bit of French um I've gone to Brazil I believe I've been there twice I've been to Rio twice I've got some family that lives there um and so I've been trying to learn Brazilian Portuguese uh so that I can go there and have more fun than I did uh without knowing Portuguese um although so where I live though there's lots and lots of Spanish speakers and so I've I've actually when covet hit I think everyone picked up like a covet hobby because you couldn't go out and do anything and my covet hobby which I've continued to this day is to practice Spanish and I'd say I've I am uh like it's as if I've taken like three semesters of Spanish I can I can speak a little bit in in past tense a little bit uh I went to Japan so anyway Spanish is is I'm working on it I have a dream of learning it I'd love to learn Italian anyways I traveled to Japan a few years ago before the pandemic and so I learned some basic phrases in Japanese and that was super fun uh being in Japan and saying uh little phrases like so Disney um I it was like it was just great and uh and I'd love to go back to Japan and learn uh more Japanese anyways learning languages I've discovered is really fun it's hard it's one of those things whereas like if I could do it all over again I would have done it as a kid uh when I lived when I grew up as a kid uh we lived on on a farm in the middle of basically nowhere and so learning a foreign language was the most abstract thing ever and I did when I was a kid I didn't travel very much uh but now that I'm adult and I'm traveling I'm like oh man I gotta learn languages so that's something um uh that uh that I'm kind of like loving and what's cool so along the lines of languages is I think I can talk about this I'm in a beta test program and I can't actually remember the name of the company but what they've been doing is they've been providing translations of my new videos not my old ones unfortunately but the new ones the last two videos that have come out have had uh Spanish and Portuguese Brazilian Portuguese overdubs uh and so the process is um I write out the script and I make sure it's all good and then they convert it into these other languages and they provide overdubs and so if you know Spanish or Spanish is your number one language or Portuguese Brazilian Portuguese is your number one language you can actually watch my new videos in those languages um so that's pretty cool and what's super cool is I think they're going to start rolling out more and more languages and so once I write the script they can just like translate into all these other languages I know they've got handy as high on the list um uh Arabic is on the list Turkish is on the list of things that might come out I hope I'm not uh saying anything I'm n
khMzi6xPbuM,2022-09-19T04:00:14.000000,Introduction to Coding Neural Networks with PyTorch and Lightning,"PyTorch plus Lightning is the coolest thing around. StatQuest! Hello!I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about an introduction to coding neural networks with PyTorch and Lightning. Lightning lets you do awesome stuff with neural networks. Yeah! ThisStatQuestisalso brought to you by the letters 'A', 'B'and 'C'. 'A'always, 'B'be, 'C'curious.Always be curious. Also, I want to give a special bam to one of my co-workers at Lightning, Adrian andWlchli, who helped me create this tutorial. Note thisStatQuest assumes that you have already seenthe StatQuest Introduction to PyTorch.If not, check out the 'Quest. Lastly, you can download all of the code in this StatQuest for free. The details are in the pinned comment below. In theStatQuest Introduction to PyTorch,we started with this super simple dataset and then we coded this super simple neural network to fit a pointy thing to the data. To do this,we created a class that contained code for the weights and biases, and for running data through the neural network. And then, completely separate from that class, we wrote code to optimize the neural network with backpropagation. Bam?Well, even though the code worked like we expected, we had to come up with our own learning rate for gradient descent and, generally speaking, figuring out a good learning rate isn't always easy. So it would be nice if there was a tool that could find a good learning rate for us. Also, it would be niceif the code for trainingthe neural networkwere easier to read and write. Lastly, we would have to make significant changes to this codeif we hadGPUs or TPUsto accelerate learning.For example, this code runs fine on my laptop, but if we wanted to accelerate it with one or more GPUs, we'd have to make a lot of changes.The good news is that we can do all of these things and more when we combine PyTorch with Lightning. Solet's go back through the code and show how we can do things easier and improve it with Lightning. Bam! First, just like, before we import torch to createtensorsto store all of the numerical values, including the raw data and the values for each weight and bias. Then we import torch dotnn to make the weight and bias tensors, part of the neural network. And torch dot nn dotfunctional for the activation functions. Then we import SGD so we can use StochasticGradient Descent to fit the neural networkto the data. So far,everything is the same as when we used PyTorch without Lightning, but now we import Lightning as L to make training easier to code. And now we need TensorDataset and DataLoader from torch dot utils dot data, which will ultimately make our lives easier when we start working with larger data sets.And just like before, we'll graphour output with matplotlib and Seaborn. Nowlet's build this neural network. Note:If all we want to do is create a pre-trained neural network and run data through it, then everything is the exact same as before,except now when we create the classwe willnameit BasicLightning andwe willinherit from LightningModule instead of nn dot module, which is what we didwhen weused PyTorch without Lightning. Other than that creating this pre-trained neural network and running data throughit is just like before. In other words, just like before we create an initialization method for the new classand the first thing we do is call the initialization method for the parent class LightningModule.And just like before we create the weights and biases for the networkby creating a new parameter initialized with a tensor set to the value for the weight or bias. And we do that for each weight and bias in the  network. Bam. Now, just like before, we need a way to make a forward pass through the neural network that uses the weights and biases that we just  initialized. So, we create a second method inside the class called forward, so we can see what's going on. Let's move the code for forward to the top of the screen. Again, just like we did when we used PyTorch without Lightning, we create a new variable,Input to Top ReLU,that is equal to the input timesthe weight w sub 0 0, plus the bias b sub 0 0.Then we pass Input to TopReLUto theReLUactivation function with f dotReLUand scale the ReLUoutput by the weight w sub 0 1. Likewise, we connect the input to the bottomReLUand scale the activation function's output. Then we add the top and bottom scale values to the final bias and use the sum as the input to the finalReLUto get the output value. Lastly, the forward function. Returns the output. So just like before we have created a new class that initializedthe weights and biases and does a forward pass through the neural network. Now, I don't know about you, but every time I write a block of code, even when it's mostly the same as something I wrote before, I like to test itto make sure it works as expected. So let's test the code by pluggingin a bunch of values between 0 and 1 that represent different doses and see if the output from forward results in thisbentshape that fits the training data.Again, just like before, we can create a sequence of numbers between 0 and 1using the linspace function from PyTorch. We store the tensor in a variable called Input Doses and we can print out and admire the input doses by just typing the variable nameInputDoses. Now, in order to run these input values through our neural network, we make a neural network thatwe'llcall model from the classwe just created, BasicLightning. Then we pass the input doses to the model, which, by default, callsthe forward method that we wrote earlier and we save the output from the neural network in a variable that we cleverly named Output Values. And now that we have both the input values to the neural network and the output values we can use them to draw this graph. So we set the Seabourn style to whitegridso the graph looks cool.And then we use lineplot to draw a graph of the data. Lastly, we set the y and x axis labels. Andthat code gives us this graph. The graph tells us that the neural network we created earlier,BasicLightning, does exactly what we expected. Hey so far, pretty much everythinghas been a review of basic PyTorch. When will we start doing cool stuff with Lightning? Right now Squatch. So we can demonstrate how Lightning makes it easier to traina modelwe'llset bsub final to 0 and make a copy of the original classwe created, BasicLightning,and change the name of the copy to BasicLightningTrain, because we want to train this neural network.Then wechange the initial value for final bias to 0.0, and we set requires grad, which remember is short for requires gradient, to True.And, because we will use a Lightning function to improve the learning rate for us,we add a new variable learning rate to store the value. Note: For now, we set learning rate equal to 0.01, but this is just a placeholder value and the actual value does not matter right now. Now, just like we did before, we can verify that this neural network no longer fits the training data by drawinga graph of the neural network's output. Only this time, we are using BasicLightningTrain instead of BasicLightning. And, because final bias now has a gradient, we calleddetachon the output values to create a new tensor that only has the values. The graph shows Effectiveness equals 17when Doseequals 0.5, which is way too high and that means we need to optimizebsub final. So we create the training data by creating a tensor called inputs with three input doses, 0, 0.5 and 1,and another tensor called labels that has the known values 0,1 and 0. However, now that we are using Lightning, we need to wrap the training data in a DataLoader. So we combine the inputs and the labelsinto a TensorDataset called dataset,and then we use the TensorDataset to create a DataLoadercalled DataLoader. DataLoaders are super usefulwhen we have a lot of data because: 1. They make it easy to access the data in batches. This is super usefulwhen we have more data than memory to storeit. 2.They make it easy to shufflethe data each epoch and3.They make it easy to usea relatively small fraction of the data if we want to do a quick and dirty training for debugging. Okay, now that we have our training data wrapped up in a DataLoader,we are ready to optimize bsub final. Now, if you remember last time, when we use PyTorch without Lightning, we optimized b subfinal with a whole lot of code. The first thing we did was create an optimizer object that used StochasticGradient Descent, SGD, to optimize bsub final. Then we coded for loops to calculate the derivatives needed for stochastic gradient descent. Specifically, we coded a loop that went through the full trainingdataset 100 times, or epochs, then for each element in the training datawe calculated the value predicted by the neural network,then we calculated the loss, which in this case was the square difference between the predicted value and the known value.Then we calledlossdot backward to calculate the derivative of the loss function with respect to the parameter we wanted to optimize. Lastly, after we calculated the derivatives for all three points in the training data, we took a small step towards an optimal value for bsub final with optimizer dot step and zeroed out the gradients with optimizer dotzero grad, so that we could start another epoch. All inall, we had to write quite a bit of code to train the neural network. Nowlet's combine PyTorch with Lightning to simplify this codea whole bunch. Well, wait, hold on a second. There's one more thing I need to review about the PyTorch code. When we only use PyTorch, before we wrote this codeto optimize bsub final,we created a class to keep track of the weights and biases and a forward functionto run data through the neural network.And then, separately, we wrote the code to optimize bsub final. In contrast when we add Lightning, we put all of the code relating to the neural network in the same place. So, when we create the class BasicLightningTrain from LightningModule, we create the init method that contains the weights and biases for the neural networkand the learning rate,the forward method to run data through the neural network, and a new method called configure optimizers that sets up the method we want to use to optimize the neural network. And just like before we'll use stochastic gradient descent. However, this time we're setting the learning rate to a variable thatwe willimprove in just a bit. Then we create another new method called training step, which takes a batch of training data from theDataLoader that we createdand the index for thatbatch. The training step functioncalculates theloss, which, just like before, is the sum of the squared residuals.Now that we've added configure optimizers and training step to our class, we're ready to optimize the neural network. So, since we just modified the class by adding two new methods, the first thing we do is make a new model. Then we create a Lightning Trainer, which we will firstuse to find a good value for the learning rate and then we will use it to optimize, or train the model. Here we are setting the maximum number of epochs to 34 because we know from before that34 epochs is enough to fit the model to the data, but if it were not, the good news is that we don't have to start from zero and try again. Because Lightning lets us add additional epochs right where we left off. Now that we have the trainer, we will use it to find an improved learning rate by calling tuner dot lrfind. In this case, we're passing lrfind the model, the training data, dataloader, the minimum learning rate, 0.001, the maximum learning rate, 1,and we're telling it to not stop early. In other words, by defaultlr find will create 100 candidate learning rates between the minimum and maximum values,and, by setting early stop threshold to ""None"", we will test all of them. Anyway, we store the output fromlrfind inlrfind results. And we can accessanimproved learning rate by calling suggestion on the results. Now,just for fun, we can print out the new learning rate that we stored in newlrto see what it is, and we see that the new learning rate is0.00214, and, lastly, we can set the learning rate variable in our model to the new learning rate. Now that we have found an improved learning rate for stochastic gradient descent,let's train the model. To train the model and optimize bsub final,we simply use the trainer to call the fit function. Fit requires the model and the training data, which we named dataloader. When we call the fit function with our model and training data, the trainer will then call our model'sconfigure optimizers function,and in this case, that means configuring a stochastic gradient descent optimizer using the new learning rate that we just set. Then the trainer calls our models training step function to calculate the loss. Then, without us having to do anything, the trainer will call optimizer dotzero grad,so that each epoch starts with a fresh gradient;loss dotbackward, to calculate the new gradient;and optimizer dot step,to take a step towards the optimal values for the  parameters. And then it calls trainingstep againand repeatsfor each epoch that we requested. In other words, the big training loop that we had to codewhen we used PyTorch without Lightning is reduced to just coding the loss in the training step function when we use PyTorch with Lightning.Bam! Now, to verify that we correctly optimized bsub final,we can print out its new value, and we get-16.0098. Hey, can we draw one last graph to verify that the optimized model fits the training data? Yes. We can verify that the optimized model fits the training data by graphing it with this code, which is the same as what we use before,except now we don't create a new model, and instead just use the one we optimized. And this is what we get, which shows that the neural network does exactly what we expect.Double Bam! What if we want to train our neural network onGPUsor use some other fancy accelerator? On a very basic computerlike a laptop,you might only have a single processor that does all of the work called a central processing unit, or CPU. In this case, our neural network and, specifically, the tensors that represent the weights and biases, would be on the CPU, as well as the tensors that represent the training data. And when all of the tensors, the ones for the weights and biases and the ones for the data, are in the same place on the CPU, then we just do the math for backpropagation to train the neural  network. However, training a neural network on a single CPU, which might only have a few computingcores, is usually relatively slow. With this simpleneural network and thissimple dataset,the speed really doesn't matter, but if we had a fancier neural networkwith tons of weights and biases and a ton of data, then running everything on a single CPUmight be too slow to train in a reasonable amount of time. So when we need speed, we often trainneural networks on one or more Graphics Processing Units, orGPUs, which can have 10 times or even 100 times more Computing  cores. And when we use PyTorch without Lightning, then we have to manually move the tensors to theGPUs,and keeping track of what tensors arewhere can get pretty complicated. And that means we can't easily test our code on our laptop with a single CPU and then ported to a system with a lot ofGPUs without having to change the code a bunch. In contrast, when we use PyTorch with Lightning, we can let Lightning automatically detect ifGPUsare available by setting accelerator to ""auto"", when we create the trainer object.And we can let Lightning determine how manyGPUsare available by setting devices to""auto"". Now,with Lightning, we can test our code on our laptop with a single CPU and then move it to a fancy computing environment with a bunch ofGPUswithout having to change the code. Triple Bam!And don't forget, you can download all of the codein this StatQuest for free!The details are in the pinned, comment below. Nowit's time for some shameless self-promotion. If you want to review statistics and machine learning offline,check out the StatQuest PDF study guides and my book The StatQuest Illustrated guide to Machine Learning atStatQuest dotorg. There's something for everyone!Hooray!We've made it to the end of another exciting StatQuest. If you liked this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, consider contributing to my patreon campaign, becoming a channel member buying one or two of my original songsor a t-shirt or a hoodie, or just donate. The links are in the description below. All right, until next time quest on!"
CHOir6-ZpkE,2022-09-04T04:00:03.000000,Three more lessons from my Pop!!!,"Three more lessons from mydad. StatQuest. Hello!I'm JoshStarmer and welcome to StatQuest. Today, we're going to talk about three more lessons from mydadand they're going to be clearly explained. LightningAI is awesome. Check them out when this video is over. Yeah. This StatQuest is also brought to you by the letters, 'A', 'B' and 'C'. 'A' always. 'B'be.'C'curious.Always be curious. September 4th is global FrankStarmer day and to celebratewe're going to talk about three lessonsmydadtaught me that he learned from hisdad. Number one:A good helper knows what tool is needed before it's needed. Back in the day, my grandfather repaired elevators for a living and on weekends, mydadwould help him. And mydadwas a good helper because every time hisdadneeded a tool, mydadhad one waiting for him. Bam! Now, we don't have to be fixing an elevator to be a good helper. We can always be a good helper. All we have to do is look around ourselves and have a little curiosity. Whenarecurious and ask questions and observe, we can learn what needs to get done. And once we learn what needs to get done, we can be a good helper and do it.Personal Note:In my various jobs, the one skill that always made me essential and got me promotions is I always tried to be a good helper. For example, whenever I haddown time at work, I went around to learn what everyone else was doing.Then I would try to find a way to help them. Bam!Number two:I never met someone that I couldn't learn something from. To be honest. I don't know if mydadlearned this at the bottom of an elevator shaft or not, but he heard it a lot from hisdad, and so did I. And it's important, so let's hear it again. I never met someone that I couldn't learn something from. What it means is that no matter how different we are from someone else, no matter how crazy that other person might be, no matter how much we might not like thatother person, if we show a little curiosityinwho they are, we can learn from them. And learning new thingsalways makes us smarter. Personal Note:I get comments all the time in my videos.Sometimes people write things that seem a little crazy. However, with a little curiosity, I learned that the people were not trying to insult me. Instead,it was just a ""Lost in Translation"" thing and they were making helpful comments. So the next time you meet or hear, or read about someone who seems totally crazy and you want to ignore them. Stop and tell yourself,""I never met someone that I couldn't learn something from."" Be curious about them and see what you can learn from them. It will always make you a better person. Double Bam! Number three:There's always room for ice cream. Hey, look, it's StatSquatch! Yum!I sure love ice cream. But what does ice cream have to do with curiosity? Well, there are a lot of ice cream flavors and if we're curious, we get to try them all. Personal Note: Pistachioice cream is green and can look kind of gross, but it's delicious. Tripleyum! In summary, 'A' always, 'B'be, 'C'curious.Always be curious. When we're curious, we can be good helpers and discover what help is needed,we can respect everyone at least a little bit and learn something new  from them and lastly we get to eat ice cream. Yum! Now, last but not least, happy birthdaydad!Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StatQuest PDF study guides and my book the StatQuest Illustrated guide to machine learning at StatQuest dot org. There's something for everyone. Hooray! We've made it to the end of another exciting StatQuest. If you liked this StatQuest and want to see more,please subscribe. And if you want to support StatQuest, consider contributing to my patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt, or a hoodie or just donate. The links are in the description below. Alright, until next time quest on!"
AsNTP8Kwu80,2022-07-11T04:00:06.000000,"Recurrent Neural Networks (RNNs), Clearly Explained!!!","Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about recurrent neural networks, and they're going to be clearly explained! Lightning and Grid are totally cool. Check them out when you've got some time. NOTE: This StatQuest assumes that you are already familiar with the main ideas behind neural networks, backpropagation and the ReLU activation function. If not, check out the 'Quest. ALSO NOTE: Although basic, or vanilla recurrent neural networks are awesome, they are usually thought of as a stepping stone to understanding fancier things like Long Short-Term Memory Networks and Transformers, which we will talk about in future StatQuests. In other words, every Quest worth taking take steps, and this is the first step. So with that said, let's say Hi to StatSquatch. Hi! And StatSquatch says, Hello! The other day I bought stock in a company called Get Rich Quick, but the next day their stock price went down and I lost money. Bummer. So, I was thinking, maybe we could create a neural network to predict stock prices. Wouldn't that be cool? That sure would be cool 'Squatch, unfortunately the actual stock market is crazy complicated and we'd probably both get in a lot of trouble if we offered advice on how to make money with it, but if we go to that mystical place called StatLand things are much simpler and there are far fewer lawyers. So let's build a neural network that predicts stock prices in StatLand. However, first let's just talk about stock market data in general. When we look at stock prices, they tend to change over time. For example, the price of this stock went up for four days before going down. Also, the longer a company has been traded on the stock market, the more data we'll have for it. For example, we have more time points for the company represented by the blue line then we have for the company represented by the red line. What that means is, if we want to use a neural network to predict stock prices, then we need a neural network that works with different amounts of sequential data. In other words, if we want to predict the stock price for the Blue Line company on day 10, then we might want to use the data from all nine of the preceding days. In contrast, if we wanted to predict the stock price for the Red Line company on day 10, then we would only have data for the preceding five days. So we need the neural network to be flexible in terms of how much sequential data we use to make a prediction. This is a big difference compared to the other neural networks we've looked at in this series. For example, in Neural Networks Clearly Explained, we examined a neural network that made predictions using one input value, no more and no less. And if you saw the StatQuest on neural networks with multiple inputs and outputs, you saw this neural network that made predictions using two input values, no more and no less. And in the StatQuest on Deep Learning Image Classification, you saw a neural network that made a prediction using an image that was six pixels by six pixels, no bigger and no smaller. However, now we need a neural network that can make a prediction using the nine values we have for the blue company and make a prediction using the five values we have for the red company. The good news is that one way to deal with the problem of having different amounts of input values is to use a Recurrent Neural Network. Just like the other neural networks that we've seen before, recurrent neural networks have weights, biases, layers and activation functions. The big difference is that recurrent neural networks also have feedback loops. And, although this neural network may look like it only takes a single input value, the feedback loop makes it possible to use sequential input values, like stock market prices collected over time, to make predictions. To understand how, exactly, this recurrent neural network can make predictions with sequential input values, let's run some of StatLand's stock market data through it. In StatLand, if the price of a stock is low for two days in a row, then, more often than not, the price remains low on the next day. In other words, if yesterday and today's stock price is low, then tomorrow's price should also be low. In contrast, if yesterday's price was low and today's price is medium, then tomorrow's price should be even higher. And when the price decreases from high to medium, then tomorrow's price will be even lower. Lastly,if the price stays high for two days in a row, then the price will be high tomorrow. Now that we see the general trends in stock prices in StatLand, we can talk about how to run yesterday and today's data through a recurrent neural network to predict tomorrow's price. The first thing we'll do is scale the prices so that low equals 0, medium equals 0.5, and high equals 1. Now let's run the values for yesterday and today through this recurrent neural network and see if it can correctly predict tomorrow's value. Now, because the recurrent neural network has a feedback loop, we can enter yesterday and today's values into the input sequentially. We'll start by plugging yesterday's value into the input. Now we can do the math just like we would for any other neural network. Beep. Boop. Beep. Boop. Boop. At this point, the output from the activation function, the y axis coordinate that we will call Y sub 1, can go two places. First Y sub 1 can go towards the output. And if we go that way and do the mathbeep, boop, boopthen the output is the predicted value for today. However, we're not interested in the predicted value for today because we already have the actual value for today. Instead, we want to use both yesterday and today's value to predict tomorrow's value. So, for now, we'll ignore this output, and instead, focus on what happens with this feedback loop. The key to understanding how the feedback loop works is this summation. The summation allows us to add Y sub 1 times W sub 2, which is based on yesterday's value, to the value from todaytimes W sub 1. In other words, the feedback loop allows both yesterday and today's values to influence the prediction. Hey, this feedback loop has got me all turned around. Is there an easier way to see how this works? Yes! There's an easier way to see what's going on. Instead of having to remember which value is in the loop, and which value is in the input, we can unroll the feedback loop by making a copy of the neural network for each input value. Now, instead of pointing the feedback loop to the sum in the first copy, we can point it to the sum in the second copy. By unrolling the recurrent neural network, we end up with a new network that has two inputs and two outputs. The first input is for yesterday's value, and if we do the math straight through to the first output like we did earlier, we get the predicted value for today. However, as we saw earlier, we can ignore this output. This second input is for today's value and the connection between the first activation function and thesecond summation allows both yesterday and today's values to influence the final output, which gives us the predicted value for tomorrow. Now, when we put yesterday's value into the first input and we do the math just like before beep boop beep boopthen we follow the connection from the first activation function to the summation in the second copy of the neural network. Now we put today's value into the second input and keep doing the math beep boop beep boop beep boop beep and that gives us the predicted value for tomorrow, zero, which is consistent with the original observation. In other words, the recurrent neural network correctly predicted tomorrow's value. Likewise, when we run yesterday and today's values for the other scenarios through the recurrent neural network we predict a correct values for tomorrow. This recurrent neural network performs great with two days worth of data, but what if we have three days of data? When we want to use three days of data to make a prediction about tomorrow's price, like this, then we just keep unrolling the recurrent neural network until we have an input for each day of data. Then we plug the values into the inputs, always from the oldest to the newest. In this case, that means we start by plugging in the value for the day before yesterday, then we plug in yesterday's value, and then we plug in today's value. And when we do the math, the last output gives us the prediction for tomorrow. NOTE: Regardless of how many times we unroll a recurrent neural network, the weights and biases are shared across every input. In other words, even though this unrolled network has three inputs the weight, W sub 1, is the same for all three inputs. And the bias, B sub 1, is also the same for all three inputs. Likewise, all of the other weights and biases are shared. So, no matter how many times we unroll a recurrent neural network, we never increase the number of weights and biases that we have to train. Okay, now that we've talked about what makes basic recurrent neural networks so cool, let's briefly talk about why they are not used very often. One big problem is that the more we unroll a recurrent neural network, the harder it is to train. This problem is called The Vanishing / Exploding Gradient Problem. Which is also known as the ""hey wait, where the gradient go?"" problem. In our example, The Vanishing / Exploding Gradient Problem has to do with the weight along the squiggle that we copy each time we unroll the network. NOTE: To make it easier to understand the Vanishing / Exploding Gradient Problem, we're going to ignore the other weights and biases in this network and just focus on W sub 2. Also, just to remind you when we optimize neural networks with backpropagation, we first find the derivatives, or gradients, for each parameter. We then plug those gradients into the gradient descent algorithm to find the parameter values that minimize a loss function, like the sum of the squared residuals. Bam. Now, even though the Vanishing / Exploding Gradient Problem starts with Vanishing, we're going to start by showing how a gradient can explode. In our example, the gradient will explode when we set W sub 2 to any value larger than one. So let's set W sub 2 equal to 2. Now, the first input value, input sub 1, will be multiplied by 2 on the first squiggle and then multiplied by 2 on the next squiggle and againon the next squiggle and again on the last squiggle. In other words, since we unrolled the recurrent neural network four times, we multiply the input value by W sub 2, which is 2, raised to the number of times we unrolled, which is4. And that means the first input value is amplified 16 times before it gets to the final copy of the network. Now, if we had 50 sequential days of stock market data, which to be honest, really isn't that much data, then we would unroll the network 50 times, and 2 raised to the 50 power is a huge number. And this huge number is why they call this an Exploding Gradient Problem. If we tried to train this recurrent neural network with backpropagation, this huge number would find its way into some of the gradients, and that would make it hard to take small steps to find the optimal weights and biases. In other words, in order to find the parameter values that give us the lowest value for the loss function, we usually want to take relatively small steps. Bam. However, when the gradient contains a huge number, then we'll end up taking relatively large steps and instead of finding the optimal parameter we will just bounce around a lot. Bummer. One way to prevent the Exploding Gradient Problem would be to limit W Sub 2 to values less than 1. However, this results in the Vanishing Gradient Problem. ""Hey, wait, where the gradient go?"" To illustrate the Vanishing Gradient Problem, let's set W sub 2 to 0.5. Now, just like before, we multiply the first input by W sub 2 raised to the number of times we unroll the network. So if we have 50 sequential input values, that means multiplying input sub 1 by 0.5 raised to the 50th power and 0.5 raised to the 50th poweris a number super close to zero. Because this number is super close to zero. This is called The Vanishing Gradient Problem. Now when optimizing a parameter, instead of taking steps that are too large, we end up taking steps that are too small. And as a result, we end up hitting the maximum number of steps we are allowed to take before we find the optimal value. Hey, Josh, these Vanishing / Exploding Gradients are a total bummer. Is there anything we can do about them? Yes, and we'll talk about a popular solution called Long Short-Term Memory Networks in the next StatQuest. Now, it's time for some Shameless Self Promotion. If you want to review statistics and machine learning offline, check out my book. The StatQuest Illustrated Guide to Machine Learning at statquest.org. It's over 300 pages of total awesomeness. Hooray! We've made it to the the end of another exciting StatQuest. If you liked this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, consider contributing to my patreon campaign, becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. Alright, until next time Quest on!"
80-Wpwf1_dg,2022-05-09T04:00:03.000000,"The StatQuest Illustrated Guide To Machine Learning, Theme Song!!!",[Music] the stat quest illustrated guide is here hooray stack quest over 300 pages jam packed with awesomeness triple bam
FHdlXe1bSe4,2022-04-25T04:00:12.000000,The StatQuest Introduction to PyTorch,the statquest introduction to pi torch is here statquest hello i'm josh starmer and welcome to statquest today we're going to talk about the stat quest introduction to pi torch this stat quest is sponsored by lightning and grid.ai lightning and grid are awesome you can do cool stuff in the cloud hooray note this stat quest assumes that you already understand the main ideas behind neural networks the main ideas behind how neural networks are fit to data with back propagation the main ideas behind the relu activation function and how tensors are used in neural networks if not check out the quests lastly you can download all of the code in this stat quest for free the details are in the pinned comment below in neural networks part 3 the rel u activation function in action we started with a simple data set that showed whether or not different drug doses were effective against a virus the low and high doses were not effective but the medium dose was effective then we talked about how this neural network used weights and biases to slice flip and stretch the relu activation functions into new and exciting shapes to fit this pointy thing to the data set bam hey look it's statsquatch hey josh this neural network is awesome can we code it in pi torch yes however before we start coding let's label the weights and the biases now we need to make sure we have the necessary python modules installed in this tutorial we will be using pi torch to create the neural network and matplotlib and seaborn to draw awesome graphs if you need help installing any of these check the pinned comment below now that we have installed everything that we'll need to implement this neural network let's get coding the first thing we do is import the python modules that we will use first we'll import pi torch which is actually called torch because that is what it was originally called before it was ported to python we'll use torch to create tensors to store all of the numerical values including the raw data and the values for each weight and bias then we will import torch.nn which we will use to make the weight and bias tensors part of the neural network then we import torch.nn.functional which gives us the activation functions then we import sgd which is short for stochastic gradient descent to fit the neural network to the data the next two things we import matplotlib and seaborn are all just to draw nice looking graphs note the seaborn package is traditionally imported as sns which stands for samuel norman seabourn a fictional character in the drama the west wing the dude that wrote seabourn is just a big fan of the west wing and names his stuff after it strange but true bam now let's build this neural network with pi torch creating a new neural network means creating a new class so we start by creating a new class that in this example we call basic nnn and basic nnn will inherit from a pi torch class called module now we create an initialization method for the new class and the first thing we do is call the initialization method for the parent class nnn.module then we initialize the weights and biases in our neural network we'll start with weight w sub 0 0 which is set to 1.70 so we create a new variable called w00 and make it a neural network parameter making this weight a parameter for the neural network gives us the option to optimize it now since weight w sub 0 0 is 1.70 we initialize the new parameter with a tensor set to 1.7 note since this is a tensor the neural network can take advantage of the accelerated arithmetic and automatic differentiation that it provides lastly because we don't need to optimize this weight we'll set requires underscore grad which is short for requires gradient to false likewise we create new variables for the bias b sub 0 0 and the weight w sub 0 1 and then we create variables for the remaining weights and biases bam we have created neural network parameters for each weight and bias now we need to connect them to the input the activation functions and ultimately to the output in other words we need a way to make a forward pass through the neural network that uses the weights and biases that we just initialized so we do that by creating a second method inside basic nnn called forward so we can see what's going on let's move the code for forward to the top of the screen now the first thing we want to do is connect the input to the activation function on top so we create a new variable input to top rail u that is equal to the input times the weight w sub 0 0 plus the bias b sub 0 0. then we pass input to top rail u to the rail u activation function with f dot rail u remember earlier we imported torch.nn.functional as f so that is where the relu function comes from then we save the output of the rail u in top rail u output now we scale top rail u output by the weight w sub 0 1 and save the result in scaled top rail u output we connect the input to the bottom rail u and scale the activation function's output then we add the top and bottom scaled values to the final bias and use the sum as the input to the final rail u to get the output value lastly the forward function returns the output thus given an input value the forward function does a forward pass through the neural network to calculate and return the output value bam now if we look at the entire class that we created basic nnn we see two methods init and forward init creates and initializes the weights and biases and forward does a forward pass through the neural network by taking an input value and calculating the output value with the weights biases and activation functions wow this is a lot of code how do we know it works and doesn't have any bugs good question squatch we can verify that the code works by plugging in a bunch of values between 0 and 1 that represent different doses and see if the output from forward results in this bent shape that fits the training data so the first thing we need to do is create a sequence of input doses and we do that with this command here we use the pi torch function linspace to create a tensor with a sequence of 11 values between and including 0 and 1 and we store the tensor in a variable called input doses note we can print out and admire the input doses by just typing the variable name input doses now the idea is to run these input values through our neural network so we'll make a neural network that we'll call model from the class we just created basic nnn note we are naming the actual neural network model because that is the standard variable name used when coding with pi torch thus from here on out i'm going to use the term model and neural network interchangeably if this freaks you out check out the statquest on models anyway now we can pass the input doses to the model which by default calls the forward method that we wrote earlier and we save the output from the neural network in a variable we cleverly named output values and now that we have both the input values to the neural network and the output values we can use them to draw this graph that has different drug doses on the x-axis and their predicted effectiveness on the y-axis first we set the seabourn style to white grid so the graph looks cool and then we use line plot to draw a graph of the data on the x-axis we put the original input doses and on the y-axis we put the corresponding output values and then we make the line green and wide enough to easily see lastly we set the y and x-axis labels and that code gives us this graph the graph tells us that the neural network we created earlier basic nnn does exactly what we expected in other words earlier we showed that when we put input values between 0 and 1 into this neural network the output was this bent shape which is the same as the graph we drew with our code double bam now that we can create a neural network in pi torch and graph what it can do can we pretend that we don't already know the optimal value for b sub final is negative 16 sure thanks squatch we'll just set b sub final to zero and we can now use pi torch to optimize b subfinal with back propagation the first thing we'll do is make a copy of the original class we created basic nnn and change the name of the copy from basic nnn to basic nn underscore train because we want to train this neural network then we change the initial value for final bias to 0.0 and we set requires underscore grad which remember is short for requires gradient to true setting requires grad to true is what tells pi torch that this parameter should be optimized we can verify that setting b subfinal to zero results in a neural network that no longer fits the training data by drawing a graph of the neural network's output like we did before only this time we create the model from basic nnn underscore train instead of basic n n and because final underscore bias now has a gradient we call detach on the output values to create a new tensor that only has the values in other words because seaborn doesn't know what to do with the gradient we strip it off with detach the original graph we drew for basic nnn shows effectiveness equals 1 when the dose equals 0.5 which is correct in contrast the graph for basic nn underscore train shows effectiveness equals 17 when dose equals 0.5 which is way too high and that means we need to train the neural network to optimize b sub final which means we need to create this training data all we have to do to create training data is create one tensor called inputs with the three input doses 0 0.5 and 1. and another tensor called labels that has the observed output values 0 1 and 0. now we are ready to optimize the last bias b sub final unfortunately this next step requires a lot of code but don't worry we'll go through it one step at a time also spoiler alert later in this series on how to implement neural networks we'll see how pi torch lightning makes this code a lot simpler anyway the first thing we do is create an optimizer object that will use stochastic gradient descent sgd to optimize b sub final remember we imported the sgd class from the torch.optin package way back at the start so in order to optimize b sub final we pass model.parameters to sgd which will optimize every parameter that we set requires grad equal to true we also set the learning rate to 0.1 in a bid we're going to use our new optimizer to optimize final bias but first just so we can see how gradient descent improves the value for final bias will print the current value the stir function converts the tensor value into a string so we can print it with other text and now we are ready to code the for loop that does gradient descent note if you're not already familiar with gradient descent and stochastic gradient descent check out the quests oh no it's the dreaded terminology alert each time our optimization code sees all of the training data is called an epoch so in this example every time we run all three points from our training data through the model we call that an epoch thus we start our optimization code with a for loop that counts the number of epochs and we set it so that we will run all three data points from the training data through the model up to 100 times now we create and initialize a variable called total loss that will store the loss which is a measure of how well the model fits the data for example if our unoptimized model fit the training data really poorly like this and we had this really large residual the difference between what the model predicts and what we know is true then the loss would be relatively large in contrast if the model fit the training data a little better and we had a smaller residual then the loss would be relatively small thus for each epoch we will use total loss to keep track of how well the model fits the data now we start a nested for loop that runs each data point from the training data through the model and calculates the total loss in this case that means the for loop starts with the first point in the training data and determines its input or dose and its known label or effectiveness then it runs that dose through the model to get a predicted output and then we calculate the loss between the predicted value and the known label with a loss function in this case we are calculating the squared residual where the residual is the difference between the output and the known value that said you can code any loss function that you want to use like the absolute value loss or you can choose from among the many loss functions like mse loss or cross entropy loss that come with pi torch anyways in this example the predicted and known value for the first point is zero so the squared residual is zero minus zero squared which equals zero now we use loss dot backward to calculate the derivative of the loss function with respect to the parameter or parameters we want to optimize in this example that means calculating the derivative of the squared residual with respect to b sub final and plugging in the predicted and known values note if any of this part is freaking you out check out the stack quest on back propagation lastly we add the squared residual to total loss so we can keep track of how well the model fits all of the data now we go back to the start of the nested for loop and select the input and label for the second point in the training data set and then we run the second point through the model to get a predicted output and then we calculate the loss the squared residual between the predicted and observed values next we use loss.backward to calculate the derivative of the loss function with respect to b sub final and and this is really important loss dot backward adds that to the previous derivative in other words loss dot backward remembers the derivative that we calculated for the first point and adds the new derivative that we calculated for the second point thus lost dot backwards accumulates the derivatives each time we go through the nested loop to be honest the first time i saw this it blew my mind this is because every time we go through the nested loop we create a brand new loss variable here and i couldn't figure out how the new loss could add to what the last one computed however it turns out that we create loss from the output value which in turn comes from the model and the model keeps track of the derivatives anyway the main point is that loss.backward accumulates the derivatives each time we go through the nested loop and we need to keep this in mind in contrast total loss does not automatically accumulate so we add the new squared residual to total loss then we go through the loop one last time for the last point in the training data set and that means we calculate the squared residual for the last point and when we call loss.backward we add the derivative for the last point to the other two derivatives and lastly we add the squared residual to total loss bam we made it through the nested loop and now that we're done with the nested loop we check to see if total loss is really small if so that means the model fits the training data really well and we can stop training so if total loss is really small we print out the number of epochs we've gone through so far and break out of the optimization loop to stop training otherwise if total loss is not small then we take a small step towards a better value for b sub final using optimizer.step note just like loss has access to the derivatives in the model when we call loss.backward optimizer.step also has access to the derivative stored in model and can use them to step in the correct direction now we need to zero out the derivatives that we're storing in model and we do that with optimizer.0grad note if we don't zero out the derivatives then the next time we enter this nested loop and call loss dot backward we'll add the new derivatives to the old derivatives from the previous step and that would be bad lastly we print out the current epoch and the current value for final bias so we can see how final bias changes each time through the loop now we've made it through the entire optimization loop and we just repeat it until total loss is small or we go through 100 epochs anyway now that we have gone through the optimization loop we print out the final value for the final bias bam now when we run this block of code we see the value for final bias before we optimize 0 and that value as we saw before gives us this graph of the output then we see the values that final bias takes on during each step of gradient descent and after 34 steps the total loss is tiny and the optimal value for final bias is negative 16.0019 which is pretty close to negative 16 the optimal value we used originally hey can we draw one last graph to verify that the optimized model fits the training data sure thing squatch we can verify that the optimized model fits the training data by graphing it with this code which is the same as what we used before except now we don't create a new model instead we just use the one we optimized and this is what we get which shows that the neural network does exactly what we expect triple bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the stack quest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
W4sYsXGeCls,2022-04-01T04:00:09.000000,"Troll 2, Clearly Explained!!!",troll two i wanna watch you but i am so afraid statquest hello i'm josh starmer and welcome to statquest today we're gonna talk about troll 2 and it's going to be clearly explained this stat quest is sponsored by lightning and grid.ai check them out they are awesome for more details follow the links in the pinned comment below note this stat quest assumes that you are already familiar with bam if not check out the quest the link is in the description below if you've watched enough stat quests chances are you've seen references to the movie troll 2 for example in the stat quest on expected values statsquatch makes a bet that the next person we meet has heard of the movie troll 2. and references to the movie troll 2 appear in a bunch of other stat quests as well so now it's time for troll 2 to be clearly explained first of all there is no consensus on when troll 2 came out the wikipedia article says 1990. this streaming platform tubby says it came out in 1991. and rotten tomatoes says it came out in 1992 and while we're on the rotten tomatoes page it's also worth noting that the critic's consensus is just three words oh my god bam another confusing thing about troll 2 is its name depending on the poster sometimes it's troll 2 and sometimes it's troll squared however one thing is certain about troll 2 troll squared it is not a sequel i mean sure there's a movie called troll that pretty much everyone agrees came out in 1986 but troll the movie is about a troll and troll 2 troll squared slash whatever you want to call it had nothing to do with the earlier movie and is not about trolls that's right at no point during the 94 minutes spent watching troll 2 will you hear or see a single reference to a troll or multiple trolls oh no it's the dreaded terminology alert although troll 2 is called troll 2 the monsters in the movie are all goblins in fact the original title for the movie was goblins however they changed the name when they thought more people would go see a movie called troll 2. double bam troll 2 is all about watching people eat including this love struck man enjoying a green pudding a child protecting his family by eating a massive bologna sandwich and someone buried in popcorn we also see a whole lot of this guy who goes by grandpa seth sometimes you see grandpa says face floating in air and sometimes you see an extreme close-up of grandpa seth's mouth anyway i don't want to give away too much of the movie but it involves a happy yet chaotically arranged family visiting the countryside and they are greeted by a family that stands in order from shortest to tallest to appreciate the differences between these two families it's good to see them both at the same time on the left we have the happy family that has no idea how to pose for a picture and on the right we have a family that can quickly arrange themselves in a photogenic way small bam anyway this kid knows there's something funny going on and ends up having to fight this creepy looking woman with lots of jewelry this lady with interesting glasses and this old dude with a wide brimmed hat does the kid win all three fights well you'll just have to watch it to find out but before you do that let me warn you troll 2 is consistently voted the worst movie ever made and that's how it caught my attention when i decided that statquest needed a new example dataset but is troll 2 really that bad i'll let you decide triple bam hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
aH3mZjHkAbs,2022-03-22T18:39:32.000000,The Binomial Distribution in 30 Seconds!!!,when it's true or it's false when it's yes or it's no when we have all these questions and we want to know use the binomial distribution for example we can use the binomial distribution to calculate the probability that seven people say yes i love pumpkin pie and three people say no i don't
jth4kEvJ3P8,2022-03-14T16:00:13.000000,UMAP: Mathematical Details (clearly explained!!!),let's do some math bam stat quest yeah hello i'm josh starmer and welcome to stat quest today we're going to talk about umap mathematical details this stat quest is sponsored by lightning and grid.ai with lightning you can design build and scale models with ease focus on the business and research problems that matter to you lightning takes care of everything else and with grid you can use the cloud to seamlessly train hundreds of models from your laptop with a single command no code changes necessary for more details follow the links in the pinned comment below note this stack quest assumes that you are already familiar with the main ideas of how umap works if not check out the quest this stat quest also assumes that you are already familiar with gradient descent and stochastic gradient descent if not check out the quest in the stat quest on umap main ideas we started with this raw high dimensional data and then transformed the raw distances into similarity scores by first deciding how many high dimensional neighbors we wanted each point to have in that example we set the number of neighbors to 3 which means that in addition to itself each point should have two other neighbors then u-map takes the log base 2 of that number and the log base 2 of 3 is 1.6 and that meant we drew curves for each high dimensional point so that the sum of the similarity scores was 1.6 so now let's look at the mathematical details behind all of this the equation for the similarity scores the y-axis coordinates is e raised to the negative raw distance minus the distance to the nearest neighbor divided by sigma note we'll talk about sigma in a bit but just so we can see the equation in action let's just let sigma equal one now when sigma equals one we get this curve for point a so given this curve let's calculate the similarity score for point b relative to point a in other words we are calculating the y-axis coordinate on the curve above b now since b is a's nearest neighbor and the distance to b is 0.5 we plug 0.5 into the equation for the distance to the nearest neighbor then we plug in the raw distance from a to b 0.5 and do the math first we subtract 0.5 from 0.5 and get 0 and 0 divided by anything is 0 and anything raised to the 0 power is 1. so the similarity score for point b relative to point a is one bam now let's calculate the similarity score for c relative to a first since b is a's nearest neighbor and the distance to b is 0.5 we plug 0.5 into the equation for the distance to the nearest neighbor now let's plug in the distance from a to c 2.4 and plug in 1 for sigma and when we do the math we get 0.15 so the similarity score for point c relative to point a is 0.15 note because points d e and f are all way far away from a their similarity scores are all super close to zero so we can ignore them and when we add up the non-zero scores 1 and 0.15 we get 1.15 now remember earlier we set the number of high dimensional neighbors each point should have to 3 and that means the sum should equal the log base 2 of 3 which equals 1.6 and that means we need to change the shape of this curve to increase the similarity scores however since the high dimensional distances are fixed and we can't change them the only thing we can do to change the shape of this curve and thus the scores is change sigma so let's increase sigma to 2 and see what happens now when we plug in the distances we get 1.0 and 0.39 for the scores and the sum of the scores is a little closer to the goal 1.6 lastly when we increase sigma to 3.5 and plug in the distances we get 1.0 and 0.6 and the sum of the scores is 1.6 which matches the log base 2 of the number of nearest neighbors that we specified 1.6 and that means we found the perfect shape for this curve bam note regardless of what sigma is the similarity score for the nearest neighbor will always be one this is because the raw distance to the nearest neighbor minus the distance to the nearest neighbor will always be zero and that makes the whole exponent equal to zero and e or anything else for that matter raised to the zero power is one small bam now when we calculate similarity scores relative to point b because b is closer to c than a is we need to reduce sigma to 2.8 and that results in a slightly different curve and as a result the similarity scores relative to b also add up to the log base 2 of 3 which equals 1.6 now we know exactly why we get different curves for each high dimensional point bam note if you're familiar with t sne the only difference at this stage is that t-sne uses the y-axis coordinates on gaussian curves to calculate the similarity scores and the widths and heights of the curves are determined by the perplexity parameter however the effect that the perplexity parameter has is very similar to the number of high dimensional neighbors that we set in umap in both cases they determine which points are considered neighbors in the high dimensional space bam now going back to umap as we saw in the main ideas stat quest because we have three different curves we can end up with asymmetrical scores like these between points b and c now if this was t sne we would make things symmetrical between points b and c by averaging the similarity scores in contrast umap uses the following formula to make the score symmetrical for example if we let s1 be the score from point b to c 0.6 and let s 2 be the score from point c to b 1.0 then when we do the math we get 1.0 so we update the scores to 1.0 now to be honest this result seems strange to me because now point b is just as similar to c as it is to a even though it is closer to a however umap uses this equation instead of the average because of the theoretical framework topology and fuzzy sets from which it is derived and this is the fuzzy union operation another small bam now we just apply the equation to all of the other pairs of scores to make them symmetrical in a fuzzy union sort of way bam and now that we know how to calculate symmetrical scores we can summarize them for both clusters in the diagrams on the right bam now umap initializes a low dimensional graph using spectral embedding and maybe one day we'll do a stat quest on spectral embedding because it sounds so cool but for now just know that the low dimensional graph is initialized in this case that means we have points spread out on a number line and now we can talk about calculating the low dimensional similarity scores using a fixed symmetrical curve based on a t distribution specifically umap uses this equation to calculate low dimensional similarity scores where d is the low dimensional distance between two points and alpha and beta control how tightly the low dimensional points can be packed together note alpha and beta can be modified with user-defined parameters for the minimum distance between low dimensional points and their spread that said by default alpha equals 1.577 and beta equals 0.8951 and we'll use those values in this example also note if we set alpha equal to 1 and beta equal to 1 then we'll get the exact same low dimensional scores that t sne uses so another difference between umap and t snee is that umap gives you more control over how tightly packed the low dimensional points end up and another small bam now let's calculate the low dimensional score for point b relative to point a first we'll plug in the default values for the parameters alpha and beta then we'll plug in the low dimensional distance between points a and b 2.1 and when we do the math we get 0.14 the y-axis coordinate above point b on this curve double bam note if two points are right on top of each other in the center of the curve so that the distance between them is zero then the highest y-axis value on the curve the highest low-dimensional score is one now that we know how umap calculates low dimensional similarity scores let's talk about how it moves the low dimensional points to reproduce the high dimensional clusters the first thing umap does is pick two low dimensional points to move closer to each other umap does this by randomly selecting a pair of points in a neighborhood proportionally to their high dimensional score so for this example let's assume that umap randomly selected points b and a and of that pair randomly decides to move point b closer to a so now we calculate the low dimensional score for b relative to a by plugging the low dimensional distance between points a and b 2.1 into the equation for low dimensional scores and when we do the math we get 0.14 now u-map picks a point that b should move away from this means u-map randomly picks one of the points that are not in b's neighborhood however unlike before this time the high-dimensional scores do not influence which point is picked so let's imagine umap picked point e which corresponds to point e on the low dimensional graph now we calculate the low dimensional score for point b relative to e and we get 0.23 so now we have two scores the first score we calculated 0.14 is the score for a point in the same neighborhood as b and that b needs to move closer to so we'll call that a neighbor score the second score we calculated 0.23 is the score for a point that is in a different neighborhood from b and that b needs to move away from so we'll call that a not neighbor score now as we move point b closer to a which is a neighbor and further from e which is not a neighbor the not neighbor score gets smaller and the neighbor score gets larger so umap uses these two scores to evaluate whether or not point b is in the right place just like we use residuals to calculate the sum of the squared residuals and evaluate how well a linear regression line fits its data however instead of plugging residuals into the sum of squared residuals cost function umap plugs the neighbor and not neighbor scores into this cost function so when point b is here we plug in the neighbor score and the not neighbor score and the cost is 2.23 and we can keep track of that on this graph that has b's current location on the x-axis and the cost associated with that location on the y-axis now if we move point b closer to a then the cost decreases to 1.17 and if we move point b even closer to a then the cost decreases even further to 0.08 lastly if we move point b way past point a then we see the cost start to increase and if we just plotted this function for point b given that a is a neighbor and e is not a neighbor then we would get this squiggle and if we calculate the derivative of the squiggle which represents the change in the cost function with respect to point b then we can use gradient ascent to move b into the correct location one step at a time and because we have a bunch of points we want to move and we only move one or a small subset at a time umap uses stochastic gradient descent to find the optimal low dimensional graph triple bam note because umap uses stochastic gradient ascent to move each point one step at a time even though every low dimensional graph for a given data set starts out the same the randomness associated with stochastic gradient descent means that each final graph might look a little different bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
eN0wFzBA4Sc,2022-03-07T17:00:10.000000,"UMAP Dimension Reduction, Main Ideas!!!",umap takes a big pile of data that you can't graph and helps you graph it hooray stat Quest hello I'm Josh starmer and welcome to stat Quest today we're going to talk about umap Dimension reduction main ideas this stack Quest is sponsored by lightning and grid. with lightning you can design build and scale models with ease focus on the business and research problems that matter to you lightning takes care of everything else and with grid you can use the cloud to seamlessly train hundreds of models from your laptop with a single command no code changes necessary for more details follow the links in the pinned comment below note this stat Quest focuses on the main ideas of how umap Works however if you're also interested in the map mathematical details be sure to check out the follow-up Quest umap mathematical details now imagine we collected weight and height measurements from a bunch of people and we plotted the people on a two-dimensional graph like this where we have weight on the x-axis the First Dimension and height on the Y AIS the second dimension seeing the data can be useful to identify outlin buers and to identify clusters of similar people so looking at data can be very useful and generally speaking is one of the first steps in any data analysis however if we wanted to include each person's age we would have to add a third AIS and now our graph is three-dimensional drawing a 3dimensional graph on a two-dimensional computer screen is awkward but possible however if we wanted to include a lot more features we'd need to draw a four or more dimensional graph and that's not possible so what do we do if we have a lot of features and we want to look at the data well one good option is principal component analysis PCA and if you're not already familiar with PCA you definitely should be so check out the quest anyway the problem with PCA is that it only works well when the first two principal components account for most of the variation in the data simply put if you have a really complicated data set PCA may not work very well so if we can't use PCA what should we do one option is called umap uniform manifold approximation and projection umap takes high-dimensional data meaning data with three or more features and outputs a low-dimensional graph meaning a graph we can easily look at umap is popular because it is relatively fast even with large data sets and similar samples tend to Cluster together in the final output so it is useful for identifying similarities and outliers bam so we can see what umap does and how it works we're going to start with a very simple two-dimensional graph and show how umap converts it to a one-dimensional number line in other words this two-dimensional graph will represent the high dimensional data that we reduce with um map to a single Dimension that will be the low dimensional graph if we look at the high-dimensional data we see that points a B and C are relatively close to each other and form a cluster and points d e and f are relatively close to each other and form another cluster and both clusters are relatively far from each other the goal of umap is to create a low-dimensional graph of this data that preserves these high-dimensional clusters and their relationship to each other the general idea is to initialize the low-dimensional points and then move the low dimensional points around until they form clusters that have the same relationships we saw in the high-dimensional data in other words because points a B and C are clustered together and are relatively far from the other cluster umap wants the low dimensional points a b and c to be close to each other and relatively far from the other cluster so let's talk about the main ideas of how umap makes this happen note if umap simply projected all of the data onto the xaxis then instead of two distinct clusters we'd see a mish mash of points and if we had a slightly more complex data set with a third cluster here then projecting the data onto the Y AIS wouldn't be any better so what umap does is calculates similarity scores to help identify clustered points so it can try to preserve that clustering in the low-dimensional graph the first thing that you map does is calculate the distances between each pair of high-dimensional points note I'm only showing two of the distances between the two clusters but just know that we calculated all of them and that the Clusters are way far apart now to calculate the similarity scores associated with point a we start by putting point a on a graph now because point a is 0.5 units away from point B we put B 0.5 units away from a on the graph and because point a is 2.4 units away from C we put C 2.4 units away from a on the graph and because the remaining points d e and f are all way far away from a we put them way far away from a on the graph now we draw a curve over the data to calculate the similarity scores the shape of this curve depends on the number of high-dimensional neighbors that you want each point to have a common default value for the number of high dimensional neighbors is 15 but in this example where we have a tiny data set and we know each cluster has three points we'll set it to three note to be clear the number of neighbors we want each point to have includes the point itself so by setting the number of neighbors to three we actually only want two other points as neighbors also later in the quest we'll talk about what happens when we change the number of neighbors we want each point to have but for now let's just set the value to three now brace yourselves things are about to get a little mathy the purpose of this math is to get an understanding of how the number of nearest neighbors parameter which is arguably the most important umap parameter you can adjust works so that said umap takes the the log base 2 of the number of high dimensional neighbors you want each point to have and since we set the number of high dimensional neighbors to three we get the log base 2 of 3 which equals 1.6 and this value 1.6 is what defines the shape of this curve the curve is shaped in such a way that the Y AIS coordinates for the nearest Neighbors which in this case are points B and C add up to the log base 2 of the number of nearest neighbors you specified which in this case is 1.6 in other words this curve is shaped the way it is so that the y- AIS coordinate for B is 1.0 and the y-axis coordinate for C is 0.6 and the y- AIS coordinates for d e and f are all pretty much zero adding these scores together gives us 1.6 the target number we are shooting for thus plus the y-axis coordinates for points B and C are their similarity scores relative to a and since the scores for d e and f are all zero we can ignore them for now so let's save the similarity scores for B and C bam now just like we did for point a let's calculate the similarity scores relative to point B first we put point B on a graph and then then we add all the other data at their respective distances and then we draw a curve over the data so that the sum of the similarity scores the y-axis coordinates for each point other than b equals 1.6 anyway just like before because points d e and f are way far away their scores are zero and we can ignore them so we only need to save scores for a and C relative to B likewise we calculate and save the nonzero scores for Point C bam note you may have noticed that the similarity score for C relative to B 0.6 is different from the similarity score for B relative to C 1.0 the difference comes from the fact that the curves for each point are different umap scales the curve so that regardless of how close or far the neighboring points are are the sum of the similarity scores will be equal to the log base 2 of the number of nearest neighbors that you specify and scaling the curves ensures that every point is similar to at least one other point in the data set however using different curves mean the similarity scores are not symmetrical so umap makes them symmetrical using a method similar to taking the average and this is one of those details we'll discuss in the follow-up stack Quest bam note in the exact same way umap calculates the similarity scores for points d e and f bam now umap initializes a low-dimensional graph but as we can see this low-dimensional graph is not ideal because point B needs to be closer to a because they are in the same high-dimensional cluster and point B needs to be further from F because they are in two different high-dimensional clusters so to make this low-dimensional graph show the same clusters we see in the high-dimensional data umap picks two low-dimensional points that it should move closer together umap does this by randomly selecting a pair of points in a cluster proportionally to their high-dimensional score in this case that means there is a higher probability that umap might randomly select points a and B because their score is 1.0 and there is a lower probability that umap might randomly select points A and C because their score is 0.8 so for this example let's assume that umap randomly selected points A and B and that means umap wants to move points A and B closer together then umap flips a coin and decides to move point B closer to a note it could have just as easily decided to move point a closer to B now that we know that we will move point B closer to a umap picks a point that b should move further from so umap randomly picks one of the points that are not in B's High dimensional cluster however unlike before this time the high dimensional scores do not influence which point is picked instead each point in a different cluster regardless of its score has an equal chance of being picked so let's imagine umap picked Point e now that umap has decided to move point B closer to a because they should be clustered together and move point B further from E because they should be in different clusters umap has to figure out how much it should move point B to do this umap calculates low dimensional similarity scores Y axis coordinates on a curve for points b and a and for b and e however now instead of using a variety of Curves like we did for the high dimensional data the low dimensional similarity scores come from a fixed bell-shaped curve that is derived from a t distribution note generally speaking a t distribution curve is like a gussian curve or normal distribution except the T distribution tends to be shorter and have fatter Tails also note the low dimensional curves are all the same size now because points A and B are in the same cluster or neighborhood umap wants to move point B closer to a in order to maximize this low-dimensional score in contrast because points B and E are in different clusters umap wants to move point B further from E so that it can minimize this low dimensional score so ultimately umap moves point B a little closer to a and a little further from e note umap only moved point B a small amount because when there is a ton of data taking small steps each turn makes it easier to get the low-dimensional graph looking Just Right double bam now umap picks another pair of points to move together in this case umap wants to move D closer to e then um map randomly picks a point in the other cluster to move D away from in this case umap decides to move D away from C now umap calculates low dimensional similarity scores to decide how to move Point d note if we move D closer to e the point we want to get closer to then we will also be moving D closer to C the point we want to get further from however this move barely increases the score for D relative to C in other words the score we want to minimize will still be pretty small in contrast the score we want to maximize the score relative to e will get much larger thus umap moves d a little closer to e likewise umap moves the points one step at a time until we have two low-dimensional clusters that are relatively far from each other just like we see in the high dimensional data triple bam note if you're familiar with how tne works you may have noticed that umap is very very similar in terms of the main ideas of how umap and tne work they are essentially the same and most of the differences are very subtle however there are two big important differences the first difference is that tne always starts with a random initialization of the low-dimensional graph in other words tne might start out with the low dimensional data looking like this and then the next time you run t- snake on the exact same data set it might start out with a low dimensional data looking like this and every time you run tne on the same data set you start with a different low-dimensional graph of the data in contrast umap uses something called spectral embedding to initialize the low-dimensional graph and what that means is that every time you use UAP on a specific data set you always start with the exact same low dimensional graph bam the other difference is that tne moves every single point a little bit each iteration in contrast umap can move just one point or a small subset of points each time and this helps it scale well with super big data sets bam lastly way back when we calculated similarity scores for the high dimensional data we said that the shape of each curve was determined in part by the userdefined parameter the number of neighbors each point has now let's talk about what happens when we change the number of neighbors generally speaking when you have a lot of data a relatively low value for the number of neighbors results in small independent clusters in some ways this is sort of like seeing the details but not the big picture in contrast a relatively large value for the number of neighbors gives you more of the big picture and less of the details so it can be worth trying different values to see what works best with your own data bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stat Quest study guides at stack quest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stat Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
L35fFDpwIM4,2022-02-28T17:00:11.000000,"Tensors for Neural Networks, Clearly Explained!!!",tensors do it for the data tensors do it for the speed tensors do it whenever you feel the need steadquest hello i'm josh starmer and welcome to statquest today we're going to talk about tensors for neural networks and they're going to be clearly explained this stack quest is sponsored by lightning and grid.ai with lightning you can design build and scale models with ease focus on the business and research problems that matter to you lightning takes care of everything else and with grid you can use the cloud to seamlessly train hundreds of models from your laptop with a single command no code change is necessary for more details follow the links in the pinned comment below note one thing that makes tensors a little confusing is that different people use the word tensor differently people in the math and physics community define tensor one way and people in the machine learning community define tensor a different way in this stat quest we're going to focus on the way tensor is used in the machine learning community within the machine learning community tensors are used in conjunction with neural networks so we need to talk about neural networks note if you're not already familiar with neural networks feel free to check out the quests the links are in the description below anyway neural networks can do a lot of things for example in the statquest neural networks part 1 inside the black box we had a simple neural network that had a single input drug dosage and used that single value to predict a single output the efficacy of the dosage then in the stat quests on back propagation we saw that even with this super simple neural network and this super simple training data with only three data points we still had to do a lot of math for the neural network to fit this squiggle to the data ugg math then in the stat quest neural network's part 4 multiple inputs and outputs we had a fancier neural network that had two inputs that corresponded to two different flower measurements and had three outputs that predicted which irish species these two measurements came from and then we saw how the neural network does a lot of math to make those predictions double ugh more math then in the statquest neural networks part 8 image classification with convolutional neural networks we had a super fancy neural network that had a 6 pixel by 6 pixel image so 36 pixels in all as the input and two outputs that predicted whether the image was of an x or an o and we walked through a whole lot of math that we needed to do in order to make those predictions triple ugg so much math note even though this neural network needs to do a whole lot of math it's still relatively simple compared to the types of neural networks that are used in practice for example the input to this convolutional neural network is a relatively small 6 pixels by 6 pixels black and white image however in practice usually the input image is much larger like 256 by 256 and that means the input has 65 536 pixels and usually the image is color instead of black and white and color images are usually split into three color channels red green and blue and since the neural network treats each channel separately that basically triples the number of pixels we have to do math on so now we're up to three times sixty five thousand five hundred thirty six which equals one hundred ninety six thousand six hundred eight pixels that we have to do a lot of math on and this is just one image and usually we need to do a ton of images to train the neural network so that means we have to do a ton of math on a ton of images and if we want to apply a neural network to video which is basically a series of images then we have even more math the good news is is that all this math is what tensors were designed for bam now let's talk about what tensors are from the perspective of someone who is creating a neural network tensors are ways to store the input data which in this example consists of three color channels for every single frame but as we saw earlier the input can also be super simple and consist of a single value and tensors also store the weights and biases that make up the neural network so from the perspective of someone creating a neural network tensors can seem really boring for example the input value for this neural network is just a single value which in most programming languages we'd call a scalar however to make things seem more exciting we can use fancy terminology and call the input which is just a single value a zero dimensional tensor when a neural network takes two input values like this one then in most programming languages we would say we store the inputs in an array however using tensor talk we will call this a one-dimensional tensor likewise when the input is a single image most programming languages would call it a matrix but we'll call it a two-dimensional tensor and when the input is video most programming languages would call this a multi-dimensional matrix or a multi-dimensional array or for you python people an nd array however using tensor talk we will call it an n-dimensional tensor so just like i said earlier from the perspective of someone creating a neural network tensors can seem really boring because all we have done is rename things that already exist so what's the big deal well unlike normal scalars arrays matrices and n-dimensional matrices tensors were designed to take advantage of hardware acceleration in other words tensors don't just hold data in various shapes like these but they also allow for all the math that we have to do with the data to be done relatively quickly usually tensors and the math they do are sped up with special chips called graphics processing units gpus but there are also tensor processing units tpus that are specifically designed to work with tensors and make neural networks run relatively quickly double bam note one thing i hinted at early on but didn't dive into the details about is that one of the things we do with neural networks is estimate the optimal weights and biases with back propagation and if you saw the stat quest on back propagation you'll know that we have to derive a bunch of derivatives and do a whole lot of the chain rule well one more cool thing about tensors is that they take care of back propagation for you with automatic differentiation this means you can pretty much create the fanciest neural network ever and the hard part figuring out the derivatives will be taken care of by the tensors triple bam in summary there are two types of tensors one type is used by mathematicians and physicists we did not talk about these today the other type of tensor is used in neural networks this is the type we talked about tensors for neural networks hold the data and the weights and biases and are designed for hardware acceleration so that neural networks can do all the math they need to do in a relatively short period of time and they take care of back propagation with automatic differentiation bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the stack quest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stack quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
PWvfrTgaPBI,2022-02-10T17:19:53.000000,"The Sensitivity, Specificity, Precision, Recall Sing-a-Long!!!",sensitivity is the percentage of actual positives correctly predicted specificity is the percentage of actual negatives correctly predicted precision is something different it's the percentage of predicted positives correctly predicted and rico hits us back to the start is the same as sensitivity is the percentage of actual positives correctly predicted bam [Music]
wtLQUjfomig,2022-01-14T19:44:47.000000,The Exponential Distribution,the exponential distribution is useful when you're estimating time between two events like the amount of time that passes between two stack quest videos stat quests
-14BImgVENA,2022-01-14T19:17:37.000000,"The mean, the median, and the mode.",the mean the median and the mode when it's normal they're the same stat quest
RDZUdRSDOok,2022-01-10T12:51:02.000000,"Clustering with DBSCAN, Clearly Explained!!!",dp scan clusters just like a person can statquest [Music] hello i'm josh starmer and welcome to statquest today we're going to talk about clustering with db scan and it's going to be clearly explained now imagine we collected weight and height measurements from a bunch of people and we plotted the people on a two-dimensional graph like this where we have weight on the x-axis the first dimension and height on the y-axis the second dimension by eye we can see two different clusters by identifying two different but relatively dense clumps of people in contrast these people that are far from everyone else look a little bit like outliers so by eye clustering this data is pretty easy however because these clusters are nested meaning the green cluster wraps around the blue cluster a relatively standard clustering method like k-means clustering might have difficulty identifying these two clusters instead because of the nesting a simple clustering method might get something weird like this where these points are assigned to the blue cluster even though they look like they belong to the green cluster so we need a clustering algorithm that can handle nested clusters also remember this two-dimensional graph only uses weight and height data but if we wanted to include each person's age we would have to add a third axis and now our graph is three-dimensional drawing a three-dimensional graph on a two-dimensional computer screen is awkward but possible however if we wanted to include four or more features we'd need to draw a four or more dimensional graph and that's not possible and if we can't draw and look at a four or more dimensional graph then we need a way to identify nested clusters that we cannot see by eye the good news is that there are clustering algorithms that can identify nested clusters in high dimensions one of these algorithms is called db scan and that's what we'll talk about today so let's go back to the original two-dimensional graph and see how db scan tries to mimic what we can easily do by eye now remember by eye we identify clusters by the densities of the points clusters are in high density regions and outliers tend to be in low density regions so let's see how dbscan uses the densities of the points to identify these two clusters bam now starting with the raw unclustered data the first thing we can do is count the number of points close to each point for example if we start with this red point and we draw an orange circle around it then we can see that the orange circle overlaps at least partially eight other points so the red point is close to eight other points note the radius of the orange circle is user defined so when using dbscan you may need to fiddle around with this parameter now this red point is close to five other points because the orange circle overlaps at least partially five other points this red point is close to six other points and this red point is close to seven other points this red point is only close to two other points and this red point is not close to any other point because the orange circle does not overlap anything else likewise for all the remaining points we count the number of close points now in this example we will define a core point to be one that is close to at least four other points note the number of close points for a core point is user defined so when using db scan you might need to fiddle with this parameter as well anyway these four points are some of the core points because their orange circles overlap at least four other points hooray but neither of these points are core points because their orange circles do not overlap four or more other points ultimately we can call all of these red points core points because they are all close to four or more other points and the remaining points are non-core now we randomly pick a core point and assign it to the first cluster next the core points that are close to the first cluster meaning they overlap the orange circle are all added to the first cluster then the core points that are close to the growing first cluster join it and extend it to other core points that are close by here we see two core points and one non-core point that are all close to the growing first cluster and at this point we only add the core points to the first cluster that said eventually we will add this non-core point but right now we are only adding core points ultimately all of the core points that are close to the growing first cluster are added to it and then used to extend it further bam note at this point every single point in the first cluster is a core point and because we can no longer add any more core points to the first cluster we add all of the non-core points that are close to the core points to the first cluster for example this point which is a non-core point is close to a core point in the first cluster so we add it to the first cluster however because this is not a core point we do not use it to extend the first cluster any further that means that this other non-core point which is close to the non-core point that was just made part of the first cluster will not be added to the first cluster because it is not close to a core point so unlike core points non-core points can only join a cluster they cannot extend it further now we add all of the non-core points that are close to core points in the first cluster to the first cluster and now we are done creating the first cluster double bam to summarize how the first cluster was formed we picked a random core point and it started the first cluster then neighboring core points joined and extended the first cluster [Laughter] and non-core points only joined the first cluster bam now because none of these core points are close to the first cluster they form a new second cluster because they are close to each other and the non-core points that are close to the second cluster are added to it lastly because all of the core points have been assigned to a cluster we're done making new clusters and any remaining non-core points that are not close to core points in either cluster are not added to clusters and called outliers and that is how the db scan algorithm works triple bam note as we just saw clusters are created sequentially that means if we had a non-core point close to both clusters then when we built the first cluster beep we would add this non-core point to the first cluster because it is close to a core point along with all of the other non-core points that were close and now that this point is part of the first cluster it is no longer eligible to be in any other cluster small bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
YtebGVx-Fxw,2021-08-25T04:00:08.000000,Entropy (for data science) Clearly Explained!!!,[Music] yes you can understand entropy the ray step Quest hello I'm Josh starmer and welcome to stack Quest today we're going to talk about entropy for data science and it's going to be clearly explained note this stack Quest assumes that you are already familiar with the main ideas of expected values if not check out the quest and ropy is used for a lot of things in data science for example entropy can be used to build classification trees which are used to classify things entropy is also the basis of something called Mutual information which quantifies the relationship between two things and entropy is the basis of relative entropy aka the colback leeler distance and cross entropy which show up all over the place including including fancy Dimension reduction algorithms like tne and umap what these three things have in common is that they all use entropy or something derived from it to quantify similarities and differences so let's learn how entropy quantifies similarities and differences however in order to talk about entropy first we have to understand surprise so let's talk about chickens imagine we had two types of chickens orange and blue and instead of just letting them randomly roam all over the screen our friend stat Squatch chased them around until they were organized into three separate areas a b and c now if stat Squatch just randomly picked up a chicken in area a then because there are six orange chickens and only one blue chicken there is a higher probability that they will pick up an orange chicken and since there is a higher probability of picking up an orange chicken it would not be very surprising if they did in contrast if stat Squatch picked up the blue chicken from area a we would be relatively surprised area B has a lot more blue chickens than orange and because there is now a higher probability of picking up a blue chicken we would not be very surprised if it happened and because there is a relatively low probability of picking the orange chicken that would be relatively surprising lastly area C has an equal number of orange and blue chickens thus regardless of what color chicken we pick up we would be equally surprised combined these areas tell us that surprise is in some way inversely related to probability in other words when the probability of picking up a blue chicken is low the surprise is high and when the probability of picking up a blue chicken is high the surprise is low bam now we have a general intuition of how probability is related to surprise now let's talk about how to calculate surprise because we know there is a type of inverse relationship between probability and surprise it's tempting to just use the inverse of probability to calculate surprise because when we plot the inverse we see that the closer the probability is to zero the larger the Y AIS value however there's at least one problem with just using the inverse of the probability to calculate surprise to get a better sense of this problem let's talk about the surprise associated with flipping a coin imagine we had a terrible coin and every time we flipped it we got heads blah blah blah blah ug flipping this coin is super boring hey stat Squatch how surprised would you be if the next flip gave us heads I would not be surprised at all so when the probability of getting heads is one then we want the surprise for getting heads to be zero however when we take the inverse of the probability of getting heads we get one instead of what we want zero and this is one reason why we can't just use the inverse of the probability to calculate surprise so instead of just using the inverse of the probability to calculate surprise we use the log of the inverse of the probability now since the probability of getting heads is one and thus we will always get heads and it will never surprise us the surprise for heads is zero in contrast since the probability for getting tails is zero and thus we'll never get Tails it doesn't make sense to quantify the surprise of something that will never happen so when we plug in zero for the probability and use the properties of logs to turn the division into subtraction the second term is the log of0 and because the log of0 is undefined the whole thing is undefined and this result is okay because we're talking about the surprise associated with something that never happens like the inverse of the probability the log of the inverse of the probability gives us a nice curve and the closer the probability gets to zero the more surprise we get but now the curve says there is no surprise when the probability is one so surprise is the log of the inverse of the probability bam note with when calculating surprise for two outputs in this case the two outputs are heads and tails then it is customary to use the log base 2 for the calculations now that we know what surprise is let's imagine that our coin gets heads 90% of the time and it gets Tails 10% of the time now let's calculate the surprise for getting heads and tails as expected because getting tails is much rare than getting heads the surprise for tails is much larger now let's flip this coin three times and we get heads heads and tails the probability of getting two heads and one tail is 0.9 * 0.9 for the heads * 0.1 for the Tails and if we want to know exactly how surprising it is to get two heads and one tail then we can plug this probability into the equation for surprise and use the properties of logs to convert the division into subtraction and use the properties of logs to convert the multiplication into addition and then plug and chug and we get 3.62 but more importantly we see that the total surprise for a sequence of coin tosses is just the sum of the surprises for each individual toss in other other words the surprise for getting one heads is 0.15 and since we got two heads we add 0.15 two times plus 3.32 for the one tail to get the total surprise for getting two heads and one tail medium bam now because this diagram takes up a lot of space let's summarize the information in a table the first row in the table tells us the probability of getting heads or tails and the second row tells us the associated surprise now if we wanted to estimate the total surprise after flipping the coin 100 times we approximate how many times we will get heads by multiplying the probability we will get heads 0.9 by 100 and we estimate the total surprise from getting heads by multiplying by 0.15 so this this term represents how much surprise we expect from getting heads in 100 coin flips likewise we can approximate how many times we will get tals by multiplying the probability we will get tals 0.1 by 100 and we estimate the total surprise from getting Tails by multiplying by 3.32 so the second term represents how much surprise we expect from getting tails in 100 coin flips now we can add the two terms together to find out the total surprise and we get 46.7 hey stat quatch is back okay I see that we just estimated the surprise for 100 coin flips but aren't we supposed to be talking about entropy funny you should ask if we divide everything by the number of coin tosses 100 then we get the average amount of surprise per coin Point toss 0.47 so on average we expect the surprise to be 0.47 every time we flip the coin and that is the entropy of the coin the expected surprise every time we flip the coin double bam in fancy statistics notation we say that entropy is the expected value of the surprise anyway since we are multiplying each probability by the number of coin tosses 100 and also dividing by the number of coin tosses 100 then all of the values that represent the number of coin tosses 100 cancel out and we are left with the probability that a surprise for heads will occur times its surprise plus the probability that a surprise for Tails will occur times its surprise thus the entropy 0.47 represents the surprise we would expect per coin toss if we flipped this coin a bunch of times and yes expecting surprise sounds silly but it's not the silliest thing I've heard note we can rewrite entropy just like an expected value using fancy Sigma notation the X represents a specific value for surprise times the probability of observing in that specific value for surprise so for the first term getting heads the specific value for surprise is 0.15 and the probability of observing that surprise is 0.9 so we multiply those values together then the sigma tells us to add that term to the term for Tails either way we do the math we get 0.47 now personally once I saw that entropy was just the average surprise that we could expect entropy went from something that I had to memorize to something I could derive because now we can plug the equation for surprise in for X the specific value and we can plug in the probability and we end up with the equation for entropy bam unfortunately even though this equation is made from two relatively easy to interpret terms the surprise times the probability of the surprise this isn't the standard form of the equation for entropy that you'll see out in the wild first we have to swap the order of the two terms then we use the properties of logs to convert the fraction into subtraction and the log of one is zero then we multiply both terms in the difference by the probability then lastly we pull the minus sign out of the summation and we end up with the equation for entropy that Claude Shannon first published in 1948 small bam that said even though this is the original version and the one you'll usually see I prefer this version since it is easily derived from Surprise and it is easier to see what is going on now going back to the original example we can calculate the entropy of the chickens so let's calculate the entropy for area a because six of the seven chickens are orange we plug in 6 / 7 for the probability then we add a term for the one blue chicken by plugging in 1 / 7 for the probability now we just do the math and get 0.59 note even though the surprise associated with picking up an orange chicken is much smaller than picking up a blue chicken there is a much higher probability that we will pick up an orange chicken than pick up a blue chicken thus the total entropy 0.59 is much closer to the surprise associated with orange chickens than blue chickens likewise we can calculate the entropy for area B only this time the probability of randomly picking up an orange chicken is 1 / 11 and the probability of picking up a blue chicken is 10 / 11 and the entropy is 0.44 in this case the surprise for picking up an orange chicken is relatively high but the probability of it happening is so low that the total entropy is much closer to the surprise of associated with picking up a blue chicken we also see that the entropy value the expected surprise is less for area B than area a this makes sense because area B has a higher probability of picking a chicken with a lower surprise lastly the entropy for area C is one and that makes the entropy for area C the highest we have calculated so far in this case even though the surprise for orange and blue chickens is relatively moderate one we always get the same relatively moderate surprise every time we pick up a chicken and it is never outweighed by a smaller value for surprise like we saw earlier for areas A and B as a result we can use enthropy to quantify the similarity or difference in the number of orange and blue chickens in each area entropy is highest when we have the same number of both types of chickens and as we increase the difference in the number of orange and blue chickens we lower the entropy triple bam PS the next time you want to surprise someone just whisper the log of the inverse of the probability bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the stack Quest study guides at stack quest.org there's something for everyone hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below all right until next time Quest on
9wCnvr7Xw4E,2021-08-16T04:00:05.000000,"Bayes' Theorem, Clearly Explained!!!!",base theorem versus statsquatch base theorem wins statquest hello i'm josh starmer and welcome to statquest today we're going to talk about bayes theorem and it's going to be clearly explained note this stat quest assumes that you are already familiar with conditional probability if not check out the quest that said let's do a quick review in the stat quest on conditional probability we took a trip to statland and asked everyone represented by a colorful dot if they loved candy and or soda these two people loved candy and soda these four people only loved candy these five people only loved soda and these three people didn't like candy and they didn't like soda bam then we calculated the probabilities for each cell in the contingency table by dividing the counts by the total number of people in statland 14. bam then we determined the total number and probability of people who loved soda and did not love soda and the total number and probability of people who loved candy and did not love candy bam then we calculated the conditional probability that someone in statland might not love candy but love soda given that we already know that they love soda we did this by dividing the five people that do not love candy but love soda by the seven people that love soda and we got 0.71 then just for fun we divided the numerator and the denominator by the total population of statland 14. doing this extra division did not change the result we still got 0.71 however now the numerator is the original unconditional probability that someone in statland does not love candy but loves soda and the denominator is the unconditional probability that someone in statland loves soda all in all the probability that someone does not love candy but loves soda given that we know that they love soda is equal to the probability that someone does not love candy but loves soda divided by the probability that someone in statland loves soda so one way to think about conditional probability is the probability that an event will happen in this case the probability that we meet someone who does not love candy but loves soda scaled by the knowledge we already have about the event in this case we know that the person loves soda note because saying someone who does not love candy and love soda given that they love soda is a little redundant many people omit writing out the and love soda part and shorten the conditional probability statement to someone who does not love candy given that they love soda while this notation is standard it makes it harder to see the relationship between the thing we want to calculate the probability for on the left and how we calculate it on the right using the slightly redundant notation for conditional probability makes it obvious that we want the probability that an event happens scaled by the knowledge we already have about the event personally this slightly redundant notation helped me get a better understanding of bayes theorem and we'll talk about this more at the end of the stat quest now let's see what happens when we change what we already know about the event from knowing that they love soda to knowing that they do not love candy now we have the probability that an event happens which in this case is the event that we meet someone who does not love candy but loves soda scaled by the knowledge we already have about the event in this case we already know that they do not love candy now we plug in the numbers and do the math and get 0.63 now let's compare this conditional probability where we already know that the person does not love candy to the conditional probability we calculated before where we already knew that the person loves soda in both cases we want to know the probability of the same event meeting someone who does not love candy but loves soda and that means in both cases the numerators are the same however since we have different knowledge in each case we scale the probabilities of the events differently and ultimately we get different probabilities hey look it's statsquatch statsquatch is our friend in statland and he always wants to make a bet i bet you one dollar that you can't solve the conditional probabilities without knowing the probability of not loving candy and loving soda in other words if we don't know the value for the numerators can we still solve for the conditional probabilities well even if we don't know the probability that someone does not love candy but loves soda we can multiply both sides of the top equation by the probability that someone loves soda and these two terms on the right cancel out and we are left with the probability that we meet someone that does not love candy but loves soda equal to this stuff on the left side likewise we can multiply both sides of the equation on the bottom by the probability of not loving candy and these two terms on the right side cancel out and just like before we end up with the probability of meeting someone who does not love candy but loves soda equal to this stuff on the left side now we have two things on the left side of the equal signs that are both equal to the probability that we will meet someone who does not love candy and loves soda now remember that statsquatch asked us to solve for this term and this term without this term and because both equations are equal to the term we want to omit both equations are equal to each other so let's move this up a little bit and move this over here now remember we want to solve for this term and we want to solve for this term we'll start with the term on the left first we divide both sides by the probability that someone loves soda and the probability that someone loves soda cancels out on the left side and we have solved for this term bam now let's move the thing on the right to the left and the thing on the left to the right and divide both sides by the probability that someone does not love candy and the probability that someone does not love candy cancels out on the left side and we have solved for the other term bam in both cases we won the bet with statsquatch because we no longer need to know the probability that someone does not love candy and loves soda but more importantly we have derived bayes theorem double bam bayes theorem tells us that this conditional probability which is based on knowing that the person loves soda can be derived from this conditional probability which is based on knowing that they do not love candy alternatively bayes's theorem tells us that this conditional probability which is based on knowing that the person does not love candy can be derived from this conditional probability based on knowing they love soda in general if we let a equal does not love candy and b equals love soda then we can rewrite each equation into the standard formula for bayes's theorem in other words the conditional probability given that we know one thing about an event can be derived from knowing the other thing about the event now statsquatch says dude you derived bayes theorem with just a little algebra what's the big deal when we have all of the data laid out in a nice colorful chart or in a contingency table then bayes theorem is not that big of a deal in fact when you have all of the data bayes theorem isn't even a small deal however most of the time we don't have all of the data in other words statsquatch might only tell us the probability that someone does not love candy given that they love soda is 0.71 and i'm not certain but i think the probability that someone loves soda is close to 0.6 and the probability that someone does not like candy is 0.57 and if this is all the data we have then we plug the numbers into bayes theorem and get approximately 0.75 and that means given this data which includes a guess about the probability someone loves soda the probability that someone loves soda given that we know they do not like candy is about 0.75 note attentive viewers may notice that when we calculated the conditional probability with bayes theorem which we used because we did not have all of the information the result is different from when we calculated the probability knowing everything this is because statsquatch didn't know the exact value for the probability that someone loved soda they just took a guess and while taking a guess might sound like a terrible thing to do it's the only option when we have a large population for example it would be almost impossible to ask every single person in india if they love soda so a lot of times we have to make a guess bayesian statistics is about understanding what it means to make a guess like this and all it implies bayes theorem is the basis for bayesian statistics which is this equation paired with a broader philosophy of how statistics should be calculated and we'll cover all these topics in follow-up stat quests however before we go i want to review the standard notation so if you research bayes theorem or bayesian statistics on your own you won't be totally lost like i said earlier when most people write conditional probabilities they do it differently from the examples i've given here specifically because they know that this person does not love candy they do not include it when stating the probability now the conditional probability reads the probability someone loves soda given that they do not love candy likewise because they know this person loves soda they do not include it when stating the probability now the conditional probability reads the probability that someone does not love candy given that they love soda in the end it looks like we want to calculate the probability of two different events however it is important to keep in mind that in both cases there is only one event and both conditional probabilities refer to the same yellow area in the drawing and the same yellow square in the contingency table and the only real difference between the two conditional probabilities is the given knowledge and that's why i prefer the longer slightly redundant way to write conditional probabilities because the longer way makes it obvious that in both cases we are talking about the exact same thing small bam no triple bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stack quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
N4ZQQqyIf6k,2021-07-13T04:00:07.000000,Using Bootstrapping to Calculate p-values!!!,bootstrap in part two calculate p value statquest hello i'm josh starmer and welcome to statquest today we're going to talk about bootstrapping part 2 calculating p-values note this stack quest assumes that you are already familiar with the main ideas behind bootstrapping if not check out the quest this stack quest also assumes that you are familiar with hypothesis testing the null hypothesis and p-values if not check out the quests in bootstrapping part one we had a new drug to treat an illness and we gave that drug to eight different people that had the illness and we calculated the mean value 0.5 but because of random things like maybe these people were healthier to begin with we were unsure if the drug actually worked and that maybe we got 0.5 instead of zero because of random stuff we cannot control ultimately we used bootstrapping to solve this problem first we showed the four steps of how bootstrapping works one make a bootstrapped data set two calculate a statistic in this example we calculated the mean but we could have just as easily calculated the median or the standard deviation etc three keep track of that calculation and four repeat the first three steps until we have a nice distribution of bootstrapped statistics we then used the distribution of means to create a 95 confidence interval for the original mean value and because the 95 confidence interval covers zero we could not reject the idea that the drug is not doing anything bam confidence intervals are great for hypothesis testing but so are p-values so now let's talk about how to use bootstrapping to calculate p-values first the null hypothesis is that the drug has no effect on the illness and that means in a perfect world where we can control everything if the null hypothesis was true then we would expect the mean value of the data to be zero however when we calculate the mean we get 0.5 so let's shift all of the measurements to the left 0.5 units so that the mean of the shifted data is 0. in other words this shifted data set with mean equal to 0 represents a true null hypothesis now let's use bootstrapping to see how the mean value varies when the null hypothesis is true first let's make a new number line and from the eight measurements that were centered on zero create a bootstrapped data set remember we sample with replacement so it's okay to get duplicates now we calculate the mean value of the bootstrapped data set and add it to our histogram then we just repeat the process a few thousand times until we have a nice distribution of mean values now because we created the bootstrap data sets from a collection of measurements with mean equal to zero the histogram gives us a sense of what would happen if the null hypothesis was true and the drug had no effect when the null hypothesis is true 36 percent of the means are between negative 0.5 and 0.5 and that tells us that the probability of observing a mean value between negative 0.5 and 0.5 is 0.36 likewise because 16 of the bootstrap means were less than or equal to negative 0.5 the probability of observing a mean less than or equal to negative 0.5 is 0.16 lastly because 48 of the bootstrap means were greater than or equal to 0.5 the probability of observing a mean greater than or equal to 0.5 is 0.48 now let's go back to the original data and remember that the original mean was 0.5 now because this distribution represents possible values for the mean if the drug on average made zero difference on how people felt we can use it to calculate the p-value for observing a mean value of 0.5 or something more extreme where more extreme means further from the null hypothesis than the observed mean which in this case means further than 0.5 units from 0. so the p-value for the observed mean 0.5 is the probability of observing a bootstrapped mean greater than or equal to 0.5 which is 0.47 plus the probability of observing a bootstrap mean less than or equal to negative 0.5 which is 0.16 adding the two probabilities together gives us 0.63 and because 0.63 is greater than 0.05 we fail to reject the hypothesis that the drug makes no difference double bam note the method we use to calculate this p-value start with raw data calculate the mean and then shift the data so that the mean equals zero and then use bootstrapping to create a histogram of means around zero and then use that histogram to test the hypothesis that the drug made zero difference will work for just about any statistic we can think of for example we could have just as easily calculated the median of the original data and shifted the data so that the median equals zero and then use bootstrapping to generate this histogram of median values around zero and use the histogram to calculate the p-value for the observed median given the hypothesis that the drug has zero effect in this case the observed median is 1.8 so the p-value for the observed median 1.8 given this histogram that represents the hypothesis that the drug has zero effect is the probability of observing a bootstrap median greater than or equal to 1.8 which is 0.01 plus the probability of observing a bootstrap median less than or equal to negative 1.8 which is 0.19 and when we do the math we get 0.2 so again we would fail to reject the hypothesis that the drug has zero effect but this time we use medians instead of means and the point of this example is that we can use bootstrapping to test whatever we want thus if the data looks like it has outliers we can use medians which are more resilient to outliers than means triple bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the stack quest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
Xz0x-8-cgaQ,2021-07-06T04:00:15.000000,Bootstrapping Main Ideas!!!,[Music] quest hello i'm josh starmer and welcome to statquest today we're going to talk about bootstrapping part one main ideas now imagine we had a new drug to treat an illness and we gave that drug to 8 different people that had the illness for 5 of these people the drug appeared to help them feel better but for three people the drug appeared to make them feel worse if we calculate the mean of the response to the drug we get 0.5 0.5 is not a huge improvement but since most of the people five of eight improved maybe this drug is better than using no drug at all however maybe these five people all felt better because they were healthier to begin with and maybe these three people all felt worse because they had unhealthy lifestyles so it is possible that the reason we got a mean value equal to 0.5 instead of 0 is because of random things that we can't control is there anything we can do to decide if the drug works or not yes one expensive and time-consuming option would be to replicate the experiment a bunch of times if we repeat the experiment a bunch of times then we can keep track of each mean value and we will end up with a histogram of mean values just by looking at this distribution we can see that mean values close to zero which suggests that the drug does not do anything are relatively likely to occur and mean values far from zero indicating that the drug does something are relatively rare however as i said earlier repeating the experiment a bunch of times is both expensive and time-consuming is there something else we can do that is less expensive and time consuming yes instead of replicating the experiment a bunch of times we can use bootstrapping bam so let's use bootstrapping to get a better sense of which results are likely and which are rare first let's create a new number line now from the eight original measurements choose one at random and add that value to the new number line now go back to the original eight measurements and choose another value at random and add it to the new number line then we repeat that process randomly selecting one of the eight original values for the new number line a total of eight times note we can randomly select the same value more than once oh no it's the dreaded terminology alert randomly selecting data and allowing for duplicates is called sampling with replacement anyway so far we've only selected six measurements so we need two more bip boop note the reason we selected eight measurements for the new number line is because the original data set that we are sampling from contains eight measurements if we had started with 10 measurements then we would need to add 10 measurements to the new number line anyway this new data set that was created using sampling with replacements so that it had the same number of values as the original data set is called a bootstrapped dataset okay now that we have a new bootstrapped dataset we calculate the mean note because the bootstrap dataset is different from the original dataset we get a different main now let's add the mean of the bootstrap data set to what will soon be a histogram of means now we start over with a fresh number line and randomly select from the eight original values for the new number line repeating a total of eight times and allowing duplications then we calculate the mean and add that to our histogram note this process of creating a bootstrap data set then calculating something in this case we calculate the main then keeping track of those calculations is called bootstrapping in other words bootstrapping consists of four steps first make a bootstrapped data set second calculate something in this case we calculated the mean three keep track of that calculation and four repeat steps one through three a bunch of times note in step two we calculated the mean but we could have just as easily calculated the median or the standard deviation or any other statistic later on i'll say more about why this flexibility is awesome for now i'll just say bam okay now that we know what bootstrapping is we just do it a bunch of times usually we use a computer to bootstrap thousands of times and after creating thousands of bootstrap samples calculating their means and adding them to the histogram we end up with this because we sampled with replacement the histogram ended up with a wide variety of mean values because there are so many combinations bootstrapping usually only creates a subset like 10 000 to estimate the full distribution in this case the histogram tells us how the mean might change if we redid the experiment a bunch of times just by looking at the histogram we can get a sense of what might happen if we redid the experiment if we redid the experiment there's a high likelihood we will get something close to the original mean and getting something really far from the original mean should be relatively rare because the histogram tells us how the mean might change if we redid the experiment a bunch of times if we want to know the standard error of the mean value from the original data set we only need to calculate the standard deviation of this distribution and a 95 confidence interval is just an interval that covers 95 percent of the bootstrap means double bam in this case we see that the 95 confidence interval covers zero so we cannot reject the hypothesis that the drug is not doing anything note what we just did with the confidence interval was a type of hypothesis testing if you want to learn more about hypothesis testing check out the quest also note just so you know there are other fancier ways to use bootstrapping to calculate confidence intervals while these fancy methods can result in better confidence intervals we'll save them for another day since the purpose of this video is to explain the main ideas behind bootstrapping small bam now so far we have used bootstrapping to calculate the standard error and a confidence interval for the mean however both the standard error and the confidence interval can be calculated directly with a formula without having to create bootstrapped data sets so what is it that makes bootstrapping so awesome the awesome thing about bootstrapping is that we can apply it to any statistic to create a histogram of what might happen if we repeated the experiment a bunch of times and we can use that histogram to calculate stuff like standard errors and confidence intervals without having to worry about whether or not there is a nice formula for example if we started out by calculating the median of the original data then we can use bootstrapping to create a distribution and use that distribution to create the confidence interval so regardless of the statistic we calculate bootstrapping allows us to see it in the context of a distribution and we can use that distribution to help us interpret the initial results triple bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
aOOaGLa8s4c,2021-06-23T20:44:27.000000,Ken Jee's #66DaysOfData Challenge Clearly Explained!!!,[Music] 66 days of data that's what i'm gonna talk about yes that quest hello i'm josh starmer and welcome to statquest today we're going to talk about ken g's 66 days of data challenge and it's going to be clearly explained every now and then when i'm on twitter or linkedin i see something like this herman says it's day 44 and they ate a lot of nachos and they watched my stat quest on pca bam hashtag 66 days of data and i'm like what the heck is this hashtag 66 days of data thing and who is this kin g person and where can i get some nachos so i called ken g on the zoom and it turns out that 66 days of data with ken g is totally awesome here's the clip what is 66 days of data well so 66 days of data is an initiative that i started i guess it's a little more than a year ago and the the big idea is that for 66 days you want to spend at least five minutes each day learning some data science and then you want to share what you learned each day on your platform of choice so it could be just texting one of your friends it could be on the discord community that's been created that has over 6 000 members it could be on twitter it could be on linkedin it could be on facebook it doesn't matter the idea is that you're you're getting into the habit of learning and then sharing your work and that has a couple benefits so the the learning process and creating a habit around that is in my mind the most important one is you know data science is this field where we're never going to be done learning there's always more to learn there's always more information to accumulate so rather than focusing on hey i have to learn this xyz information now i actually think when you're starting out it's more important to focus on how you learn that information and creating a habit around that learning process and you know something for me that i've always struggled with has been a little bit of motivation and one great way to circumvent the need for motivation is creating habits another way to do that is through accountability so when you're putting your work out there when you're sharing it everyone who knows you're doing the challenge in the community in your twitter or whatever it is they're going to be able to hold you accountable and that little extra social pressure to keep you going is something that i think is really valuable to be able to get past that initial learning hump where it can be really scary cool cool i love it um so here's a question what's your favorite thing about the 66 days challenge honestly that is the easiest question you could have asked because by far it is the community oh i would have never guessed we would have gotten so many people involved so many people asking questions so many admins willing to help out so many different viewpoints and so many different individuals that are willing to help out and pitch in and make this community just better what do most people do so there's a couple different routes and i don't think any one route is correct i will say that i think project-based learning is more effective for me and it also makes what you share a little bit more interesting to everyone else i see that engagement with posts where people are building something is significantly higher than just talking about um you know like the either the concept they learned or the certificate they're working on but there's a a couple different routes so the the first that i see very commonly is people use this as a great excuse to start a certificate course or online course the other approach is the project-based approach where it does take a little bit more planning to say hey these are the projects i want to work on it's also a little bit difficult to time scope a data science project to begin with so you know people are like oh i'm going to do these three projects and they eventually end up just doing one i would also say that a third route some this is usually more advanced data scientists who already have a portfolio uh they're using it to go back and review the math or the theory i think that's something that i i encourage is that after you've applied these things after you've built stuff i always like to go back and review the theory review the math because now i have something to stick it on to so i think that you know it's kind of choose your own adventure here but based on the stage of your career or the stage of your learning it might make more sense to use this uh to to just focus on the areas where you would like to improve most again like it is your always your ocean you can choose to study whatever you'd like bam note starting july 1st 2021 i will be participating in 66 days of data with ken g on twitter and so should you i'll be using the 66 days of data to say what i've learned about light gbm cat boost natural language processing feature engineering and a bunch of other cool stuff obviously you can do whatever you want but if you need ideas i've made a special 66 days of data playlist just for you the link is in the description below double bam in summary the 66 days of data with ken g is any 66 consecutive days that you promised to spend at least five minutes doing something related to data that will help you grow personally or professionally you then share what you learned with hashtag 66 days of data on the social media platform of your choice like twitter linkedin or the 66 days of data discord server triple bam hooray we've made it to the end of another exciting stack quest if you like this stack quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
OSPr6G6Ka-U,2021-06-22T04:00:06.000000,Expected Values for Continuous Variables!!!,stat quest stat quest stat quest yeah stat quest hello i'm josh starmer and welcome to statquest today we're going to talk about expected values part 2 continuous variables note this stack quest assumes that you are already familiar with the concept of an expected value if not check out the quest in expected values part 1 we travel to the mystical and magical place called statland and we showed how to calculate expected values for two really bad bets made by our friend statsquatch for example statsquatch made this bet i bet you one dollar that the next person we meet has heard of the movie troll 2 and we use the probabilities of people in statland having heard of troll 2 or not having heard of troll 2 and the amount of money we would lose if we met someone who had heard of troll 2 in the amount of money we would gain if we met someone who had not heard of troll 2 to calculate the expected value of the overall amount of money we would gain or lose per bet if we made the bet a lot of times and the result was 0.66 meaning that we expect to gain on average 60 cents per bet if we make the bet a bunch of times oh no it's the dreaded terminology alert when we calculate the expected value for a bad bet like this we say that we are calculating the expected value for a discrete variable in this case the discrete variable is the bet and it has two outcomes lose one dollar or gain one dollar in general any time we have discrete outcomes we have a discrete variable bam now let's talk about expected values for continuous variables continuous variables come from measuring things and the outcomes are wait for it continuous for example imagine you and your friend statsquatch are walking around statland and statsquatch says i wonder how long we would have to wait per person to see people in other words statsquatch wants to know the expected value for waiting time which is something we can measure and after 10 seconds you meet someone yo what's up squatch so you and statsquatch decide to keep track of that by putting a dot on this number line at 10. the next person we meet shows up after 30 seconds and the next person shows up immediately and the next person shows up in 10 seconds etc etc etc then statsquatch says ugh collecting all this data is taking forever then statsquatch notices the gaps in the data and says gasp gaps gaps in the data mean we still have more to collect then statsquatch notices that the data are plotted using 10 second intervals and says but what if we want different interval sizes like 5 or 2.5 seconds so you tell stat squatch chill out squatch instead of spending the rest of our lives collecting data and worrying about the interval size we can model the waiting times with an exponential distribution bam this curve that skims the top of the data is an exponential distribution and this is the equation for the exponential distribution lambda which is also called the rate is a parameter that defines the shape of the curve in this example the rate refers to the number of people we meet per second because that is the unit on the x-axis and if we set lambda to 0.05 we get a curve that fits the data we have already collected however if we set lambda to 0.1 meaning we meet more people per second then we get this green curve that has a steeper slope close to zero and if we set lambda to 0.01 meaning we met fewer people per second then we get this orange curve that barely bends but is much higher on the right side compared to the other curves but like we said when lambda equals 0.05 then the curve fits the data now if we want to calculate the probability we meet someone in 10 seconds or less then we calculate the area under the curve between zero and ten in other words we integrate the exponential distribution from zero to ten for now i'll spare you this math and just tell you that when lambda equals 0.05 the integral is 0.39 which means that the probability we will meet someone in 10 seconds or less is 0.39 alternatively if we wanted to know the probability of meeting someone between 25.302 seconds and 30.122 seconds then we can calculate the area under the curve between 25.302 and 30.122 in this case the area under the curve is 0.06 which means that the probability we will meet someone in this range of time is 0.06 in summary the exponential distribution fits the data that we have collected so far but it doesn't have any gaps or missing values and we can use it to make calculations on any interval we want note i call the y-axis likelihood because the y-axis coordinates generated by this equation are the likelihood values that we use for maximum likelihood estimation and if you want to learn about how these y-axis values are used and maximum likelihood with the exponential distribution check out this quest also note the y axis is scaled so that the total area under the curve is equal to one lastly in theory this curve should go all the way to positive infinity on the x-axis but we don't have enough room to draw a graph that goes all the way to infinity so we'll stop drawing at 90 seconds because at that time the curve is pretty close to zero on the y-axis okay enough about the exponential distribution itself now let's talk about how to calculate the expected value and i know we just talked about how awesome it is to have a continuous distribution but for a few minutes let's pretend that this is actually a discrete distribution and let each 10 second interval represent an outcome note in this case we've drawn each rectangle so that the curve goes through the midpoint of each top side so for example because the interval is 10 seconds long the curve intersects the first rectangle at 5 seconds and the curve intersects the second rectangle at 15 seconds etc etc etc now instead of having to integrate the function to get the area under the curve we can approximate the area under the curve for each outcome with the corresponding area of each rectangle for example the probability of meeting someone in the first 10 seconds is approximately the width of the first rectangle which is 10 times the height to calculate the height we need to find the y-axis coordinate for where the top edge of the rectangle intersects the curve and that means we need to find the y-axis coordinate for this exponential distribution when time equals five so we plug x equals five into the equation and do the math and we get 0.04 so the height of the rectangle is 0.04 and the area of the rectangle is the height times the width which is 0.4 and that means the probability of meeting someone in the first 10 seconds is approximately 0.4 and compared to the exact probability calculated with the integral 0.39 the approximation is not terrible so let's put 0.4 inside the first rectangle so we don't forget likewise we use the exponential distribution to calculate the height for each rectangle and the probabilities for each outcome bam now if we want to approximate the expected value of the exponential distribution we can plug the outcomes and their approximated probabilities into the equation for discrete outcomes for example the first outcome is meeting people in 10 seconds or less and the probability is 0.4 so the first term is 10 times 0.4 the second outcome is the 10 second interval that ends at 20 seconds and the associated probability is 0.2 so the second term is 20 times 0.2 likewise we add the remaining terms and when we do the math we get 22 and that suggests that on average we expect to wait 22 seconds between each time we meet someone bam now if we want to improve our approximation we can cut the intervals in half so that each one lasts 5 seconds instead of 10. and when we do the math plugging in each outcome and its corresponding probability we get 21.8 now to improve the estimate of the expected value even more we can keep decreasing the width of each rectangle until the width of the rectangles goes to zero and the number of rectangles goes to infinity note when we have an infinite number of rectangles with zero width then we are no longer approximating the area under the curve but calculating it exactly now remember that the probability of observing a specific outcome is the height times the width of the associated rectangle and that the height the y-axis coordinate of the top of each rectangle is the likelihood at that point and the width can be written as delta x now if you remember from high school calculus if the sum of the number of rectangles goes to infinity while the width of each rectangle delta x goes to zero then we end up with an integral now i know this is a huge mess of math so let's summarize everything when we have a discrete distribution like this the expected value of the corresponding discrete variable is the sum of the outcomes times their associated probabilities and when we have a continuous distribution like this then the expected value of the corresponding continuous variable uses an integral instead of a sum and the rest of the equations are very similar except we replace the probability with the likelihood the y-axis coordinate note although we have been using the exponential distribution as an example this formula works for any continuous variable double bam now that we have a formula for the expected value for continuous variable let's calculate the expected value for a continuous variable from the exponential distribution since we use the exponential distribution equation to calculate the likelihoods let's plug it into the equation for the expected value and because the exponential distribution is defined for all values greater than or equal to zero we will integrate everything from zero to infinity now we just do the math first because we can split this into two functions we can use integration by parts to find the solution note if you are not familiar with integration by parts there are helpful links in the description below anyways we'll start by setting f of x equal to x now since integration by parts requires the derivative of f of x we will put that here now we'll set the derivative of g of x g prime of x to be equal to the second term in the integral and because we need g of x the antiderivative of g prime of x we need to figure it out now you might have an awesome strategy for finding anti-derivatives but i do not so i start by observing that the derivative of e to the x is e to the x which is close to what we want but among other things is missing the negative lambda in the exponent so let's put negative lambda in the exponent to match what we want and the derivative via the chain rule is this negative lambda times e raised to the negative lambda x and that is almost the derivative we are shooting for except g prime of x does not have this negative sign so let's try putting a negative sign in front of the equation now when we take the derivative with the chain rule we get the same thing as g prime of x so this equation must be the anti-derivative g of x small bam now we just plug these functions and their derivatives into the integration by parts formula first let's plug in f of x now let's plug in the derivative of f of x and since multiplying by one doesn't change anything we can omit it now let's plug in the derivative of g of x and lastly let's plug in g of x and move the minus sign outside of the parentheses now let's do some more math first let's tackle this term by evaluating it when x equals infinity and when x equals zero so we set x equal infinity then we subtract the term and set x equal to zero now since the exponent in the first term is negative we can turn it into a fraction however it's not obvious what this fraction is equal to 0 infinity a quick google search on l'hopital's rule tells us that the limit as x goes to infinity of a of x divided by b of x is equal to the limit as x goes to infinity of a prime of x divided by b prime of x so in our case we have the limit as x goes to infinity of x divided by e raised to the lambda x then we take the derivatives of the numerator and the denominator and plug in infinity for x and that equals zero because we are basically dividing one by infinity so we can replace the first term with zero and we can replace the second term with zero two because zero times anything is zero lastly zero minus zero is zero and we are done computing the first term of the integration by parts now let's compute the second term first we recognize that we are subtracting the second term from the first then we solve for the antiderivative of the stuff inside the integral using the trial and error approach we saw earlier and evaluate it at infinity and zero just like before since the exponent in the first term is negative we can turn it into a fraction and one divided by infinity is zero now since the exponent in the second term is multiplied by zero the whole exponent is zero and anything raised to the zero power is one and that leaves us with one divided by lambda lastly we commute this minus sign do the math and we see that the expected value is 1 divided by lambda so let's move this up here now given this specific exponential distribution where lambda equals 0.05 we calculate the expected value by plugging in 0.05 in for lambda and we see we expect to wait on average 20 seconds between meeting people so going back to the original question that statsquatch asked how long will we have to wait per person to see people we answer 20 seconds and then statsquatch says triple bam in summary the expected values for discrete and continuous variables are very similar the only two differences are one we replace the sum with the integral and two we replace the probability with the likelihood bam note if you would like to know how we estimate the value for lambda with a pile of data check out the quest on the exponential distribution and maximum likelihood the link is in the description below also note if you watch this video hoping to learn exactly why we divide the sample variance by n minus 1 know that you have taken a big step towards understanding this mystery now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the stackquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
ilUbD7EoQnk,2021-05-27T04:00:10.000000,Three (3) things to do when starting out in Data Science,hip hip hooray there's just three things to do stat quest hello i'm josh starmer and welcome to stat quest today we're going to talk about three things to do when starting out in data science thing number one be curious about the data in data science or bioinformatics or statistics or machine learning or whatever we work with a lot of data for example we might have clinical trial data or we might get data from a research lab or we might get data about how people interact with a website or from sensors at a weather station or from community surveys or political polls or any other source the possibilities are endless given that regardless of the source data usually consists of numbers and words in a spreadsheet it is tempting to think that is all there is to data just a bunch of stuff in a spreadsheet but data doesn't just magically appear in a spreadsheet it has to be collected usually by people from a source which is usually other people and anything that involves people tends to be more interesting than the numbers in a spreadsheet might suggest for example maybe we are studying lung cancer and this person has lung cancer and we isolate a bunch of cells from the tumor and then we get measurements from each cell what can we learn from this data well that depends first we need to know about this person do they smoke are they old or young do they have a race or ethnic identity the answers to these questions give us context for the data and limit what we can extrapolate from it if this person smokes then we may need to restrict our conclusions to smokers if this person is young we may need to restrict our conclusions to young people and we may need to restrict our conclusions to a specific race or ethnic identity and there are a ton of other factors that narrow the scope of our conclusions people often talk about bias and analytics and this is where a lot of that bias can occur for example if we only collected lung cancer samples from white males then our results may not apply to the majority of the population so it's super important to know about the source of the data now what do we know about the person who extracted the cells were they an expert who had extracted lung cancer cells a million times or was this their first time the answers to these questions suggest how pure or contaminated the sample is if we believe the sample is contaminated we may have less confidence in the results so it's important to know how the samples were handled lastly who prepared this table were they an expert or was this their first time if it was their first time did they do it right if not the data we input to our analysis algorithms may not be what we think it is so it's super important to know how the data were processed so we see that at every step of the way there is a person that influences the data and ultimately how the data should be interpreted so my advice is to go to the clinic and meet the patients you may notice something that no one else noticed and that thing could be critical to the analysis then meet the people collecting the cells if you're lucky they may let you observe the process again this gives you a chance to notice things that other people who are not focused on the analysis might miss lastly meet the people that create the data set to learn about what they do this gives you a chance to validate the assumptions you have about the data in the end we see that data isn't just data it's not just numbers and words in a spreadsheet instead it is a process and each step in the process influences how the numbers and words in the spreadsheet should be interpreted so maybe instead of calling it data science we should call it people science because it's as much about understanding people and what they do as it is about the data in summary be curious about the data bam thing number two be curious about the methods data science or bioinformatics or whatever is chock full of methods statistics machine learning database this is a blessing because we have a lot of tools to use but it is also a curse because it takes time to keep up with them all however it's important to keep up with the methods in order to understand their limitations here's an example when i made the stat quest on neural networks i started with a really simple data set that showed whether or not different drug dosages were effective against a virus the high and low dosages were not effective but the medium dosage was effective then i fit a neural network to this data and got this green squiggle and the green squiggle fits the data pretty well when the dosage is low the green squiggle is close to zero and the neural network predicts that the drug is not effective and that is consistent with the data we use to train the neural network when the dosage is 0.5 the green squiggle is close to 1 and the neural network predicts that the drug will be effective and this prediction is also consistent with the data we use to train the neural network lastly when the dosage is high the green squiggle is close to zero and the neural network predicts that the drug will not be effective and that too is consistent with the data we use to train the neural network so we see that the green squiggle fits the three points in the training data set pretty well but let's be honest this bump looks a little weird if you asked me to fit a squiggle to the data i might fit a nice arc like this this arc makes sense to me because the output values are all between 0 and 1 just like the original data and as the dosage increases from 0 to 0.5 our confidence that the dosage will work also increases in a nice consistent way likewise as the dosage increases from 0.5 to 1 our confidence that the dosage will work decreases in a consistent way but the neural network did not fit an arc to the data instead the neural network fit a green squiggle that has this weird bump that goes over one and the training data never went over one so where does this weird bump come from well if we knew how neural networks worked we would know that the green squiggle is only penalized where there is data in other words the neural network will be penalized if the green squiggle is far from these three points and as a result the green squiggle is close to all three points however the green squiggle is not penalized where there is no data and as a result the green squiggle can do whatever it wants between the points so because of how neural networks are randomly initialized i ended up with this bump but i could have just as easily ended up with a neural network that had a bigger bump or two bumps or whatever the point i'm trying to make is that just because it makes more sense to us to fit a nice arc to the data like this it doesn't mean the neural network won't fit something crazy like this instead and that means it can be hard to trust the output from a neural network that is making predictions between data points i think about these in-between data predictions whenever someone talks about using a neural network to drive a car people are very good at extrapolating between experiences in ways that other people can relate to in contrast neural networks do whatever they want between experiences and other drivers may not relate to that and that's just one example of why knowing how the method works can be important so be curious about how the methods work double bam thing number three just be curious what we do in data science or bioinformatics or whatever is usually a small but super important piece of a much larger puzzle we aren't just plugging in data and hitting return but we are also helping to cure diseases or helping lift people out of poverty or bringing smiles to people's faces or helping people in new and exciting ways and i believe it is important to see the big picture our work does not exist in a cave but is always in the service of people and we should always be aware of how our work is used is our work being used to make the world a better place or not the only way to find out is to be curious about the other pieces and to figure out what they do and see the big picture and i can't show you that big picture you have to find it yourself so be curious triple bam hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
_L39rN6gz7Y,2021-04-26T04:00:08.000000,"Decision and Classification Trees, Clearly Explained!!!",i like decision trees how about you stat quest hello i'm josh darmer and welcome to statquest today we're going to talk about decision and classification trees and they're going to be clearly explained here is a simple decision tree if a person wants to learn about decision trees then they should watch this stat quest in contrast if a person does not want to learn about decision trees then check out the latest justin bieber video instead in general a decision tree makes a statement and then makes a decision based on whether or not that statement is true or false it's no big deal when a decision tree classifies things into categories it's called a classification tree and when a decision tree predicts numeric values it's called a regression tree in this case we're using diet to predict a numeric value for mouse size note for the remainder of this video we are going to focus on classification trees however if you want to learn more about regression trees fear not there's a whole stat quest dedicated to regression trees the link is in the description below now here's a more complicated classification tree it combines numeric data with yes no data so it's okay to mix data types in the same tree also notice that the tree asks about exercising multiple times and that the amount of time exercising isn't always the same so numeric thresholds can be different for the same data lastly the final classifications can be repeated for the most part classification trees are pretty easy to work with you start at the top and work your way down and down until you get to a point where you can't go any further and that's how you'll classify something note so far i've been labeling the arrows with true or false but usually it is just assumed that if a statement is true you go to the left and if a statement is false you go to the right so sometimes you see true and false labels sometimes you don't it's no big deal oh no it's the dreaded terminology alert the very top of the tree is called the root node or just the root these are called internal nodes or branches branches have arrows pointing to them and they have arrows pointing away from them lastly these are called leaf nodes or just leaves leaves have arrows pointing to them but there are no arrows pointing away from them bam now that we know how to use and interpret classification trees let's learn how to build one from raw data this data tells us whether or not someone loves popcorn whether or not they love soda their age and whether or not they love the 1991 blockbuster cool as ice starring vanilla ice so we will use this data to build this classification tree that predicts whether or not someone loves cool as ice now pretend you've never seen this tree before and let's see how to build a tree starting with just data the first thing we do is decide whether loves popcorn love soda or age should be the question we ask at the very top of the tree to make that decision we'll start by looking at how well loves popcorn predicts whether or not someone loves cool as ice to do this we'll make a super simple tree that only asks if someone loves popcorn and then we'll run the data down the tree for example the first person in the dataset loves popcorn so they go to the leaf on the left and because they do not love cool as ice we'll keep track of that by putting a 1 under the word no the second person in the data set also loves popcorn so they also go to the leaf on the left and because they also do not love cool as ice we increment no to two the third person does not love popcorn so they go to the leaf on the right and because they love cool as ice we put a 1 under the word yes likewise we run the remaining rows down the tree keeping track of whether or not each one loves cool as ice bam now let's do the exact same thing for love soda at the two little trees we see that neither one does a perfect job predicting who will and who will not love cool as ice specifically these three leaves contain mixtures of people that do and do not love cool as ice dread it's another terminology alert because these three leaves all contain a mixture of people who do and do not love cool as ice they are called impure in contrast this leaf only contains people who do not love cool as ice because both leaves in the love's popcorn tree are impure and only one leaf in the love soda tree is impure it seems like love soda does a better job predicting who will and who will not love cool as ice but it would be nice if we could quantify the differences between love's popcorn and love soda the good news is that there are several ways to quantify the impurity of the leaves one of the most popular methods is called genie impurity but there are also fancy sounding methods like entropy and information gain however numerically the methods are all quite similar so we will focus on genie impurity since not only is it very popular i think it is the most straightforward so let's start by calculating the genie impurity for love's popcorn to calculate the genie impurity for love's popcorn we start by calculating the genie impurity for the individual leaves the genie impurity for the leaf on the left is 1 minus the probability of yes squared minus the probability of no squared so we start out with one then we subtract the squared probability of someone in this leaf loving cool as ice which is one the number of people in the leaf who loved cool as ice divided by the total number of people in the leaf four and then the whole term is squared lastly we subtract the squared probability of someone in this leaf not loving cool as ice which is three the number of people in the leaf who did not love cool as ice divided by the total number of people in the leaf squared and when we do the math we get 0.375 so let's put 0.375 under the leaf on the left so we don't forget it now let's calculate the genie impurity for the leaf on the right just like before we start out with one then we subtract the squared probability of someone in this leaf loving cool as ice and the squared probability of someone in this leaf not a loving cool is ice and when we do the math we get 0.444 now because the leaf on the left has four people in it and the leaf on the right only has three people in it the leaves do not represent the same number of people thus the total genie impurity is the weighted average of the leaf impurities we start by calculating the weight for the leaf on the left the weight for the left leaf is the total number of people in the leaf four divided by the total number of people in both leaves seven then we multiply that weight by its associated genie impurity 0.375 now we add the weighted impurity for the leaf on the right which is the total number of people in the leaf 3 divided by the total number of people in both leaves 7 times the associated genie impurity 0.444 and when we do the math we get 0.405 so the genie impurity for love's popcorn is 0.405 likewise the genium purity for love soda is 0.214 now we need to calculate the genie impurity for age however because age contains numeric data and not just yes no values calculating the genie impurity is a little more involved the first thing we do is sort the rows by age from lowest value to highest value then we calculate the average age for all adjacent people lastly we calculate the geniu impurity values for each average age for example to calculate the gd impurity for the first value we put age less than 9.5 in the root and because the only person with age less than 9.5 does not love cool is ice we put a 0 under yes and a 1 under no then everyone with age greater than or equal to 9.5 goes to the leaf on the right now we calculate the genie impurity for the leaf on the left and get zero and this makes sense because every single person in this leaf does not love cool as ice so there is no impurity then we calculate the genie impurity for the leaf on the right and get 0.5 now we calculate the weighted average of the two impurities to get the total genie impurity and we get 0.429 likewise we calculate the genie impurities for all of the other candidate values these two candidate thresholds 15 and 44 are tied for the lowest impurity 0.343 so we can pick either one in this case we'll pick 15. however remember that we are comparing genie impurity values for age loves popcorn and love soda to decide which features should be at the very top of the tree earlier we calculated the genie impurity values for love's popcorn and love soda and now we have the genie impurity for age and because love soda has the lowest genie impurity overall we know that its leaves had the lowest impurity so we put love soda at the top of the tree bam now the four people that love soda go to a node on the left and the people that do not love soda go to a node on the right now let's focus on the node on the left all four people that love soda are in this node three of these people love cool as ice and one does not so this node is impure so let's see if we can reduce the impurity by splitting the people that love soda based on love's popcorn or age we'll start by asking the four people that love soda if they also love popcorn because two of the four people that love soda also love popcorn they end up in the leaf on the left the remaining two people that love soda but do not love popcorn end up on the right and the total genie impurity for this split is 0.25 so let's put 0.25 here so we don't forget now we test different age thresholds just like before only this time we only consider the ages of people who love soda and age less than 12.5 gives us the lowest impurity zero because both leaves have no impurity at all so let's put zero here now because zero is less than 0.25 we will use age less than 12.5 to split this node into leaves note these are leaves because there is no reason to continue splitting these people into smaller groups likewise this node consisting of the three people who do not love soda is also a leaf because there is no reason to continue splitting these people into smaller groups now there is just one last thing we need to do before we are done building this tree we need to assign output values for each leaf generally speaking the output of a leaf is whatever category that has the most values in other words because the majority of the people in these leaves do not love cool as ice the output values are does not love cool as ice and because the majority of the people in this leaf love cool as ice the output value is love's cool as ice hooray we finished building a tree from this data double bam now if someone new comes along and we want to predict if they will love cool as ice then we run the data down our tree and because they love soda they go to the left and because they are 15 so age less than 12.5 is false they end up in this leaf and we predict that they will love cool as ice triple bam okay now that we understand the main ideas of how to build and use classification trees let's discuss one technical detail remember when we built this tree only one person in the original data set made it to this leaf because so few people made it to this leaf it's hard to have confidence that it will do a great job making predictions with future data and it is possible that we have overfit the data note if the term overfit is new to you don't don't instead check out the stack quest on bias and variance in machine learning regardless in practice there are two main ways to deal with this problem one method is called pruning and there's a whole stack quest dedicated to it so check it out alternatively we can put limits on how trees grow for example by requiring three or more people per leaf now we end up with an impure leaf but also a better sense of the accuracy of our prediction because we know that only 75 percent of the people in the leaf love to cool as ice note even when a leaf is impure we still need an output value to make a classification and since most of the people in this leaf love cool as ice that will be the output value also note when we build a tree we don't know in advance if it is better to require three people per leaf or some other number so we test different values with something called cross validation and pick the one that works best and if you don't know what cross validation is check out the quest bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
Llqoz1u8Pfk,2021-04-01T04:00:10.000000,"Silly Songs, Clearly Explained!!!",silly songs are different from serious songs because this silly stat quest hello i'm josh starmer and welcome to statquest today we're going to talk about silly songs and they're going to be clearly explained note this stat quest assumes that you are already familiar with bam if not check out the quest the link is in the description below you know the deal it's late at night and you can't sleep because you're wondering what is a silly song if that's the case then you came to the right stat quest let's see if a classification tree can help us understand silly songs the first thing we do is ask is there a ukulele if there is a ukulele it might be a silly song if there isn't a ukulele is the singing out of tune if the singing is out of tune then it might be a silly song otherwise check back later unfortunately this classification tree is totally useless when it comes to identifying silly songs oh no it's the dreaded terminology alert when something is totally useless the technical term is totes useless since the classification tree is totes useless let's use a super fancy neural network this super fancy neural network should help us classify songs as silly songs or super serious songs here's our super awesome training data set it has a bunch of stat quest theme songs which we know are silly and it has these songs from a heavy metal band and heavy metal music is not silly if it was silly people would call it silly metal music and no one does that so let's see what happens when we input a silly song it's a super duper silly song it's a super duper silly song hooray now the neural network does the math and the super fancy neural network says it's a silly song bam now let's see what happens when we input a serious song serious song yeah it's a serious song dang now the neural network does the math and the super fancy neural network says it's a silly song it looks like the super fancy neural network classifies everything as a silly song and that means our super fancy neural network is totes useless since both the classification tree and the super fancy neural network are totes useless how can we identify a silly song it's easy if a song starts a stat quest it's a silly song triple bam hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
KLs_7b7SKi4,2021-03-29T04:00:03.000000,"Expected Values, Main Ideas!!!",today we're gonna talk about what to expect when you're expecting a value stat quest hello i'm josh starmer and welcome to statquest today we're going to talk about expected values and they're going to be clearly explained let's start by taking a trip to the magical place called statland once we get to statland our friend statsquatch says hey i bet the next person we meet has heard of the movie troll 2 and we say are you kidding me troll 2 is one of the worst movies ever made why would anyone know about it and then stat squash says i bet you one dollar that the next person we meet has heard of the movie troll 2. so we think to ourselves good thing we recently asked everyone in statland if they'd heard of the movie troll 2. here's the data there are two types of people in statland people that have heard of the 1990 movie troll 2 and people who have never heard of troll 2 in other words there isn't anyone who has both sort of heard of troll 2 and sort of hasn't heard of troll 2. each person in statland is in one group or the other 37 of the people in statland have heard of troll 2 and the remaining 176 people in statland have never heard of troll 2 that means there are 213 people living in statland so statland is not a huge place looking at the raw numbers is useful because they tell us that statland is pretty small and that whatever analysis we do only applies to a handful of people that said by looking at the numbers we can get a general sense of the trends in statland in this case we see that most of the people in statland have never heard of troll 2 but at a glance it's not super obvious how the number of people who have never heard of troll 2 relates to the total population of statland the good news is that we can make this relationship super obvious by calculating probabilities so let's calculate the probabilities of people in statland that have and haven't heard of troll 2 if we want to know the probability that a randomly selected person in statland has heard of troll 2 we simply divide the number of people that have heard of troll 2 37 by the total 213 and that gives us 0.17 that means the probability that a randomly selected person in statland has heard of troll 2 is 0.17 which is relatively low likewise the probability that a randomly selected person in statland has never heard of troll 2 is 0.83 which is relatively high now remember that bet our friend statsquatch wanted to make i bet you one dollar that the next person we meet has heard of the movie troll 2. the good news is that we can use these probabilities to decide if we should agree to the bet so let's move the probabilities to the top so we can focus on them now if the next person we meet has heard of troll 2 then we will lose the bet and that means we will lose one dollar so let's put negative one here to represent the outcome of losing one dollar if we meet someone who has heard of troll two in contrast if the next person has not heard of troll two then we will win the bet and that means we will win one dollar so let's put one here to represent the outcome of winning one dollar if the next person we meet has not heard of troll 2. so the left side of this table tells us that the probability we will lose one dollar is 0.17 and the right side tells us that the probability that we will win one dollar is 0.83 in other words the probability that we will win one dollar is much higher than the probability that we will lose one dollar and that makes it seem like it would be a good idea to accept the bet however even though there is a high probability that we will win the bet there is still a low probability that we will lose and no one likes to lose money so we say can we make this bet 100 times or is this just a one-time offer and our friend statsquatch says we can make this bet 100 times now if we make this bet 100 times we will probably win some and we will probably lose some and we can use this table to predict how much we will win and lose if we make this bet 100 times we can approximate how many times we will lose by multiplying the probability we will lose 0.17 by 100 and if we do the math we get 17. so that means we expect to lose about 17 times in 100 bets since we will lose one dollar each time we lose the bet we can estimate the total amount of money we will lose by multiplying the number of times we expect to lose by negative one so this whole term represents how much money we expect to lose in 100 bets and when we do the math we get negative 17 and that means we expect to lose about 17 dollars now since we can make the bet 100 times we can approximate how many times we will win by multiplying the probability we will win 0.83 by 100 when we do the math we see that we expect to win about 83 times since each time we win the bet we will win one dollar we can estimate how much money we will win by multiplying the number of times we expect to win by one so this whole term represents how much money we expect to win in 100 bets and when we do the math we see that we expect to win 83 dollars now that we have a term for the expected amount of money lost and a term for the expected amount of money won we can add the two terms together to find out the total of how much we expect to win or lose and when we do the math we see that we expect to gain approximately dollars after 100 bets however we can also calculate the average amount of money we will gain per bet by dividing everything by the number of bets 100 doing the math gives us 66 divided by 100 which is 0.66 so on average we expect to gain 66 cents every time we bet note even though i win or lose one dollar each time i bet on average i expect to gain 66 cents each time in statistics lingo 66 cents is the expected value for the bet bam using fancy statistics notation we could write e of bet equals 0.66 or if we wanted to make it look even more cryptic and more in line with what you might find in a textbook we could write e of x equals 0.66 where x represents the bet now let's talk about why i left this messy math in the middle of everything since we are multiplying each probability by the number of bets 100 and dividing by the number of bets 100 then all of the values that represent the number of bets 100 cancel out and we're left with the probability that someone in statland has heard of troll 2 times the outcome negative 1 plus the probability that someone has not heard of troll 2 times the outcome 1. and when we do the math we get the exact same thing we got before 0.66 thus the expected value represents what we would expect per bet if we made this bet a bunch of times note we can rewrite the expected value using fancy sigma notation using fancy sigma notation the expected value e of x is the sum of each outcome x times the probability of observing each outcome x so for the first term herd of troll 2 the outcome is negative 1 and the probability of observing the outcome is 0.17 so we multiply those values together then the sigma tells us to add that term to the term for not heard of troll 2 now the outcome is 1 and the probability of observing the outcome is 0.83 so either way we do the math we get 0.66 that means that if we can make this bet a bunch of times even though we will lose some of the time we should make money in the long run double bam now imagine statsquatch saying because it is relatively rare for someone in statland to have heard of troll 2 i will pay you 10 if the next person we meet has heard of troll 2 but if they have not you pay me one dollar will we win money or lose money if we can make this bet a bunch of times let's calculate the expected value to find out the outcome for when someone has heard of troll 2 is 10 because we will gain 10 dollars and the outcome for when someone has not heard of troll 2 is negative 1 because we will lose one dollar so these are the two outcomes now let's calculate the expected value the expected value is the sum of each outcome times its associated probability so let's start by plugging in the numbers for herd of troll two first the outcome x is ten and the probability of observing that outcome is 0.17 now we add the term for never heard of troll 2. the outcome x is negative 1 and the probability of observing that outcome is 0.83 now we just do the math and the expected value is 0.87 and that means we expect to gain on average 87 cents every time we make this bet and that means stat squatch is the worst gambler ever triple bam note in this stat quest we have only talked about how to calculate expected values for discrete events like whether or not someone has heard of troll 2. however we'll talk about how to calculate expected values for continuous events like how much time passes between text messages on your phone in another stat quest lastly if you watch this video hoping to learn exactly why we divide the sample variance by n minus 1 just know that this is the first of several steps to that answer and will get there someday soon now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
HGwBXDKFk9I,2021-03-08T05:00:12.000000,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),convolutional neural networks are used for image classification and other stuff statquest hello i'm josh starmer and welcome to statquest today we're going to talk about neural networks part 8 image classification with convolutional neural networks note this stack quest assumes that you are already familiar with the main ideas behind neural networks the main ideas behind back propagation the main ideas behind the relu activation function and you should know about neural networks with multiple inputs and outputs if not check out the quests the links are in the description below now imagine we have a friend named statsquatch and statsquatch wants to play tic-tac-toe against their computer unfortunately stat squad can't remember if you start with the letter x or the letter o so each time statsquatch plays tic-tac-toe against their computer the computer has to figure out if statsquatch drew the letter x or the letter o the good news is that the computer can figure out if stat squads drew the letter x or o using a convolutional neural network bam because the letter o and the letter x are drawn on a computer screen we can zoom in on the letter o and the letter x and see that each image is just a bunch of pixels and in this case each pixel is represented by either a zero for a white pixel or a one for a black pixel so let's walk through step by step how a computer can classify this image as the letter o and this image as the letter x we will start with the image of the letter o now because this image is so small just 6 pixels by 6 pixels it is possible to make a normal everyday neural network that can correctly classify it we simply convert this 6 by 6 grid of pixels into a single column of 36 input nodes and connect the input nodes to a hidden layer so here we have 36 connections from the 36 input nodes to this node in the hidden layer now remember each connection has a weight that we have to estimate with back propagation so that means we need to estimate 36 weights in order to connect to this node however usually the first hidden layer has more than one node and each additional node adds an additional 36 weights that we need to estimate like i said because the original image is small 6x6 and black and white something like this could work however if we had a larger image like 100 pixels by 100 pixels which is still pretty small compared to real world pictures then we would end up having to estimate 10 000 weights per node in the hidden layer so this method doesn't scale very well another problem is that it's not clear that this neural network will still perform well if the image is shifted by one pixel for example if this is the image we used for training then it is not clear that the neural network will still recognize this letter o correctly if each pixel is shifted to the right by one lastly even complicated images like this teddy bear tend to have correlated pixels for example any brownish pixel in this image tends to be close to other brown pixels and any white pixel tends to be near other white pixels and it might be helpful if we can take advantage of the correlation that exists among each pixel thus classification of large and complicated images is usually done using something called a convolutional neural network convolutional neural networks do three things to make image classification practical one they reduce the number of input nodes two they tolerate small shifts in where the pixels are in the image and three take advantage of the correlations that we observe in complex images so let's go back to our tic-tac-toe game and see how a convolutional neural network can recognize this letter o the first thing a convolutional neural network does is apply a filter to the input image in convolutional neural networks a filter is just a smaller square that is commonly 3 pixels by 3 pixels and the intensity of each pixel in the filter is determined by back propagation in other words before training a convolutional neural network we start with random pixel values and after training with back propagation we end up with something more useful to apply the filter to the input image we overlay the filter onto the image and then we multiply together each overlapping pixel and then we add each product together to get a final value which in this case is three oh no it's the dreaded terminology alert in fancy math lango we call this sum of products a dot product by computing the dot product between the input and the filter we can say that the filter is convolved with the input and that's what gives convolutional neural networks their name small bam now we add a bias term to the output of the filter and put the final value into something called a feature map now in this example we slide the filter over 1 pixel however other convolutional neural networks might move over two or more pixels but in this example we just move over one pixel and calculate the dot product of the filter and overlapping pixels in the image add the bias term and put the final value into the feature map then we shift the filter over again and repeat until we have filled up the feature map bam we filled up the whole feature map to summarize we started with an input image of the letter o and then applied a filter to it in other words we can evolved the filter with the input and added a bias term to the values and that gave us a feature map because each cell in the feature map corresponds to a group of neighboring pixels the feature map helps take advantage of any correlations there might be in the image bam now typically we run the feature map through a relu activation function and that means that all of the negative values are set to zero and the positive values are the same as before in this case that means everything gets set to zero except for these two points so let's move this stuff over to make some room for the next step now we apply another filter to the new feature map however unlike before we simply select the maximum value and this filter usually moves in such a way that it does not overlap itself oh no it's another dreaded terminology alert when we select the maximum value in each region we are applying max pooling to see the effect that max pooling has in this example let's go back to the input image here we see that the upper left hand corner in the input image has an exact match with the filter and this results in the highest possible value in the feature map and then the max pooling step reduce this region to the one spot where the filter did the best job matching the input image in other words this region in the input image corresponds to this part of the feature map and the max pooling step selected the spot where the filter did the best job matching the input image likewise this region in the input image corresponds to this part of the feature map and the max pooling steps selected the spot where the filter did the best job matching the input image so we see that max pooling selects the spots where the filter did the best job matching the input image oh no it's another terminology alert alternatively we could calculate the average value for each region and that would be called average or mean pooling now going back to the max pooled layer let's move and shrink things to give us more room now let's convert the pooled layer into a column of input nodes lastly let's plug the input nodes into a normal everyday neural network this neural network has four input nodes a single hidden layer with a single node using the relu activation function and two output nodes one for the letter o and one for the letter x so given this image we run it through the filter to create the feature map then we run the feature map through a relu activation function then we select the maximum value in each area and end up with these values in the input nodes now we multiply the values in the input nodes by their associated weights and add each term together and then add the bias and we end up with 0.34 and thus the x-axis coordinate for the activation function is 0.34 now we plug 0.34 into the relu activation function and the output is 0.34 because 0.34 is greater than 0. now this connection from the hidden layer to the output for the letter o gives us one for the letter o and the connection from the hidden layer to the output for the letter x gives us 0 for the letter x so when the input is a picture of the letter o this convolutional neural network classifies it as a picture of the letter o bam now let's see what happens when the input is a picture of the letter x note even though we have changed the input the filter is the same as before and we're doing max pooling just like before in the neural network with its weights and biases is the same as before so just like before we run the filter over the input to create the feature map now we run the feature map through the rel u and do max pooling and the results of max pooling become the values for the input nodes now we run the values in the input nodes through the neural network and the result is 0 for the letter o and 1 for the letter x so when the input is a picture of the letter x this convolutional neural network classifies it as a picture of the letter x double bam now remember that we said convolutional neural networks help reduce the number of inputs in the neural network in this case we started with a 6x6 image or 36 potential inputs and compress those down to just four inputs into the neural network we also said that convolutional neural networks take correlations into account and this is accomplished by the filter which looks at a region of pixels instead of just one at a time lastly we said that convolutional neural networks can tolerate small shifts in where the pixels are in the image so let's see what happens when we shift the picture of the letter x one pixel to the right in other words will this convolutional neural network still decide if the input is a picture of the letter x and we see that the output value for the letter x 1.23 is much closer to 1 than the output value for the letter o negative 0.2 so this convolutional neural network decided that the input image is of the letter x triple bam note if we wanted to we can make the output easier to interpret by running it through the soft max function or the argmax function also note this is about as simple a convolutional neural network as you can get however no matter how fancy the convolutional neural network is it's still based on filters aka convolution applying an activation function to the filter output and pooling the output of the activation function now the next time statsquatch decides to play tic-tac-toe with their computer statsquatch can start with the letter x or the letter o and a convolutional neural network we'll figure it out bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the stack quest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
xBEh66V9gZo,2021-03-01T05:15:01.000000,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,cross entropy derivatives and back propagation so cool stat quest hello i'm josh starmer and welcome to statquest today we're going to talk about neural networks part 7 cross-entropy derivatives and back propagation note this stat quest assumes that you already understand the main ideas behind neural networks and back propagation the main ideas behind neural networks with multiple inputs and outputs and soft max and the main ideas behind cross entropy now consider this neural network it takes petal and sepal width measurements as input and by using a soft max layer at the end outputs predicted probabilities of the species of iris we measured and since we're using the soft max function for training we evaluate how well the neural network fits the data with cross entropy and that means if we want to optimize parameters with back propagation we need to take the derivative of the equation for cross entropy with respect to the different weights and biases in the neural network to demonstrate the basic principles behind how we do back propagation with cross entropy we're going to work through an example that optimizes this bias b sub 3. but let's start by reviewing how this neural network makes predictions to begin with first we put values from 0 to 1 into the inputs for petal and sepal widths and these values are modified by weights and biases and run through an activation function to create this blue bent surface which is scaled by the weight negative 0.1 likewise we create an orange bent surface with the other activation function which is scaled by the weight 1.5 now we add the blue and orange bent surfaces together to get this green crinkled surface lastly we add a final bias term b sub 3 and that gives us the final green crinkled surface in other words this blue bent surface plus this orange bent surface plus a bias creates a green crinkled surface and the green crinkled surface represents the raw output for setosa likewise to create the raw output values for versicolor we add a blue bent surface to an orange bent surface plus a bias to create a red crinkled surface finally to create the raw output values for virginica we add a blue bent surface to an orange bent surface plus a bias to get a purple crinkled surface now we run the raw output values through the soft max function to get the final predicted probabilities for setosa versicolor and virginica bam for example if we had this training data with petal and sepal width measurements for setosa then we would plug the measurements into the neural network and get these raw output values that are converted by the soft max function into these predicted probabilities and thus 0.57 is the predicted probability for setosa now to evaluate how good the prediction is we plug it into the equation for cross entropy and we get 0.56 now that we know how to calculate a cross entropy value let's talk about how changes in this bias b sub 3 can change the cross entropy value first remember that b sub 3 is the bias that finalizes the green crinkled surface and thus b sub 3 finalizes the raw output for setosa and by changing b sub 3 and thus the raw output for setosa we will change the soft max value for setosa because the raw output for setosa appears in the numerator and denominator and it will change the soft max value for versicolor and virginica because it appears in those denominators now when we finally plug a predicted probability into the equation for cross entropy because the observed species is setosa the corresponding predicted probability is for setosa so we plug this equation into the cross entropy however if we had measurements from virginica then the soft max equation for virginica would give us the corresponding predicted probability in other words if we have different observed species in the data then we can end up with different ways to calculate the cross entropy and because these equations are all a little different we end up with different derivatives of cross entropy with respect to the bias b sub 3. so in this stag quest we'll derive these different derivatives for cross entropy however you may have noticed that the bottom two derivatives are the same so i'm actually just going to derive one of these and leave the other for homework so let's get started with the derivative of the cross entropy for setosa with respect to b sub 3. first let's talk a little bit more about the predicted probability for setosa remember the predicted probability for setosa is the output from the soft max function and the output from the soft max function comes from the raw output values for setosa versicolor and virginica of these three crinkled surfaces only the green crinkled surface is directly influenced by b sub 3. remember the green crinkled surface is the sum of the blue and orange crinkled surfaces plus b sub 3. now we want to use gradient descent to optimize b sub 3 and that means we need to take the derivative of the cross entropy with respect to b sub 3. and because the cross entropy is linked to b sub 3 by the predicted probability for setosa and the raw output for setosa we can use the chain rule solve for the derivative of the cross entropy with respect to b sub 3 the chain rule says that the derivative of the cross entropy with respect to b sub 3 is the derivative of the cross entropy with respect to the predicted probability for setosa times the derivative of the predicted probability for setosa with respect to the raw output for setosa times the derivative of the raw output for setosa with respect to b sub 3. bam now let's clean our workspace and move these equations out of the way now we can solve for the derivative of the cross entropy with respect to the predicted probability for setosa by first plugging in the equation for the cross entropy and then just plugging in the derivative of negative one times the natural log and thus the derivative of the cross entropy with respect to the predicted probability for setosa is negative one divided by the predicted probability for setosa now let's solve for the second part the derivative of the predicted probability for setosa with respect to its raw output value we start by plugging in the softmax equation for the predicted probability and if you remember from neural networks part 5 argmax and softmax or better yet from the soft max derivative step by step this derivative is the predicted probability for setosa times one minus the predicted probability for setosa if you don't remember this derivative you can one take my word for it or two check out the quest either way we plug it into the chain rule now let's solve for the final derivative the derivative of the raw output for setosa with respect to b sub 3. we start by plugging in the equation for the raw output for setosa remember that the blue and orange bent surfaces were created before we got to b sub 3. so the derivative of the blue bent surface with respect to b sub 3 is 0 because the blue bent surface is independent of b sub 3 and the derivative of the orange bent surface with respect to b sub 3 is also 0. lastly the derivative of b sub 3 with respect to b sub 3 is 1. now we just add everything up and the derivative of the raw output value for setosa with respect to b sub 3 is 1. so we multiply the other derivatives by 1. so technically when the observed data is for setosa and thus the predicted probability for setosa is used to calculate the cross entropy then this product is the derivative of the cross entropy with respect to b sub 3. however it is usually simplified by multiplying everything by one and all that does is make the times one term go away poof then we cancel out these two predicted probabilities for setosa and multiply everything by this negative one and lastly we move the negative 1 to the right so this looks like everyone else's solution and at long last when the predicted probability for setosa is used to calculate the cross entropy the derivative of the cross entropy with respect to b sub three is the predicted probability for setosa minus one bam in summary when the observed data is for setosa and we use the predicted probability in other words the soft max output for setosa to calculate the cross entropy then the derivative of the cross entropy with respect to the bias b sub 3 is this the predicted probability for setosa minus 1. now let's see what happens when we calculate the derivative of the cross entropy for virginica with respect to b sub 3. remember the predicted probability for virginica is the output from the soft max function and the output from the soft max function comes from the raw output values for setosa versicolor and virginica of these three crinkled surfaces only the green crinkled surface is directly influenced by b sub 3. the green crinkled surface represents the raw output for setosa and is one of the inputs to the soft max function and the green crinkled surface is the sum of the blue and orange crinkled surfaces plus b sub 3. now because the cross entropy is linked to b sub 3 by the predicted probability for virginica and the raw output for setosa we can use the chain rule to solve for the derivative of the cross entropy with respect to b sub 3. the chain rule says that the derivative of the cross entropy with respect to b sub 3 is the derivative of the cross entropy with respect to the predicted probability for virginica times the derivative of the predicted probability for virginica with respect to the raw output for setosa times the derivative of the raw output for setosa with respect to b sub 3 bam now just like before we can solve for the derivative of the cross entropy with respect to the predicted probability for virginica by plugging in the equation for the cross entropy and then just plugging in the derivative of negative one times the natural log the derivative of the predicted probability for virginica with respect to the raw output for setosa is the negative predicted probability for setosa times the predicted probability for virginica if you don't remember that check out the quest now we just plug it into the chain rule lastly the derivative of the raw output value for setosa with respect to b sub 3 is one just like we got earlier so we multiply the other derivatives by one so technically when the observed data is for virginica and thus the predicted probability for virginica is used to calculate the cross entropy this product is the derivative of the cross entropy with respect to b sub 3. however it is usually simplified by multiplying everything by one poof then we cancel out these two predicted probabilities for virginica and multiply everything by negative one and at long last when the predicted probability for virginica is used to calculate the cross entropy the derivative of the cross entropy with respect to b sub 3 is the predicted probability for setosa double bam in summary when the observed data is for virginica and we use the predicted probability in other words the soft max output for virginica to calculate the cross entropy then the derivative of the cross entropy with respect to the bias b sub 3 is this the predicted probability for setosa note before we move on i want to mention that this predicted probability for setosa comes from using the measurements for petal and sepal widths in the second row of the training data and this predicted probability for setosa comes from using the measurements for petal and sepal widths in the first row of the training data this difference isn't super important now but will become important later when we do back propagation small bam now when the observed measurements are for versicolor then following the same steps we use for virginica the derivative is also the predicted probability for setosa note before we move on i want to remind everyone that b sub 3 the parameter we want to optimize only directly influences the raw output for setosa and that is why these three derivatives are in terms of the predicted probability for setosa in other words if we had taken the derivatives with respect to b sub 4 which only directly influences the raw output for versicolor then by the same process we went through earlier we would end up with different derivatives and lastly if we had taken the derivatives with respect to b sub 5 which only directly influences the raw output for virginica then by the same process we went through earlier we would end up with these derivatives now going back to the original derivatives with respect to b sub 3 let's use this training data and use these derivatives to optimize the bias b sub 3 with back propagation first let's set the bias b sub 3 to some starting value in this case we'll use negative two now just so we can make sure back propagation is improving things let's calculate the total cross entropy for the training data by running it through the neural network da in the total cross entropy for when b sub 3 equals negative 2 is 2.67 and we can plot that on a graph with values for b sub 3 on the x axis and total cross entropy on the y axis note if we just plugged in a bunch of values for b sub 3 and calculated the total cross entropy then we would get this pink curve now let's optimize the bias b sub 3 with back propagation to find the value for b sub 3 at the low point in the curve in other words let's find the value for b sub 3 that minimizes the total cross entropy and that starts with the derivative of the cross entropy with respect to b sub 3. now since we have three observations and we'll make one prediction per observation we expand the summation so that there is one term per prediction now let's focus on the prediction made for the first observation because the observed species is setosa we solve for the cross entropy using the predicted probability for setosa and the derivative of the cross entropy for setosa with respect to b sub 3 is the predicted probability for setosa minus one so we plug that in now we plug the petal and sepal measurements into the neural network do the math and plug the predicted probability for setosa 0.15 into the equation bam now let's focus on the prediction for the second observation because the observed species is virginica we solve for the cross entropy using the predicted probability for virginica and the derivative of the cross entropy for virginica with respect to b sub 3 is the predicted probability for setosa so we plug that in now we plug the petal and sepal measurements into the neural network do the math and plug the predicted probability for setosa 0.04 into the equation double bam now let's focus on the prediction for the third and final observation because the observed species is versicolor we solve for the cross entropy using the predicted probability for versicolor and the derivative of the cross entropy for versicolor with respect to b sub 3 is the predicted probability for setosa so we plug that in now we plug the petal and sepal measurements into the neural network do the math and plug the predicted probability for setosa 0.04 into the equation now we just add up the derivatives and get negative 0.7 and thus the slope of this tangent line is negative 0.77 now we plug the slope into the gradient descent equation for step size and in this example we'll set the learning rate to 1 and that means the step size is negative 0.77 now we use the step size to calculate the new value for b sub 3 by plugging in the old value for b sub 3 negative 2 and the step size negative 0.77 and the new value for b sub 3 is negative 1.23 then we just repeat the process each time using the new value for b sub 3 until the predictions no longer improve very much or we reach a maximum number of steps or we meet some other criteria in this case the predictions stop improving when b sub 3 equals negative 0.03 so we're done triple bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
6ArSys5qHAU,2021-03-01T05:00:01.000000,Neural Networks Part 6: Cross Entropy,if you made a neural network on the surface of mars you'd be a long long way away from me that's the truth statquest hello i'm josh starmer and welcome to statquest today we're going to talk about neural networks part 6 cross-entropy note this stat quest assumes that you already understand the main ideas behind neural networks the main ideas behind back propagation and softmax and argmax if not check out the quests the links are in the description below now before we get started with cross entropy let me remind you that in the statquest on backpropagation main ideas we had a simple neural network with a single output that in theory could give us any output value in cases like this we commonly use the sum of the squared residuals to determine how well the neural network fits the data bam now when we have a neural network with multiple output values we often run the data through arg max to make the output easy to interpret but because arg max has a terrible derivative we can't use it for back propagation so in order to train the neural network we use the soft max function and the soft max output values are predicted probabilities between 0 and 1 and when the output is restricted to values between 0 and 1 we often use something called cross entropy to determine how well the neural network fits the data cross entropy is one of those things that sounds super fancy and complicated but when it comes to neural networks is super simple to see how super simple it is let's start with this super simple training data set that has pedal and sepal widths for known or observed iris species now let's plug in the petal and sepal widths for the first observed species setosa and run the numbers through the neural network and run the raw output values through the softmax function now because we know the data are from setosa the cross entropy is the negative log base e of the soft max output value for setosa 0.57 in other words we plug the predicted probability for the observed species into the cross entropy function note if you have seen the cross-entropy function before this version may look different to you the difference is because neural networks only need a simplified form of this general equation in this summation m is the number of output classes in this case m equals 3 because we have three output classes setosa versicolor and virginica thus if we expanded the summation we would get one term for setosa one term for versicolor and one term for virginica now because we know that the data from the first row comes from setosa the observed probability that the data comes from setosa is one and the observed probabilities that the data came from versicolor and virginica are both zero and that means the terms for versicolor and virginica go away poof and we are left with negative one times the log of the predicted probability for setosa anyway going back to where we plugged in the predicted probability for setosa when we do the math we get 0.56 bam so let's add the predicted probability for setosa to the table and the corresponding cross entropy value now let's plug in the petal and sepal widths for the second observed species virginica and run the numbers through the neural network and run the raw output values through the soft max function now because we know the data are from virginica we plug the predicted probability for virginica 0.58 into the cross entropy equation and the cross entropy value for virginica the second row in the training data is 0.54 likewise we plug in the measurements on the third row run the numbers through the neural network and soft max and because we know the data are from versicolor we plug the predicted probability for versicolor 0.52 into the cross entropy equation and the cross entropy for versicolor is 0.65 now to get the total error for the neural network all we do is add up the cross entropy values in this case we get 1.75 as the total error and we can use back propagation to adjust the weights and biases and hopefully minimize the total error double bam now at this point you might be wondering if i can calculate these probabilities for each observed species then i can calculate residuals the difference between the observed probabilities and the predicted probabilities for example for the first row in the data the observed species is setosa and thus the observed probability that the petal and sepal measurements came from setosa is 1 and the predicted probability is 0.57 thus the residual is 0.43 and if we can calculate a residual we can square it and ultimately that means we can calculate the sum of the squared residuals so you may be wondering why we don't just calculate the squared residuals instead of the cross entropy well the first thing we do is remember that the soft max function only gives us values between 0 and one and if the prediction for setosa is really good it will be close to one and if the prediction is really terrible it will be close to zero in this case the prediction for setosa is kind of in the middle however we can just plug in values for the predicted probability from 0 to 1 into the cross-entropy function and plot the output the y-axis is the loss which is a measure of how bad the prediction is when we use cross-entropy as the prediction gets worse and worse the loss kind of explodes and gets really really big in contrast if we plug in values for the predicted probability from 0 to 1 into the squared residual then the change in loss between 0 and 1 is not as large as it is for cross entropy and you may remember from neural networks part 2 back propagation main ideas that the step size for back propagation depends in part on the derivatives of these functions and the derivative or slope of the tangent line for cross entropy for a bad prediction will be relatively large compared to the derivative for the same bad prediction with squared residuals so when the neural network makes a really bad prediction cross entropy will help us take a relatively large step towards a better prediction because the slope of the tangent line will be relatively large triple bam now with all this talk about back propagation you're probably dying to see how it is done with cross entropy my friend the news is good the next stat quest is all about how to use cross entropy with back propagation bam now it's time for some shameless self promotion if you want to review statistics and machine learning offline check out the stackquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
M59JElEPgIg,2021-02-08T03:34:35.000000,"The SoftMax Derivative, Step-by-Step!!!",if you're watching this then your hardcore stat quest hello i'm josh starmer and welcome to statquest today we're going to talk about the soft max derivative and we're going to go through it step by step note this stack quest assumes that you already understand the main ideas behind soft max if not check out the quest the link is in the description below in the stat quest on soft max we had this fancy neural network that predicted if an iris was setosa versicolor or virginica and since the output values were all over the place we ran them through a soft max layer and i was like here's the derivative of the soft max for setosa with respect to the raw output value for setosa bam now let's solve for that derivative one step at a time well first remember that the soft max for setosa and thus this fraction is equal to the predicted probability for setosa now the derivative of the predicted probability requires the quotient rule don't worry i don't expect you to remember the quotient rule i had to google it too since we are taking the derivative with respect to the raw output value for setosa the quotient rule tells us to take the derivative of the numerator with respect to the raw output value for setosa and the derivative of the numerator e raised to the raw output value for setosa with respect to the raw output value for setosa is e raised to the raw output value for setosa then we just plug in the denominator then we subtract the derivative of the denominator with respect to the raw output value for setosa which again is just e raised to the raw output value for setosa so we plug that in and then we plug in the numerator lastly we square the original denominator and plug that in now let's move the equation up a little bit and shorten setosa versicolor and virginica so the equations don't run off the screen and now just do some algebra first since e to the s multiplies the rest of the numerator we can split it out likewise we can split this numerator into two terms now we notice that the first term is equal to the predicted probability for setosa so we can replace the first term with the predicted probability for setosa and this term has the same thing in the numerator and the denominator so it is equal to one and the last term is also equal to the predicted probability for setosa so we plug that in bam so we see that the derivative of the predicted probability for setosa with respect to the raw output value for setosa is the predicted probability for setosa times one minus the predicted probability for setosa where in this case the predicted probability for setosa equals 0.69 so when we do the math we get 0.21 now because the raw output values for versicolor and virginica play a role in the soft max output value for setosa we also need the derivatives of the predicted probability for setosa with respect to versicolor and virginica so let's quickly look at how to calculate the derivative with respect to versicolor and we'll leave virginica for homework just like before remember that the soft max value for setosa and thus this fraction is equal to the predicted probability for setosa likewise the soft max value for versicolor and thus this fraction is equal to the predicted probability for versicolor now just like before the derivative of the predicted probability for setosa requires the quotient rule the big difference this time is that now we are taking the derivatives with respect to versicolor this difference makes things easier because the derivative of the numerator e raised to the raw value for setosa with respect to versicolor is zero since versicolor is not in the numerator and that means this whole first term is zero then we subtract the derivative of the denominator with respect to versicolor times the numerator and divide everything by the denominator squared now let's shorten setosa versicolor and virginica so the equations don't run off the screen and now we just do some algebra first we split the numerator into two separate terms and the first term is the same as the negative predicted probability for setosa and the second term is the same as the predicted probability for versicolor double bam so we see that the derivative of the predicted probability for setosa with respect to the raw output value for versicolor is the negative predicted probability for setosa times the predicted probability for versicolor where the predicted probability for setosa equals 0.69 and the predicted probability for versicolor equals 0.1 and when we do the math we get negative 0.07 likewise the derivative with respect to virginica is the negative predicted probability for setosa times the predicted probability for virginica and that gives us negative 0.15 [Music] now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
KpKog-L9veg,2021-02-08T03:34:23.000000,Neural Networks Part 5: ArgMax and SoftMax,argh max soft max statquest hello i'm josh starmer and welcome to statquest today we're going to talk about neural networks part 5 arg max and softmax note this stat quest assumes that you already understand the main ideas behind neural networks the main ideas behind back propagation and how neural networks work with multiple input and output nodes if not check out the quests the links are in the description below in the statquest on neural networks with multiple input and output nodes we had a fancy neural network that used petal and sepal widths to predict iris species the neural network made predictions with crinkled surfaces for setosa versicolor and virginica then we plugged in the petal and sepal widths from an iris we found in the woods and ran the numbers through the neural network and predicted that this iris is versicolor because that output value 0.86 is closest to 1. bam now let's see what happens when petal width equals 0 and sepal width equals 1. when we run the numbers through the neural network the output values are 1.43 for setosa negative 0.4 for versicolor and 0.23 for virginica so one thing we notice is that the raw output values are not always values between 0 and 1. sometimes a raw output value can be greater than 1 like in the case for setosa with 1.43 and sometimes a raw output value can be less than zero like the case reversi-color with negative 0.4 this broad range of values makes the raw output harder to interpret than it needs to be and this is one of the reasons that when there is more than one output like we have here the raw output values are sent to either an arg max layer or a soft max layer before a final decision is made argmax which sounds like something a pirate might say arg max simply sets the largest value to 1 and all of the other values to 0. in this example setosa has the largest value 1.43 so the arg max function sets the final output value for setosa to 1. and the final output values for versicolor and virginica to zero thus when we use arg max the neural network's prediction is simply the output with a 1 in it and that makes the output super easy to interpret bam the only problem with argmax is that we can't use it to optimize the weights and biases in the neural network this is because the output values from arg max are constants 0 and 1. to see why this is a problem let's plot the second largest output value 0.23 on a graph since this is the second largest value arg max will output 1 for any other output value that is greater than 0.23 and it will output 0 for any other output value that is less than 0.23 because the slopes of these two lines are both zero their derivatives are also zero and that means if we wanted to find the optimal value for any of the weights and biases in the neural network then we would end up plugging zero into the chain rule for the derivative of arg max and then the whole derivative would be zero and if we plug zero into gradient descent we won't step towards the optimal parameter values and that means we can't use arg max for back propagation [Music] so that leads us to the soft max function when people want to use arg max for output they often use softmax for training however first let me say that softmax sounds like a brand of toilet paper but since we're already using a roll of toilet paper to represent the soft plus activation function let's use a picture of a soft teddy bear to represent the soft max function now let's see the soft max function in action first let's solve for the soft max output for setosa we raise e to the raw output value for setosa and divide by the sum of e raised to each raw output value so in this case we plug 1.43 in for setosa negative 0.4 for versicolor and 0.23 for virginica and when we do the math we get 0.69 as the soft max output value for setosa so let's put 0.69 here so we don't forget it now let's calculate the softmax output value for versicolor when we calculate the softmax value for versicolor the only thing that changes is the numerator now instead of e raised to the raw output value for setosa we are using the raw output value for versicolor so let's plug in the raw output values do the math and we get 0.1 as the soft max output value for versicolor lastly let's calculate the soft max output value for virginica just like before the only thing that changes is the numerator now e is raised to the raw output value for virginica so let's plug in the raw output values do the math and we get 0.21 as the softmax output value for virginica bam now let's take a moment to look at the three soft max output values first notice that the largest raw output value 1.43 for setosa is paired with the largest softmax output value 0.69 likewise the second largest raw output value 0.23 for virginica is paired with the second largest softmax output value 0.21 lastly the lowest raw output value negative 0.4 for versicolor is paired with the lowest softmax output value 0.1 so we see that the softmax function preserves the original order or ranking of the raw output values bam second notice that all three soft max output values are between zero and one this is something that the soft max function ensures regardless of how many raw output values there are the soft max output values will always be between zero and one double bam third notice that if we add up all of the softmax output values then the total is one that means that as long as the output values are mutually exclusive then the soft max output values can be interpreted as predicted probabilities note i put the word probabilities in quotes because you should not put a lot of trust in their accuracy the reason you should not put a lot of trust in the accuracy of these predicted probabilities is that they are in part dependent on the weights and biases in the neural network and the weights and biases in turn depend on the randomly selected initial values and if we change the initial values we can end up with different weights and biases that give us a neural network that is just as good at classifying the data but give us different raw output values and different raw output values give us different soft max output values and that means that different randomly selected initial values for the weights and biases result in different predicted probabilities in other words the predicted probabilities don't just depend on the input values but also on the random initial values for the weights and biases so don't put a lot of trust in the accuracy of these predicted probabilities small bam now let's go back to the original neural network with the original predicted probabilities and look at the general form of the softmax equation this i refers to an individual raw output value for example when i equals 1 then we are talking about the raw output value for setosa and that means we put setosa here and we put the raw output value for setosa in the numerator in the denominator we just have the sum of e raised to each raw output value now remember that we started by talking about arg max which is easy to interpret but cannot be used for back propagation because its derivative is totally lame in contrast soft max has a derivative that can be used for back propagation for example if we have the soft max function for setosa then the derivative of the predicted probability with respect to the raw output value for setosa is the predicted probability for setosa times 1 minus the predicted probability for setosa where in this case the predicted probability for setosa equals 0.69 and so when we do the math we get 0.21 note some of you may wonder where this derivative came from some of you may not for those who want to know i've created a video that walks you through every single step so check it out bam now because the raw output values for versicolor and virginica play a role in the soft max output value for setosa we also need the derivatives of the predicted probability for setosa with respect to versicolor and virginica the derivative of the predicted probability with respect to the raw output value for versicolor is the negative predicted probability for setosa times the predicted probability for versicolor where the predicted probability for setosa equals 0.69 and the predicted probability for versicolor equals 0.10 and when we do the math we get negative 0.07 lastly the derivative with respect to virginica is the negative predicted probability for setosa times the predicted probability for virginica and when we do the math we get negative 0.15 thus unlike the arg max function which has a derivative equal to 0 or it's undefined the derivative of the soft max function is not always zero and we can use it for gradient ascent so we see why neural networks with multiple outputs often use softmax for training and use argmax which has super easy to understand output to classify new observations triple bam now before we go i need to mention that way back in the stack quest on back propagation main ideas we use the sum of the squared residuals to determine how well the neural network fit the data however when we use the soft max function because the output values are predicted probabilities between 0 and one we often use something called cross entropy to determine how well the neural network fits the data and we'll talk about cross entropy in the next stack quest in this series bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stack quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
83LYR-1IcjA,2021-02-01T05:00:03.000000,Neural Networks Pt. 4: Multiple Inputs and Outputs,way up north there's an island out in the sea and way out there they've got neural networks and they're cool statquest hello i'm josh starmer and welcome to statquest today we're going to talk about neural networks part 4 multiple inputs and outputs note this stack quest was supported in part by ital also i thought i'd mention that the inspiration for this stat quest came from my friend michael in svalbard lastly this stat quest assumes that you already understand the main ideas behind neural networks and the relu activation function if not check out the quests the links are in the description below so far the neural networks that we've looked at have been super simple and only predict whether or not the dosage of a drug will be effective the neural networks just have one input node and one output node when there is only one input node then the data we are using to make predictions in this case dosages can all fit on the x-axis of this graph in other words the input is one-dimensional since it only needs one axis in the graph likewise a single dimension the y-axis represents the output values combined we get a two-dimensional graph with the input dosage on the x-axis and the output drug effectiveness on the y-axis because the input and output combine to form a two-dimensional graph we can see how the weights and biases in this neural network slice flip and stretch the curved or bent activation functions into new shapes that are added together to make a two-dimensional squiggle or shape that fits the data bam now let's look at a more complicated network that has more than one input node and more than one output node now this neural network may look really fancy but all it does is take two measurements from an iris flower the width of a petal which is this part of the flower and the width of a sepal which is this part of the flower and with that information it predicts the species either setosa versicolor or virginica now raise your hand if you already knew that this was a sepal and not a petal not me i thought they were all petals anyway to keep things simple at the start let's begin with both input nodes but just one output node for setosa later on we'll add the other two output nodes but for now let's keep things simple and just use one output node now let's see what happens when we plug values for petal width and sepal width into this simplified neural network since we have two inputs and one output if we're going to draw a graph of what's going on then we need a three-dimensional graph the inputs petal width and sepal width each get an axis in the output the prediction for setosa gets the y-axis note to keep the math simple i scaled the inputs to be between 0 for the smallest value and one for the largest value so let's start in this corner where petal and sepal width equal zero and plug those values into the neural network first let's determine the x-axis coordinate for the top node in the hidden layer we multiply the petal width by the weight associated with the connection to the top node in the hidden layer negative 2.5 and we multiply the sepal width by the weight associated with its connection to the top node in the hidden layer 0.6 then we add the two terms together and add the bias 1.6 and that gives us the x-axis coordinate for the activation function which is 1.6 now we plug 1.6 into the relu activation function to get the y-axis coordinate 1.6 because 1.6 is greater than 0 and 1.6 corresponds to this blue point on the graph when petal and sepal widths are both zero now let's increase petal width to 0.2 but keep sepal width at 0. now when we do the math we get 1.1 for the x-axis coordinate and 1.1 for the y-axis coordinate and 1.1 corresponds to this blue point on the graph likewise when we increase pedal width to the maximum value 1 but keep sepal width at 0 we get these blue dots now let's increase sepal width to 0.2 and run values for petal width from 0 to 1 through the neural network likewise if we keep increasing sepal width to 1 for different values of petal width we get this blue bent surface the bend corresponds to the points where the relu activation function set the y-axis values to 0. now we multiply the y-axis value for each point by negative 0.1 for example the original y-axis value for this point when petal and sepal widths are both zero is 1.6 and 1.6 times negative 0.1 equals negative 0.16 so the final point is here likewise when we multiply all of the other y-axis coordinates by negative 0.1 we get this final blue bent surface now we do the exact same thing for the connections to the bottom node in the hidden layer and we end up with this orange bent surface where the bend occurs where the relu activation function set the y-axis values to zero then we multiply each y-axis coordinate by 1.5 to get the final orange bent surface now we add the y-axis coordinates on the blue bent surface to the y-axis coordinates on the orange-bent surface for example the y-axis coordinate for this blue point is negative 0.16 and we add the y-axis coordinate for this orange point 1.05 to get 0.89 the y-axis coordinate for this green point anyways we do that for every single point and ultimately we end up with this green crinkled surface now the last thing we do is add the final bias 0 to each y-axis coordinate and since adding 0 doesn't change the green crinkled surface this is the output for setosa bam looking at the green crinkled surface we see that the value for setosa is highest when the petal width is close to zero and the value for setosa is lowest when the petal width is close to one note remember that we scaled the inputs to be between zero and one and thus pedal width equals zero does not imply that the pedal is zero centimeters wide instead 0 refers to the smallest width in the training data set likewise 1 means the largest width in the training data set small bam now to review the concepts so far when we have two inputs the neural network creates curved or bent surfaces that are added together to make a new crinkled surface that in this case we can use to make predictions about whether or not the species of an iris is setosa for example if we found this iris while walking in the woods and the scaled petal width was 0.5 and the scaled sepal width was 0.37 then we can look at the y-axis value on the green crinkled surface that corresponds to these measurements and see that this particular iris is probably not setosa because the y-axis value is closer to zero than one and this is confirmed when we run the numbers through the neural network and get 0.09 bam now that we have a green crinkled surface for setosa let's determine the output for the second species fursy color just like before we'll start with the connections to the top node in the hidden layer and because the weights and biases are the same as before we start out with the same blue bent surface however because we will multiply the y-axis coordinates by 2.4 let's change the range of the y-axis from 0 to 2 to negative 6 to 6 bam now multiplying the y-axis coordinates by 2.4 gives us this final blue bent surface now we create the orange pen surface from the bottom node in the hidden layer and multiply the y-axis coordinates on the orange-bent surface by negative 5.2 now we add the y-axis coordinates from the two bent surfaces together to create this red crinkled surface lastly we add the final bias 0 to the y-axis coordinates on the red crinkled surface and that gives us the final surface for predicting if the iris species is versicolor now i'll admit it's hard to see what values for petal or sepal widths will give versicolor a high score on this red crinkled surface but when we change the y-axis scale from negative 6 to 6 to negative 0.5 to 1 we see that when petal width is close to 0.4 we will get a high value for versicolor double bam now just like we did for setosa and versicolor let's determine the crinkled surface for virginica just like before we start with the blue bent surface from the top node in the hidden layer but now we multiply the y-axis coordinates by negative 2.2 and just like before we create the orange bent surface from the bottom node in the hidden layer but now we multiply the y-axis coordinates by 3.7 now we add the y-axis coordinates from the two bent surfaces together and get this purple crinkled surface lastly we add the final bias one to the y-axis coordinates on the purple crinkled surface and that gives us the final surface for predicting if the iris species is virginica now so we can see what's going on let's change the scale for the y-axis from negative six to six to zero to one now we see that when petal width is close to 1 then we will get a high score for virginica triple bam at long last we have crinkled surfaces for setosa versicolor and virginica now we can plug in the petal and sepal widths from the iris we found and run the numbers through the neural network and predict that this iris is versicolor because that output value 0.86 is closest to 1. that said usually when there are two or more output nodes the output values are sent to either something called arg max arg or something called softmax before a final decision is made and we'll talk about argmax and softmax in the next stat quest in this series bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
0nK6iPpqNPU,2020-12-08T01:00:16.000000,US Census Data and Contest!!!,gotta give me some data gotta get it now statquest yeah hello i'm josh starmer and welcome to statquest today we're gonna talk about the u.s census data and competition note this stat quest has two parts first i'll show you some awesome data sets that are free and anyone can access second if you're a high school student or teacher residing in the u.s or u.s territories i'll show you how you can use this data to enter a contest and have a chance to win five thousand dollars in prizes you know the deal you're sitting at home on a terrible chair that makes your back hurt and you have a data science project due soon that means you need some data there are lots of places to get data these days but two cool places are census reporter.org and data.census.gov bam the links are in the description below let's start by taking a look at censusreporter.org here's the website for censusreporter.org it's jam-packed with cool summarizations of census data since i live in orange county north carolina let's check out the census data for orange county north carolina at the top of the results we see the overall population size scrolling down we see that population broken down into different units for example we see that 53 percent of the people that live in orange county north carolina are female by clicking on show data we can then see that that percentage is high relative to the state and the country scrolling down even further we see that the median household income is 74299 and this is larger than the average for the entire state if we want to we can break this into smaller units or we can just scroll down and see more summary statistics for orange county in summary for orange county north carolina we saw two things one the percentage of females 53 percent is higher than the state average and two the median household income is higher than the state average does this mean that the more females we have living in a county the larger the median income in order to answer this question we need to download some data and the best place for that is data.census.gov so let's take a look at it here's an example of using data.census.gov in this example let's type in income the first thing we get is the median household income for the entire united states now we need to break that down into smaller pieces we can do that by clicking on the customize table button and then clicking on the geo button select county then select the state we're interested in in this case that's north carolina and then we select all counties in north carolina now we have income data for every single county in north carolina in order to download the data we simply click on the download button and then click on the download now button in this example we just downloaded the median household income per county in north carolina then i downloaded the number of males and females in each county then i combined the percentage of females in each county with the median income in each county this outlier which has a lot fewer females than the other counties in north carolina is home to a very large marine base camp lejeune anyway looking at the remaining counties makes me suspect that the percentage of females is not related to the median household income this might seem like a bummer because it's a negative result however we've used the census data to tell a story using census data for north carolina we see the percentage of females in a county does not appear related to the median income it's not a fancy story but it's still a story and now that we have a story from the data we can submit it to the let's make it count national census data competition bam oh no it's a small print alert note the let's make it count national census data competition is open to high school students and high school teachers based in the united states and u.s territories also note the contest ends on january 1st 2021 so get your entry in asap step one sign up for the contest to sign up you go to let's make it count.org then click on join the competition then click on join the competition again and scroll down to create your account creating an account is easy you enter your email your super secret password confirm your super secret password and then your first and last name then click sign up bam step 2 put together your census data story our census data story starts with orange county north carolina we saw that the percentage of females 53 percent is higher than the state average and we saw that the median household income is higher than the state average but statewide the percentage of females appears to be independent of the median household income note this is just one way to tell a story with census data if you are better with words than pictures then you can write your story out the point being you should tell the story the way that works best for you and submit it and that leads us to step three submit your census data story to the contest when you're ready to submit your data story go back to the let's make it count.org website click join the competition then click join the competition again and then scroll down and click on have an account sign in then just type in your email and password and sign in now scroll down and enter your first name last name and email address city state or territory and the name of your high school the next thing you need to do is upload your data story by clicking on the choose files button lastly just verify that you're eligible and you agree with the terms and then click join the competition for more information resources and help check out the let's make it count virtual summit at letsmakeitcount.org summit and if you don't mind a little shameless self-promotion check out the stat quest statistics fundamentals videos and playlists at statquest.org triple bam hooray we've made it to the end of another exciting stat quest if you like this stack quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
68BZ5f7P94E,2020-11-23T09:05:29.000000,Neural Networks Pt. 3: ReLU In Action!!!,[Music] some people say i mispronounce value but that is okay at least it's okay with me stackquest hello i'm josh starmer and welcome to statquest today we're going to do neural networks part 3 the relu activation function in action note this stat quest assumes that you are already familiar with the main ideas behind neural networks if not check out the quest the link is in the description below in neural networks part 1 inside the black box we started with a simple data set that showed whether or not different drug dosages were effective against a virus the low and high dosages were not effective but the medium dosage was effective then we talked about how a neural network like this one that uses the soft plus activation function in the hidden layer can fit a green squiggle to the data set bam now let's see what happens if we swap out the soft plus activation function in the hidden layer with one of the most popular activation functions for deep learning and convolutional neural networks the rel u activation function which is short for rectified linear unit and sounds like a robot and as a bonus because it is common to put an activation function before the final output we'll do that too bam remember to keep the math simple let's assume dosages go from 0 for low to 1 for high so if we plug in the lowest dosage 0 the connection from the input to the top node in the hidden layer multiplies the dosage by 1.70 and then adds negative 0.85 and the result is an x-axis coordinate for the activation function so if we plug in 0 for dosage then the x-axis coordinate for the activation function is negative 0.85 now we plug negative 0.85 into the relu activation function the relu activation function outputs whichever value is larger 0 or the input value which in this case is negative 0.85 and because 0 is greater than negative 0.85 the output from the relu activation function is zero and the corresponding y-axis value is zero so let's put a blue dot at zero for when dosage equals zero now if we increase dosage to 0.2 the x-axis coordinate for the activation function is negative 0.51 and again because 0 is greater than negative 0.51 the output from the relu activation function is 0 and the corresponding y-axis value is 0. so let's put a blue dot at 0 for when dosage equals 0.2 and if we increase the dosage value to 0.4 we get 0 for the y-axis coordinate again however when dosage equals 0.6 the x-axis coordinate is 0.17 now when we plug 0.17 into the rail u activation function the output is 0.16 because 0.17 is greater than 0 and the corresponding y-axis value is 0.17 and if we continue to increase the dosage values all the way to one the maximum dosage we get this bent blue line then we multiply the y-axis coordinates on the bent blue line by negative forty point eight and the new bent blue line goes off the screen note for those of you drawing this at home your final bent blue line will have a steeper slope than mine because for the sake of clarity i am not drawing things perfectly to scale tiny bam now when we run dosages through the connection to the bottom node in the hidden layer we get the corresponding y-axis coordinates that go off the screen for this straight orange line now we multiply the y-axis coordinates on the straight orange line by 2.70 and we end up with this final straight orange line now we add the bent blue line and the straight orange line together to get this green wedge now we add the final bias term negative 16 to the y-axis coordinates on the green wedge lastly because we included the rail u activation function right in front of the output we use the green wedge as its input for example the y-axis coordinate for this point on the green wedge is negative 16 which corresponds to this x-axis coordinate for the relu activation function and when we plug that into the relu activation function we get 0 because 0 is greater than negative 16. and 0 corresponds to this green dot likewise the y-axis coordinate for this point on the green wedge is negative 9.2 and just like before when we plug that into the rail u activation function we get 0 and 0 corresponds to this green dot now in contrast to the last two points we fed into the rel u activation function this one is positive 0.49 and when we plug it into the relu activation function we get 0.49 and 0.49 corresponds to this green dot likewise the remaining positive y-axis coordinates stay the same and the negative values are set to zero and at long last we end up with this green pointy thing double bam thus the relu activation function may seem weird because it's not curvy and the equation is really simple but just like for any other activation function the weights and biases on the connection slice them flip them and stretch them into new shapes which are added together to get an entirely new shape that fits the data triple bam oh no it's another technical detail alert some of you may have noticed that the relu activation function is bent and not curved this means that the derivative is not defined where the function is bent and that's a problem because gradient descent which we use to estimate the weights and biases requires a derivative for all points however it's not a big problem because we can get around this by simply defining the derivative at the bent part to be zero or one it doesn't really matter small bam and now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the stack quest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
GKZoOHXGcLo,2020-11-02T05:15:01.000000,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,omg let's do the chain rule with me it's gonna be cool you'll see statquest hello i'm josh starmer and welcome to statquest today we're going to talk about back propagation details part 2. note this stat quest assumes that you have already seen back propagation details part 1. if not check out the quest the link is in the description below since you've already seen back propagation details part 1 you already know that in this stat quest we're going to go totally bonkers with the chain rule and gradient radiant descent in order to optimize all of the weights and biases in this neural network so that it can fit a green squiggle to this data bam note the derivatives that we derived in part 1 for bias b sub 3 and weights w sub 3 and w sub 4 do not change so we can just plug these derivatives into the gradient descent algorithm however now we need to derive the derivatives of the sum of the squared residuals with respect to w sub 1 b sub 1 w sub 2 and b sub 2. let's start with the derivative of the sum of the squared residuals with respect to w sub 1. remember the neural network starts by multiplying input sub i by w sub 1. then it adds the bias b sub 1 and that gives us an x axis coordinate for the activation function that we call x sub 1 comma i we then plug x sub 1 comma i into the activation function and that means plugging x sub 1 comma i into this equation because we are using the soft plus activation function and that gives us a y-axis value of y sub 1 comma i and remember y sub 1 comma i times w sub 3 gives us the final blue curve which we add to the final orange curve and the bias b sub 3 to get the green squiggle and the predicted values lastly remember that we use the predicted values to calculate the sum of the squared residuals now let's clear the screen and spread out the equations now we can see that the sum of the squared residuals are linked to w sub 1 first by the predicted values then y sub 1 comma i the y axis values that come from the activation function and lastly x sub 1 comma i the x x-axis values that are input for the activation function thus the chain rule tells us that the derivative of the sum of the squared residuals with respect to w sub 1 is the derivative of the sum of the squared residuals with respect to the predicted values times the derivative of the predicted values with respect to y sub 1 times the derivative of y sub 1 with respect to x sub 1 times the derivative of x sub 1 with respect to w sub 1. as we've seen before the derivative of the sum of the squared residuals with respect to the predicted values is the sum of negative 2 times the observed minus the predicted values now the derivative of the predicted values with respect to y sub 1 is w sub 3 for the first term and 0 for the other two terms so the derivative of the predicted values with respect to y sub one is w sub three now we need to solve for the derivative of y sub one with respect to x sub 1. this requires knowing that the derivative of the log of z is one divided by z and the derivative of e to the x is e to the x the chain rule tells us to take the derivative of the stuff outside of the parentheses the log function times the derivative of the stuff inside the parentheses 1 plus e to the x if we let z equal one plus e to the x then the derivative of the stuff outside is one divided by one plus e to the x and the derivative of the stuff inside is zero for the first term since it does not include x plus e to the x for the second term and this whole thing simplifies to e to the x divided by one plus e to the x lastly the derivative of x sub 1 with respect to w sub 1 is the input value for the first term plus 0 for the second term because it does not include w sub 1 which simplifies to just the input values plugging in the derivatives gives us a big fancy equation or bfe for short hooray we solved for the derivative of the sum of the squared residuals with respect to the first weight w sub 1. note keep in mind that the x's in the e to the x terms are the individual x-axis coordinates for the activation function for each input value bam now let's find the derivative of the sum of the squared residuals with respect to b sub 1. the good news is that the exact same equations that linked the sum of the squared residuals to w sub 1 also link the sum of the squared residuals to b sub 1. thus the chain rule tells us that the derivative of the sum of the squared residuals with respect to b sub 1 is the derivative of the sum of the squared residuals with respect to the predicted values times the derivative of the predicted values with respect to y sub 1 times the derivative of y sub 1 with respect to x sub 1 times the derivative of x sub 1 with respect to b sub 1. and the derivative of the sum of the squared residuals with respect to the predicted values is the same as before and so is the derivative of the predicted values with respect to y sub 1 and the derivative of y sub 1 with respect to x sub 1 is also the same as before and the only thing different is the derivative of x sub 1 with respect to b sub 1 which is 0 for the first term since it does not include b sub 1 plus 1 for the second term which simplifies to just one plugging in the derivatives gives us another big fancy equation double bam we've solved for the derivatives of the sum of the squared residuals with respect to w sub 1 and b sub 1. now we need to solve for the derivatives of the sum of the squared residuals with respect to w sub 2 and b sub 2. the good news is that the only difference is that instead of using the top node in the hidden layer we use the bottom node and that means now we multiply input sub i by w sub 2 and add b sub 2 and that gives us x sub 2 comma i which we plug into the activation function to get y sub 2 comma i then we multiply y sub 2 comma i by w sub 4 to get the final orange curve which we add to the final blue curve and b sub 3 to get the green squiggle and the green squiggle gives us the predicted values which gives us the residuals and the sum of the squared residuals thus just like before the sum of the squared residuals are linked to w sub 2 and we use the chain rule to derive the derivative of the sum of the squared residuals with respect to w sub 2. now calculating the derivatives is just like before but with slightly different parameter names and plugging in the derivatives gives us another bfe so in the end this is the derivative of the sum of the squared residuals with respect to w sub two similarly we can derive the derivative of the sum of the squared residuals with respect to b sub now we just combine these derivatives with all of the others and use gradient ascent to optimize all of the parameters simultaneously first we initialize the weights and biases in this example we picked numbers from a standard normal distribution for the weights oh no it's a technical detail alert i think this is the first time that's ever happened just so you know using a standard normal distribution is just one of many ways to initialize weights small bam now we initialize the bias terms to zero because bias terms frequently start at zero now starting with the derivative of the sum of the squared residuals with respect to w sub one we expand the summation then we plug in the observed values and the values predicted by the green squiggle remember we get the predicted values on the green squiggle by running the dosages through the neural network now we can plug in the current value for w sub 3 and the x-axis coordinates for the activation function in the top node x sub 1 comma i lastly we plug in the input values then we do the math and get 0.76 then we solve for all of the other derivatives and then we use each derivative to calculate a step size and a new value then we update the parameters in the neural network and repeat until the predictions no longer improve very much or meet some other criteria now let's check out a fancy animation that shows the gradient descent in action these gray dots represent the data that we are using to train the neural network and the orange and blue curves represent the orange and blue curves and the green squiggle which fits the data so poorly that it's going off the screen represents the sum of the orange and blue curves plus b sub 3. now watch how gradient descent fits the green squiggle to the data after 450 steps we went totally bonkers with the chain rule and gradient descent and we finally have a neural network that fits a green squiggle to the data triple bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
iyn2zdALii8,2020-11-02T05:00:06.000000,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,the sun is out and it's nice outside it's the perfect weather for statquest yeah hello i'm josh starmer and welcome to statquest today we're going to talk about back propagation details part 1. note this stat quest assumes that you have already watched neural networks part 2 back propagation main ideas if not check out the quest the link is in the description below in back propagation main ideas we had this super simple data set that showed whether or not different drug dosages were effective against a virus then we had this simple neural network that already had optimal values for all of the parameters except for the last bias term b sub 3. then using everything in the neural network except for the last bias b sub 3 we drew this green squiggle then we demonstrated the main ideas behind back propagation by optimizing b sub 3. we first used the chain rule to calculate the derivative of the sum of the squared residuals with respect to the unknown parameter which in this case was b sub 3. then we initialized the unknown parameter with a number and in this case we set b sub 3 equal to 0 and used gradient descent to optimize the unknown parameter hooray we can optimize the last bias term b sub 3. now let's pretend we don't know b sub 3's optimal value and start working our way backwards so that along with b sub 3 we optimize the last two weights w sub 3 and w sub 4. note the goal of this quest is to learn how the chain rule and gradient ascent applies to multiple parameters and to introduce some fancy notation in the next part we'll go completely bonkers with the chain rule and learn how to optimize all seven parameters in this neural network simultaneously bam so let's go back to not knowing the optimal values for w sub 3 w sub 4 and b sub 3 and just like before we'll assume that the other weights and biases are already optimized the first thing we do is initialize the weights w sub 3 and w sub 4 with random starting values and in this example that means we randomly select two values from a standard normal distribution then we initialize the last bias b sub 3 to zero because bias terms frequently start at zero now if we run dosages from 0 to 1 through the connection to the top node in the hidden layer then just like before we get the corresponding y-axis coordinates and this blue curve now we multiply the y-axis coordinates on the blue curve by w sub 3 which starts out with the random value 0.36 and we get this new blue curve now if we run dosages from 0 to 1 through the connection to the bottom node in the hidden layer then just like before we get the corresponding y-axis coordinates for this orange curve now we multiply the y-axis coordinates on the orange curve by w sub 4 which starts with the random value 0.63 and we get this new orange curve now we add the blue and orange curves together and get this green squiggle lastly since the initial value for b sub 3 is 0 adding it to the y axis values on the green squiggle does not change anything in other words given the current parameters for this neural network some of which are optimal and some of which are not optimal we end up with this green squiggle now just like before we can quantify how well the green squiggle fits the data by calculating the sum of the squared residuals and we get the sum of the squared residuals equals 1.4 now even though we have not yet optimized w sub 3 and w sub 4 we can still plot the sum of the squared residuals with respect to b sub 3. and just like before if we change b sub 3 then we will change the sum of the squared residuals and that means just like before we can optimize b sub 3 by finding the derivative of the sum of the squared residuals with respect to b sub 3 and plugging the derivative into the gradient descent algorithm to find the optimal value for b sub 3. and just like before because the predicted values in the sum of the squared residuals come from the green squiggle and the green squiggle is the sum of the blue and orange curves plus b sub 3 then the sum of the squared residuals are linked to b sub 3 by the predicted values so by the chain rule the derivative of the sum of the squared residuals with respect to b sub 3 is the derivative of the sum of the squared residuals with respect to the predicted values times the derivative of the predicted values with respect to b sub 3. note this is the exact same derivative that we calculated in back propagation main ideas the point of this is that even though we are now optimizing more than one parameter the derivatives that we have already calculated with respect to the sum of the squared residuals do not change bam now let's talk about how to calculate the derivatives of the sum of the squared residuals with respect to the weights w sub 3 and w sub 4. unfortunately before we can do that we have to introduce some fancy notation first let's remember that the i in this summation notation is an index for the data in the data set for example when i equals one we are talking about observed sub one which is zero and we are talking about predicted sub 1 which is 0.72 however we can also talk about dosage sub i and when i equals 1 we are talking about dosage sub 1 which is 0. when i equals 2 we're talking about dosage sub 2 which is 0.5 and when i equals 3 we're talking about dosage sub 3 which is 1 and because dosage sub i is the input value we call it input sub i and that means this connection multiplies input sub i by weight w sub 1 which is 3.34 and it adds bias sub 1 which is negative 1.43 to get an x-axis coordinate for the activation function in the top node in the hidden layer meanwhile the other connection multiplies input sub i by weight w sub 2 which is negative 3.53 and adds bias b sub 2 which is 0.57 to get an x-axis coordinate for the activation function in the bottom node in the hidden layer so we have two different x-axis coordinates for input sub i in order to keep track of things let's call this x axis coordinate x sub 1 comma i where the one in one comma i refers to the activation function in the top node and the i in one comma i tells us that it corresponds to input sub i likewise let's call this x axis coordinate x sub 2 comma i where the 2 in 2 comma i refers to the activation function in the bottom node and the i in 2 comma i tells us that it corresponds to input sub i for example if i equals 3 then we're talking about the third dosage dosage sub 3 and that means we're talking about input sub 3 which is 1 the maximum dosage and that means the x-axis coordinate for the activation function in the top node x sub 1 comma 3 is equal to 1.91 and the x-axis coordinate for the activation function in the bottom node x sub 2 comma 3 is equal to negative 2.96 bam if we plugged in all values for i into dosage sub i we get x sub 1 comma i values in this red box and x sub 2 comma i values in this red box now in order to get the y axis coordinates for the activation function in the top node we plug x sub 1 comma i into the activation function which in this example is the soft plus function and that gives us y sub 1 comma i just like before the one in one comma i tells us that we are talking about the activation function in the top node and the i tells us which dosage we are talking about likewise in order to get the y-axis coordinates for the activation function in the bottom node we plug x sub 2 comma i into the activation function and that gives us y sub 2 comma i bam now that we understand the fancy notation we can talk about how to calculate the derivatives of the sum of the squared residuals with respect to the weights w sub 3 and w sub 4. first remember that y sub 1 comma i represents the y axis coordinates for the top activation function and they form this initial blue curve however we get the final blue curve by multiplying the y axis coordinates y sub 1 comma i by w sub 3. and that means we can plug y sub 1 comma i times w sub 3 into the equation for the predicted values likewise w sub 4 multiplies the y axis coordinates y sub 2 comma i from the bottom activation function to create the final orange curve and that means we can plug y sub 2 comma i times w sub 4 into the equation for the predicted values now since this sum creates the green squiggle and the green squiggle gives us predictions that we evaluate with the sum of the squared residuals then the sum of the squared residuals are linked to w sub 3 and w sub 4 by the predicted values that means we can use the chain rule to determine the derivative of the sum of the squared residuals with respect to w sub 3 and with respect to w sub 4. the chain rule says that the derivative of the sum of the squared residuals with respect to w sub 3 is the derivative of the sum of the squared residuals with respect to the predicted values times the derivative of the predicted values with respect to w sub 3. likewise the derivative with respect to w sub 4 is the derivative of the sum of the squared residuals with respect to the predicted values times the derivative of the predicted values with respect to w sub 4. double bam not yet note in both cases the derivative of the sum of the squared residuals with respect to the predicted values is the exact same as the derivative used for b sub 3. just to remind you we start by substituting the sum of the squared residuals with its equation then we use the chain rule to move the square to the front and then we multiply that by the derivative of the stuff inside the parentheses with respect to the predicted values negative one lastly we simplify by multiplying two by negative one and this is the derivative of the sum of the squared residuals with respect to the predicted values so we just plug it in now to solve for the derivative of the predicted values with respect to w sub 3 we plug in the equation for the predicted values and the derivative of the first term with respect to w sub 3 is y sub 1 comma i and the derivatives of the other terms are both zero since they do not contain w sub 3 and we end up with just y sub 1 comma i so we multiply the derivative of the sum of the squared residuals with respect to the predicted values by y sub 1 comma i likewise the derivative of the predicted values with respect to w sub 4 is 0 for the first term plus y sub 2 comma i for the second term plus 0 for the third term which is just y sub 2 comma i so we multiply the derivative of the sum of the squared residuals with respect to the predicted values by y sub 2 comma i double bam now that we have the derivatives of the sum of the squared residuals with respect to w sub 3 w sub 4 and b sub 3 we can plug them into gradient descent to optimize w sub 3 w sub 4 and b sub 3. first we initialize w sub 3 and w sub 4 with random values and set b sub 3 equal to 0. now starting with the derivative of the sum of the squared residuals with respect to w sub 3 first we expand the summation then we plug in the observed values and plug in the predicted values from the green squiggle remember we get the predicted values on the green squiggle by running the dosages through the neural network now we plug in the y-axis coordinates for the activation function in the top node y sub 1 comma i lastly we do the math and get 2.58 likewise we calculate the derivative of the sum of the squared residuals with respect to w sub 4 and with respect to b sub 3. now we use the derivatives to calculate the new values for w sub 3 w sub 4 [Music] now we repeat that process until the predictions no longer improve very much or we reach a maximum number of steps or we meet some other criteria now let's check out a fancy animation that shows the gradient descent in action these gray dots represent the data that we are using to train the neural network and the orange and blue curves represent the orange and blue curves and the green squiggle represents the sum of the orange and blue curves plus b sub 3. now watch how the green squiggle fits the data after 175 steps in gradient descent bam so after a bunch of steps we see how gradient descent optimizes the parameters triple bam in the next stack quest we'll go totally bonkers with the chain rule and show how to optimize all of the parameters in a neural network simultaneously now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the stackquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stack quest if you like this stack quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
IN2XmBhILt4,2020-10-19T04:00:02.000000,Neural Networks Pt. 2: Backpropagation Main Ideas,"Backpropagation is a really big word, but it's not a really big deal. StatQuest! Hello! I'm Josh Starmer and welcome to StatQuest! Today we're going to talk about Neural Networks, Part 2: Backpropagation Main Ideas. Note: this StatQuest assumes that you are already familiar with neural networks, the chain rule, and gradient descent.  If not, check out the quests.  The links are in the description below. In the StatQuest on Neural Networks Part 1, Inside the Black Box, we started with a simple dataset that showed whether or not different drug dosages were effective against a virus. The low and high dosages were not effective, but the medium dosage was effective. Thenwe talked about how a neural network like this one fits a green squiggle to this data set. Remember, the neural network starts with identical activation functions, but, using different weights and biases on the connections, it flips and stretches the activation functions into new shapes, which are then added together to get a squiggle that is shifted to fit the data. However, we did not talk about how to estimate the weights and biases. So let's talk about how backpropagation optimizes the weights and biases in this, and other, neural networks. Note: backpropagation is relatively simple, but there are a ton of details, so I split it up into bite-sized pieces. In this part we talk about the main ideas of backpropagation. One: using the chain rule to calculate derivatives, and Two: plugging the derivatives into gradient descent to optimize parameters. In the next part we'll talk about how the chain rule and gradient descent apply to multiple parameters simultaneously, and introduce some fancy notation. Then we will go completely bonkers with the chain rule and show how to optimize all seven parameters simultaneously in this neural network. Bam! First, so we can be clear about which specific weights we are talking about, let's give each one a name: we have w1, w2, w3, and w4. And let's name each bias: b1, b2, and b3. Note: conceptually, backpropagation starts with the last parameter and works its way backwards to estimate all of the other parameters. However, we can discuss all of the main ideas behind a backpropagation by just estimating the last bias, b3. So, in order to start from the back, let's assume that we already have optimal values for all of the parametersexcept for the last bias term, b3. Note: throughout this, and the next StatQuests, I'll make the parameter values that have already been optimized green, and unoptimized parameters will be red. Also, note: to keep the math simple, let's assume dosages go from 0, for low, to 1, for high. Now, if we run dosages from 0 to 1 through the connection to the top node in the hidden layer, then we get the x-axis coordinates for the activation function, that are all inside this red boxand when we plug the x-axis coordinates into the activation function which, in this example, is the soft plus activation function, we get the corresponding y-axis coordinates, and this blue curve. Then we multiply the y-axis coordinates on the blue curve by negative 1.22 and we get the final blue curve. Bam! Now, if we run dosages from zero to one through the connection to the bottom node in the hidden layer, then we get x-axis coordinates inside this red box. Now we plug those x-axis coordinates into the activation function to get the corresponding y-axis coordinates for this orange curve. Now we multiply the y-axis coordinates on the orange curve by negative 2.3 and we end up with this final orange curve. Bam! Now we add the blue and orange curves together to get this green squiggle. Now we are ready to add the final bias, b3, to the green squiggle. Because we don't yet know the optimal value for b3, we have to give it an initial value,and because bias terms are frequently initialized to 0, we will set b3 equal to 0. Now, adding zero to all of the y-axis coordinates on the green squiggle leaves it right where it is. However, that means the green squiggle is pretty far from the data that we observed. We can quantify how good the green squiggle fits the data by calculating the sum of the squared residuals. A residual is the difference between the observed and predicted values. For example, this residual is the observed value, zero, minus the predicted value from the green squiggle, negative 2.6. This residual is the observed value, one, minus the predicted value from the green squiggle, negative 1.61. Lastly, this residual is the observed value, 0, minus the predicted value from the green squiggle, negative 2.61. Now we square each residual and add them all together to get 20.4 for the sum of the squared residuals. So when b3 equals 0, the sum of the squared residuals equals 20.4. And that corresponds to this location on this graph that has the sum of the squared residuals on the y -axis and the bias, b3, on the x-axis. Now, if we increase b3 to 1, then we would add one to the y-axis coordinates on the green squiggle and shift the green squiggle up one. And we end up with shorter residuals. When we do the math, the sum of the squared residuals equals 7.8,and that corresponds to this point on our graph. If we increase b3 to 2, then the sum of the squared residuals equals 1.11. And if we increase b3 to 3, then the sum of the squared residuals equals 0.46. And if we had time to plug in tons of values for b3, we would get this pink curve, and we could find the lowest point, which corresponds to the value for b3 that results in the lowest sum of the squared residuals, here. However, instead of plugging in tons of values to find the lowest point in the pink curve, we use gradient descent to find it relatively quickly. And that means we need to find the derivative of the sum of the squared residuals with respect to b3. Now, remember the sum of the squared residuals equals the first residual squared, plus all of the other squared residuals. Now, because this equation takes up a lot of space, we can make it smaller by using summation notation. The greek symbol sigma tells us to sum things together, and 'i' is an index for the observed and predicted values that starts at one. And the index goes from one to the number of values, 'n', which in this case is set to 3. So, when 'i' equals one, we're talking about the first residual. When 'i' equals two, we're talking about the second residual. And when 'i' equals three, we are talking about the third residual. Now let's talk a little bit more about the predicted values. Each predicted value comes from the green squiggle, and the green squiggle comes from the last part of the neural network. In other words, the green squiggle is the sum of the blue and orange curves, plus b3. Now remember, we want to use gradient descent to optimize b3, and that means we need to take the derivative of the sum of the squared residuals with respect to b3. And because the sum of the squared residuals are linked to b3 by the predicted values, we can use the chain rule to solve for the derivative of the sum of the squared residuals with respect to b3. The chain rule says that the derivative of the sum of the squared residuals with respect to b3 is the derivative of the sum of the squared residuals with respect to the predicted values, times the derivative of the predicted values with respect to b3. Now, before we calculate the derivative of the sum of the squared residuals with respect to the predicted values, let's clean up our workspace and move these equations out of the way. Now we can solve for the derivative of the sum of the squared residuals with respect to the predicted values by first substituting in the equation, and then use the chain rule to move the square to the front, and then we multiply that by the derivative of the stuff inside the parentheses with respect to the predicted values,negative one. Now we simplify by multiplying two by negative 1, and we have the derivative of the sum of the squared residuals with respect to the predicted values. So let's move that up here,and now we are done with the first part. Now let's solve for the second part: the derivative of the predicted values with respect to b3. We start by plugging in the equation for the predicted values. Remember, the blue and orange curves were created before we got to b3. So the derivative of the blue curve with respect to b3 is 0, because the blue curve is independent of b3. And the derivative of the orange curve with respect to b3 is also 0. Lastly, the derivative of b3, with respect to b3, is 1. Now we just add everything up, and the derivative of the predicted values with respect to b3, is one. So we multiply the derivative of the sum of the squared residuals with respect to the predicted values by 1. Note: this times 1 part in the equation doesn't do anything, but I'm leaving it in to remind us that the derivative of the sum of the squared residuals with respect to b3 consists of two parts: the derivative of the sum of the squared residuals with respect to the predicted values, and the derivative of the predicted values with respect to b3. Bam! And at long last we have the derivative of the sum of the squared residuals with respect to b3. And that means we can plug this derivative into gradient descent to find the optimal value for b3. So let's move this equation up and show how we can use this equation with gradient descent. Note: if you're not familiar with gradient descent, check out the quest the link is in the description below. Anyway, first, we expand the summation. Then, we plug in the observed values and the values predicted by the green squiggle. Remember, we get the predicted values on the green squiggle by running the dosages through the neural network. Now, we just do the math and get negative 15.7. And that corresponds to the slope for when b3 equals zero. Now we plug the slope into the gradient descent equation for step size, and, in this example, we'll set the learning rate to 0.1. And that means the step size is -1.57. Now we use the step size to calculate the new value for b3 by plugging in the current value for b3, zero, and the step size, -1.57. And the new value for b3 is 1.57. Changing b3 to 1.57 shifts the green squiggle up, and that shrinks the residuals. Now, plugging in the new predicted values and doing the math gives us -6.26, which corresponds to the slope when b3 equals 1.57. Then, we calculate the step size and the new value for b3, which is 2.19. Changing b3 to 2.19 shifts the green squiggle up further, and that shrinks the residuals even more. Now we just keep taking steps until the step size is close to zero. And because the step size is close to 0 when b3 equals 2.61, we decide that 2.61 is the optimal value for b3. Double bam! So, the main ideas for backpropagation are that, when a parameter is unknown, like b3, we use the chain rule to calculate the derivative of the sum of the squared residuals with respect to the unknown parameter, which in this case was b3. Then we initializethe unknown parameter with a number, and in this case we set b3 equal to zero, and used gradient descent to optimize the unknown parameter. Triple bam! In the next StatQuest we'll show how these ideas can be used to optimize all of the parameters in a neural network. Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StatQuest study guides at statquest.org. There's something for everyone. Hooray! We've made it to the end of another exciting StatQuest. If you like this StatQuest and want to see more, please subscribe.  And if you want to support StatQuest, consider contributing to my patreon campaign, becoming a channel member, buying one or two of my original songs, or a t-shirt or a hoodie, or just donate the links are in the description below.  Alright, until next time.  Quest on!"
SEwxvjfxxmE,2020-09-07T04:00:08.000000,What is AutoML? A conversation with Gnosis Data Analysis,this stat quest is sponsored by jad bio just add data and their automatic machine learning algorithms will do the rest of the work for you for more details follow the link in the pinned comment below let's learn about it right now we've got a special guest we're going to talk to him grace quest hello i'm josh starmer and welcome to statquest today we're talking to giannis a professor at the university of crete and the ceo of gnosis data analysis whose product is jab bio just add data bio a product that specializes in auto ml so hello giannis glad to be here all right thank you for joining me i really appreciate it um i'm really excited about this interview and i guess we're just going to dive right in with some questions what is auto ml well automl stands for automated machine learning and it is the effort to fully automate end to end the machine learning process now what this means in practice is that we try to build systems where you just throw your data in you click a button and get results as we say you declare what you want and the system knows how to compute it now that of course like sounds extremely ambitious so the term automl should be taken with a grain of salt uh we gradually try to automate bigger and bigger portions of machine learning but as of now there are absolutely no systems that can fully automate all types of machine learning from predictive modeling with let's say sequences to learning with graphs to causal discovery for example our system as you mentioned it's called the jud bio fully automates predictive modeling with tabular data but not unsupervised learning now some people confuse autumn ml with what is called technically cache standing for combined algorithm selection and hyper parameter optimization cache are algorithms for optimizing the combination of algorithms to uh use at each step of the analysis like transformations imputation of missing values feature selection modeling and the like and they're hyper parameters but in the end they just return a predictive model the most famous example of a cache library is the very successful admittedly auto psychic learn system but it only returns a predictive model however to me automl should be much much more it should not just return a model this is not all a user needs automl should return estimates of what a predictive performance to expect from the model it should perform feature selection help interpreting and explaining the final model help the user like make decisions put the model into operational environment monitor its execution raise alarm predictions starting to go off and uh much more it should provide everything a good analyst would provide to you where he or she to analyze your data that's what autumn mill means to me so i guess the question is uh if automl does everything who is it for is it for academics or industry people or is it for people that are already experts or is it just for beginners who are trying to learn machine learning or who is it for right i think the beauty of automl is that it should be for anyone and i mean anyone not just like computer coders from ceos who analyze their company data to a home dad who wants to analyze their expenses excel sheet automl should not require any coding math statistics or machine learning yeah odml should be democratic and bring machine learning to the masses my vision is to build optimal systems that follow the path of excel here's what i mean initially excel seemed to appeal only to accountants now everybody uses it from a teacher who's told student grades to a high level executive now uh it may seem at first that automl is meant for novices and non-experts but this is absolutely not true it really boosts the product activity of expert analysts too we have seen this with numerous clients of ours and in fact some of the people who best appreciate automl are experts who are fully aware of the difficulties and challenges of doing it yourself and can feel the pain and effort that autumnal takes away programmers used to program in assembly language decades ago another program in python which is much more automatic programs programmers benefited from automated uh programming languages and making programming languages higher level so i think it's the same thing with automl instead of scripting all the details of an analysis experts can now automate many parts of it and they can focus on other aspects of the analysis like data representation and results in presentation interpretation if you're new to automl what are the first things i need to know absolutely this is this is very important uh to know um that uni users need to be aware of certain things uh and namely correctness of performance estimation so let me explain what i mean uh when trying lots of analysis pipelines as auto mel uh does automatically uh meaning you try lots of different combinations of algorithms for each step of the analysis with different hyper parameter values and the like a statistical phenomenon occurs the technical uh term for this phenomenon is uh multiple comparisons problem in induction algorithms okay this is not the best one it's also related to what is called the winner's cursing statistic uh in statistics and it basically goes like this say you try 1000 analysis pipelines that lead to 1000 models and the best one has cross-validated accuracy of say 90 and even though this is cross-validated because you have chosen it among 1000 models it is an overestimation on average so when you cross-validate one algorithm you get the right performance estimate when you cross-validate 1000 algorithms or pipelines and then you choose the best you overestimate performances and for small sample sizes this or imbalanced data very imbalanced data this overestimation is simply unacceptable it's not negligible and we show this in several scientific papers we have published now you could leave a holdout set to estimate the final performance but then you lose these samples to estimation you don't not uh don't learn from these samples and so if you don't have that many samples this is also unacceptable so you need some statistical procedures to remove the optimism uh from your estimations just like in statistics where we have a multiple testing problem uh you have an exact model fitting problem right um yeah so exactly exactly this is the right analogy is the conceptual equivalent of multiple statistical multiple hypothesis testing and statistics um and and it sounds like you guys have at least at least jad bio has a way of compensating for this effect sort of like the bone ferrone correction or false discovery rate you guys have a way of adjusting for that which sounds fantastic yes how does auto ml compare to manually optimizing a bunch of models is it better to do it by hand or is it or is it is it better to use automl right that that that's a good question and and a lot of people are actually asking this uh and we hear about it i mean they a lot of people are thinking like okay sure you cannot donate analysis but i can i bet i can do a better job you know that's the that's the attitude we encounter a lot of times uh which is understandable of course now it's difficult to prove or disprove like scientifically uh this statement i mean doesn't matter like uh do a better job than human experts because which human expert do you actually compare against uh so it would take very big studies with lots of experts with different levels of expertise to get sound results and then once you do that autumnal will have improved uh since they are proving constantly and any such study will be obsolete so uh you know without without like scientific evidence i can at least tell you my experience one like uh story i have for you is we recently participated in an fda prediction challenge to predict the survival time of break cancer brain cancer patients so we downloaded the data uploaded them to job bio clicked and got results with within a few minutes of manual effort at least and we got third place out of 30 participating groups now the winner submitted a model that had just uh one uh point of c index uh higher than ours on the holdout set oh uh just to recall remind like uh recall what the c index is is it's similar to the auc metric for measuring performance for survival analysis tasks so the difference was uh not that big and it was within the confidence intervals within the uncertainty of the estimation so it may have just been uh luck uh now i don't know how long the winning team spent on the challenge but i bet it was measured in the hundreds of hours instead of like uh uh minutes as with manual effort yeah um so uh another major difference with the winner of that challenge is that judd buyer estimated the c index that we would expect from the training set to test it uh to within like uh almost like um uh one decimal point let's see now instead the winning team estimated their performance to be 100 100 percent and they actually got 74 of c index or 0.74 so this is very misleading to the user they overestimated their ability to make predictions now having said that many analysis problems do require an expert to pre-process data clean the data represent them in a suitable format formulate a problem as a machine learning task and make it ready for automl so that's where the human is still unbeatable but once the data is ready be aware of the otml will automl replace data science jobs or just make those jobs more interesting what do you think well when something like this did ever happen before i mean never you go from assembly language to use my example before to python and instead of firing programmers now global economy needs more and more programmers yeah uh so right now only about five percent uh five percent of the global data is actually ever analyzed and what i think is going to happen is that the demand for analysis will increase with automl because people businesses scientists will be able to unlock the value of the data that they're sitting on and they will be hungry for more uh so automl will boost productivity but at least for the foreseeable future it will not eliminate the need for an expert you will still need the expert to represent and formulate the problem as machine learning interpret the results uh apply the model correctly and and this type of stuff now the difference however is that the experts will have to be more specialized more educated sophisticated than before right now people perform a cross-validation of a random forest in python and they feel entitled to write data scientists in their resume so these simple tasks i think will be automated and performed with much higher quality uh no errors no overfitting by automl so the next generation of data scientists will have to add more value than that to the whole process to justify their salaries what do you think is the future for automl well a very very bright one i think i believe that the era of manual scripting for machine learning is reaching critical point it is changing it is evolving um i predict that within five years at the most most data analysis will be completely performed by all the ml systems or at the very least there will be an automated system like an automl system uh as a major part of the analysis uh now experts may still be scripting but the scripts will be calling automl systems and configuring automl systems and customizing them instead of uh being from scratch so it will be scripting on a higher level i think there are great opportunities uh business wise for startups like ours and investments in the space and i think we'll see ultimate systems with different uh specializations like chad bio now is a quite general tool for everybody but caters more to bioinformatics data as an example uh and initially i think there will be many many ultimate systems but eventually they will gradually consolidate to a few survival overs and uh i think um i also believe lots of different types of companies will need to have an automl internally embedded in their products in order to survive for example database vendors data ranking companies cloud providers and so forth will need to own an automl system to convert automatically the data stored by their users to actionable knowledge i also believe we'll see automl systems embedded within other software too and we may not even know it for example uh we we will have like automl within laptops cars and other devices to keep analyzing the data from our behavior and our usage of these devices and our interaction with the environment of these systems to make them smarter so that's i think about the future of ml well it sounds very bright and uh once again giannis i want to thank you for participating in this conversation with me i learned a lot about automl that i did not know before so it's my pleasure i i really appreciate it and um so thank you very much and until next time quest on
CqOfi41LfDw,2020-08-31T04:00:03.000000,The Essential Main Ideas of Neural Networks,"Neural networks... seem so complicated, but they're not! StatQuest! Hello! I'm Josh Starmer and welcome to StatQuest! Today, we're going to talk about neural networks, part one: inside the black box! Neural networks, one of the most popular algorithms in machine learning, cover a broad range of concepts and techniques. however, people call them a black box because it can be hard to understand what they're doing. the goal of this series is to take a peek into the black box by breaking down each concept and technique into its components and walking through how they fit together, step by step. in this first part, we will learn about what neural networks do, and how they do it. in part two, we'll talk about how neural networks are fit to data with backpropagation. then, we will talk about variations on the simple neural network presented in this part, including deep learning. note: crazy awesome news! i have a new way to think about neural networks that will help beginners and seasoned experts alike gain a deep insight into what neural networks do. for example, most tutorials use cool looking, but hard to understand graphs, and fancy mathematical notation to represent neural networks. in contrast, i'm going to label every little thing on the neural network to make it easy to keep track of the details. and the math will be as simple as possible, while still being true to the algorithm. these differences will help you develop a deep understanding of what neural networks actually do. so, with that said, let's imagine we tested a drug that was designed to treat an illness and we gave the drug to three different groups of people, with three different dosages: low, medium, and high. the low dosages were not effective so we set them to zero on this graph. in contrast, the medium dosages were effective so we set them to one. and the high dosages were not effective, so those are set to zero. now that we have this data, we would like to use it to predict whether or not a future dosage will be effective. however we can't just fit a straight line to the data to make predictions, because no matter how we rotate the straight line, it can only accurately predict two of the three dosages. the good news is that a neural network can fit a squiggle to the data. the green squiggle is close to zero for low dosages, close to one for medium dosages, and close to zero for high dosages. and even if we have a really complicated dataset like this, a neural network can fit a squiggle to it. in this StatQuest we're going to use this super simple dataset and show how this neural network creates this green squiggle. but first, let's just talk about what a neural network is. a neural network consists of nodes and connections between the nodes. note: the numbers along each connection represent parameter values that were estimated when this neural network was fit to the data. for now, just know that these parameter estimates are analogous to the slope and intercept values that we solve for when we fit a straight line to data. likewise, a neural network starts out with unknown parameter values that are estimated when we fit the neural network to a dataset using a method called backpropagation. and we will talk about how backpropagation estimates these parameters in part 2 in this series. but, for now, just assume that we've already fit this neural network to this specific dataset, and that means we have already estimated these parameters. also, you may have noticed that some of the nodes have curved lines inside of them. these bent or curved lines are the building blocks for fitting a squiggle to data. the goal of this StatQuest is to show you how these identical curves can be reshaped by the parameter values and then added together to get a green squiggle that fits the data. note: there are many common bent or curved lines that we can choose for a neural network. this specific curved line is called soft plus, which sounds like a brand of toilet paper. alternatively, we could use this bent line, called ReLU, which is short for rectified linear unit, and sounds like a robot. or, we could use a sigmoid shape, or any other bent or curved line. oh no! it's the dreaded terminology alert! the curved or bent lines are called activation functions. when you build a neural network you have to decide which activation function, or functions, you want to use. when most people teach neural networks they use the sigmoid activation function. however, in practice, it is much more common to use the ReLU activation function, or the soft plus activation function. so we'll use the soft plus activation function in this StatQuest. anyway, we'll talk more about how you choose activation functions later in this series. note: this specific neural network is about as simple as they get. it only has one input node, where we plug in the dosage, only one output node to tell us the predicted effectiveness, and only two nodes between the input and output nodes. however, in practice, neural networks are usually much fancier and have more than one input node, more than one output node, different layers of nodes between the input and output nodes, and a spider web of connections between each layer of nodes. oh no! it's another terminology alert! these layers of nodes between the input and output nodes are called hidden layers. when you build a neural network one of the first things you do is decide how many hidden layers you want and how many nodes go into each hidden layer. although there are rules of thumb for making decisions about the hidden layers, you essentially make a guess and see how well the neural network performs, adding more layers and nodes if needed. now, even though this neural network looks fancy, it is still made from the same parts used in this simple neural network, which has only one hidden layer with two nodes. so let's learn how this neural network creates new shapes from the curved or bent lines in the hidden layer, and then adds them together to get a green squiggle that fits the data. note: to keep the math simple, let's assume dosages go from zero, for low, to one, for high. the first thing we are going to do is plug the lowest dosage, zero, into the neural network. now, to get from the input node to the top node in the hidden layer, this connection multiplies the dosage by negative 34.4 and then adds 2.14, and the result is an x-axis coordinate for the activation function. for example, the lowest dosage 0 is multiplied by negative 34.4, and then we add 2.14, to get 2.14 as the x-axis coordinate for the activation function. to get the corresponding y-axis value we plug 2.14 into the activation function, which in this case is the soft plus function. note: if we had chosen the sigmoid curve for the activation function then we would plug 2.14 into the equation for the sigmoid curve. and if we had chosen the ReLU bent line for the activation function, then we would plug 2.14 into the ReLU equation. but, since we are using soft plus for the activation function, we plug 2.14 into the soft plus equation. and the log of one plus e raised to the 2.14 power is 2.25. note: in statistics, machine learning, and most programming languages, the log function implies the natural log, or the log base e. anyway, the y-axis coordinate for the activation function is 2.25, so let's extend this y-axis up a little bit and put a blue dot at 2.25 for when dosage equals zero. now, if we increase the dosage a little bit and plug 0.1 into the input, the x-axis coordinate for the activation function is negative 1.3, and the corresponding y-axis value is 0.24. so, let's put a blue dot at 0.24 for when dosage equals 0.1. and, if we continue to increase the dosage values all the way to 1, the maximum dosage, we get this blue curve. note: before we move on I want to point out that the full range of dosage values, from 0 to 1, corresponds to this relatively narrow range of values from the activation function. in other words, when we plug dosage values, from 0 to 1, into the neural network, and then multiply them by negative 34.4 and add 2.14, we only get x-axis coordinates that are within the red box. and thus, only the corresponding y-axis values in the red box are used to make this new blue curve. bam! now we scale the y-axis values for the blue curve by negative 1.3. for example, when dosage equals zero the current y-axis coordinate for the blue curve is 2.25, so we multiply 2.25 by negative 1.3 and get negative 2.93. and negative 2.93 corresponds to this position on the y-axis. likewise, we multiply all of the other y-axis coordinates on the blue curve by negative 1.3 and we end up with a new blue curve. bam! now, let's focus on the connection from the input node, to the bottom node in the hidden layer. however, this time, we multiply the dosage by negative 2.52, instead of negative 34.4, and we add 1.29, instead of 2.14, to get the x-axis coordinate for the activation function. remember, these values come from fitting the neural network to the data with backpropagation, and we'll talk about that in part two in this series. now, if we plug the lowest dosage, zero, into the neural network, then the x-axis coordinate for the activation function is 1.29. now we plug 1.29 into the activation function to get the corresponding y-axis value, and get 1.53. and that corresponds to this yellow dot. now, we just plug in dosage values from 0 to 1 to get the corresponding y-axis values, and we get this orange curve. note: just like before, i want to point out that the full range of dosage values, from 0 to 1, corresponds to this narrow range of values from the activation function. in other words, when we plug dosage values from 0 to 1 into the neural network we only get x-axis coordinates that are within the red box. and thus, only the corresponding y-axis values in the red box are used to make this new orange curve. so we see that fitting a neural network to data gives us different parameter estimates on the connections and that results in each node in the hidden layer using different portions of the activation functions to create these new and exciting shapes. now, just like before, we scale the y-axis coordinates on the orange curve, only this time we scale by a positive number: 2.28. beep boop beep! for every number written on the screen] and that gives us this new orange curve. now the neural network tells us to add the y-axis coordinates from the blue curve to the orange curve, and that gives us this green squiggle. then, finally, we subtract 0.58 from the y-axis values on the green squiggle, and we have a green squiggle that fits the data. bam! now, if someone comes along and says that they are using dosage equal to 0.5 we can look at the corresponding y-axis coordinate on the green squiggle and see that the dosage will be effective. or, we can solve for the y-axis coordinate by plugging dosage equals 0.5 into the neural network, and do the math. [Music] and we see that the y-axis coordinate on the green squiggle is 1.03, and since 1.03 is closer to 1 than 0, we will conclude that a dosage equal to 0.5 is effective. double bam! now, if you've made it this far you may be wondering why this is called a neural network. instead of a big fancy squiggle fitting machine. the reason is that way back in the 1940s and 50s, when neural networks were invented, they thought the nodes were vaguely like neurons, and the connections between the nodes were sort of like synapses. however, i think they should be called big fancy squiggle fitting machines, because that's what they do. note: whether or not you call it a squiggle fitting machine, the parameters that we multiply are called weights, and the parameters that we add are called biases. note: this neural network starts with two identical activation functions, but the weights and biases on the connections slice them, flip them, and stretch them into new shapes, which are then added together to get a squiggle that is entirely new. and then the squiggle is shifted to fit the data. now, if we can create this green squiggle with just two nodes in a single hidden layer, just imagine what types of green squiggles we could fit with more hidden layers and more nodes in each hidden layer. in theory, neural networks can fit a green squiggle to just about any dataset, no matter how complicated, and i think that's pretty cool. triple bam! now it's time for some shameless self-promotion! if you want to review statistics and machine learning offline check out the StatQuest study guides at statquest.org. there's something for everyone. hooray! we've made it to the end of another exciting StatQuest. if you like this StatQuest and want to see more, please subscribe. and if you want to support StatQuest consider contributing to my patreon campaign, becoming a channel member, buying one or two of my original songs, or a t-shirt, or a hoodie, or just donate. the links are in the description below. alright, until next time. quest on!"
GrJP9FLV3FE,2020-08-01T18:13:06.000000,XGBoost in Python from Start to Finish,xg boost is extreme but so is this webinar it's totally extreme yes steadquest [Music] hooray i'm josh starmer and welcome to the stat quest webinar on xgboost in python from start to finish this is the jupiter notebook that we're going to go through today we're going to use xgboost to build a collection of boosted trees one of which is illustrated below so this guy right here and use continuous and categorical data from the ibm base samples website to predict whether or not a customer will stop using a company's service in business lingo this is called customer churn you can download the telco churn data set or use the file provided with the jupyter notebook and if you want to learn more about the telco churn data set you can click on this link in the jupyter notebook it's live or if you just want to learn more about the base samples there's all kinds of data sets here that you could use for other experiments in machine learning so it's a it's a great resource for just testing out these models in general all right xgboost is an exceptionally useful machine learning method when you don't want to sacrifice the ability to correctly classify observations but you still want a model that's fairly easy to understand and interpret in this lesson you will learn about importing data from a file missing data including dealing with missing data xg boost style which is relatively unique and then we're going to format data for xg boost including using one hot encoding and actually because of the way xgboost uses handles missing data we're going to have a mini stack quest in the middle of this webinar to explain sort of the specifics of one hot encoding and how it relates to how missing data is handled after that we're going to build a preliminary xg boost model and then we're going to optimize parameters with cross validation and grid search and then lastly we're going to draw validate and build obviously and and then we're going to interpret eval and evaluate the optimized xg boost model note this tutorial already assumes that you know the basics of python and are familiar with the theory behind xgboost cross validation and confusion matrices if not check out the stat quest by clicking on the links for each topic also note i strongly encourage you to play around with the code playing with the code is the best way to learn from it so the very first thing we do is load in a bunch of python modules python itself just gives us a basic programming language these modules give us extra functionality to import the data clean it up and format it and then build evaluate and draw the xgb boost model note you'll need python 3 and at least these versions of the versions of the following modules pandas numpy scikit-learn and xg boost i've got instructions on how to install all this stuff right here and also instructions on if you want to actually draw that beautiful tree i've got instructions on how to install graphviz as well to make it all work out all right so this is where we're going to load in the data since this is a jupiter notebook we can load uh we can run the the python code by either clicking on the play button uh or we can select run selected cells or we can select this keyboard combination um to run the cells note i'm using a macintosh so that's my keyboard combination on your computer the keyboard combination may be different so i'm going to run this and we get a number over here that tells us that we've run the code so that's great if if by the way if python is still cranking away if you're doing something a little more complicated than loading in some modules you'll see a star over here while python is cranking away but we just get a number so that's good okay now we're ready to import the data we're going to load in a data set from the ibm base samples specifically we're going to use the telco churn data set which can allows us to predict if someone's going to stop using telco services or not using a variety of continuous and categorical data types note when pandas reads in data it returns a data frame which is a lot like a spreadsheet data are organized in rows and columns and each row can contain a mixture of text and numbers the standard variable name for a data frame is the initials df and that's what we're going to use here so what we're doing is we're creating a new data frame that we're calling df and we're setting it to the output of this pandas function read csv so this is a csv file that we're loading in um so let's do that bam now that we've loaded in the data into a data frame called df we're going to look at the first five rows using the head function so this is our data frame df and there's an associated function called head that we're going to run and what it does is it prints out the first five rows so it says five rows by 33 columns and we scroll over we see all these columns not all of them have been printed to the screen i don't know if you can see there's a dot dot dot between gender and contract and that just means that even though there's 33 columns we didn't print all of them to the screen um note the last four columns churn reason cltv churn score um these are um what are these these are sort of uh exit interview uh data uh that were collected from people that left telco and only people that left telco provided answers here so we don't want to use this data in our prediction because generally speaking someone's not going to do the exit interview before they leave the company um and so since these things will give us perfect uh prediction ability perfect predictive abilities we want to remove them from the data set um so we do that using the drop function so we've got data frame this is our data frame and we're going to use the drop function and then we list the columns that we want to drop we set access equals to 1 to specify that we're dropping columns instead of rows and we set in place to true which means we want to modify df this data frame called df directly we're not making a copy uh that that of of the data frame and only in the copy uh is that or uh containing these or without these columns it has these columns dropped and then after what we're after that what we're going to do is we're going to print the first five rows again just to verify that we did everything correctly so let's do that okay so when we scroll over to the right we see that churn reason and cltv those things are gone some of the other columns in this data set only contain a single value and will not be useful for classification for example this column count we just see a bunch of ones we can verify that the only value in this column is one by with our data frame we specify we want to look at the count column and then we call the unique function to print out all the unique values in that column and when we run that we see that the only unique value is one so this column count only contains one and that makes it useless for making predictions likewise if we look at the unique values in country just looking at it we see united states a bunch of times and if we print out the unique values in the country column we see that the only value is united states similarly in the state column we see a bunch of entries for california let's print out all the unique values in state again we just see one value california so that means we can omit count country and state from the analysis because they're not going to help with predictions in contrast city a column called city contains a bunch of different city names so let's look at that so yeah we see los angeles beverly hills huntington park standish all kinds of stuff so we're going to leave city in because city may help us with making predictions however we're also going to remove customer id and we're going to remove this we're going to remove customer id because there's a different value for every single person so that's not going to be very helpful for predictions and also there's a column called lat long which contains both latitude and longitude of of the resident of of that was a customer but we also have separate columns for latitude and longitude so we don't need the column that merges those two things so um so we're gonna uh we're gonna drop those columns as well just like we did before so we've got our data frame dot drop so we're using the drop function then we pass in array of the columns that we want to drop specify that we're dropping columns instead of rows and again just like before we're using in place set to true so that we modify the data frame df itself and as always we're going to print out the first five rows to make sure we did everything correctly hooray looks like we did it just right okay now we're down to just 24 columns note although it's okay to have white space in city names uh so you see we've got we've got los angeles and there's a there's a blank space between loss and angelus and you see that up here um when we printed out the unique values for each city name we've got a blank space between beverly and hills and a blank space between huntington and park those blank spaces are perfectly okay for xg boost xg boost doesn't care partially because we're going to use one hot encoding for this however we can't have any white space or blank space if we want to draw the actual tree like we did all the way up here if we want to draw this tree later on at the very end we need to remove the white spaces um so we're going to do that since we know there's a lot of white spaces in the city column we specify we've got our data frame and we specify that we were interested in the city column and then we can use the replace function and this is a just a search and replace uh function like you'd find anywhere else we're searching for blank spaces and we're going to replace it each blank space with an underscore we set regex to true that's regex is short for regular expression and if you're not familiar with the term regex or regular expressions don't worry just think of it as advanced features for a search and replace function and again just like we've been doing before we're going to make these modifications in place so we're going to modify df directly and then print out the first five rows to make sure we did it right there we go and we see an underscore between los angeles and we can also print out the first 10 um city names uh the first 10 unique city names and verify that we've got underscores between beverly and hills uh between huntington and parked between marina and dell and dell and ray so we've we've eliminated those white spaces also we need to eliminate the white spaces in the column name so we're going to replace those with underscores as well we do this slightly differently so we've got our data frame df and we specified that we're interested in the columns not in a specific column but the column names and what we do is we convert those column names to strings and then we call the replace function and again this is just a search and replace we're searching for white space and replacing with underscore and just like we've done before we're going to print out the first five rows bam so now we see an underscore between zip and code senior and citizen tenure and months etc etc hooray we've removed all of the data that will not help us to create an effective xg boost model and we've reformatted the column names and city names so that we can now draw a tree okay so now we're ready to identify and deal with missing data unfortunately the biggest part of any data analysis project is making sure that the data are correctly formatted and fixing it when it's not the first part of this process is identifying missing data missing data is simply a blank space or a surrogate value like n a that indicates that we failed to collect one or more features we failed to collect data for one or more features for example if we forgot to ask someone's age or forgot to write it down then we would have a blank space in this data set next to that person's age one thing that's relatively unique about xgboos is that it has a default behavior for missing data it knows how to handle missing data it's expecting it so all we have to do is uh is identify the missing values and make sure they are set to zero now um in the webinar that i've given a couple times now uh this has confused a lot of people because what happens if there is a zero already in your data set um i'm going to have a little mini mini stat quest about halfway through that shows how we can have zeros that code for things as well as using xero to code for missing data and it actually works out and even in situations i'm just going to show you one scenario and even if there's some scenario where it does not work out the way i'm demonstrating i'm i will throw this out as well the author of xgboost has said that even when you code something with xero and you use zero to mean missing data xgboost still does a great job it doesn't so for some reason that doesn't really interfere with how well it performs um so the first thing we're going to do is we're going to see what sort of data is in each column this is what i always do when i'm looking for missing data i print out the data types for each column because that can tell us if something is messed up or not we see that a lot of the columns are object and that's okay because above here when we uh when we did head we saw that senior citizen has a bunch of no's and it probably has some yeses in there partner has yes's and no's dependence has yes's and no's so it makes sense that a lot of these columns are object um because they have text responses like yes and no however we should always verify that we're getting what we expect in each column so for example we'll look at the phone service column and we'll use our handy unique function to check to see what we're getting and what we get is yes and no and that's perfect because that means there's not question marks in this column there's not an n a place holder for a missing data so so we can verify that this column only contains yeses and no's and that's great okay now in practice we should do that for every single column verify that it has uh the type of data that we're expecting and it only has the responses that we're expecting and trust me i did this but right now we will focus on one specific column that looks like it could be a problem and that's total charges so if we looked at this output over here we see that total charges looks like a bunch of numbers however if we look at the data type for total charges we see that it's an object and that we usually get that object when we get a mixture of numbers and and characters so one thing we can do is just print out the unique values and total charges and kind of see what we what we see so let's do that bam and when we do that we see that there are too many values to print we've got this dot dot dot right in the middle um but what we see looks like a bunch of numbers however if we try to convert the column to numeric data or numeric values and i'll try that right here when we run this code we get an error so i'm going to comment it out just in case you want to run all the code all at once um and let's look at this error the the nice thing about this error and the reason why i wanted to show you this error is it actually tells you what's wrong um with the data it says unable to parse string quote nothing or blank space end quote and what does that tell us that tells us that there are blank spaces in this in this column um in total charges uh and so so we need to deal with those okay so now we're ready to deal with missing data xg boost style like i've said before one thing that's relatively unique about xgboost is that it determines default behavior for missing data so all we have to do is identify missing values and make sure they are set to zero however before we do that let's see how many rows are missing data and if it's a lot then we might have a problem on our hands that is bigger than what xg boost can do on its own and if it's not that many we'll just set them to zero so uh so we do this by um searching for we're using the location uh function so we've got our data frame and we use we say tell give me the rows where this is true so the value in the total charges column is equal to zero and then we're wrapping all of that in the len or length function and that counts the number of rows that have blank spaces in the total charges column and we see there's only 11 rows that have missing values so since it's only 11 we can look at them so we're going to print them out bam so we see if we go over here we see uh that in the total charges column uh we have uh no values uh we also see that in the tenure months column we've got zero for everybody and that means that the reason why these people have total charges equal to blank is that they have not been charged for anything yet they just subscribed and so they're on a plan they've got uh we've got an expected amount of money we're going to be getting from them but they have not paid us anything yet and since it's just a handful of people we're going to set these up these total uh um to um uh we're just gonna set total charges to zero and the way we do that is a lot like what we did before only this time we're specifying instead of having having uh loc return the entire row which is what it was doing before we're specifying that we just want the total charges and again we're interested in just the the the locations where total charges equals blank space and we're setting that value in the total charges column to zero so let's do that we can verify that we modified total charges correctly by looking at everyone had 10-year months set to zero note i'm not looking at total charges equal to zero because there could be other people that have not paid a dime even though they've been on board for a couple of months they they might have zeros there already um and so but i'm pretty certain that everyone who had tenure months equal to zero had a blank space because they just signed up and they hadn't had they hadn't paid a bill yet so we do that and we see that we've got the 10-year month's equal zero and when we scroll over to the right we've got total charges equal to zero so that worked bam we have verified that our data frame df contains zeros instead of spaces for missing values note total charges still has the object data type and that's no good because xgb boost only allows int float and boolean data types so we're going to fix this by converting that column with two numeric there are multiple ways to convert columns from one type to another this is just one of them so i'm using two numeric and uh it's a pandas function and i'm specifying that i want to convert the total charges column and i'm saving it in the original total charges slot and when i'm done i'm printing out the data types to verify that we've done it correctly so let's do that bam and we go down here and we see total charges is now float 64. so hooray now that we've dealt with the missing data what we're going to do is we're just going to replace all of the other white spaces in all of the columns with underscores there could be other columns that have white spaces and we're just going to do this all at once um so we're going to do it just like we did before we've got our data frame and we're using the replace function however this time we're not specifying a specific column we're just going to do this data frame wide we're replacing blank space with underscore and then we're going to print out the first five rows to verify that we did it correctly so there we go bam and if we scroll over to the right we see that one of the things that we fixed was under under the payment method column uh instead of blank spaces between mailed and check now we have a um an underscore an electronic in check we've got underscores so bam and remember just to clarify the only reason why we're replacing all these blank spaces is so that we can print out a nice pretty looking tree xjboot xgboost itself doesn't really care partially because we're going to one hot encode these things later anyways okay so now that we've dealt with all those issues uh we can now start formatting the data for an xg boost model and the first step is to break it into two parts the uh we want we want to separate the columns that we will use to make classifications from the column that we want to predict so we're going to use the conventional notation of capital x to represent the columns of data that we will use to make classifications and we're going to use lowercase y to represent the thing that we want to predict in this case we want to predict churn value let's look at this this is one for people that left and it's zero for people that did not leave um and so what the what we're doing is we're creating uppercase x here and we've got our data frame and we're just like before we're calling the drop function and we're specifying um that we want to drop churn value however we're not doing in place like we did before and we're saving the results in a new variable and we're going to print out the first five rows of that new variable so there we go and if we scroll over we see that churn value is missing we've got everything else which is great that's exactly what we wanted now we're going to make this lowercase y variable and it's going to be just the column in data frame called churn underscore value and then we're going to print out the first five rows to verify that it looks the way it should and there it is bam now that we've created capital x which has the data that we want to use to make predictions and we've made lowercase y which has the data that we want to predict we're ready to continue formatting x or capital x so that it is suitable for making a model with xg boost so that brings us to one hot encoding okay now that we've split the data frame up into two pieces we need to take a closer look at the variables within capital x so the list below which i got from the um the ibm website for this data set tells us whether or not each column should be afloat or categorical so we see city is a category a longitude is float gender is a category senior citizen is a category we've got a bunch of categories tenure months is a float so we've got lots of columns now just to review we're going to look at the data types in x to remember how python is seeing the data right now so we see that latitude longitude monthly charges and total charges those are all float64 which is great that's exactly what we want however all of the other columns that are object those need to one we need to inspect those to make sure each one only contains a reasonable value and then we also need to modify them with one hot encoding one hot encoding is a trick for taking um taking data that is categorical and splitting it up into a format that xg boosts and a lot of other algorithms can use so the problem is is that xgboost and a lot of other machine learning algorithms they natively support continuous data like monthly charges and total charges but they do not natively support categorical data like phone services which has two different categories so if we want to use categorical data with our model we have to uh convert it with one hot encoding to kind of get around this limitation okay so at this point you may be wondering what's wrong with treating categorical data like continuous data can't we can can't we just can convert the categories to numbers and be done with it and to answer this question uh we're going to look at an example and i've chosen payment method uh to be that example we see it has a bunch of a bunch of options we've got mailed check we've got electronic check we've got bank transfer and we've got credit card if we converted those categories to numbers one two three and four and treated them like continuous data then we would assume that four which means credit card is more similar to three which means a bank transfer than it is to one or two which are other forms of payment that means the extra xg boost tree would be more likely to cluster the people with fours and threes together uh than fours with ones in contrast uh if we treat these payment methods like categorical data then we will treat each one as a separate category and that is no more or less similar to any other category thus the likelihood of clustering people who pay by mailed check with people who pay by electronic check is the same as clustering with mail check and credit card so this approach seems more reasonable to me note there are there are many different ways to do one hot encoding in python there are two main popular ways and i describe the pros and cons of these two approaches in the jupiter notebook in this paragraph however for the purpose of this webinar we're going to use git dummies because i feel like it's the best way to teach what one hot encoding does so what we're going to do is we're going to use this pandif function so pd is short for pandas we're going to use this pandas function called git dummies and we pass in the data frame that we're interested in processing and the columns that we want to one hot encode now i'm not saving the results i just want to see what happens so i'm just going to print out the first five rows to show you how this column payment method is is one hot encoded so we run this code and we and we see that all the columns that we did not modify are on the left side of the data frame but if we scroll to the right we see payment month payment method bank transfer so we've got that column we've got another column for payment method credit card another column for payment method electronic check and another column for payment method mail to check and in each column we've got a one if for in the in the bank transfer column we've got a one if they use bank transfer and a zero for any other option for credit card we've got a one in that column and a zero for any other option for electronic check we've got a one in that column and zero for any other option and lastly for male check we've got a one in that column and zero for any others note if you're familiar with linear regression or logistic regression if you're not familiar with linear regression or logistic regression don't worry about what i'm about to say but if you are familiar with those two things one hot encoding is different from the way you would encode it for the same data the one hot encoding gives us a result that is different from what we would do for linear and logistic regression so just keep in mind that one hot encoding is is is not for linear and logistic regression but it works great for um so now that we know what git dummies does and we know that it works we're going to use it on all of the categorical columns and we're going to save the result and note in a real situation and not in a tutorial like this we would go through each individual column and make sure it only contains reasonable data however since this is just a tutorial and i've already done all that work we're just going to skip to the next step which is to run this pandas function pd we're going to run the pandas function called git dummies we're going to pass in our data frame and all of this of the columns that we want to convert into uh categorical all of the categorical columns and we're going to save that in a new data frame called x underscore encoded and then when we're done we're going to print out the first five rows so let's do that bam um so now we see that we've encoded a bunch of columns there's this dot dot dot still because there's too many columns to print and if you look down here obviously we've got five rows because we use the head function which only prints out five rows but we now have 1178 columns dang a lot of those are because we've got a different column for each city name so we've so you see all these all these city names that just kind of bleed into the dot dot dot we've got a lot of different city names and we've got a column for each one now one last thing before we build an xg boost model we're going to verify that y only contains ones and zeros with a unique function so we've got y dot unique and we run that and it only has one and zero and that's a double bam so we finally finished formatting the data for making an xg boost model however before we get uh before we do that i want to do a mini stack quest to show how one hot encoding works especially when we're coding missing values with zero so imagine the favorite color was a column or feature or variable in our data set two people loved the color blue two people loved the color green and two people had missing data so just like we did in the jupiter notebook we replaced the missing data with zeros now we convert favorite color with one hot encoding just like we just did there are ones in the blue column for the two people that liked blue and zeros in the green column because those people did not like green likewise there are ones in the green column for the two people that liked green and zeros in the blue column because those people did not like blue lastly both the blue and green columns get zeros for the people with missing data let's move this table to the left side now the question is should the people with missing data be clustered with the people that like blue or should they be clustered with the people that like green xgboost answers these questions by comparing these two different ways to split the data on the left side we are splitting people who like blue from everyone else and that means we are clustering the people who like green with the people with missing data on the right side we are splitting the people who like green from everyone else and that means we are clustering the people who like blue with the people who have missing data xgboost then chooses the split that gives the best value for gain okay i get how xgboost deals with missing data but doesn't keeping track of all these zeros take up a lot of memory because xgboost uses sparse matrices it only keeps track of the ones and it doesn't allocate memory for the zeros and that means that this branch is really just checking to see if memory is allocated for blue if memory is allocated for blue then we go to the left and if memory is not allocated for blue then we go to the right likewise this branch is only asking if memory is allocated for green if memory is allocated for green then we go to the left and if memory is not allocated for green then we go to the right this is how xgboost deals with missing data and is memory efficient at the same time bam all right now let's return to the jupiter notebook and and build our preliminary xg boost model okay so i know we just said that we were gonna build the uh um the xg boost model but the first thing we need to do is we need to split the data into training and testing data sets however let's first observe that this data is imbalanced by dividing the number of people who left the company where y equals one by the total number of people in the data set so since the this this column y lowercase y only contains zeros and ones and it only contains ones for people that left the company we can add up all the values in this column to get the number of people that left and if we divide it by the length of that column we'll get the percentage of people that left and we see that only 27 percent of the people in the data set left the company because of this when we split the data into training and testing data sets we will split using stratification in order to maintain the same percentage of people who left the company in both the training set and the testing data set so we're using a function called train test split and we're passing in x encoded lowercase y we're setting the random state to 42 so that hopefully hope against hope you will get the same results that i got and we're setting stratify to y for yes and this will return four variables and we're or four or four data sets we're storing them in x underscore train x underscore test y underscore train and y underscore test so let's run this bam now let's verify that stratify worked as expected now we're just doing the same math we did before only this time we're doing it on the training data set so we see that 27 of the training data set are contains people that left and let's look at the testing data set and we see that 27 percent of the people in the testing data set left so bam stratify worked as expected so now what we're going to do is we're ready we've got our training set we've got our testing data set we're ready to build our xgboost model and the way we 
Y5tyrAxhmUc,2020-07-20T04:00:03.000000,The Elements of StatQuest (Webinar),"hello I'm Josh Starr Murr before we get started I want to let you know that the following presentation and question-and-answer session was supported by XP incorporated if you'd like to learn more about xB Incorporated there's a link to their website in the description below alright let's get started hello I'm Josh stormer and welcome to my presentation for XP incorporated today we're going to be talking about the elements of stack quest or how to clearly explain complex topics alternatively we could call this stack quest clearly explained note this presentation does not assume you have ever heard about stack quest before so if it is new to you don't sweat it stack quest is my youtube channel everyday over 30,000 people around the world watch stack quest to learn statistics machine learning and other data science subjects but it wasn't always like this I used to work in a genetics lab and helped people with statistics I didn't want them to think what I was doing was magic so I made a little PowerPoint presentation to help explain the concepts eventually On February 3rd 2015 I uploaded a little presentation to YouTube that explained our squared my goal was simply to have it as a reference for coworkers in other words I didn't think anyone else in the world would watch it however over the next year a hundred and twenty three people watched my video which it was pretty cool and two people subscribed which was super cool and over the next few months I uploaded a bunch of cooking videos and other stat quest videos for friends and co-workers so I had a video on how to cut an onion and a video on how to chop garlic Oh in any way my channel stayed small which was fine with me but then I uploaded this video which attempted to demystify some of the technical jargon associated with high-throughput sequencing and if you don't know what high throughput sequencing is don't sweat it the important point is that in the first year 7000 people watched this video and 48 people subscribed and that was 57 times more people than watched my R squared video in other words things in stat quest land were going bonkers however to put my 7000 views into perspective in the same year 2015 Taylor Swift's bad blood music video had something like 1 billion views anyways to make a long story short I kept adding videos to my channel and I switched from using PowerPoint to keynote and now people have watched my videos over 14 million times and over 290 thousand people have subscribed more importantly people say that stat quest helps them win data science competitions pass exams graduate from college or university get jobs and so far no one has told me that stat quest help them get married but maybe one day that'll happen and that would be a cool thing too people watch stat quest because it makes complicated sounding things easy to understand here's a quote from someone they said it's the best video one can find to understand SVC support vector classifier z' and SVM support vector machine algorithms and this person said I'm always blown away by how you make statistics and machine learning algorithm so simple to understand and how you graciously share your knowledge this guy said that they clearly understood it and this you know someone else said that they they thought I was good at explaining concepts more people goodness so anyways let's talk about the things that I do to clearly explain complicated sounding things rule number one focus on the main ideas this is a lesson I learned from my father he always told me to focus on the main ideas so imagine we wanted to teach someone how to drive a car should we start by teaching them about fuel injection or should we start by describing the master cylinder and what about the head gasket who knows what that does even though fuel injection master cylinders and the head gasket they're all critical parts of cars they are not the main idea in other words we don't need to know about these parts in order to drive in fact even though I would bet most of the people watching this seminar know how to drive I'd be surprised if they knew about these car parts if I wanted to teach someone how to drive I would focus on the main ideas I would start by teaching them about the brake pedal so they can stop the car stopping the car is the most important thing to teach first so that we can avoid hurting other people and ourselves then I would teach about the steering wheel so they can turn the car we can now go left and right we can avoid hitting things the steering wheel is the second most important thing because we can use it to avoid hurting people lastly I would teach them about the gas pedal so they can move the car the brake and gas pedals and a steering wheel are the main ideas of driving because we need all three to drive but you don't need anything else knowing about the brake and gas pedals and the steering wheel may not make someone a great driver but at least they know how to practice and they can easily learn more once they've started so bail that's rule number one similarly when I want to teach someone about the normal distribution I start by describing this graph for this specific normal curve the x-axis represents height measurements of women in Brazil the main idea the first main idea of the normal distribution is that the red area under the curve represents the probability that a Brazilian woman's height will be within a range of possible values for example 95 percent of the area under the curve is between 142 and 169 and that means that 95 percent of the Brazilian women were between 142 in 169 centimeters tall in other words there's a 95 percent probability that each time you measure a Brazilian woman their height will be between 142 and 169 centimeters the main idea number two is that the average height 155 point 7 is at the center of the bell-shaped curve and main idea number 3 the y-axis represents the likelihood of measuring someone with a specific height in other words because the curve peaks at the average height the average height has the maximum likelihood value bail now that we understand the main ideas behind the normal curve we can dive into the math behind it however I'm gonna save that for later since right now we are focusing on the main ideas so this is rule number one and it is the most important rule of all it seems very simple but it is easy to be distracted by things that are not the main ideas and I would say the hardest part of my job is sifting through all of the information to find these nuggets that are the main ideas because most people don't present information or when you I read a book they're not organized in terms of main ideas you have to search for these things okay that brings us to rule number two have empathy for the audience everyone has different experiences backgrounds and perspectives and it is important to keep this in mind when explaining anything for example if I'm going to teach someone how to drive and they know how to write a bike then I'll explain how to drive in terms of how to ride a bike the handlebar is the steering wheel and the pedals are the gas etc in contrast if they knew how to ride a horse then I would explain how to drive in terms of how to ride a horse giddyup and if someone had piloted a boat before then I would explain how to drive in terms of how to pilot a boat Anchors Away lastly when you don't know your audience try your best to explain in a way that anyone could relate to and try to anticipate questions from people that have different experiences one way to explain things so that anyone can relate to the subject and helps anticipate questions is to use pictures and that leads us to rule number three news pictures a lot of people are visual learners if you're not a visual learner and it's easier to look at this then look at this they need to make sure you understand rule number two have empathy for the audience regardless of whether or not you are a visual learner visual cues often make things easier to remember here's an example of some unvisible directions to the grocery store one go straight for 731 meters to turn pi divided by two radians three go straight for 1196 meters for turn three times pi divided by 2 radians 5 go straight for 52 meters BAM did that get us to the grocery store technically it did here's a pop quiz can anyone remember the first step in the directions to the grocery store go straight for 731 meters this may be precise and accurate but it's very hard to remember here's an example of a visual direction to the grocery store go straight till you get to the gas station turn left go straight till you get to the church turn right go straight till you get to the grocery store Mihal to summarize this rule we almost always have a choice between using pictures to explains between not excuse me we almost always have a choice between not using pictures to explain something and we could just step through these very precise and very accurate directions that are very hard to remember and we can always instead we can use pictures to explain something and when we use pictures a lot easier remember because pictures are one easy to relate to often help people answer their own questions and they're easy to remember so BAM rulz 4 & 5 repetition is helpful and do the math no matter how simple the equation plugging in a few numbers makes it way easier to explain in doing this more than once makes it way easier to remember however for complex equations plugging in numbers is crucial and can provide deeper and more memorable insights for example this is the equation for the normal curve the main Moo for this specific curve is 25 so we can plug in 25 here and the variance Sigma squared which controls how tall and wide the curve is is 4 so we plug in 4 here and we plug in 4 here we can now plug in different values along the x-axis for X for example if we plug in x equals 20 then the equation will give us the corresponding y axis value so let's do the math and figure out this y axis coordinate first we subtract 25 for 20 and we get negative 5 then we square negative 5 and get 25 then we multiply by negative 1 and get negative 25 which we divide by 2 times 4 which is 8 and we get negative 3.125 in the exponent e to the negative three point one two five is zero point zero four so we end up with this term times zero point zero four and the whole thing is zero point zero zero eight and that means when x equals 20 the y-axis coordinate is zero point zero zero eight now if we wanted to find the Y access value for the highest part of the curve then we would plug in x equals 25 because that is the middle of the curve now when we do the subtraction we get zero and that means this term is zero so the whole experiment is zero and e to the zero is one so we end up with this term times one and that just leaves us with just this term which equals 0.2 and that means that the highest part of the curve is 0.2 to review what we just did we started with this big fancy equation but when we plugged in 25 for X the same value as the mean the exponent ends up being zero and we ended up with this term times one and that left us was just this term to determine the y axis coordinate for the peak of the curve note if we had another curve with another mean value so in this case mu equals 15 and we wanted to know the Y access coordinate for the peak of the curve then we could plug in x equals 15 here and that makes this term zero and and the whole exponent becomes zero so this term equals one and that means all we really needed to to determine the maximum y axis value on this curve or any normal curve is just calculate this part in this case the variance equals two so we plug in two then we just do the math and the maximum y axis value is 0.28 now we have seen two examples where when we want to know the y-axis coordinate for the peak of the curve we can ignore this part of the equation and that means we don't have to plug in values for X or for the mean mu because the whole term cancels out and all we have to do is calculate this part BAM now if we want to find y axis coordinates for the highest point on this curve we don't need to know the mean mu or some value for x instead all we need is the variance Sigma squared in this case the variance is 16 so we plug in 16 do the math and we get zero point one so zero point one is the maximum Y access coordinate for this graph curve ow okay now remember the rules we just illustrated here repetition is helpful and do the math we just did both of those and we just went through plugging numbers into this equation a few times and we learned that we can omit this part when we just want to know the y-axis value for the peak all right rule number six don't repeat other people's explanations this is oh this might be personal for me because my job is to explain things to people but my rule is if someone already someone else already has a great explanation for something what I do is I direct people to it I don't feel like I need to re explain it if I can't do any better than that then I'm just gonna say hey if you want to understand this concept check out this other website check out this other video don't come to me because that other person has done the best job out there otherwise if there is no other good explai explanation you have no choice but to come up with something new now in YouTube land and on the Internet there's sort of this echo chamber effect and once somebody explains something regardless how good or bad that explanation in a lot of people just copy it word-for-word or generally speaking they might use the same data set as an example they might use the same terminology and the same graphs but that's not very helpful because if someone goes to some website or a YouTube video and they watch something and they read something and it doesn't make sense to them well if you just if someone else just repeated that same stuff that's not gonna help that person they're gonna go to the next link down in the Google search and they're gonna find the exact same explanation maybe with a slight change in words but there won't be big difference in how its explained and that means that if that person didn't understand the first explanation they won't understand the next explanation and they won't understand any of those explanations unless someone comes up with something new with a new way to explain something and that leads us to rule number 17 limit the presentation to three main ideas and I know this is kind of like a little funny because I'm on rule number seven so I've already violated my own rule but when I'm making videos if you've ever watched a stat quest you'll notice that I have a through I do three Bam's I I have a main idea and I go hey BAM that's great and then we go on for a little bit longer and I get really excited because we're covering really cool material and then I say hey double BAM and then we go just a little bit longer and then we finally reach the climax and that's the triple BAM and the reason why I use three bounds is to accentuate the fact that there are three main ideas that I'm covering I've found that when I'm making a video and I want to add extra Bam's I've gone too far I swear I've swerved and deviated from the point of the video and I'm covering subjects that I don't need to talk about or it would be better separated in a different lesson and I found that just in general if you can keep whatever it is you're trying to explain as simple as three main points then then your explanation will be easier for people to understand it okay I do have some other random thoughts that may be helpful when presenting Soph one I know this sounds crazy but try not to use a laser pointer you'll notice that right now I'm not using a laser pointer um I don't have an option to use a laser pointer on youtube you can't use a laser pointer I guess so I have no choice but to use sort of still images oh and I've got strong feelings it's funny when I when I when I first started doing static quest videos and I switched to doing things on YouTube I was very scared that I didn't have a laser pointer because I was like well how am I gonna point to things that are important um and I've just since discovered that not only can you make a presentation without a laser pointer and give a presentation with a lighter corner a laser pointer is actually a crutch that can get in your way and let me explain why I have two kittens the one on the left is named red and the one on the right is named PO those aren't the main ideas but I just thought you might want to know they love it when I point at things with a laser pointer but they are not paying attention to what I'm pointing at instead they are obsessed with the green dot itself they love it because they have an instinct to track motion and so wherever I point that green laser pointer dot my cats will chase all over the house to try to get at that dog they don't really care that one time I'm pointing it at the pillow and then I'm pointing it at the rug they could care less about the rug or the pillow what they're interested in is the green dot people are not kittens but we are not that much different because it's moving around we can to see the dot instead of what it's pointing to so instead focus people's attention by putting a box around it and pointing to it with a stationary arrow now we can all focus on this specific part of the equation that were interested in talking about and try to remove everything that's not directly related to what you're focusing on so now when we look at this part of the equation we can meditate on it without any distractions and that's very important to make sure you've got people's concentration focused on what you want them to be focused on and not just a green dot that's been moving around on the screen okay my second advice is dare to look stupid if I went up to the average person on the street and I said let me teach you about this equation there's a great chance that they would have run away screaming no that looks complicated I can't do that stuff and that would make me look smart but I would not have explained anything to anyone in contrast if I say let me explain this curve to you there's a chance they will say hey that's too simple you know you must not be very smart oh you know and they could ridicule me that's okay that's a risk I'm willing to take and I hope you're willing to take it too because if we start simple and build up the no I'm I haven't lost you you know if we start with a real simple example and we build on it very slow and very in a concerted way in a in a in a directed way I know you're following what I'm talking about but if I start with something very complicated at the very beginning there's a good chance I've already lost you and adding things to that complicated equation or whatever it is that was complicated will not help so I like to start simple and at the risk of looking silly and looking not smart we're gonna build from there and we're gonna get too complicated soon or enough sooner or soon enough yeah excuse me in summary I believe it's stat quest my youtube channel is successful because rule number one we like to focus on the main ideas rule number two we have empathy for the audience and that means trying to understand where they are coming from what their experiences are do they know about data science do they know about the normal distribution in advance or do they not know about it you have to start from scratch you have to know about your audience in order to know where you can start the conversation and if you don't know your audience start simple and build up rule number three use pictures I know I'm not using a picture right here but you get the idea remember if we're trying to tell someone directions we don't just list a bunch of directions arbitrarily we if we give them pictures or we even just describe things in terms of pictures if I say hey just go down the street until you see the snack shop internal left there someone has an image in their head they're thinking oh I'll just look for the snack shop and then I'll take a left in contrast if I say go down the road five hundred fifty three meters who knows what they're gonna do they may go down far enough they may not it's hard to measure that when you're just walking and you're not really concentrating on the exact distance of every step rulz four and five repetition is helpful and do the math if there's one thing I think that sets my channel apart from a lot of the other channels is that I do the math and I do it repeatedly and and it helps people people go oh I just I see how you're just going through and you're plugging the numbers in I can do that myself and I can take this this lesson and I can plug in my own numbers now now that I've seen how how stat quest does and how Josh does it I can plug in my own numbers and see what I get do I get the same results people do that all the time in the end that's the funny thing is they often find that I'm not very good at doing the math technically and I have a lot of errors but the main idea is just plug in some numbers and you'll get something out rule number six don't repeat other people's explanations if they have a great explanation then just say hey check out this explanation it explains the normal distribution better than anyone else in the world but if it's not the best explanation you looked at stat quest you looked at some other websites and you're like I don't think this is the best well then you have to come up with your own and you can't just repeat what other people are saying and rule number seven which revile ated all over the place limit the presentation to three main ideas all right that's the end of the talk I know we've got a bunch of questions we've got plenty of time for those I'm gonna is it okay if I exit oh we've got a bunch of questions over here let me just reload my window okay so someone said I'm just gonna read through these questions and give you the answers or my answers there's there's no right answer or wrong answer so someone said you are the most didactic guy on the Internet what recommend what recommendations would you give data scientists to communicate with non-technical co-workers inside of a company well I guess that presentation that I just gave you answered this question one focus on the main ideas - I can't actually remember the next one is it use pictures [Music] and three you know repeat use and plug in numbers and Oh have empathy for the audience that's the one I unless - skip that one it's kind of funny I'm not a good example of my own teaching technique but anyways if you have empathy for your audience and you know they're non-technical try to keep it non-technical don't just throw around a lot of fancy jargon like this is the posterior distribution of the beta binomial hypothesis test they're not going to understand a word of that however if you can give them a general sense of what the main idea is the jargon is never the main idea the main ideas are always the concept so focus on the concepts of what that posterior probability of that beta binomial distribution is what's the main idea behind that not the jargon not the words but the concept ah so that's that's question number one question number two any thought on the future of the Big Data machine learning and applied statistics as a profession how can one study and prepare himself or herself well my thoughts are the big data it's just gonna get bigger and more of it people are measuring everything and our ability to amass data is is not gonna go away there's not going to be something that like completely breaks the internet we're just gonna keep collecting more and more data and it's gonna be fine it you know it used to be you know ten years ago if you just if you had a client and and you knew basic demographics like their age then you were doing pretty good dowa days we've got so much data about all of our clients we we know we might know their age we might know it down to the day of the month we might know of what their shopping preferences are what websites they like to go to how much time they spend on the internet it sounds kind of creepy and Big Brother II but this is this is the reality we live in we live in a connected world so it's it's and people are gonna be measuring these things and so we we're just gonna collect more and more data I used to work in a genetics lab and when I started working in that genetics lab you know we'd be lucky if we could measure one or two genes at a time and then all of a sudden we could measure all of them all 20,000 human genes and at a single go and so we went from having you know being data poor to being very data rich and now what they're doing is instead of it used to be that they have to average the 20,000 genes over a large group of cells so if we were interested in a tumor for example or cancer we would we would apply this research technique to a big blob of that tumor and it would give us the average gene activity for all of the genes in that tumor but now we can do it on a single-cell basis and everything is made of millions of cells and so in biology the data is just exploding okay so you don't need me to talk more about oh you know what the future of email is because we're just gonna have more and more data and in machine learning and apply statistics are just going to become more and more critical every single day so the last part is how can we study and prepare for this profession uh well there's stat quests there's the internet you can watch my videos of course but there's just lots of things you can do I have my favorite book on machine learning is a free download it's called the introduction to statistical learning and our um it's written by a bunch of super geniuses and I love it it's I'm not I'm gonna be honest with you it's kind of hard to read it's not the easiest reading ever it doesn't have a lot of pictures on every page however it does have one thing going for it in that I've made a lot of stat quest videos to correspond to the chapters in that book so what you can do is you can read that book or skim through that book or do whatever you can with that book and when you get confused you can say hey I wonder if there's a stat quest video and you can oftentimes there is so if you want to learn about regularization and your reading in the book about regularization or like it's kind of confusing well you can check out the video it's there and it's waiting for you when you need it um there's also a bunch of websites that I love the UCLA they have a online statistics consulting website and I think I'm not really certain but I assume that if you are an employee at UCLA and you need statistics you contact those people but this website is actually for everybody they have great explanations of like what logistic regression is or all these all these data science and statistical learning techniques and they've got a great examples they've got it in all these different computer languages so you could learn how to do it in R you could learn how to do it SAS you could learn how to do in spss so whatever sort of however maybe in Microsoft Excel but whatever you use to do data science they usually have an example of how to do it in that language I just love the way they write up stuff it's very intuitive for me and it's a place that I go to when I'm researching my own videos oh another place I really love is there's a university in Pennsylvania it's it's not B University of Pennsylvania it's Penn State or Pennsylvania State they have a wonderful online statistical research resource and I love reading through those pages okay so uh that's the answer that question oh oh what is my opinion about Auto ml tools both for model and feature engineering so if for those of you who are not familiar Auto ml is um is like so in machine learning land we have a lot of different ways to apply machine learning to the same data set there's all these fancy algorithms like neural networks or cat boost or XG boost or support vector machines there are all these fancy machine learning methods and we can apply them all to our data what Auto ml does is it applies all of them simultaneously to the data and it tries to contrive to optimize them so one thing about machine learning is every single method neuro-networks XG boost support vector machines they all have these things called hyper parameters which are really fancy terminology for fudge factors all of those things have little little parameters that we can tweak and if we change one value sort of arbitrarily it can change how good or bad our machine learning algorithm is an auto ml attempts to one apply a bunch of different statistics and machine learning methods to our data and to it tries to optimize them and and also it can also be used for something called feature engineering which is sort of like selecting what are the most important features in your data set you should be focusing on oftentimes we collect a lot of data some of its useful some of its not useful and feature engineering is all is is a lot about that process of filtering out the parts of the data that are not very useful and and sort of streamlining it to just the good stuff so so it's a so autom ml is a relatively new thing on the scene and I on the one hand it sounds awesome right because there's all these different methods getting them all optimized and measuring them all is a terrible thing to have to do that's a lot of work it's not very fun to do so that's very cool although I will say you have to be careful with how you do it you know if we apply if we throw like a hundred different methods at our data there's a good chance that by random chance one of those is going to do really well and perform really well does that mean that that same method is going to work well in the long term in other words it just so happened that that this one method worked really well because it just or fit the data in just the right way that with once we give it new data it can no longer perform really well it does great in the testing situation in a testing environment but not of them so there has to be some way for compensating for the fact that we're looking at all these different models and just by chance we might have one that baby is giving us good results just because of the data set we have and not because of its good in a long-term um so when you do auto ml you should you should look and make sure that you're aware of of those features oh I should stop sharing so you can see my lovely face oh so what else do we have someone said in my opinion what is the future of data science 10 to 20 years from now what will be the skills required this is kind of an interesting question oh I think I'll tell you I'll tell you immediately off the top of my head with whatever it is we won't call it data science so when I went to graduate school I have a PhD and something called bioinformatics when I went to school it was they just invented that term by informatics was a new term no one had ever heard it before but there was a program at NC State University North Carolina State University for bioinformatics and to be honest I've never heard of it either um in fact I didn't even apply to this program I'd applied to something else but the people that saw my application said this guy would be great for bioinformatics let's let's pass his application over to those people and see what they think and the bioinformatics people contacted me and said hey we got your application we think you'd be perfect won't you come interview so I went interviewed and I asked the head of the diaphragmatic s-- Department I said hey what is bioinformatics and the head of the Department of this guy named Bruce we're said behind for Maddox is a hat we put on and when we put on this hat were eligible for certain funding that we were not eligible for before when we called ourselves statisticians and a lesson there is that um Bruce Weir was a statistician before he called himself a Baphomet Isham and after he called himself a bioinformatician he was still a statistician the skills we learn in machine learning and data science these things are going to be useful for all of eternity for example I'm not all of it I mean that might be an exaggeration but think about the t-test the t-test is one of the most basic statistical tests it's been around forever or at least a hundred years uh linear regression is still a very popular and very useful machine learning method it's been around for almost as long knowing about these core concepts these core idea"
wl1myxrtQHQ,2020-07-13T04:00:18.000000,"The Chain Rule, Clearly Explained!!!",the chain rule is cool stat quest yeah [Music] hello i'm josh starmer and welcome to statquest today we're going to talk about the chain rule and it's going to be clearly explained note this stat quest assumes that you are already familiar with the basic idea of a derivative and just want a deeper understanding of the chain rule that said let's do a super quick review imagine we collected these measurements from a bunch of people on the x-axis we measured how much they liked statquest and on the y-axis we measured awesomeness we can then fit this orange parabola to the data the equation for the parabola is awesomeness equals likes statquest squared the derivative of this equation tells us the slope of the tangent line at any point along the curve the slope of the tangent line tells us how quickly awesomeness is changing with respect to like's stat quest we can calculate the derivative of awesomeness with respect to like's stat quest by using the power rule the power rule tells us to multiply like's stat quest by the power which is 2 and raise stat quest by the power 2 -1 and since minus one equals one and raising something by one is the same as omitting the power we end up with two times like's statquest okay bam that's the review now let's dive into the chain rule with a super simple example imagine we collected weight and height measurements from three people and then we fit a line to the data now if someone tells us they weigh this much we can use the green line to predict that they are this tall bam now imagine we collected height and shoe size measurements and we fit a line to the data now if someone tells us that they are this tall we can use the orange line to predict that this is their shoe size bam now if someone tells us that they weigh this much then we can predict their height and we can use the predicted height to predict shoe size and if we change the value for weight we see a change in shoe size bam now let's focus on this green line that represents the relationship between weight and height we see that for every one unit increase in weight there's a two unit increase in height in other words the slope of the line is 2 divided by 1 which equals 2 and since the slope is 2 the derivative the change in height with respect to a change in weight is two now since the slope of the green line is the same as its derivative two the equation for height is height equals the derivative of height with respect to weight times weight which equals two times weight note the equation for height has no intercept because the green line goes through the origin now let's focus on the orange line that represents the relationship between height and shoe size in this case we see that for every one unit increase in height there is a one-quarter unit increase in shoe size and i admit that it's hard to see the one-quarter unit increase in shoe size so just trust me anyway because we go up one quarter unit for every one unit we go over the slope is one quarter divided by one which equals one quarter and since the slope is one quarter the derivative or the change in shoe size with respect to a change in height is one quarter now since the slope of the orange line is the same as its derivative the equation for shoe size is shoe size equals the derivative of shoe size with respect to height times height which equals one-quarter times height and again because the orange line goes through the origin the equation for shoe size has no intercept now because weight can predict height and height can predict shoe size we can plug the equation for height into the equation for shoe size now if we want to determine exactly how shoe size changes with respect to changes in weight we can take the derivative of shoe size with respect to weight and the derivative of the equation for shoe size with respect to weight is just the product of the two derivatives in other words because height connects weight to shoe size the derivative of shoe size with respect to weight is the derivative of shoe size with respect to height times the derivative of height with respect to weight this relationship is the essence of the chain rule plugging in numbers gives us one half and that means for every one unit increase in weight beep boop beep there is a one-half unit increase in shoe size bam now let's look at a slightly more complicated example imagine we measured how hungry a bunch of people were and how long it had been since they last had a snack as time since the last snack increases on the x-axis people got hungrier and hungrier at a faster rate so we fit an exponential line with intercept one-half to the measurements to reflect the increasing rate of hunger then we measured how much people craved ice cream and how hungry they were the hungrier someone was the more they craved ice cream but after a certain amount of hunger the craving did not continue to increase very much so we fit a square root function to the data to reflect how the increase in craving tapers off now if we want to see how the rate of craving ice cream changes with respect to the time since the last snack plugging the equation for hunger into the equation for craves ice cream gives us an equation without an obvious derivative to convince yourself that taking the derivative of this is no fun at all pause the video and give it a try however because hunger links time since last snack to craves ice cream we can use the chain rule to solve for this derivative first the power rule tells us that the derivative of hunger with respect to the time since the last snack is two times time likewise the power rule tells us that the derivative of craves ice cream with respect to hunger is one divided by two times the square root of hunger so with these two derivatives the chain rule tells us that the derivative of craves ice cream with respect to time is the derivative of craves ice cream with respect to hunger times the derivative of hunger with respect to time since last snack so we plug in the derivatives and plug in the equation for hunger and cancel out the twos and we get the derivative of craves ice cream with respect to time since last snack this derivative tells us how quickly or slowly our craving for ice cream changes with respect to time double bam in this last example it was obvious that hunger was the link between time since last snack and craves ice cream and we had an equation for hunger in terms of time and an equation for craves ice cream in terms of hunger however usually these relationships are not so obvious instead of having two separate equations we usually get the first equation jammed into the second and when all you have is this it's not so obvious how the chain rule applies so we can talk about how to apply the chain rule in this situation let us scooch the equation to the left so we have room to work now one thing we can do in this situation is look for things in the equation that can be put in parentheses for example the square root symbol can be replaced with parentheses now we can say that the stuff inside the parentheses is time squared plus one half and craves ice cream can be rewritten as the square root of the stuff inside now the chain rule tells us that the derivative of craves ice cream with respect to time is the derivative of craves ice cream with respect to the stuff inside times the derivative of the stuff inside with respect to time the power rule gives us the derivative of craves ice cream with respect to the stuff inside and the power rule gives us the derivative of the stuff inside with respect to time now we just plug the derivatives into the chain rule and plug in the equation for the stuff inside cancel out the twos and we get the derivative of craves ice cream with respect to the time since last snack and that's exactly what we got before bam now let's look at how the chain rule applies to the residual sum of squares a commonly used loss function in machine learning note if this does not make any sense to you just imagine i said now let's look at one last example imagine we measured someone's weight and height and we wanted to fit this green line to the measurement now to keep things simple let's assume we can only move the green line up and down the equation for the green line is height equals the intercept plus 1 times weight and we can change the intercept but to keep things simple we can't change the slope which is set to 1. if we set the intercept to 0 then this location on the green line is the predicted height and we can calculate the residual the difference between the observed height and the value predicted by the line and we can plot the residual on this graph which has the intercept on the x-axis and the residual on the y-axis if we change the intercept here then we can see the change in the residual here and because a common way to evaluate how good the green line fits the data is the squared residual we can plot the squared residual here where we have the residuals on the x-axis and the squared residuals on the y-axis now if we change the intercept here then we change the residual here and here and changing the residual here changes the squared residual here in order to find the value for the intercept that minimizes the squared residual we are going to find the derivative of the squared residual with respect to the intercept and then we're going to find where the derivative equals zero because given the function y equals the residual squared the derivative is zero at the lowest point the chain rule says that because the residual links the intercept to the squared residual then the derivative of the squared residual with respect to the intercept is the derivative of the squared residual with respect to the residual times the derivative of the residual with respect to the intercept the power rule tells us that the derivative of the residual squared is just two times the residual so let's plug that in to solve for the derivative of the residual with respect to the intercept we move the equation for the residual over here so we have room to work then we plug in the equation for the predicted height then in order to remove these parentheses we multiply everything inside by negative one now the derivative of the residual with respect to the intercept is zero because this term does not contain the intercept plus negative one because the derivative of the negative intercept equals negative one plus zero because the last term does not contain the intercept now do the math and we are left with negative one and that makes sense because the derivative is just the slope of the orange line and by i we can see that the slope of the orange line is negative one so let's plug this derivative in here and do a little math and plug in the equation for the residual now we have the derivative for the residual squared in terms of the intercept note if instead of starting with separate equations for the residual and the residual squared we started with just the equation for the residual squared with the equation for the predicted height jammed into it then just like before we can use parentheses to help us out in this case we'll call everything between the outermost parentheses the stuff inside which equals the observed minus the intercept minus one times weight and that means the residual squared can be rewritten as the square of the stuff inside now we can use the chain rule to determine the derivative of the residual squared with respect to the intercept it's the derivative of the residual squared with respect to the stuff inside times the derivative of the stuff inside with respect to the intercept just like before the derivative of the residual with respect to the stuff inside is two times the stuff inside so we plug that into the chain rule and the derivative of the stuff inside with respect to the intercept is negative one so we plug that into the chain rule now we just plug in the stuff inside multiply two with negative one and we end up with the exact same derivative as before bam now we want to find the value for the intercept such that the derivative of the residual squared equals zero so we plug in the observed height and the observed weight set the derivative equal to 0 and solve for the intercept and at long last we see that when the intercept equals one we minimize the squared residual and we have the best fitting line triple bam hooray we've made it to the end of another exciting stack quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of the statquest study guides or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
0oc49DyA3hU,2020-07-06T04:00:01.000000,"Hypothesis Testing and The Null Hypothesis, Clearly Explained!!!","stat question the mornin stat quest at night stat quest in the afternoon it's alright stat quest hello I'm Josh stormer and welcome to stat quest today we're gonna talk about hypothesis testing and the null hypothesis I'm not going to name names but imagine there was a virus and we had two drugs we could use to treat it so we give drug a to three people and measure how long it takes each person to recover from the virus the first thing we notice is that not everyone recovered in the exact same amount of time person number one recovered the fastest in person number two recovered the slowest it's possible that person number one eats healthy food and exercises and already has a strong immune system and that helped them recover quickly and maybe person number two doesn't get as much exercise or maybe person number two has a stressful job or lives where there is a lot of air pollution the point is is that even though all three people had the same virus and took the same drug they did not all recover in the exact same amount of time and that might be due to a lot of random things like exercise or job stress that we cannot control now let's give drug b to three different people that have the virus and measure how long it takes them to recover again we see that even though three people had the same virus and took the same drug they did not all recover in the exact same amount of time and this is probably due to random stuff that we can't control like how much exercise each person gets or how much candy they eat overall it looks like people taking drug a took less time to recover than people taking drug B and when we calculate the mean or average value for drug a and the mean value for drug B we see that on average there is a 15-hour difference between drug a and drug B so after seeing this preliminary data it might seem reasonable to form the following hypothesis people taking drug a need on average 15 fewer hours to recover than people taking drug B and now that we have this hypothesis we can test it by repeating the experiment now when we calculate the means we see that on average people taking drug a need 35 more hours than people taking drug be compared to our preliminary data this result is very unexpected in fact it is the opposite of the original hypothesis but it is also possible that all three people that took drug a in the second experiment have super stressful jobs and unhealthy lifestyles and maybe that's why it took them so long to recover and maybe everyone taking drug be was well rested and super healthy to begin with and maybe that's why they recovered so quickly but it is also possible that we mislabeled drug a and drug B and did the wrong experiment so we repeat the experiment and again the results are totally backwards from the preliminary experiment and totally backwards from the hypothesis that we made so again just to make sure we didn't miss label things we redo the experiment and again these results are the opposite of the original hypothesis so we just keep repeating the experiment each time double-checking every little detail and every time we do the experiment we get the opposite result of the original hypothesis so after doing all of these repeated experiments where we double-checked every little step we can confidently reject this hypothesis that we came up with after doing the preliminary experiment BAM now let's imagine we had two more drugs C and D just like before we gave drug C to three people and measured how long it took each person to recover from the virus then we gave drug D to three different people and measured how long it took them to recover from the virus and based on this data we can create a hypothesis about drug C and drug D people taking drugs C need on average 13 fewer hours to recover than people taking drug D now just like before we decide to test this hypothesis by repeating the experiment only this time instead of getting something that's the exact opposite of what we expected we get something that is only slightly different in this case the difference is in the same direction but it is only 12 hours then we repeat the experiment again and again we get something slightly different from the preliminary experiment and hypothesis the difference is in the same direction but this time it is 13.5 hours the good news is that we probably didn't mislabel the drug like we did last time in the differences between the three experiments might be due to random things we cannot control like maybe these people exercised a lot and had relatively healthy diets compared to these people who took longer to recover but regardless the hypothesis says that people taking drug C needed 13 fewer hours to recover but when we repeated the experiment the first replicate said the difference between averages was 12 which is different from the hypothesis and the second replicate said the difference was 13.5 which is also different from the hypothesis and let's be honest the only reason the hypothesis says 13 fewer hours is because that was the result from the first experiment however we could have just as easily put 12 fewer hours in the hypothesis because that's what we got the second time or we could have put 13.5 fewer hours in the hypothesis because that's what we got the third time so if we just pick one experiment like the first one and use that to define the hypothesis then we have two experiments that are not different enough to give us confidence to reject the hypothesis but because there is just as much data suggesting that the difference is 12 hours and there is just as much data suggesting that the difference is 13.5 hours these experiments don't make a super confident that the hypothesis of 13 fewer hours is correct again maybe drug a reduces recovery by 13 fewer hours but maybe it reduces recovery by 12 hours or 13.5 because the results from the repeated experiments are not different enough to cause us to reject the hypothesis and because they don't convince us that the hypothesis is correct either the best we can do is fail to reject the hypothesis small BAM to summarize what we've covered so far we can create a hypothesis and if data gives us strong evidence that the hypothesis is wrong then we can reject the hypothesis but when we have data that is similar to the hypothesis but not exactly the same then the best we can do is fail to reject the hypothesis because it's unclear if the hypothesis should be based on this result or this other slightly different result or this result or any other possible outcome double bam now let's take a closer look at the hypothesis itself you may remember that the only reason the hypothesis is 13 fewer hours is that it was the first result but we could have just as easily gotten a 12 hour difference or a 13.5 hour difference and ended up with a different hypothesis and if 12 and 13.5 are reasonable hypotheses then so is 12.25 or 13.1 in other words there are a lot of reasonable hypotheses how do we know which one to test since the goal is to see if drug C is different from drug D we simply test to see if there is no difference between the drugs oh no it's the dreaded terminology alert the hypothesis that there is no difference between things is called the null hypothesis so let's take a look at two examples of the null hypothesis in action now imagine we are testing two new drugs E and F and this time we only get a 0.5 hour difference this person recovered the fastest but it is easy to imagine that if they had exercised a little less or had a slightly worse diet then they might have taken a little longer to recover likewise if this person was just a little healthier to begin with then they might have recovered a little more quickly these small random differences give us a slightly different result now instead of drug f being slightly better by 0.5 hours drug ii is slightly better by 0.25 hours because these small random differences give us slightly different results we can use the null hypothesis so we don't have to worry about whether or not the difference is exactly 0.25 or 0.5 hours instead we simply see if the data convinces us to reject the hypothesis that there is no difference between drug II and drug f in this case the original result was zero point five hours in favor of drug f but small random things could have easily changed the result to be a 0.25 hour difference in favor of drug II and thus the data does not overwhelmingly convince us to reject the null hypothesis so we fail to reject the null hypothesis that there is no difference between the drugs in contrast if we tested the drugs on a lot of people and little random things would not change the results very much then we could confidently reject the null hypothesis that there is no difference between drug II and drug f BAM note without the null hypothesis we need preliminary data in order to make a statement that we can test and follow-up experiments this is because we don't know if we should test if the difference is 13 hours or 13,000 hours until we get some data in contrast the null hypothesis does not require preliminary data because the only value that represents no difference is zero triple bam in summary rather than get stressed out over a large number of possible hypotheses that we could test to see if drug C is different from drug D we use the null hypothesis to determine if there is a difference if we do an experiment with a bunch of people and a lot more people taking drug C had shorter recovery times than people taking drug D so many that it would be hard to imagine that the results were due to random things like everyone taking drug C had better diets or got more exercise than the people taking drug D then we could reject the null hypothesis and then we know that there is a difference between drug C and drug D alternatively if little random things could easily shift the result from one drug to the other and then back again then we would fail to reject the null hypothesis BAM but wait what about the alternative hypothesis because the alternative hypothesis is super important it has its own quest so check it out and if you don't already know about p-values they would make a wonderful follow-up lastly if you want to review statistics and machine learning offline check out the stat quest study guides at stat questo RG there's something for everyone hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of the stack quest study guides or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on"
5koKb5B_YWo,2020-07-06T04:00:00.000000,Alternative Hypotheses: Main Ideas!!!,if it's raining outside stay inside and watch stat quest if it's sunny outside go outside and watched a quest on a mobile device stat quests hello I'm Josh stormer and welcome to stat quest today we're going to talk about alternative hypotheses so that you understand the main ideas note this stat quest follows up on the stat quest on hypothesis testing and the null hypothesis if you haven't already seen that one check out the quest the link is in the description below either way let's do a super quick review in the stat quest on hypothesis testing we learn that rather than get stressed out over a large number of possible hypotheses that we could test to see if two drugs are different we simply use the null hypothesis to determine if there is a difference we learn that if we do an experiment with a bunch of people and a lot more people taking drugs see had shorter recovery times than people taking drug D so many that it would be hard to imagine that the results were due to random things like everyone taking drug C had better diets or got more exercise than the people taking drug D then we can reject the null hypothesis and then we would know that there is a difference between drug C and drug D alternatively we learn that of little random things could easily shift the result from being in favor of one drug to another then we would fail to reject the null hypothesis and then we said triple BAM now that we're done with our review let's talk about the alternative hypothesis first here's some data that shows how quickly people taking drugs C and D recovered from a virus the goal of collecting all of this data is to determine if we should reject or fail to reject the null hypothesis in order to decide if we should reject or fail to reject the null hypothesis we run the data through something called a statistical test and the output of the statistical test is a decision to reject or fail to reject the null hypothesis a statistical test needs three things one it needs data - it needs a null or primary hypothesis ie it needs something to reject or fail to reject and three it needs an alternative hypothesis in this case the alternative hypothesis is simply the opposite of the null hypothesis warning things are about to get a little hand wavey the idea is to give you a general sense of why the alternative hypothesis is important and is used in statistical tests not to give you all the details of how those tests work that said if you want the details there's a stat quest playlist that goes through examples step by step the link is in the description below now one way to test the null hypothesis that there is no difference between drug C and D is to calculate a mean value for all the data from both drugs and calculate the distances between each observation and the mean and compare those two distances calculated from individual means for drug C and drug D the distances around the single mean represent the null hypothesis that there is no difference in the distances around the two separate means represent the alternative hypothesis if the distances around two means are much shorter than the distances around the single mean then that suggests that using two means to summarize the data makes more sense than using one so we reject the null hypothesis alternatively if the data looked like this and the distances from the single mean were not dramatically different from the distances around the separate means then that would suggest that the difference between two means only reflects little random things that we can't account for for example it could be that the subtle difference in the means is due to this one guy getting less exercise than everyone else if he had exercised just a little bit more he might have recovered from the illness a little faster and then we would no longer see a difference between the two means so in this case we would fail to reject the null hypothesis just if you're familiar with machine learning lingo failing to reject the null hypothesis it's the same thing as realizing that using two averages means that you have overfit the data if you're not familiar with machine learning lingo ignore what I just said or better yet check out the machine learning stack Quest's note when we only have two groups of data the alternative hypothesis is pretty obvious because it is simply the opposite of the null hypothesis however when we have three or more groups the alternative hypothesis becomes more interesting in this case the null hypothesis is that there is no difference between drug C D and E and like before we can represent the null hypothesis by measuring the distances from the data to a single mean value however now we have choices for the alternative hypothesis one alternative hypothesis could be that all three drugs are different and in this case we would measure the distances from a separate mean for each drug or the alternative hypothesis could be that there is no difference between drug C and D but drug E is doing its own thing in this case we would calculate the distances from a single mean value for drug C and D and a separate mean for drug E so far we have two different alternative hypotheses in depending on which one we use in the statistical test we can end up making a different decision about the null hypothesis and that is why it is important to clearly state which alternative hypothesis we want to use however regardless of the alternative hypothesis we used in the test we only reject or fail to reject the primary or null hypothesis if we tested the null hypothesis using this alternative hypothesis and we rejected the null hypothesis we might say that we rejected it in favor of this alternative hypothesis however we would still not say we accept the alternative hypothesis because just like we saw in the stat quest on hypothesis testing other alternatives might be better in other words there are too many possibilities to test to know if we have accepted the correct one and this is why we only reject or fail to reject the null or primary hypothesis BAM in summary a statistical test needs three things one it needs data - it needs a null or primary hypothesis and three it needs an alternative hypothesis when we only have two groups of data the alternative hypothesis is super obvious because it is just the opposite of the null hypothesis but when we have three or more groups we have options for the alternative hypothesis and depending on which one we use in the statistical test we can end up making a different decision about the null double bam note if you're not already familiar with p-values these stat quests would be an awesome follow-up to this one and if you want to learn more about statistical testing check out the playlist on linear models it sounds fancy but if you've made it this far it will be a snap now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the stack West study guides at stat quest RG there's something for everyone hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of the stack quest study guides or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
8A7L0GsBiLQ,2020-06-30T17:34:48.000000,Support Vector Machines in Python from Start to Finish.,"support vector machines quest hey yeah alright let's get started first thing I need to do is share my screen with you let's get let's get back going alright here we go oh so welcome hello I'm Josh dormer and welcome to the stack quest on support vector machines in Python from start to finish in this lesson we will build a support vector machine for classification using scikit-learn and the radial basis function our training dataset contains continuous and categorical data from the UCI machine learning repository to predict whether or not a person will default on their credit card and note throughout this jupiter notebook all these links are live so if you want to learn more about the UCI machine Rapala scheen learning repository just click on the link and there it is if you want to learn more about the data set that we're gonna be using just click on the link and there it is and you see we've got attribute information this describes the data and the data set and so so there's lots lots of sort of stuff you can click on and learn more about here is a picture of the decision service that we will create with our support vector machine we'll do this at the end but I just want to give you a sneak preview of what's gonna happen it's kind of a messy figure but it's it's what we're gonna do so let's just let's move on okay so support vector machines why would you want to do them they are one of the best machine learning algorithms out there for when getting the correct answer is a higher priority than actually understanding why you get the correct answer they work really well with relatively small data sets and they tend to work well out of the box in other words they tend to not require much optimization um so in this lesson we're gonna learn about importing data from a file we're gonna learn about missing data down sampling data formatting the data for support vector machines and we're gonna build a preliminary support vector machine then we're gonna optimize it and then we're gonna build evaluate draw and interpret a final support vector from support vector machine and we're gonna compare it to that preliminary machine mm-hmm and we're gonna see if it does better or it doesn't doesn't do better um one of the things about support vector machines is because they're great out of the box sometimes optimizing doesn't give you a huge bonus like when you're using classification trees when we did that it was night and day it was like we went from a huge tree to a prune tree and that prune tree was like infinitely better at doing classification um in contrast support vector machines tend to be pretty good out of the box so let's just see what happens note this tutorial assumes that you already know the basics of Python and are familiar with the theory behind support vector machines the radial basis function regularization cross validation and confusion matrices if not you can check out those questions by clicking these links and you're good to go also I want to strongly encourage all of you to play around with the code once you get it playing with code is the best way to learn from it there's alternative ways to do what what we want to do in this um in this Jupiter notebook and there are different ways to optimize and there's different ways there's you can even just plug in different machine learning algorithms for that matter and just see what happens there's a lot of learning that can take place by just playing around and that's when I think that's them that's the exciting part I mean this is pretty fun too so don't get me wrong ok so the very first thing we're gonna do is we're going to import the modules that will do all the work Python itself just gives us a basic programming language these modules give us extra function extra functionality to import the data clean it up and format it and then bill evaluate and draw the support vector machine note you're gonna need Python three and I've got instructions on how to install that and make sure all your modules are up to date down here we don't need to go through that because I've already got this set up on my own computer however I do want to say that we're using a jupiter notebook and as you can see jupiter notebooks are a nice way to convey combined text with code and so here is some code this is where we're importing the modules or importing pandas to load and manipulate the data and we're using it for one hot encoding there's also a numpy that we're gonna use for data manipulation we're importing menthe mat plot Lib for graphs and making things look cool and we're also importing a lot of scikit-learn stuff to do support vector machines and do confusion matrices and whatnot now when you have a jupiter notebook the way you run the code is you click somewhere in that block of code and you can either click on this play button up here to run the code there's a run menu and you can say run selected cells or on your computer there's a key combination on a Macintosh it's command or command enter or control enter on different computer systems it's different so just click the the run menu and it'll tell you what to do um so what I'm gonna do is I'm gonna click in this block of code and we're gonna run it and when you run it briefly you might have missed it there was a star there before it turned into a number one the star means that python is doing something and and at that time it was loading the modules and when it gives you a number that means it's done doing whatever it was doing so it loaded the modules pretty quickly okay now we're ready to import the data now we load the data set from the UCI machine learning repository specifically we are going to use the credit card default data set this data set will allow us to predict if someone will default on their credit card payments based on their sex age and a variety of other metrics note when pandas which is what we're going to use to read in the data when it reads in data it returns a data frame which is a lot like a spreadsheet the data are organized in rows and columns and each row can contain a mixture of text and columns excuse me text and numbers the standard variable name for a data frame is the initials DF and that's what we're gonna use here and I've got two blocks of code here one is to read in the file that we're gonna use this is a file that will be included with a cheaper notebook so you don't have to download it or anything we're gonna be using read underscore Steve CSV to read it in the file has a header row so the very first row actually it's the second second row in the data set has the header information the first row has some other kind of nonsense and I don't actually remember what it was and it's tab delimited so we're setting this separator to the tab escape sequence however you can also read this directly from the UCI machine learning repository directly you don't have to download the file first um and this is commented out so we won't be running but when you play around the code later you can feel free to play with it yourself okay so we'll load in the data and there we are we've got a number here that means we ran the code and so now that we've loaded the data into a data frame called DF we're gonna look at the first five rows using the head function um let's just run that so we've got D F dot head and that returns us the first five rows and we see that we have a bunch of columns for the variables collected for each customer the columns are ID that's just an ID number I'll limit balance is the is the is the credit limit for that customer sex is male or female education marriage age payment these these columns tell us whether or not the last payment was on time or later or how late it was there's there's one column for different months of payments so so we found just one last month's payment its it goes back six months then we have the bill amounts for the last six months last six bills and how much was paid for the last six bills and then lastly we have default payment next month this is the variable that we're going to try to predict and here I've listed the column names as well but note the last column name this one default payment next month that is a mouthful so we're gonna change it to just be default um so did they default or not um and we do that by doing data frame don't rename and we pass in the column name that we want to change and then the name we want to change it to when we set access to columns um so we're specifying that we want to change a column name lastly we're saying we want to do it in place meaning we're gonna modify this data frame that we're not gonna I'll make a copy and then save it as a new variable and then what we're gonna do is we're going to print out the next the first five rows and we're going to verify that we rename the column correctly okay so we look over here and we see that the last column name is now default and that's great that's what we wanted the other thing is this is ID column these are just random numbers that were assigned to each customer and they're not informative so we're gonna drop it so we'll do that with D F dot drop and we're specifying the ID column again this is a column and we're gonna do this in place just like we did before and we're gonna print out the first five rows to verify that we were moved the column and there it is like we've got ID up here and now we no longer have the ID so hooray we've cleaned up the columns a bit and now that we have the data in a data frame called DF we are ready to identify and deal with missing data unfortunately the biggest part of any data analysis project is making sure that the data are correctly formatted and fixing it when it is not the first part of this process is identifying and dealing with missing data missing daily data is simply a blank space or a surrogate value like n/a that indicates that we failed to collect data for one of the features for example if you forgot to ask someone's age or forgot to write it down then we would have a blank space in the data set for that person's age there are two main ways to deal with missing data we can remove the rows that contain missing data from the data set or we can impute the values that are missing and in this context impute is just a fancy way of saying we can make an educated guess about what that value should be so in this section we're gonna focus on identifying missing data in the data set first let's see it what sort of data is in each column to do that we've got our data frame and we're asking for the data type so we're going to check out the data types with the d-types command and when we run that code we see that every column is inte 64 which is good or at least it looks good because it doesn't tell us off the bat that the person mixed letters and numbers um in other words there are no na values and that suggests that maybe things are in good hands and because they didn't use character based placeholders for missing data and data frame that's just that said we should still make sure that each column contains acceptable values the list below describes what are allowed in each column and was based on the column descriptions in credit card in the on the credit card default web page so the limit balance of the credit balance or credit limit excuse me is the amount available available credit and that's an integer sex is a category we have one for male and two for female education is also a category we've got four categories for that graduate school is 1/2 equals University 3 is high school and 4 as others marriage is also a category we have one for married to for single and three for other ages and integer pay this is the these are a series of columns that tell us when the last six bills were paid negative one means on time and then we have how much after that we've got how much it's been delayed we've got bill amount pay amount and default which is a binary column that has zero for did not default and one for defaulted so we're gonna start by making sure sex only contains numbers 1 & 2 and we do that with our data frame and in square brackets we specify that we want to look at the column named sex and we've got that in single quotes and we're what we want to do is we want to look at the unique values in that column and we do that with the unique function so let's run that and we see that the unique values in this column named sex are 2 & 1 so BAM it does um now we're going to look at education to make sure it only contains 1 & 2 3 & 4 so we do the exact same thing we did before only we swapped sex with education when we run that we see that for reasons unknown in addition to the allowed numbers 1 2 3 & 4 education also contains 0 5 & 6 it is possible that 0 represents missing data and 5 & 6 represent categories not mentioned in the original specification but that's just a guess oh now we're going to look at marriage and make sure it only contains 1 2 & 3 so we do that the exact same code as before only this time we're specifying the column named marriage and like education marriage contains 0 which I'm guessing represents missing data now note this data set is part of an academic publication that is not open access it's owned by a company called Elsevier so in theory I could pay Elsevier a lot of money to get the article and find out if 0 represents missing data or not there's a good chance that the author's even didn't even mention in the article so there's a good chance that even if I paid a lot of money for the article I still wouldn't know and because this is just a demo I'm not gonna worry too much about being correct and instead we're just gonna see what happens when we treat 0 as missing data and I will give you I will say this I tried it both ways and the support vector classifier performs better when we treat zero as missing data so I at least tried it both ways and and I said well since it works better this way we'll just assume that that means missing data since scikit-learn support vector machines do not support datasets with missing values we need to figure out what to do with the zeros and the data set we can either delete these columns from the training data set or impute values for the missing data first let's see how many rows contain missing values we do that by using the Lok or location function that is associated with our data frame and what you do is you specify a sort of a logical statement that if it's true it'll return [Music] of those those specific rows that were interested and were over what we're interested in if we're interested in finding rows that have zero in the education column or and that's a logical or and an old-school computer language talk we call that a pipe the pipe character but it's a logical or so we want to eat so we want all of them we want we want all the lines in the data set where there's a zero in the education column or or there's a zero in the marriage column and we wrapped all of this up in the Len function which is short for length and so we're gonna count the number of rows that have zeros in those columns and when we run that we see that there are 68 rows that have missing values so now we're gonna count the total number of rows in the data set and we're gonna use that Len or length function just like we did before only this time we're not specifying which rows we want and when we don't specify specific rows we'd get them all and we see that there are 30,000 rows in the data set to begin with and so 68 of the 30,000 rows or less than one percent contain missing values since that still leaves us with more data than we need for a support vector machine we will remove the rows with missing values rather than try to impute their values and like I said we're gonna try to do imputing imputation in a future webinar hopefully in two months so the way we do this we select all the roads that do not contain zero in either education or marriage and the way I said that was a little awkward because we're actually going to use an and a logical and in this statement so just like before we're getting all the rows we're using this loke function to get the rows that where this logical statement is true and what we want are rows that do not have 0 in education and do not have 0 in marriage and we're gonna save all those rows that don't have zeros in education and ah and they don't have zeros in in marriage we're gonna save all those rows in a new data frame called data frame no missing all so we'll run that and since data frame no missing has 68 fewer rows in the original data frame it should only have twenty nine thousand nine hundred thirty-two rows so we're gonna count the number of rows using that length function again and there we got it hooray the math works out however we can also make sure that education no longer contains zeros by printing out the unique values this is exactly what we did before however now we're specifying data frame no missing rather than just data frame the original data set and now we see that there's no zero there and when we do the same thing for marriage we print out the unique values for marriage now we just have one two and three so BAM we have verified the data frame no missing does not contain any missing values all right and we're ready to move on to the next section where we down sample the data set so like I've said support vector machines are great with small data sets they the the data set that we used for classification trees is relatively small and support vector machines do pretty well with that data set um however uh like I said they can take a long time with large data sets and this is relatively large so we're gonna down sample both categories customers who did not default and customers that did down to a thousand each so first thing we're gonna do is we'll remind ourselves how many rows of data were working with because we remove some of them so we use the length function again and that tells us we've got twenty nine thousand nine hundred and thirty-two samples that's relatively large so we're going to down sock down sample it to a thousand of each category so the way we do that is we've created this we've already have this data frame called data frame no missing and we're specifying we want all the rows where the someone did not defaults a default is zero and we're gonna store that in a new variable called DF no default and then we're doing the same thing for the people that default it we're storing them in another data frame so we're splitting the data into two variables here one for people that default it and one for people that did not default and now what we're doing is we're down sampling the people that did not default we're using the resampled function and we're passing it the the data frame that consists of people that did not default we're setting replace to false so that means when we pull something out of there we don't and we put it in our new data frame we don't put that back in the pool of possible people that we could select again and we're saying that we want a thousand samples and we're setting the random state to 42 that's a random seed and all that does is make sure that you get the same answer that I get all and then what we're gonna do is we're gonna print out the length of this new data frame that we're creating data frame no dough no default downsampled so let's run that oh no I got there too early I didn't split the data set and a half so yeah so when you see this pink that means there that means something went wrong and instead of feeling great shame what you got to do is just got a default Deebo debug it and so let's rerun this again because before the first time I ran it I hadn't run this chunk of code first and so this data frame no default this guy did not exist but now it exists so let's see what happens and it runs and tells us we've got a thousand rows and our new data frame data frame no default downsampled now we're gonna do the exact same thing but this time we're using the people that defaulted and we're going to print out the number of rows there and there so we've got two new variables each containing a thousand rows each and now what we want to do is we want to merge them back into a single data frame and print out the total number of rows to make sure everything is hunky-dory to merge the two data frames that were created we're using this pandas function called concat which will concatenate the two two things so there we go BAM mm so now that we've taken care of the missing data we are ready to start formatting the data for making a support vector machine the first step is to split the data into two parts we're gonna have one part that contains the columns of data that we will use to make classifications and one part is going to be a column of data that contains the things we want to predict so we're gonna use the conventional notation of capital X to represent the columns of data that we will use to make classifications and we're gonna use lowercase Y to represent the thing we want to predict in this case we want to predict default whether or not someone defaulted on their payments um so here's how we're going to do that we've got our down sample data set and what we're doing is we're gonna drop the default column and we're going to copy that we're gonna store that into uppercase X and that's gonna be our set of columns that we're going to use to make predictions and then we're gonna print out the first five rows so let's do that BAM okay you'll notice we scroll over to the right we no longer have that column called default so this data set does not contain the thing we want to predict now here or we're going back or going back to that date data frame that contains the down sampled samples ah and now we're just specifying that one column default and we're making a copy of it and we're storing it in a new variable called Y and then we're gonna print out the first five rows BAM okay now we've created uppercase X which is the data we're going to use to make predictions and lowercase Y which has the data we want to predict we are ready to continue formatting x over here so that it's suitable for making a support vector machine oh okay so now we're on to one hot encoding now we have to split the data frame into two pieces uppercase X which contains okay we've already done this we've split things up now what we need to do is take a look at the variables in X and this list tells us whether the variables supposed to be an integer or a category so we see limit balance is an integer that's the credit limit sex is a category that's male or one for male two for female education has a bunch of categories 1 2 3 & 4 we've already talked about these okay so it looks like sex education marriage and pay are supposed to be categorical and they need to be modified this is because scikit-learn support vector machine support vector machines while they natively support continuous data like limit balance and age they do not natively support categorical data like marriage which contains three different categories thus in order to use the categorical data with scikit-learn support vector machines we have to use a trick that converts the column of categorical data into multiple columns of binary values and this trick is called one hot encoding um so at this point you may be wondering what's wrong with treating categorical data like it's continuous so to answer that question let's look at an example for the marriage columns we have three options one married to single and three other if we treated those values one two and three like continuous data then we would assume that three which means other is more similar to two which means single than it is to 1 which means married and that means the support vector machine would be more likely to cluster people with threes and twos together than people with 3s and ones together and in contrast if we treat these numbers like categorical data then we will treat each one as a separate category that is no more or less similar to any of the other categories thus the likelihood of clustering people with twos and threes is the same as clustering threes and ones and that approach is more reasonable note I have a huge treatise on different ways to do one hot encoding there's two very common ways to do it there's a there's a function called column transformer from scikit-learn and there's a function called get dummies from pandas because I believe that get dummies is bed it better for teaching how one hot and coding works we're gonna use it however just know there's alternatives and you should definitely read this description at your leisure to learn about the pros and cons of these two different approaches okay first before we commit to converting columns with one hot and coding I just want to show you what happens when we convert marriage without saving the results so to make this easy to see we're gonna use get dummies get dummies as a Panda function and so we pass it X capital X that's the columns that we want to transform and we specify the columns that we want to transform for this demonstration we're just gonna transform marriage and then we're gonna print out the first five rows to see what it did okay scroll over here and you'll see that on the left side of the data frame are the columns that we did not touch and on the right side we've got three columns for marriage we used to just have one that contained three values now we have three columns and each one contains a 0 and a 1 1 in this column marriage 1 there's a 1 if there was a 1 in the original marriage column there and a 0 otherwise for marriage - there's a 0 if where there's a 1 if they originally had a a 2 in the marriage column and a 0 otherwise and marriage 3 is 1 if they originally had a 3 in the original column zero otherwise so that is how one hot encoding works okay now we're gonna do it for all of the categorical um columns and then we're gonna print out the first five rows just to see what it looks like alright so here is our new data frame we've got a dot dot dot specifying that some columns are not shown but you see that hey these columns have all been converted into new columns so double bam the last part of formatting the data for a support vector machine is to Center and scale the data um the radial basis function that we're going to use assumes that the data are centered in scale scaled so in other words each column should have a mean of 0 and a standard deviation of 1 so what we're gonna do is first what we're gonna do is we're gonna split the data into training and test data sets so we're using train tests split again we're setting the random state so that you could reproduce what I've got we're passing it in X encoded that's the one haunted coded X these columns over here that are that are gonna make the predictions and we're passing em Y that's the thing we want to predict and we're creating X train X test Y train and why test I believe the default setting is for 70% of the data to go into the training data set and 30% to go into testing don't quote me on that I just believe that's the case after we do that we are scaling the data sets using the scale function now right long um now we're gonna talk about building a preliminary support vector machine um we've done a lot of stuff we've almost been a whole hour just formatting data and now we're finally getting to the good part the way we do that is we call SVC for support vector classifier we set the random state to 42 and what this does is it kind of makes an untrained shell of a support vector classifier the next step and we're saving that shell as classifier underscore support vector machine the next step is to call fit using that shell and so what we're doing is we're fitting it or we're training it on the training data so we've got X train scale and y train oh and we know we don't scale y train because that's just 0 on one and my brain was left my head for a second so anyway so that's correct and we run it and it just prints out the support vector classifier plus the default settings that it has and now that we've trained the classifier we can see how it performs with confusion matrix and we're going to use the test data set we're passing in we're using plot confusion matrix for PLAs passing in our trained support vector machine and we're passing in the test X data set and a test Y and below we're just formatting it so it looks pretty so here is our class here's our confusion matrix it did all right it did not do great of the 257 people that did not default 79 percent were correctly classified of the 243 people in this row that defaulted 61% were correctly classified so the support vector machine was not awesome so we're going to try to improve that using cross-validation to optimize the parameters to optimize the parameters we're gonna use grid search cross-validation or grid search CV and when we optimize a support vector machine it's all about finding the best value for gamma and potentially the regularization parameter C so what we're gonna do is when we use grid search CV we specify the parameters that we want to try in this you know in a matrix um and so we've got this is the parameter C which is the regularization parameter and we're gonna try these values um note the regularization parameter Scott to be greater than zero oh and there's gamma and these are the values were specifying for gamma in theory we could try other kernels if we wanted to and specify those as well that ends up taking a long time so we're just gonna stick with the radial basis function typically it gives us the best performance okay so then we run grid search C V and we pass in the shell of a support vector classifier we pass in the parameters the C V is the number of folds of Kraft cross-validation we want to do then we pass in the scoring metric that we want to use there are lots of options and I tried a bunch of them and true to its word support vector machines tend to be good out of the box and I was trying to find a metric that would give us huge improvements and I wasn't able to find one but you can uncomment these and try different metrics there's even more I'm using here um anyways um this Wilkin will return something we're saving in a variable called optimal parameters and then what we do is we run the cross-validation on the parameter values by running optimal values fit using the training data set and then we're going to print out the optimal best parameters so we'll do that this is gonna take a little long you'll see this a star here that means a Python is hard at work doing cross-validation for us when that turns into a number it'll print out the output down here and this is one of the reasons why we downscaled the dataset we went from 30,000 rows to just 2,000 and it still took a little bit of time I mean we're not talking hours but it still took a little bit of time these are the optimal parameters by the way we're done running we've got a number here oh and we can see that the ideal value for C because that's a literation is 100 which means that we will use regularization and the ideal value for Gamma is 0.001 so now we're ready to build evaluate draw and interpret the final support vector machine so now we're doing exactly what we did before however this time we're specifying C equals 100 and gamma equals 0.001 and then we are so that creates the shell and then we train it using the training data so we'll run that BAM and now we're going to print out a new confusion matrix with our new support vector machine and we're going to run that and here is our all put and these are the results the optimized support vector machine and they are just a little bit better than before for more people or correctly classified as not defaulting however one person was incorrectly classified as defaulting so that mean depending on how you want it depending on whether or not it's more important classify these people or these people we may have improved or we may have slightly gotten worse but that just tells you again that support vector machines are pretty good out of the box optimization didn't help us that much okay now we can actually draw what the decision boundary is um this is a pretty complicated procedure and at best it is an approximation of of what is really going on and we can quantify how good that approximation is various ways that we can quantify how good that approximation is going to be and we'll go over that really quickly the first thing we're gonna do though is we're gonna look at see how many columns in our our data set so we've got 24 columns in our data set and this is a problem because that mean"
isEcgoCmlO0,2020-06-15T15:36:26.000000,"Live 2020-06-15!!! Bootstrapping, Main Ideas","hello I'm Josh Dahmer and welcome to my live stream strap it it sounds so complicated strategy it's not so complicated stick quest hello I'm Josh Dahmer and welcome to my live stream I guess I got it backwards today there's always something I mess up at the intro however I'm super excited to be here and I just want to say I'm super excited about where everyone is watching from we got people in New York Pennsylvania Malaysia Mexico Germany Indonesia India yeah if someone said nice hair I just combed my hair a little bit it's really long these days I'm debating whether or not I should get a haircut this week because it's really getting long and I just don't know what to do I don't know if you get some advice put it in the chat maybe I could cut it myself I've thought about it oh we've got South Korea Montreal which is super exciting Virginia we've got some people in North Carolina my home state watching which is cool Moscow China all the way Holy Smoke the other side of the world Melbourne Australia Sweden Japan all over um super exciting to have everyone Jordan holy smokes all over the world watching a stat quest live stream on bootstrapping oh we've got Romania Georgia more India Hyderabad Iraq Iran got that area covered Wow Colombia India Brazil Brazil let your hair be says somebody kerala india I love Kerala what a beautiful place Malta wow I've never been to Malta Argentina Sicily oh I'd love to go to Sicily Morocco Austin Texas Hungary Hungry's where all the mathematicians live isn't that right or at least historically Baja rain the more Germany u.s. giorgia Portugal Canada more Brazil or Brazil Vancouver Canada India Wow Morocco as well it's very I had a friend from Morocco when I was in college he was super cool Jaipur and Wow okay London Chile more morocco bangalore this is super exciting alright I'm gonna get started with the stat quest so let's move on and we're going to talk today we're going to talk about bootstrapping main ideas bootstrapping is a simple yet powerful technique that's used a lot in both statistical inference and machine learning bootstrapping may sound fancy but it's not that's the that's the cool thing about it today we're going to go through an example that shows how bootstrapping can be used for statistical inference the plan is for the next live stream I think that'll be the first Monday in July is to then show how it can be used in machine learning bootstrapping is kind of a cool thing that has a wide variety of uses and has kind of been the savior of of statistics catching up and through the machine learning machine learning is moving so fast but one thing in statistics but that's been able to keep up with machine learning has been the bootstrap okay so without any more jibber jabber introductory nonsense stuff let's get started ok imagine we had a new drug to treat an illness and we gave that drug to eight different people that had the illness for five of those people the drug helped them feel better but for three people the drug made them feel worse wha-wha okay if we calculate the mean of the response to the drug we get 0.5 0.5 isn't a Hugh improvement but since most of the people five of eight improved maybe this drug is better than using no drug at all however maybe those eight people excuse me those five people hahaha maybe these five people let me just change that while we're at it huh okay maybe those five people all felt better because they were healthy to begin with and maybe these three people all felt worse because they had unhealthy lifestyles so it's possible that the reason we got a mean value equal to 0.5 instead of zero it's because of random things that we can't control so we create a hypothesis on average the drug has no effect on whether people feel better or worse in other words the hypothesis says that the only reason we see a difference between the mean of the data 0.5 and zero is due to random chance now we need a p-value to decide if we should reject the hypothesis there are a lot of ways to calculate p-values but today we're going to use bootstrapping BAM okay first let's before we start doing the bootstrapping what we're going to do is we're going to shift all of the measurements to the left 0.5 units so that the mean is shifted to zero yes so all we did is we just move those 8 measurements over 0.5 units so now the mean is centered right on 0 okay now we can calculate a p-value by applying the bootstrap to the shifted dataset first he knows it she knows ah feels better now okay first let's make a new number line and from the 8 measurements that are centered on 0 choose one at random and add that value to the new number line now we go back to the original eight measurements that were centered on zero and choose another random value and add it to the new number line then we repeat the process randomly selecting one of the eight values centered on zero for the new number line a total of eight times note we can randomly select the same value more than once oh no it's a terminology alert randomly selecting data and allowing for duplicates is called sampling with replacement it's one of the key things that makes bootstrapping what bootstrapping is that's an awkward scent sentence let me rephrase that sampling with replacement is one of the key aspects of and it makes bootstrapping it makes it's what makes bootstrapping bootstrapping maybe words aren't my specialty today it's it's a key aspect of bootstrapping okay anyway so far we've only selected six measurements so we need two more beep-boop note the reason we selected eight measurements for the new number line is because the data set we are sampling from contains eight measurements if we'd started with ten measurements up here then we would have needed to add ten measurements down here terminology alert oh no this new data set that was created using sampling with replacement so that it had the same number of values as the original is called a bootstrap data set okay now that we have a new bootstrap data set we can calculate the mean and add that mean to what will soon be a histogram of means so it's not much of a histogram yet because we've only added one value to it but we're gonna add more stuff to it and over time it's gonna look like a cool histogram okay now we start over with a fresh number line and randomly select one of the eight values centered on zero for the new number line repeating a total of eight times in allowing duplications beep-boop then we calculate the mean and add that to our histogram note this process of creating a bootstrapped dataset and then calculating something in this case we're calculating the mean and keeping track of those calculations is called bootstrapping no big deal in other words bootstrapping consists of three steps one make a bootstrapped dataset to calculate something in this case we calculated the mean three keep track of that calculation note in step two we calculated the mean but we could have just as easily calculated the median or the standard deviation or anything else we wanted to test or learn more about later on I'll say why this flexibility is awesome for now I'll just say BAM okay now that we know what bootstrapping is we just do it a bunch of times usually we use a computer to bootstrap thousands or ten thousands of times I'm not gonna go through every single one we're just gonna skip to the histogram okay so after creating thousands of bootstrap samples calculating their means and adding them to this histogram we end up with this because we sampled with replacement the histogram ended up with a wide variety of mean values for example all these different value or in this bin in the histogram and because we are randomly selecting with replacement from eight values they are up to eight to the eighth or 16,777,216 combination of observed values and possible means that we could put into that histogram because there are so many combinations bootstrapping usually only creates a subset like ten thousand to estimate the full distribution in this case because we created the bootstrap samples from a collection of measurements with mean equal to 0 then the histogram tells us how the means from each bootstrap sample are distributed around 0 in this case because 36 percent of the bootstrapped means were between negative 0.5 and 0.5 the probability of observing a mean between 0 negative 0.5 and 0.5 is 0.36 so the percentage is essentially the probability the percentage of observations in a region in the histogram equals the probability that you'll get a randomly selected mean or bootstrapped mean in that range and because 16% of the bootstrap means were less than or equal to negative 0.5 the probability of observing a mean with a value less than or equal to negative 0.5 is 0.16 and because 48 percent of the bootstrap means were greater than or equal to 0.5 the probability of observing a mean with a value greater than or equal to 0.5 is 0.48 yeah that's right BAM I just want to double-check that I didn't have a big mistake right there but I think we're all good okay cool okay now let's finally go back to the original sample and focus on its mean value 0.5 because we wanted to know if we got 0.5 instead of 0 because of random things like maybe these people had healthy lifestyles and maybe these people had unhealthy lifestyles we formed this hypothesis on average the drug has no effect on whether people feel better or worse then we shifted the sample over so that it was centered on 0 so there it is centered on 0 BAM and then we use bootstrapping to make this histogram of means and because the shifted sample is centered on 0 the histogram represents possible values for the mean if the drug on average made 0 difference in how people felt so the p-value for the observed mean 0.5 given this histogram that represents the hypothesis that the drug had zero effect is the probability of observing a bootstrap mean that is greater than or equal to 0.5 which is 0.47 that's the probability plus the probability of observing a bootstrap mean less than or equal to negative 0.5 which is 0.16 adding those two probabilities together gives us 0.63 and because 0.63 is greater than 0.05 we fail to reject the hypothesis that that the drug makes no difference double Bamm okay now let's quickly review the bootstrap bootstrapping consists of three steps one make a bootstrap dataset to calculate something in this example we calculated the mean but we can calculate other things like the median or the standard deviation 3 keep track of that calculation okay now let's talk about why step 2 leads to awesomeness first let's go back to the original data last time we calculated the mean and then shifted the data over so that it had mean equal to zero and then use bootstrapping to create this histogram of means around zero and then we use the histogram to test the hypothesis that the drug made zero difference however we could have just as easily calculated the medium for the original data and shifted the data so the median equals zero and then used bootstrapping to generate this histogram of median values around zero note that's a funky looking histogram but it tells you that you know medians have strange distributions and that's why we don't often use them in statistics although they are great because they're resistant outliers they've got weird properties and we can see that sort of in this in this histogram but with bootstrapping who cares we just generate this distribution and we can use the histogram to calculate the p-value for the observed median given that the hopf hypothesis given the hypothesis that the drug has zero effect in this case the observed meeting is 1.8 so the p-value for the observed median 0.8 given this histogram that represents the hypothesis that the drug has zero effect is the probability of observing a bootstrapped median greater than or equal to one point eight which is 0.01 plus the probability of observing a bootstrapped median which is less than or equal to negative one point eight which is zero point one nine and when we do the math we get zero point two so again we would fail to reject the hypothesis that the drug has zero effect but this time we use medians instead of means and the point of this example is that we can use bootstrapping to test whatever we want and that means that if our data looks like it has outliers we can use medians which are more resilient to outliers than averages triple bam okay usually triple of am means we're totally done but before we're done I want to mention a few more things and make some shameless self-promotion okay although we first shifted the data sets before bootstrapping to calculate p-values we could have just as easily bootstrap the original data set and created a histogram of means or medians or whatever just using the original data set and then we can use the histogram to calculate the standard error of the main or or if we use the median we get calculate the standard error of the median or if we calculated the the standard deviation we can calculate the standard error of the standard deviation or we can use this histogram to calculate confidence intervals around the mean the standard error is just the standard deviation of this distribution so that's a that's easy and a 95% confidence interval is just an interval that covers 95% of the bootstrapped amines or medians or standard deviations or whatever no big deal shameless self-promotion for more details about using bootstrapping for confidence intervals or standard errors check out these quests these are some like prehistoric quests this is these are from years ago when I was just starting out making these videos to begin with so in some ways they're classics in some ways they're clunkier than my new ones which are still pretty clunky but these old ones are kind of like old gems if you want to see me you like completely like unscripted and just sort of winging it check out these videos anyways with that we're done with this stack quest so we've got a little extra time on our hands and so what I'm gonna do is I'm gonna go over to the chat window and I'm gonna see if we got any questions I know we got a lot of people from calling in or not callin watching from all over the place which is super cool let me see if I missed anyone before Chile Morocco Bangalore Sela pora interesting Austria Turkey Wales Thailand Eugene Oregon Poland hello Poland all the world here Gujarat Bangalore Mumbai Vietnam good morning Vietnam Mexico New Delhi Algeria more Korea what else poon Nepal hello open Nepal I went to Katmandu a long time ago when I was 10 years old and I loved it more Russia okay what else do we have do we have any good questions why we shifted before the bootstrapping so the only reason in this example because we wanted to calculate P values because we wanted to calculate P values let's go back to this the goal we wanted to calculate a p-value for that for the red mean we wanted to decide if the observed mean oh the light just went out so it's a little darker in here we wanted to know if the observed meaning 0.5 if that it's if the only reason why it's different from zero is due to random chance and so what we want to do is we want to create a distribution of means around zero to see how rare or rare similar means might be to what we observed with the real data so that's why we're shifting things over we shift things over so that when we create this histogram down here when we create this histogram this histogram will be centered on zero and that histogram will then tell us how means are distributed around zero and we can then go back and look at the original mean that we calculated up here right here that original mean and we can see that when we have mean centered on zero it's actually pretty common to have it's actually pretty common to have mean centered but around zero to still be pretty far away from zero and because that's pretty common we we don't have any confidence that the observed mean 0.5 is super-special anymore so we fail to reject the hypothesis so that's why we shift however I want to make a point that shifting is only necessary when we went when do we want to test the hypothesis that there's no difference and if you want to know more about hypothesis testing check out the last livestream that we did you can go to stat questo RG and there's a link to the video index and at the very bottom I've got all the live streams you can go down to the live stream and you can check out hypothesis testing and that's when you'll find out why we're interested in testing theer's ero difference let's see if we got some see if we got some more questions someone asked if I have a video on back propagation and I'll just tell you right now absolutely not however I'm working on it Oh back propagation is all about neural networks and neural networks have two that back propagation algorithm is based on the on being able to do the chain rule and then applying gradient descent on the derivative of these activation fronts so we use the chain chain rule from high school calculus which for some people was a long time ago and so what I'm doing right now is I'm doing a video on the chain rule and maybe not the coolest of topic topics but it's one of these things that if you understand very well and very deeply understanding that back perkupp propagation understanding back back propagation will be a snap so what this video on chain the chain rule is is a stepping stone so we're gonna get the chain rule out of the way and then we're gonna cover back propagation and neural networks and we're gonna look at that and go hey I'm so solid in the chain rule because I watch the stat quest on it and now when I'm seeing this thing on back propagation it's all super easy and obvious so we're getting there we're gonna get two back propagation sooner or later I'm excited about that somebody else said bootstrapping triggers my statistical commuting computing course and that is that is exactly right bootstrapping is all about statistical computing but it's also like I was saying early on bootstrapping is sort of because it's so adaptable and so flexible and we don't have to like come up with some theoretical distribution we can always just bootstrap and create the distribution on the fly bootstrapping is a modern technique that uses computers but it allows us to just kind of generate new statistical tests on the fly without having to spend years and years doing theoretical fancy fancy stuff what else do we have any more questions I'm still just scrolling scrolling through bootstrapping someone asked if it's related to the bootstrapping used in structural equation modeling and it's very similar and we're gonna be talking about how bootstrapping is used in machine learning it's used in a slightly different way but also a very related way obviously when we're doing statistics we want to calculate p-values and confidence intervals and when we're doing machine learning maybe we want to do those things plus something else that's a little us also interesting so well we'll see how this bootstrapping concept in our next live stream which I think will be the first Monday in July we're gonna see how this concept can be applied in different settings kind of doing different things um someone asked if we should shift any other point estimates it just depends on your hypothesis whatever your hypothesis is that you want to either reject or fail to reject you shift it to that so say like our hypothesis was that this drug should have a mean response of 5 well then we'd shift everything so that the mean of the original data was now over 5 coming up with hypotheses like that is it's a little bit of a crapshoot and sometimes people do it most of the time however we do what's called the null hypothesis where we just shift everything to zero however if you've got a special hypothesis go ahead shift your samples over to that special mean someone asked if we can do support vector machines next time I've actually already got stat quest on support vector machine so if you have specific questions about those watch the videos you can find them on my channel or you can find them on the stat quest orj website again go to the video index and then you can just search the paid there's a ton of videos on that page they're kind of organized from simple to complicated and I've bracketed them in statistics up top and then below is machine learning so you're gonna go down to the second half of that page and and you're gonna see you're gonna find support vector machines and you'll watch those videos and post questions there and and if there's a bunch of good questions that I can't answer in those little tiny comments we can we can bring it up in a live stream but check out those videos first someone else says how do you do any implementation of bootstrapping if not would you do it for us Thanks so yeah bootstrapping is such a simple algorithm um most programming languages that do data analysis all like are or I'm pretty sure in scikit-learn or not scikit-learn but psych it in Python or numpy or something like that there's there's a function that will do the bootstrapping sampling for us and so then it's a very simple like two line for loop so for you know a thousand or ten thousand iterations create a bootstrapping dataset calculate the mean or the median or the standard deviation or whatever it is you want to calculate save it in an array and bam so I guess that's a three line for loop so it's not that not that big a deal um someone asked why we're using both tails if we're evaluating a 0.5 as our mean why we're doing a two-tailed test and the reason why we're doing a two-tailed test is I love two-tailed tests and I actually talked about that in my video on how to calculate p-value so if you haven't seen that video you may already know how to calculate p-values but if you go to the end uh YouTube now has these chapters and you can actually you can if you hover your mouse over the timeline on the bottom of the video there's a chapter where I describe why I like two sided p-value so much so just go to that part of the video check it out you'll understand why if someone says the idea of bootstrapping sounds like bagging and there's a good reason why it sounds like back and that's because backing is short for bootstrapped aggregate so if you take bootstrap and you in the word aggregate and you Jam them aggregating and you Jam them together you get bagging so if you're familiar with random forests we do bagging in that and that means we use bootstrapping to generate datasets or that are the same size as before so we do that in random forests we do it in adaboost XG boost a lot of a lot of machine learning algorithms will apply bootstrapping in some way and we'll talk about that in the next live stream which I believe will be the first Monday in July so that's what we're shooting for all right well we've been here for 30 minutes I don't want to be talking all day I'm really excited you guys are all here it means a lot to me that you join me for my live stream until next time until next time quest on"
q90UDEgYqeI,2020-06-07T01:50:06.000000,Classification Trees in Python from Start to Finish,decision trees from stock to finch in python we're gonna do it today hip hip hooray stat quest great well thank you guys very much for joining me for my uh webinar in decision trees from start to finish in python i'm going to share the screen right here uh can you guys all see that i'm sharing the uh this jupiter notebook um i hope everyone can see it yes i got a yes that's great so what we're gonna go through today is this jupiter notebook and i'm going to email you every single one of you guys a copy of this uh it will include the jupiter notebook which has to be opened up within jupiter but also a copy uh that can be run directly in python so if you don't have jupiter installed on your computer but you won't but you have python you still should be able to run everything that we talk about and all of the um all of the writing in here there's lots of writing lots of comments uh all of that will be in comments in the code so so you will get everything one way or the other um yeah so today we're going to use scikit-learn and cost complexity pruning to build this classification tree right here which uses continuous and categorical data from the uci machine learning repository to predict whether or not a patient has heart disease note all these things are hyperlinks so if you want to learn more about the uci machine learning repository or you want to learn more about the specific data set we're using uh you can click on the links and learn more um so so there there are lots of hyperlinks in here anyways the classification trees are an exceptionally useful machine learning method when you need to know what how the decisions are being made for example if you have to justify the predictions to your boss classification trees are a good method because each step in the decision making process is easy to understand now i know classification trees some people think they're not the sexiest of machine learning thing methods out there but they are super practical and are actually very frequently used in the medical profession because the decisions you can trace exactly what the rationale is for for everything and that's important in in certain fields and so um and i also like them just for like exploring data uh in terms of like looking to see which um features or variables are the most important so there's a lot of cool things we can do with decision trees and so we're going to learn all about them so we're going to learn about importing data which is not very exciting but it's important we're going to talk about how to deal with missing data identifying it dealing with it we're going to talk about formatting the data for decision trees um specifically we're going to talk about one hot encoding um we're also going to build a preliminary classification tree and it's going to not be very good but then we're going to optimize that thing using cost complexity pruning and then once we've optimized it we're going to build draw interpret and evaluate the final classification tree and this all covers a lot of material so we're going to move pretty quickly however if you have questions at the end of each section feel free to uh to bring them up uh you can also you should all have my email the statquest.bam gmail.com you can email me questions later um so with that let's just dive right in oh by the way i s once you get this code i strongly encourage you to play around with it um playing with the code is the best way to learn from it uh and i've got um alternative ways to do things in the comments so you can try like the way we're gonna do it today but you can also try alternative versions and see if you get the same results or not okay so um someone just asked we're going to share this notebook link i'm going to actually email everyone this uh this notebook you're going to get everyone's going to get a copy of it plus all the code so you'll get everything um so don't worry about if i move fast and you're having trouble taking notes or something like that don't worry about it you're going to get everything and i've commented and written everything up uh uh with lots and lots of details some of which we'll cover today and there's actually stuff we won't get to so you can read more and learn more once you get the notebook okay so the very first thing we do is load in a bunch of python modules python itself as many of you may know but some of you might not just gives us a basic programming language these modules give us extra functionality to import the data clean it up and format it and then build evaluate and draw the classification tree note i'm doing everything in python 3 and if you've got your own installation of python going you're going to need certain versions of of the modules these are listed here and i've got a little blurb on how to update modules should you need to do that but uh since i don't have to do that we're just going to skip right here but uh and these are the modules we're going to load we're going to load pandas we're going to do that for manipulating data and for one hot encoding we're loading num pi to calculate the mean and standard deviation and then we're importing matplotlib to draw some graphs and then a bunch of scikit learn modules and and bits to do classification trees and confusion matrices and cross validation so with a jupiter notebook some of you guys may know this some of you may not uh if you want to run code you just click in the pane that has the code and it gets highlighted and then you can go up here and you can click that play button and it'll it'll run everything there's also key combinations for doing the same thing or you can go to the run menu so we could do um control enter to run this selected cell or we could click here when we do um you will see a star um show up here and that means python is working and when it's done working it'll put a number there um the actual number is not very important so don't worry about that but when you try to run this you may see a little star for a little bit but when the star turns into a number you should be good to go so we've imported uh the modules um so now we're ready to move on to the next thing we're going to import the data we're going to load in a data set from the uci machine learning repository specifically we're going to use the heart disease data set and this data set will allow us to predict if someone has a heart disease based on their sex age blood pressure and a bunch of other metrics so we're going to use pandas to read that data frame in and when it does it returns a data frame which is a lot like a spreadsheet the data are organized in rows and columns and each row can contain a mixture of text and columns and the standard variable name for a data frame is the initials df for data frame so that's what we're going to use here we're going to try to stick to the the python convection conventions um so we've got this code um we've got data frame that's going to be our new data df that's going to be our data frame it's our new variable and we're setting it to um to the data we're going to read in using read csv which is a pandas function um the data is i'm also going to email you the data it's a relatively small file so you'll get the data as well however as you see below we can also read it directly from the machine learning repository by just plugging in the url for the data so i'm going to run this code bam okay now we've got the data loaded into a data frame called df and we're going to look at the first five rows using the head function so we've got df dot head and that will print out the first five rows i'm going to use control enter and that prints that out diff by the way different uh computers i've got a macintosh that i'm using right now uh if you're on uh windows or a pc or linux it may be a different key combination to run the code just go up to the run menu and figure out what it is on your platform of choice all right so we see um the first six rows they're kind of a mess we got row numbers and column numbers however we do not have column names and since nice column names would make it easier to know how to format the data we're going to replace the column numbers with the following column names i got these names by the way off the uci website so i didn't just make them up we've got age sex chest pain resting blood pressure uh cholesterol fasting blood sugar resting electrocardiographic results uh this maximum heart rate achieved exercise induced angina um and a bunch of other things the point is we're going to set the column names by with df.columns and they're going to be set to this array of column names and then once we've set the column names we're going to print out the first five rows like we just did and hopefully we'll see nice pretty looking column name so let's run this bam okay so uh so now instead of column numbers we've got nice column names which are much easier to remember and manipulate okay so now that we've got the data in our data frame and we've got nice column names we are ready to identify with missing data i identify and deal with missing data i apologize um so i've broken this uh into two parts the first part is going to focus on identifying missing data and then the second part is going to be um focused on dealing with missing data and unfortunately the biggest part of any data analysis project is making sure that the data is correctly formatted and fixing it when it is not the first part of this process is identifying and dealing with missing data missing data is simply a blank space or a surrogate value like n a that indicates that we failed to collect data for one of the features for example if we forgot to ask someone's age or forgot to write it down then we would have a blank space in the data set for that person's age there are two main ways to deal with missing data one is just to remove if it's if it's a single column or row that has a lot of missing data we can remove that row of data or we can move that column alternatively we can impute the values that are missing and in this context impute is just a fancy way of saying we can make an educated guess about what the value should be um so if we were missing a value for age uh instead of throwing out that entire row of data we might fill in a missing value with the average age or the median or use some more sophisticated approach to guess an appropriate value so first what we're going to do is we're going to see what kind of data we have in our data frame and we'll do that with the d types uh by looking at d types so we've got d data frame dot d types and we run that and that tells us that age is a float which is good because age is supposed to be a number sex is a float maybe that's good maybe that's not um so we got a bunch of floats and then we've got um ca and foul both have the object data type and one column hd which is just short for heart disease whether or not someone has heart disease is an integer so the fact that the ca and foul columns have object data types suggest that there's something funny going on in them object data types are used when there is a mixture of things like a mixture of numbers and letters and in theory both ca and false should just have a few values representing different categories and i know this from the uci website so you can if you want to learn more about this data set i've actually got more about the data set further down in but you can also read about it on the on the uci website anyways so what we're going to do to investigate what's going on in these columns is we're going to print out their unique values so we're going to start with ca so we've got data frame and then in square brackets and single quotes we've we've identified the column we are interested in we're interested in the ca column and we're interested in seeing the unique values we're going to use the unique function to print out those values so we run it and we see that ca contains numbers 0 3 2 and 1 and question marks the numbers represent the number of blood vis vessels that were lit up during fluoroscopy which is some sort of diagnostic procedure i actually don't know the details about it it's not super important to be able to follow along with what's going on in this webinar and the question marks however those represent missing data uh now we're going to look at the unique values in the column called fal which is short for thallium heart scan and we're doing the exact same code that we had before we've got the data frame and in square brackets and single quotes we specify which column we are interested in looking at and then we print out the unique values and again we see that foul contains a mixture of numbers representing the different diagnoses for the thallium heart scan and question marks which represent different mis represent missing values so now that we've identified some missing values we need to deal with them and that leads us to missing data part two dealing with missing data since scikit learns classification trees do not support data sets with missing values we need to figure out what to do with these question marks we can either delete these patients from the training data set or impute the missing data or impute values for the missing data so first we're going to see how many rows contain missing values we do that we're going to count the number of rows so we're going to use the len which is short for length function and we're specifying with with this line we want to look at rows um in the data frame the location of which this is true so uh is there a question mark for the ca value or that's a pipe which represents a logical or or a bitwise or uh or we want the rows that have a question mark uh in the foul spot so we're going to run this code and we see that there are only six rows that have missing values and since that's not very many we're just going to print them out so we're going to run the exact same code we just ran however we're not going to wrap it in the length function or the len function so we're not going to count the number of rows we're just going to print them out so let's run that here we are um we can see a question mark here in the foul column a question mark here question mark here blah blah blah we've got these question marks so that's what the those are that's what the data looks like um with the question marks now we're going to see how many rows are in the full data set so we're using that length function again only this time we're not specifying which rows we want to look at we're just saying let's count all of the rows when we do that we see we've got 303 rows and so 6 of the 303 rows are 2 percent contain missing values and since 297 is still plenty of data relatively speaking uh to build this classification tree we're going to remove of the rows with missing values rather than try to impute their values note imputing missing values is a big topic that we will tackle in another webinar because there's lots of ways to do it there's lots of nuance and so that's real high on my to-do list for what the next webinar is going to be so by taking the easy route by just deleting the rows of missing values we can we can stay focused on what we want to talk about today which are decision trees because we still have a lot to talk about with decision trees so what we're going to do is we're going to remove the rows with missing values by selecting all of the rows that do not contain question marks in either the ca or foul columns so this looks a lot this looks very similar to the code we were just running when we wanted to print out the rows however instead of looking for rows um that have the question mark we're looking for rows that do not so that not equal says do not match a question mark and we want to do that for both of these uh columns and we want to use the logical and to get all of the rows that do not have a question mark here or here we want everything but those rows so we'll run that and since and we oh by the way we saved um the results in a new variable called df no missing so this is our data frame with no missing values and since uh df no missing has six fewer rows in the original data frame it should have 297 rows we can verify that with this command uh and we see that it we got it the math works out so hooray the math works out however we can also make sure that ca no longer contains question marks for printing its unique values so this is just like what we did before only this time we're calling on df no missing instead of uh just df alone and we see that we just have numbers and there's no question mark here so that's good now we're going to do the same thing for thal and again we see we just have the numbers so bam we have verified that data frame no missing or the data frame with no missing values does not contain any missing values note ca and thal still have the object data type that's okay now we're ready to format the data for making a classification tree all right the first thing we need to do when we format the data for a classification tree is split the data into two parts we want to have the columns of data that we will use to make classifications and we want the one column of data that we want to predict with the data over here and we're going to use the conventional notation of capital x to represent the columns that we will use to make the classifications and predictions and lowercase y to represent the thing we want to predict in this case we want to predict hd the column specified by hd which is short for heart disease and the reason why we deal with missing data before we split it into x and y is that if we need to remove rows splitting afterwards ensures that each row and x will correctly correspond to a row and y if we do it the other way around everything's going to get mixed up so uh so what we're going to do is we're going to copy all of the rows uh excuse me all of the columns except for the one column that has that is named hd and i've got some alternative ways to do this code so you can you can play around with it once you get the jupiter notebook and then what we're going to do is once we copy everything but hd we're going to look at the first five rows just to verify that we did it correctly so there we go and we see in the ver on the right side where we no longer have that column called hd so that worked out well and now we are uh we're gonna just copy the hd column into our new variable called y all right okay now that we've created x which has the data we want to use to make predictions and y which has the data we want to predict we are ready to continue formatting x so that it is suitable for making a decision tree all right here we get to the fun part one hot encoding a lot of you people may already know what one hot encoding is if you don't don't worry this is something we're going to go into in detail now we have to split the data frame into now that we have split the data frame into two pieces x which contains the data we want to use to make classifications and why which contains the known classifications in our training data set we need to take a closer look at the variables in x the list below tells us what each variable represents and the type of data float or categorical it should um uh it should contain okay so uh uh so we've got age uh which should be a float because that can be any number uh and we've got sex that should be a category that should be a value of either zero for females and one for males we have chest pain which should also be a category we've got four different categories we'll go through those categories in more detail later um but we see in this list we've got resting blood pressure that's just a number so we're going to save that as a float um serum cholesterol that's also just a number so that's a float um and then we've got categories and different things like that however just review let's go look at the data types in x to remember how python is seeing the data so this is how the data should be considered as as as some things are floats and some things are categories but when we go to xd types or x dot d types and we see what data type each column has we see that a lot of these things that are supposed to be categories are um like slope is supposed to be a category um but we have it stored as a float okay so that's there's some there's a problem with that uh however before we get to the problem i'm going to say that we we see that age resting blood pressure cholesterol and to latch are all float 64 which is good because we want them to be floating point numbers that's the way the data is supposed to be all of the other columns however need to be inspected to make sure that they only contain reasonable values and some of them need to change this is because while scikit-learn decision trees natively supports continuous data like resting blood pressure and maximum heart rate they do not natively support categorical data like chest pain which contains four different categories thus in order to use categorical data with scikit-learn decision trees we have to use a trick that converts a column with categorical data into multiple columns of binary values and this trick is called one hot encoding okay at this point you may be wondering what's wrong with treating categorical data like continuous data and to answer that question we're going to look at an example for the cp chest pain column we have four options one typical angina two atypical angina three non-anginal pain and four asymptomatic now if we treated these values one two three and four like continuous data then we would assume that 4 which means asymptomatic is more similar to 3 which means non-anginal pain than it is to 1 or 2 which are other types of chest pain that means the decision tree would be more likely to cluster the patients with fours and threes together than patients with fours and ones together in contrast if we treat these numbers like categorical data then we treat each one as a separate category that is no more or less similar to any of the other categories thus the likelihood of clustering patients with fours and threes is the same as clustering fours and ones and that approach is more reasonable partly because i don't really know what these what this means is are our one and two more similar i don't know because i don't know i'm gonna use one hot encoding to force psychic learn to treat this like categorical data rather than continuous data so now let's inspect and if needed convert the columns that contain categorical and integer data into the correct data types we'll start with the chest pain by inspecting its unique values okay so the good news is that chest pain only contains the values it is supposed to contain one two three and four so we'll convert it using one hot encoding into a series of columns that only contain zeros and ones um note i've got a long description on the different ways to do one hot encoding um there's two major methods one is called column transformer from scikit learn and the other is called get dummies from pandas both methods have pros and cons we're going to use get dummies today because i think it's the best way to teach [Music] how to do one hot encoding i think it's i think it by far is the best way to teach it however column transformer is more commonly used in production systems so uh make sure you're familiar with both um uh and one way to do that is just read the uh read this write-up that i've provided you it provides you with all the pros and cons of the different methods so at your leisure you can go through that however so uh so we're gonna we're just gonna use get dummies uh because i think it's better for teaching um so what we're gonna do is we're gonna start with chest pain and just to see what happens when we can convert chest band we're going to do this without saving the results um just so we can see how git dummies works so what we're doing is we're going to use this panda function get dummies or passing it our data uh our data frame which we're calling x that's the that's the data we're using to make predictions and we're specifying one column we're just going to specify the chest pane column we could specify a bunch of columns and convert them all at once but right now we're just going to specify chest pane and we're going to print out the first five rows to see what it does to the chest pane column so let's run that and we can see in the printout above that git dummies puts all of the columns it does not process in front and it puts chest pain at the end right here so everything we did not touch is up here on the left side and everything that we did touch was with just chest pain is on the right side it also splits chest pain into four columns just like we expected it to do chest pain 1.0 is one for any patient that scored a one for chest pain and zero for all other patients chest pain 2.0 is one for every patient that scored two for chest pain and zero for all other patients likewise we have chest pain three and chest pain four and this accounts for all four different options we had for chest pain um so now that we see how git dummies works we're going to use it on the four categorical columns that have more than two categories and we're going to save the result this time we're not just going to print it out okay note in a real situation and not a tutorial like this what you should do is verify that all five of these columns only contain the accepted categories uh i feel like every data set i've ever worked with uh always has someone just typing in something completely random and we need to get rid of that stuff so use that unique function to make sure that each one of these columns is correctly formatted however for this tutorial i've already done that so we're going to skip that step so here we're we're doing the exact same thing we did before except now we're specifying four columns to process and then we're when we're saving it in a new data frame called x encoded and then we're going to print out the first five rows of x encoded bam there it is um so we've got chest pain resting electrocardiogram we've got slope and we've got foul and so they've all been one hot encoded now we need to talk about the three categorical columns that only contain zeros and ones sex fasting blood sugar and exercise induced angina um as we can see one hot encoding converts a column with with more than two columns excuse me more than two categories like chest pain into multiple columns of zeros and ones since sex fasting blood sugar and exercise induced angina only have two categories to begin with and only contain zeros and ones we do not have to do anything special to them so we're done formatting the data for the classification tree hooray note again in practice we would use unique uh to verify that they only contain zeros and ones but to save times just trust me um now one last thing before we build a classification tree um we have uh y uh and that's what we're trying to predict and it doesn't just contain zeros and one instead it has five different levels of heart disease um zero for no heart disease and one through four for various degrees of heart disease we can see this with a unique function so y dot unique bam so we see that we've got all these different values in the y column however in this tutorial we're just going to make a tree that does simple classification and only care if someone has heart disease or not so we're going to convert all numbers greater than 0 to 1. and the way we're going to do that is we're going to store the indices of every time this statement is true every time the value in y is greater than zero we're going to save that index and then we're going to set all of those indexes to one or all the values that those indexes to want and then we're going to verify that we only have zeros and ones we're going to run this code bam and we did it actually that's a double bam we finally finished formatting the data for making a classification tree and now over here in the in the chat i see there's a question naomi thompson asks is there a limit to the number of types of chest pain uh to make sense to use in one hot encoding is the method better is one method better than other when one has hundreds of classifying values um it depends if all of those hundreds of classifying values are unique categories in and of themselves um then yes we need to use one hotend coding and um if we i mean that could happen if we have a massive data set uh if we've got hundreds and hundreds of categories um for a single value variable we we'd have to have you know a huge data set and that can happen um uh and yeah so we would apply one hot encoding uh and we would then end up with this data frame our x x encoded data frame would then have hundreds and hundreds of extra columns added to it um i've never used a data set like that before in scikit learn so i cannot guarantee that it will it will not cause the machine to crash however there are machine learning methods like xgboost that are designed to deal with situations like that specifically um so that's another uh webinar that we'll do uh in the next couple of months i've actually already got the jupiter notebook ready for xgboost uh so we'll be i'm actually just a sneak preview next month we're doing support vector machines and then we're going to do the following month we're going to do xgboost and then i think after that we're going to do imputing data imputing missing values and going through all of the various ways for doing that so that's a little shameless self-promotion right there now let's move on and build a preliminary classification tree this is preliminary because there are lots and lots of actually someone just raised a hand so before we get too into this i'll go back and address this if we use dummy variables don't we run the risk of perfect cholera and co-linearity among dummies if yes how do we deal with them um as you saw i mean i guess it's it is possible uh to get co-linearity uh among the variables uh the nice thing is with regression trees is they are relatively immune to that as a problem um typically what regression trees do is they order the columns alphabetically or numerically there's some way it goes through the um uh through the through the variables and it just picks the first one that it gets to and if i've got multiple columns that have the exact same data and uh and this in this call that first column is really good for um classifying and so it does a great job separating and the other columns would do just as well it just uses that first one every time um so it ends up not being an issue if we have redundancy in our data set and that's one of the nice things about um decision trees um all right so i think we are ready to move on um all right so we're going to build a preliminary classification tree this is a classification tree uh that is not optimized that's why it's preliminary then we'll go through how to optimize it once we get this going uh so the first thing we're going to do is we're going to split our our data into testing and training data uh sets subsets so we've got x underscore train x underscore test y underscore train y underscore test and we're using train test split to take y x encoded and y and split them into uh training and testing pieces and i've set the random state to 42 so that when you run this code you will get the exact same results that i get after we split the data we're initializing a decision tree classifier and then we are going to fit uh the data to the training data um and so let's run this and it's uneventful because we didn't print anything out and we didn't uh draw anything however uh this piece of code will draw um uh the decision tree that we just created it's a huge tree um i'm using the plot tree function that comes with scikit-learn um and we just pass it the tree that we created and trained the classification decision tree uh and and we've got a few uh parameters that were passing it to make it easier to look at so let's let's draw this there it is this is a monster decision tree it's a lot a lot bigger than the um than the um than the tree i showed you at the very top of this uh jupiter notebook i also by the way i see some people are raising their hands i'll get to those questions once we're done with the section we're almost done um okay so we've built this classification tree this monster we're gonna see how and so far it's only seen the training data set so we're going to see how how it performs on the testing data set by running the testing data set down the tree and then drawing a confu
H3EjCKtlVog,2020-06-03T12:36:37.000000,"Gaussian Naive Bayes, Clearly Explained!!!",[Music] quest hello i'm josh starmer and welcome to stat quest today we're going to talk about gaussian naive bayes and it's going to be clearly explained note this stack quest assumes that you are already familiar with the main ideas behind multinomial naive bayes if not check out the quest the link is in the description below this stack quest also assumes that you are familiar with the log function the normal or gaussian distribution and the difference between probability and likelihood if not check out the quests the links are in the description below imagine we wanted to predict if someone would love the 1990 movie troll 2 or not so we collected data from people that love troll 2 and from people that do not love troll 2 we measured the amount of popcorn they ate each day how much soda pop they drank and how much candy they ate the mean for popcorn for the people who love troll 2 is 24 and the standard deviation is 4 and a gaussian or normal distribution with mean equals 24 and standard deviation equals 4 looks like this likewise the average amount of popcorn for people who do not love troll 2 is 4. and the standard deviation is 2 and that corresponds to this gaussian or normal distribution now we calculate the mean and standard deviation for soda pop for people that love troll 2 and draw the corresponding gaussian distribution then we do the same thing for the people that do not love troll 2. lastly we draw the gaussian distributions for candy gaussian naive bayes is named after the gaussian distributions that represent the data in the training data set now someone new shows up and says they eat 20 grams of popcorn and drink 500 milliliters of soda pop and eat 25 grams of candy every day let's use gaussian naive bays to decide if they love troll 2 or not the first thing we do is make an initial guess that they love troll 2. this guess can be any probability that we want but a common guess is estimated from the training data for example since 8 of the 16 people in the training data loved troll 2 the initial guess will be 0.5 so we'll put that up here so we don't forget likewise the initial guess for does not love troll 2 is 0.5 so let's put that here so we don't forget oh no it's the dreaded terminology alert the initial guesses are called prior probabilities now the score for love's troll 2 is the initial guess that the person loves troll 2 times the likelihood that they eat 10 grams of popcorn given that they love troll 2. note the likelihood is the y-axis coordinate on the curve that corresponds to the x-axis coordinate and we multiply that by the likelihood that they drink 500 milliliters of soda pop given that they love troll 2 times the likelihood that they eat 25 grams of candy given that they love troll 2. the initial guess that someone loves troll 2 is 0.5 the likelihood for popcorn is 0.06 the likelihood for soda pop is 0.004 and the likelihood for candy is a really really small number note when we get really really small numbers it's a good idea to take the log of everything to prevent something called underflow the general idea of underflow is every computer has a limit to how close a number can get to zero before it can no longer accurately keep track of that number when a number gets smaller than that limit we run into underflow problems and errors occur so we use the log function to avoid underflow note any log will do but the natural log or log base e is the most commonly used log in statistics and machine learning so we take the log of everything and the log turns the multiplication into the sum of the individual logs the log base e of 0.5 is negative 0.69 the log of 0.06 is negative 2.8 the log of 0.004 is negative 5.5 and the log of this really really small number is negative hundred fifteen now we just add this up and we get negative one hundred twenty four so the log of the love's troll two score is negative one hundred twenty four bam now let's calculate the score for not loving troll 2. we start with the initial guess that someone does not love troll 2 times the likelihood that they eat 20 grams of popcorn given that they do not love troll 2 times the likelihood that they drink 500 milliliters of soda pop times the likelihood that they eat 25 grams of candy so let's plug in the numbers and take the log of everything and that turns the multiplication into the sum of logs now we just do the math and we get negative 48 and since the score for does not love troll 2 is greater than the score for love's troll 2 we will classify this person as someone who does not love troll 2. double bam note when we look at the raw data it almost looks like we should have classified this person as someone who loves troll 2. after all they ate a lot more popcorn than the average person who doesn't love troll 2 and they drank as much soda as the average person who loves troll 2. however the big thing is that they ate a lot more candy than the people who loved troll 2 and the log of the likelihoods for candy are way different and this difference is what made us classify the new person as someone who does not love troll 2. in other words candy can have a much larger say in whether or not someone loves troll 2 than popcorn and soda pop and this means we might only need candy to make classifications we can use cross validation to help us decide which things popcorn soda pop and or candy help us make the best classifications shameless self-promotion if you don't already know about cross-validation check out the quest the link is in the description below triple bam oh no it's another shameless self-promotion one awesome way to support statquest is to purchase the gaussian naive bayes stat quest study guide it has everything you need to study for an exam or job interview it's seven pages of total awesomeness and while you're there check out the other stat quest study guides there's something for everyone hooray we've made it to the end of another exciting stack quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
O2L2Uv9pdDA,2020-06-03T12:35:57.000000,"Naive Bayes, Clearly Explained!!!",I'm at home during lockdown working on my step quest yeah I'm at home during lockdown working on my stack quest yeah stack quest hello I'm Josh starburns welcome to static quest today we're gonna talk about naive Bayes and it's gonna be clearly explained this stack quest is sponsored by jad bio just add data and their automatic machine learning algorithms will do the rest of the work for you for more details follow the link in the pinned comment below note when most people want to learn about naive Bayes they want to learn about the multinomial naive bayes classifier and that's what we talk about in this video however just know that there is another commonly used version of naive Bayes called Gaussian naive Bayes classification and I cover that in a follow-up stat quest so check that one out when you're done with this quest BAM now imagine we received normal messages from friends and family and we also received spam unwanted messages that are usually scams or unsolicited advertisements and we wanted to filter out the spam messages so the first thing we do is make a histogram of all the words that occur in the normal messages from friends and family we can use the histogram to calculate the probabilities of seeing each word given that it was in a normal message for example the probability we see the word dear given that we saw it in a normal message is eight the total number of times deer occurred in normal messages divided by 17 the total number of words in all of the normal messages and that gives us 0.47 so let's put that over the word dear so we don't forget it likewise the probability that we see the word friend given that we saw it in a normal message is 5 the total number of times friend occurred in normal messages divided by 17 the total number of words in all of the normal messages and that gives us zero point two nine so let's put that over the word friend so we don't forget it likewise the probability that we see the word launch given that it is in a normal message is 0.18 and the probability that we see the word money given that it is in a normal message is 0.06 now we make a histogram of all the words that occur in the spam and calculate the probability of seeing the word dear given that we saw it in the spam and that is two the number of times we saw deer in the spam divided by seven the total number of words in the spam and that gives us zero point two nine likewise we calculate the probability of seeing the remaining words given that they were in the spam BAM now because these histograms are taking up a lot of space let's get rid of them but keep the probabilities oh no it's the dreaded terminology alert because we have calculated the probabilities of discreet individual words and not the probability of something continuous like weight or height these probabilities are also called likelihoods I mention this because some tutorials say these are probabilities and others say they are likelihoods in this case the terms are interchangeable so don't sweat it we'll talk more about probabilities versus likelihoods when we talk about Gaussian naive Bayes in the follow-up Quest now imagine we got a new message that said dear friend and we want to decide if it is a normal message or spam we start with an initial guess about the probability that any message regardless of what it says is a normal message this guess can be any probability that we want but a common guess is estimated from the training data for example since 8 of the 12 messages are normal messages our initial guess will be 0.67 so let's put that under the normal messages so we don't forget it oh no it's another dreaded terminology alert the initial guests that we observe a normal message is called a prior probability now we multiply the initial guess by the probability that the word dear occurs in a normal message and the probability that the word friend occurs in a normal message now we just plug in the values that we've worked out earlier and do the math beep-boop beep-boop it and we get 0.09 we can think of 0.09 as the score that dear friend gets if it is a normal message however technically it is proportional to the probability that the message is normal given that it says dear friend so let's put that on top of the normal messages so we don't forget now just like we did before we start with an initial guess about the probability that any message regardless of what it says is spam and just like before the guests can be any probability we want but a common guess is estimated from the training data and since four of the twelve messages are spam our initial guess will be 0.33 so let's put that under the spam so we don't forget it now we multiply that initial guess by the probability that the word dear occurs in spam and the probability that the word friend occurs in spam now we just plugged in the values that we worked out earlier and do the math BIP BIP BIP BIP BIP and we get 0.01 like before we can think of 0.01 as the score the dear friend gets if it is spam however technically it is proportional to the probability that the message is spam given that it says dear friend and because the score we got for normal message 0.09 is greater than the score we got for spam 0.01 we will decide that dear friend is a normal message double BAM now before we move on to a slightly more complex situation let's review what we've done so far we started with histograms of all the words in the normal messages and all of the words in the spam then we calculated the probabilities of seeing each word given that we saw the word in either a normal message or spam then we made an initial guess about the probability of seeing a normal message this guest can be anything between zero and one but we based hours on the classifications in the training data set then we made the same sort of guess about the probability of seeing spam then we multiplied our initial guests that the message was normal by the probabilities of seeing the words dear and friend given that the message was normal then we multiplied our initial guests that the message was spam by the probabilities of seeing the words dear and friend given that the message was spam then we did the math and decided that dear friend was a normal message because 0.09 is greater than 0.01 now that we understand the basics of how naive Bayes classification works let's look at a slightly more complicated example this time let's try to classify this message lunch money money money money note this message contains the word money four times and since the probability of seeing the word money is much higher in spam than in normal messages then it seems reasonable to predict that this message will end up being spam so let's do the math calculating the score for a normal message works just like before we start with the initial guess then we multiply it by the probability we see lunch given that it is in a normal message and the probability we see money four times given that it is in a normal message when we do the math we get this tiny number however when we do the same calculation for spam we get zero this is because the probability we see lunch in spam is zero since it was not in the training data and when we plug in zero for the probability we see lunch given that it was in spam then it doesn't matter what value we picked for the initial guess that the message was spam and it doesn't matter what the probability is that we see money given that the message was spam because anything times zero is zero in other words if a message contains the word lunch it will not be classified as spam and that means we will always classify the messages with lunch in them as normal no matter how many times we see the word money and that's a problem to work around this problem people usually add one count represented by a black box to each word in the histograms note the number of counts we add to each word is typically referred to with the Greek letter alpha in this case alpha equals one but we could have said it to anything anyway now when we calculate the probabilities of observing each word we never get 0 for example the probability of seeing lunch given that it is in spam is 1/7 the total number of words in spam plus for the extra counts that we added and that gives us 0.09 note adding counts to each word does not change our initial guess that a message is normal or the initial guess that the message is spam because adding a count to each word did not change the number of messages in the training data set that are normal or the number of messages that are spam now when we calculate the scores for this message we still get a small number for the normal message but now when we calculate the value for spam we get a value greater than zero and since the value for spam is greater than the one for a normal message we classify the message as spam spam now let's talk about why naive Bayes is naive the thing that makes naive Bayes so naive is that it treats all word orders the same for example the normal message score for the phrase dear friend is the exact same for the score for friend dear in other words regardless of how the words are ordered we get 0.08 treating all word orders equal is very different from how you and I communicate every language has grammar rules and common phrases but naivebayes ignores all of that stuff instead naivebayes treats language like it is just a bag full of words and each message is a random handful of them naive bayes ignores all the rules because keeping track of every single reasonable phrase in a language would be impossible that said even though naive bayes is naive it tends to perform surprisingly well when separating normal messages from spam in machine learning lingo we'd say that by ignoring relationships among words naivebayes has high bias but because it works well in practice naive Bayes has low variance shameless self-promotion if you are not already familiar with the terms bias and variance check out the quest the link is in the description below triple spam oh no it's one last shameless self-promotion one awesome way to support stack quest is to purchase the naivebayes stack quest study guide it has everything you need to study for an exam or job interview it's eight pages of total awesomeness and while you're there check out the other stack quest study guides there's something for everyone hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
hGoTUyBnbxg,2020-06-01T15:35:49.000000,Live 2020-06-01!!! Hypothesis Testing,hypothesis testing can be really confusing hypothesis testing we're going to break it down into small pieces and hopefully it'll make sense by the end of this live stream stat quest bam hello i'm josh starmer and welcome to the statquest live stream uh i've we've got people uh watching uh in uruguay in peru uh looks like bangkok india new york city more india um got all kinds of people um watching and so that's super exciting today we're going to be talking about hypothesis testing which is one of those basic fundamental concepts and statistics that always seems backwards and i'm hoping that we can kind of break it down into small pieces and we'll see that it actually kind of makes sense uh once you uh once you see the small pieces all right uh so with that uh that said um oh i see we got some people from uh virginia and mexico we got some brazil uh uh which is really exciting um and so let's get i've got some a couple announcements to make so we're gonna we're gonna make a few announcements in june we're going to do webinars on support vector machines in python from start to finish i'm really excited about this the response for the decision tree statquest was really exciting um and so we're gonna um we're just gonna keep going and we're gonna cover new topics so this month we're gonna do support vector machines i should announce the dates soon in the next couple of days so check the stat quest community blog i'll also post it on linkedin as well so check those spots if you want to if you want to check things out i've also got some new study guides out which i'm really excited about we've got a new linear discriminant analysis study guide it's five pages of total awesomeness and and that gives us a bam and we also have gaussian naive bayes which is seven pages of total awesomeness double bam so we're going to talk about hypothesis to testing testing i see a bunch of more people have signed on so i want to give them a shout out because it's really exciting to have a big crowd we got a lot more people from india we've got someone from indiana uh we've got um more brazil malaysia china um uh france more more brazil uh oi brazil um colombia uh bam from mumbai iran um serbia istanbul germany more indonesia egypt uh bulgaria all kinds of places so we've got a bun and more people from mexico uh and more colombia's and denmark holy smokes washington anyways we've got everything covered well not everything we got nepal and pakistan uh we got a lot a lot of places covered uh including delaware so let's just move on with a hypothesis testing because we've got a lot to cover because this is one of those confusing topics that um i hope to demystify and and hopefully you'll have it for the rest of your life in your head okay i'm not going to name any names but imagine there was a virus any virus and that we had two drugs that we could use to treat it so we give people uh give drug a to three people and measure how long it takes each person to recover from the virus the first thing we notice is that not everyone recovered in the exact same amount of time person number one recovered the fastest and person number two recovered the slowest it's possible that person number one eats healthy food and exercises and already has a strong immune system and that helped them recover quickly and maybe person number two doesn't get as much exercise or maybe person number two has a stressful job or lives where there's a lot of air pollution or there's some other reasons why maybe they're just not as healthy as person number one the point is that even though all three people had the same virus and took the same drug they did not all recover in the exact same amount of time and that might be due to a lot of random things like exercise or job stress and these are things we cannot control now let's give drug b to three different people that have the virus and measure how long it takes them to recover again we see that even though three people had the same virus and took the same drug they did not all recover in this in the exact same amount of time and this is probably due to random stuff that we can't control like how much exercise each person gets or how much candy they eat but overall it looks like people taking drug a took less time to recover than people taking drug b and when we calculate the mean or average for drug a and the mean value for drug b we see that on average there's a 15 hour difference between drug a and drug b so after seeing this preliminary data it might seem reasonable to form the following hypothesis people taking drug a need on average 15 fewer hours to recover than people taking drug b and now that we have this hypothesis we can test it and the way we test it is by repeating the experiment i got to put this ukulele down i'm starting to make a lot of racket with it okay now when we calculate the means we see that on average people taking drug a need almost 35 more hours than people taking drug b compared to our preliminary data this result is a very unexpected in fact it is the original it is the opposite of the original hypothesis but it's also possible that all three people that took drug a in the second experiment have super stressful jobs and unhealthy lifestyles and maybe that's why it took them so long to recover and maybe everyone taking drug b was well rested and super healthy to begin with and maybe that's why they recovered so quickly but it's also possible that maybe we mislabeled drug a and drug b and did the wrong experiment so we repeat the experiment and again the results are totally backwards from the preliminary experiment and totally backwards from the hypothesis we made so again just to make sure we don't didn't mislabel things we redo the experiment and again these results are the opposite of the original hypotheses hypothesis excuse me hypothesis is one of those weird words where it's hard to know like what the plural is and what the singular is there's just a lot of s in that word at the end so it's easy to get it confused okay anyways we just keep repeating the experiment each time double checking every little detail [Music] and every time we do the experiment we get the opposite result of the original hypothesis so after doing all of these repeated experiments where we double checked every little step we can confidently reject this hypothesis that we came up with after doing the preliminary experiment bam bam i got ahead of myself okay since we just covered a super important part point about hypothesis testing let's summarize it we collected some preliminary data we used that data to form a hypothesis about the difference between drug a and drug b then we did a bunch of follow-up experiments and they gave us an overwhelming amount of evidence that suggested the hypothesis was wrong so we rejected the hypothesis and then we said bam all right now let's imagine these extra experiments never happened and go back to the original preliminary experiment and hypothesis now just like before we decide that this test oh excuse me now just like before we decide to test this hypothesis by repeating the experiment only this time instead of getting something that's the exact opposite of what we expected we get something that's slightly different in this case the difference is in the same direction but is only 12 hours then we repeat the experiment again and again we get something slightly different from the preliminary experiment and hypothesis the difference is in the same direction but this time the difference is 19 hours so the good news is we probably didn't mislabel the drug like we did last time and the differences between the three experiments might be due to random things we cannot control like maybe these people exercised a lot and had relatively healthy diets compared to these people who took longer to recover but regardless the hypothesis says people taking drug a needed 15 fewer hours to recover but when we repeated that experiment the first replicate said the difference between averages was 12 which is different from the hypothesis and the second replica said the difference was 19 hours which is also different from the hypothesis and let's be honest the only reason the hypothesis says 15 fewer hours is because that was the result from the first experiment however we could have just as easily put 12 fewer hours in the hypothesis because that's what we got the second time or we could have put 19 fewer hours in the hypothesis because that's what we got the third time so if we just pick one experiment like the first one and use that to define the hypothesis then we have two experiments that are not different enough to give us confidence to reject the hypothesis but because there's just as much data suggesting that the difference is 12 hours and there's just as much data suggesting the difference is 19 hours these experiments don't make us super confident that the hypothesis of 15 fewer hours is correct i mean that was just a guess and again maybe drug a reduces recovery by 15 hours but maybe it reduces recovery by 12 hours or 19. because the results from the repeated experiments are not different enough to cause us to reject the hypothesis and because they don't convince us that the hypothesis is correct either the best we can do is fail to reject the hypothesis small bam failing to reject the hypothesis is kind of like it's a little bit of a bummer it's kind of a weak statement we're just more wow we we blew it to summarize what we've covered so far we can create a hypothesis and if data give us strong evidence that that hypothesis is wrong then we can reject the hypothesis but when we have data that is similar to the hypothesis then the best we can do is fail to reject the hypothesis because that hypothesis was really just a guess because it's unclear if that hypothesis should be based on the first result we got or this other slightly different result or this result or any other possible outcome that's close to 15. so double bam so we've learned that we can reject a hypothesis or we can fail to reject the hypothesis we reject because we have overwhelming data that says that the hypothesis is wrong and we fail to reject because that hypothesis was sort of a guess to begin with and other data suggests maybe something else similar but not exact could be that hypothesis okay now let's take a closer look at the hypothesis itself oop you may remember that the only reason the hypothesis is 15 fewer hours is that was the first result we got but we could have just as easily gotten a 12 hour difference and ended up with that with a different hypothesis or we could have gotten a 19 hour difference and ended up with another hypothesis and if 12 and 19 are reasonable hypotheses then so is 15.5 or 15.0001 or 14.99999 dot dot dot in other words there are a lot of reasonable hypotheses how do we know which one to test since the goal is to see if drug a is different from drug b we simply test to see if there is no difference between drug a and drug b now when we do an experiment and we get a difference of 0.5 hours or we get a 0.5 hour difference we know that it is possible that we could have randomly selected someone who just happens to exercise a little less than that person and that might and that person who exercises just a little less might take a little longer to recover and it's possible that we could randomly select someone with slightly with a slightly better diet than that person and that person might recover a little more quickly and since small random differences give us slightly different results rather than worry about whether or not the difference is 0.5 or 0.25 hours we simply see if the data convinces us to reject the hypothesis that there is no difference between drug a and drug b in this case the result 0.5 hours in favor of drug b is close to zero but small random changes could easily result in a 0.25 hour difference in favor of drug a in other words the data does not overwhelmingly convince us to reject the hypothesis so we fail to reject the hypothesis that there is no difference between the drugs in contrast if we tested the drugs on a lot of people and there was a pretty big difference and little random things would probably not change the results very much then we can confidently reject the hypothesis that there is no difference between drug a and drug b double bam okay terminology alert oh no the hypothesis that there's no difference between things is called the null hypothesis in summary rather than get stressed out over a large number of possible hypotheses that we could test to see if drug a is different from drug b we use the null hypothesis to determine if there is a difference if we do an experiment with a bunch of people and a lot more people taking drug a had shorter recovery times than people taking drug b so many that it would be hard to imagine that everyone taking drug a had better diets or got more exercise than people taking drug b then we could reject the null hypothesis and then we would know that there is a difference between drug a and drug b and we could estimate how different they are using measures of standard error or something like that alternatively if it's hard to tell the difference between drug a and drug b because the results overlap a lot then we would fail to reject the null hypothesis in this case we could do a power analysis to make sure our sample size was large enough to reject the null triple bam woohoo the end okay so we've made it to the end of this stat quest um let's go back to just me being big and large and you're just going to see me looking over at the um at the chat to see if we got any good questions um uh uh [Laughter] someone just said no one knows where calgary is that was part of the shout out at the beginning uh someone's from portugal wow uh we got poland um and some other stuff argentina holy smokes um oh someone asked uh should these group samples have some attributes regarding their lifestyles um you know should what if we kept track of their diets and their exercise and that's a great idea um and we could do that and we would have a better experiment and we would be better able to isolate the difference between the medications and the drugs however the point i'm trying to make is that even when we keep track of people's diet and we keep track of their exercise they're still random things like maybe their genetic makeup or like i said where they live air pollution or like i said their jobs there's all these like random things there's always something else that could have an effect on how long it takes for me or someone else to recover from that illness and because we can't make a model that includes every single thing all those impossibly infinite things that could cause us to get a slightly different result when we do when we do experiments we have to take in account that there could be some random noise in there that's uh that's that could change it and if we replicated the experiment we might get us a different value so so yes we can keep track of diet and exercise but we have to know that there's always going to be something else that there's that we probably cannot control and that's the fundamental concept of statistics if we always got the same result you know if we could keep track of everything and we could say people that got this much exercise and had this job and lived in this place they all recovered in the exact same amount of time if they all did that then we wouldn't even need statistics we would just say those people recover in five hours or whatever you know and so a fundamental thing about statistics is dealing with all those little random things that we can't include in our model because some of the stuff we can anticipate but there's stuff that you know who knows maybe we can't anticipate it could be anything there could be like some weird like gamma ray coming from a different solar system that causes something to change um and that might give us a different value so i think that's an excellent question because it really touches on the fundamental reason why we do statistics to begin with um uh what else do we have over here um we've got someone said oh so someone said what happened earlier when we got the exact opposite i think in that exam experiment we can conclude that when we did the preliminary result experiment we had everything backwards and so um you know so from that experiment we could just kind of say we rejected the hypothesis that we generated from the preliminary thing because we we decided oh we must have swept swapped the labels um so that was an example of rejecting the hypothesis someone asked if i could talk about z-scores and i will definitely do that i will put that on the short list z-scores i mean i'll be honest z-scores are one of the last kind of annoying things they're a little bit of a hold over from when statistics was taught like 50 years ago before computers um and the only way to look up p-values and stuff like that you can get a p-value to calculate it you couldn't get a computer to calculate a p-value for you instead you had to look it up in a book and z statistics are related to that and and and a lot of fancier statistics are were developed around the time when when we were using c statistics to to look up p-values in a table rather than use a computer and since the statistics were built back in that age or created in that age uh they used the z-statistics as a way of sort of explaining things and so it's sort of like it's sort of this relic uh um from a from long ago that continues to be with us um sort of like a prehensile tail that we just carry around anyways um uh uh what else do we got we got all kinds of stuff going on here western australia sao paulo brazil all kinds of questions someone asked do we always need to do preliminary analysis to come up with the null hypothesis and the answer is no that's the magic of the null hypothesis i'll try to include that bit uh in the final ver you know when i tr when i make this stat quest when i make it when i take this live stream and i make it a full-blown stat quest i'll try to include that the beauty of the null hypothesis is because we just say the hypothesis is that there's no difference we don't need preliminary data to suggest that we're just going to say it and then we'll gather data that will either reject that hypothesis or will fail to reject that hypothesis so that's a cool thing instead of having to pick a hypothesis based on preliminary data which is kind of stressful because if we did the experiment different times we get different values uh rather than having to deal with that uh with the null hypothesis we just state it from the get-go and go with it um uh what else do we got uh um oh uh someone asked what do we do when we don't know the sample size uh beforehand um i don't know you need to know the sample size uh in order to get a good estimate however now that i say that uh we have measured measurements of um of sort of effect size and things like that like imagine the standard error which incorporates the sample size into it so if we have a large sample size usually we that means the standard error is going to be relatively small and so and so they don't tell us the sample size but they tell us the standard errors then we can still do hypothesis testing based on those standard errors um uh what else we got uh what does z-score have to do with hypothesis testing uh we can talk about that in the z-score live stream which i'm sure will come out soon because i think that's a that's a great kind of follow-up to this one so hopefully that's what we do next i can't make a promise that that's what we'll do next someone says why use hypothesis testing if we could just use bayesian credibility intervals and that's an excellent question and also a raging debate in all of statistics so hypothesis testing is there's is is part of a branch that people call frequent statistics um and there's another branch called bayesian statistics and we touched a little bit about bayesian statistics in the last live stream when we when we uh derived bayes's theorem um and they have different approaches to kind of rejecting accepting hypotheses that's what we do in frequentist land and in bayesian land they have an alternative way to deal with those problems called credibility intervals so why do this hypothesis testing when we could do the credibility intervals and um there's a lot of reasons for that one is uh one is legacy again frequentist statistics are just sort of part of how people do statistics uh you know that they were they were done earlier and people just said hey that's what i learned i'm gonna teach that uh but there's also some fundamental uh sort of annoying things about bayesian statistics in that they're not always relatively easy to do and um and since bayesian statistics and frequently frequent statistics will often give us the same result people will choose the easier route to get to that result bayesian statistics sometimes involves not always but sometimes involves sort of complicated um uh what it kind of a random sampling or what's what's the good word for it uh when we're we're trying to simulate uh what would go on they so yeah so bayesian statistics often have these rather relatively complicated simulations that are used to derive this credibility interval that you use to make decisions um and uh like um and and that can be kind of a pain to generate relatively speaking whereas frequent statistics we can just plug stuff into microsoft excel and get our p-value and bam we're done and so there's and since oftentimes they'll both give us the same result people will choose sort of the more direct approach um however i don't want to downplay bayesian districts there's a lot of cool interesting stuff going on in there and it's worth knowing what it's all about um uh oh we got someone from lithuania that's exciting um uh someone asked uh if i'm is a little off topic but if i'm ever gonna do a sort of diploma course am i gonna publish something on uh udemy or something like that and the answer is we're headed that direction um and i just i i feel like i feel like if you look at the stat quest list of of videos that i have if you go to the index that i have on the statquest website um there's a lot of variety there it's because i didn't just start from the beginning kind of work my way down there's and as a result there are are gaps you know in the in in in the coverage of topics i kind of like because the way stat quest originated was someone i worked with would say hey josh i got this question and i go hey i'm going to work on a stack quest that answers your question because that's how statquest started it wasn't like hey i'm going to start from the beginning of stat of statistics and just kind of build up real slow because it was sort of like i was just answering questions as they came to me there's a little gap there are little gaps throughout my index and collection of videos and i'm trying to fill those gaps and once i feel enough of them yes we're going to do a stat quest diploma and i'm super excited about that someone asked about the tabla behind me yes that's a tabla um uh so uh uh so that's that um anyways we're at the 30 minute marker and so it's time for me to go away i just want to thank everyone for coming to my live stream today i'm really excited um like i said we've got new study guides available at the statquest.org website and we've got um we're gonna be doing support vector machines and webinars so just stay stay tuned on on either youtube on the youtube community board or um or on linkedin and i will post the registration and sign up pages for those if you want to participate in those um all right until next time quest on you
Xm2C_gTAl8c,2020-05-19T04:00:08.000000,"Ridge vs Lasso Regression, Visualized!!!",Ridge regression versus lasso regression which one will survive stat quest hello I'm Josh stormer and welcome to stat quest today we're gonna talk about rage versus lasso regression and the differences are going to be visualised note this stack quest assumes that you are already familiar with rage and lasso regression if not check out the quests to show you the difference between Ridge and lasso regression we're going to use a very simple dataset that consists of weight and height measurements and we'll start by fitting this horizontal line to the data this horizontal line represents a terrible fit and we can measure how bad that fit is by calculating residuals the difference between the observed and predicted values in order to compare the horizontal line to other lines fit to the data we will plot the sum of the squared residuals on a graph the y-axis on this graph is the sum of the squared residuals and the x-axis represents different slope values for the fitted line in this case the slope for the horizontal line is zero now let's increase the slope to zero point two and calculate a new value for the sum of the squared residuals now let's increase the slope to zero point four and calculate a new value for the sum of the squared residuals we can keep plugging in new values for the slope and plotting the sum of the squared residuals or we can just plot the curve for this equation we can see that the best fitting line is at the bottom of the parabola in other words when the slope equals 0.45 we get the lowest sum of the squared residuals in this example we simply calculated the sum of the squared residuals for different slopes now let's add the ridge regression penalty aka the l 2-norm note if you asked me it should be called the squared penalty since that's what it is and for me way easier to remember the thick blue line that we just drew represents lambda equals zero this is because when lambda equals zero the penalty is equal to zero regardless of the slope and we are left with the original sum of squared residuals now let's see what happens when we set lambda equal to 10 and just like before we'll start with a horizontal line only this time the line is orange just like before we can calculate the residuals and we can calculate the sum of the squared residuals plus lambda times the slope squared in this case lambda equals 10 in the slope of the horizontal line is 0 so the penalty is 0 so we plot the sum of the squared residuals here now let's increase the slope to 0.2 note the residuals are smaller than before so the sum of squared residuals is smaller than before but now the penalty is 0.4 and that gives us this point on the graph now let's increase the slope to 0.4 and the residuals are even smaller but now the penalty is 1.6 and that gives us this point on the graph and like we did before we can keep plugging in new values for the slope and plotting the sum of the squared residuals plus the penalty or we can just plot the curve with a thick orange line that represents lambda equals 10 the bottom of the parabola is where the slope gives us the lowest sum of squared residuals plus penalty and that corresponds to this specific line and when we compare that to the optimal slope when lambda equals zero we see that setting lambda equal to ten results in a smaller optimal slope note we can also see that when lambda equals ten the lowest point in the parabola is closer to zero than when lambda equals zero so either way we look at it we see that the larger value for lambda Shrunk the optimal value for the slope likewise the thick green line represents lambda equals 20 we see that the minimum value is closer to zero and the optimal slope has shrunk some more the purple lines represent lambda equals 40 and it shrinks the slope even more in other words as we increase lambda for the ridge regression penalty aka the l2 penalty aka the square penalty the optimal slope gets closer and closer to zero but it does not equal zero BAM now let's see what happens if we use the lasso penalty aka the l1 norm or if you asked me I'd call it the absolute value penalty unfortunately no one asked me again the thick blue line represents lambda equals zero so there is no extra penalty this is because when we plug lambda equals zero into the equation the penalty becomes zero and we are left with the original sum of the squared residuals note just like before we'll keep track of the best fitting line plus penalty in this graph on the left the thick orange line represents lambda equals 10 so now we are turning on the penalty and shrinking the slope note when lambda equals Tim we start to see a kink in the curve where the slope is zero the thick green line represents lambda equals 20 and this kink at zero is becoming more prominent lastly the thick purple line represents lambda equals 40 and now the kink at zero is super obvious now the lowest point in the purple curve aka the optimal slope given the absolute value penalty when lambda equals 40 is zero and that means the slope of the optimal line is zero and that means when lambda equals 40 we ignore weight as a variable when predicting height double bam in summary when we increase the Ridge regression penalty aka the l2 penalty aka the square penalty the optimal slopes shift towards zero but we retain a nice parabola shape and even when we set lambda to something crazy high like 400 we still end up with an optimal value greater than zero in contrast when we increase the lasso penalty aka the l1 penalty aka the absolute value penalty the optimal value shifts towards zero but since we have a kink at zero zero ends up being the optimal slope BAM hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
ONCOkccpk3w,2020-05-18T15:42:14.000000,Live 2020-05-18!!! Bayes' Theorem,I wish I wish that I started I wish that I got my hair cut right before they locked me down I wish that I got my hair cut right before right before right before right before right before they locked me stat quest hello and welcome to stat quest one day I'm gonna get that theme song and I'm gonna nail it something about doing it live makes it a little tricky I guess anyways I'm really glad you guys are all here um let me put the guitar down I'm gonna look over at the comments let's see where people are coming in from I'm in Chapel Hill North Carolina where are you guys at I hope everyone's doing great I've got a big a big a big stack quest to do today and I'm all excited about that so so we're just gonna dive into that pretty soon but before we do that I have a big announcement and the big announcement is the stat quest study guides are here and for example I've got a stat quest study guide on the principle component analysis and here's a sneak peek at what it looks like it's actually six pages jam-packed with Awesomeness I've just shown you two of those pages I've got a study guide for classification trees and here's a sneak peak of what that one looks like and that's actually eight pages jammed packed with awesomeness and I've got a study guide on classification trees and on linear regression and naive Bayes and PCA and there are more to come I'm working I'm dedicating a few hours every day to work on these so put your requests in the chat or comments because I want to hear what you guys want steady study guides on and the stat quest study guides are they at stat quest without oor G and full disclosure I am charging some money for that that is part of the Josh dormer needs enough money so we can buy groceries fund so I am charging a little bit of money for these things there's a limit they're only three dollars each but I do I don't want you to be too disappointed to think I'm handing them out but I do appreciate the support so so that's the big news and it looks like we've got a bunch of shoutouts going on over here in the chat so I'm gonna see we got someone from Dallas India Istanbul South Korea Mumbai India Istanbul Vienna Chennai Karnataka Helsinki Turkey Lima Peru which is a place I've always wanted to go to so that's a crate I mean I've always wanted to be a lot of places but I've never actually been to Lima Peru and I'd love to go there and that looks that looks like we've got the crowd taken care of and so now what we're gonna do is we're gonna move on to the meat of today's livestream we're going to talk about Bayes theorem and I just want to give a note usually these live streams we start from scratch and there's not you know where we're not building on other live stream so you can just join in but I want to give you a heads up that today we're following up on the last live stream where we talked about conditional probability and there's a little bit of a review here but if it moves too fast and you get confused don't worry about it just check out the last live stream and then rewatch this one and it'll all come back and before I dive in I just want to give some more shoutouts because because because this thing started scrolling the chat we've got Russia we got Honduras we got Germany Malaysia Miami Ireland holy smokes Mexico Kolkata Tunisia Kenya holy smokes Kenya Borneo Madras Hyderabad Taiwan China Ukraine Saudi Arabia Honduras Denmark a place I can't pronounce I don't even know what language that is Indonesia Brazil hooray Brazil Oh Brazil and what else we got anyways India in a bunch in Seattle holy smokes okay Banga a bulky area Ukraine Italy India Brazil okay I don't want to just be screaming off France I don't want to be screaming off country names for the next 30 minutes we got a lot to cover so we're gonna just dive in and do a little bit of a review so when we talked about conditional probability we took a trip to stat land and asked everyone the entire population of stat land and and in this case they're represented by a colorful dot if they loved candy and or soda and two people these two people they love candy and soda and these four people only loved candy these five people only loved soda and these three people didn't like candy and they didn't like soda BAM then we calculated the probabilities for each cell in the contingency table by dividing the counts by the total number of people in stat land 14 so stat land is a very small place even compared to a small towns that land is tiny anyways we did we did the math BAM okay then we determined the total number and probability of people who love soda by adding up the first row and the total number and probability of people who did not love soda by adding up the second row in the total number and probability of people who loved candy by adding up the first column and the total number and probability of people who did not love candy by adding up this second column BAM okay then we calculated the conditional probability that someone in stat land might not love candy but love soda given that we already know that they love soda we did this by dividing the five people that do not love candy but love soda by the two plus five equals seven people that love soda and doing the math gave us 0.71 then just for fun we divided the numerator and denominator by the total population of stat land 14 and now the numerator is the original unconditional probability that someone in stat land does not love candy but love soda and the denominator is the unconditional probability that someone in stat land love soda Boop all in all the probability that someone does not love candy but love soda given that we know that they love soda is equal to the probability that someone does not love candy but love soda divided by the probability that someone in stat land love soda and when we do the math we get zero point seven one so one thing about conditional probability is the probability that an event will happen in this case that probability is that we will meet someone who does not love candy but love soda scaled by the knowledge we already have about the event in this case we already know that the person loves soda note this is super important so I'm going to repeat it conditional probability is the probability that an event happens scaled by the knowledge we already have about that event note by saying someone who does not love candy and love soda given that they love soda is a little redundant because we just kept talking about how they love soda we we want to know the probability that they love soda given than they love soda so there's a little bit of redundancy in there pretty much everyone everyone omits writing out the and love soda parked and it shortens it to someone who does not love candy given that they love soda okay so that's what it shortens it to that said I don't like this notation it makes it harder to see the relationship between the thing we want to calculate the probability for on the left and how we calculate it on the right using my clunky notation for conditional probability where I include all the redundancies it's obvious that we want to calculate the probability that an event happens scaled by the knowledge we already have about that event note we're gonna talk more about the different notations at the end because it's important to actually know the standard notation I have a very clunky notation and I like using it myself but since the rest of the world has another notation where you have to cover that so that when you research other things you'll look at that notation I go and you go I know what that means even though it's not the notation that Josh Dahmer uses most of the time but I think if you just watch the way I do it it will over like you'll see the conventional notation and you'll translate it into my clunky notation and it actually might make more sense it does for me I have a lot of trouble with these conditional probabilities without my clunky notation because because there's stuff and you'll see later there's stuff that looks different but is actually the same anyways we'll talk more about that at the end I don't want to get too far ahead of myself okay now let's see what happens when we change what we already do about the event from knowing that they love soda to knowing that they do not love candy now we have the probability that an event happens which in this case is the event that we meet someone who does not love candy but love soda so this is the exact same event we were calculating the probability for before but now we're scaling by the knowledge that we already have about the event which in this case means we know that they do not love candy so we have different knowledge about the same event now we plug in the numbers and do the math and we get 0.63 now let's compare this conditional probability where we already know that the person does not love candy - the conditional probability we calculated before where we already knew that the person loves soda in both cases we want to know the probability of the same event meeting someone who does not love candy but loves soda and that means in both cases the numerators are the same however since we have different knowledge in each case we scale the probabilities of the events differently and ultimately we get different probabilities BAM hold on I have to go close my door real quick I says hello okay stat guy is our friend in stat land and he always wants to make a bet and he says I bet you $1 that you can't solve the conditional probabilities without knowing the probability that someone does not love candy but they love soda deep so he wants us to get rid of that can we still solve for the conditional probabilities without knowing that so can we still solve for the conditional probabilities well even if we don't know the probability that someone does not love candy but love soda we can multiply both sides of the top equation by the probability that someone loves soda and these two terms on the right cancel out beep and we are left with the probability that we meet someone that does not love candy but love soda equal to this stuff on the left side likewise we can multiply both sides of the equation on the bottom by the probability that someone does not love candy so we do that beep and these two these two terms on the right side cancel out boo and just like before we end up with the probability of meeting someone who does not love candy but love soda equal to this stuff on the left side now we have two things on the left side of the equal signs and they are both equal to the probability that we will meet someone who does not love candy and love soda and that means these things are equal to each other so let's move this up a little bit and let's move this over here okay now we have two options first we can divide both sides by the probability that someone loves soda the those two terms cancel out and that's what we end up with BAM alternatively we can divide both sides by the probability that someone does not love candy so we do that oh so what I'm doing right here is I'm taking that side of the right side of the equation I'm moving it to the left side of the screen and I'm taking the left side of the equation I'm moving it to the right side of the screen no big deal anyways now we're dividing by the probability that someone does not love candy and those two terms cancel out and this is what we end up with BAM okay in both cases we won the bet with stat guy because we no longer need to know the probability that someone does not love candy but they also love soda so we won the bet ray however more importantly we also derived bayes's theorem or Bayes theorem double-damn bayes theorem tells us that this conditional probability which is based on knowing that the person loves soda can be derived from this conditional probability which is based on knowing that they do not love candy and so what it means is well I just said what it means we can take this thing on the left side and we can say what's the probability of this knowing one thing and we can derive it from the probability of that same thing knowing something else alternatively Bayes theorem tells us that this conditional probability which is based on knowing that the person does not love candy can be derived from this conditional probability based on knowing that they love soda so we can go either way we can depending on what we know we can reform al formulate Bayes theorem so that we can derive something that we don't know from what we do know all right Bayes theorem is just a little bit of algebra what's the big deal when we have all of the data laid out in a nice colorful chart or in a contingency table then Bayes theorem is not a big deal in fact when you have all of the data Bayes theorem isn't even a small deal however most of the time we don't have all the data in other words stag guy might only tell us the probability that someone does not love candy given that they love soda and I'm not certain but I think the probability that someone loves soda is close to 0.6 and the probability that someone does not like candy is 0.5 7 I had big plans on giving stat guy a special accent and I forgot up until just that last little second so maybe next time when we have stat guy and he comes back I'll do a better job giving him a special accent anyways I just want you to be able to distinguish that guy for me anyways anyways if this is all the data we have then we can plug in the numbers and do the math and we get 0.75 so when we don't have all the data we can we can still solve for what we want to solve and that means the probability that someone loves soda given that they do not like candy is about 0.75 note attentive viewers may notice that this value is different than what we got earlier on in this stat quest and this is because stack stat guy didn't know the exact value for the probability that someone loves soda he just took a guess and while taking a guess might sound like a terrible thing to do it's the only option when we have a large population for example it would be almost impossible to ask every single person in India if they loved soda so a lot of times we have to make a guess Bayesian statistics is about understanding what it means to make a guess like this and all that it implies Bayes theorem is the basis for Bayesian statistics which is this equation paired with a broader philosophy on how statistics should be calculated unfortunately we don't have time to go into the philosophy right now but maybe some other day we'll dive into it and I'll be honest it is kind of a can of worms because we're dealing with things that we don't always know and the equations aren't always solvable so we have to use we have to use kind of like machine learning methods where we're where instead of like using a specific equation that we solve we have to approximate it I don't know if you're familiar with gradient descent but that's what we use a lot of machine learning to approximate sort of optimal parameter settings and and and values Bayesian statistics has another way of sort of simulating what it means to have asked everyone if they like love soda or not and and it and doing that simulation is kind of a can of worms there's lots of details lots of debate on the best way to do it but it's a topic that I have a dream that one day we will talk about however in the time being I'm still kind of like in machine learn and in my head and we're working on in the big picture we're working on neural networks so hopefully I have my first neural network video coming out in the next couple of weeks that I'm really excited about okay so however we don't before we go I want to review the standard notation so if you research bayes's theorem or Bayesian statistics on your own you won't be totally lost like I said earlier when most people write conditional probabilities they do it differently from the examples I've given here specifically because they know that this person does not love candy they do not include it when stating the probability and now the conditional probability reads the probability that someone loves soda given that they do not love candy likewise because we know that this person loves soda they do not include it when stating the probability and now the conditional probability reads the probability that someone does not love candy given that they love soda and now these two conditional probabilities look likely are inverted versions of each other in other words if we focused on this conditional probability it looks like if we swapped what we were calculating the probability for love soda with the knowledge we were given does not love candy then we would end up with this conditional probability so it kind of looks like we take one conditional probability and we just flip it and we get the other conditional probability however it's important to keep in mind that in both cases the probability statement on the left side is referring to this yellow area in the drawing and the same yellow square in the contingency table so even though it says the probability that we love soda or the probability they they don't love candy so even though those statements sound different we're actually referring to the exact same square that yellow square in the contingency table and the only real difference between the two conditional probabilities is the given knowledge on the left the given knowledge that the person does not love candy refers to the column total for doesn't love candy and on the right the given knowledge that the person love soda refers to the row total for love soda small BAM no we're gonna say triple BAM because we made it do this whole stat quest BAM I'm really excited about that so what I'm gonna do right now is I'm gonna go over to the chat window and I'm gonna see what we got over here oh and I just saw some ones from Turku Finland I had a really good friend from Turku I don't know if I'm pronouncing that right but he was super cool and he was a good friend someone's asking if this is gonna be available later and the answer is yes what else we have aren't there assumptions in Bayes theorem we talked a little bit about well not in Bayes theorem itself in Bayes theorem all we have all we have is this this equation which we derived and it doesn't require any assumptions Bayesian statistics however assumes that we don't know all of the proper abilities exactly and it comes up with ways for dealing with guesses and so so yeah so that is so it's not really an assumption of Bayes theorem but it's but it's a way of approaching Bayesian statistics and it says I assume that we don't know all this stuff and actually that's a pretty good assumption and that's sort of the the one of the fundamental differences between what they call its baiting statistics and on the other branch of statistics and I'm totally blanking on the name right now a frequentist they call it frequentist so most of the statistics that we do like t-tests are anova linear models most of those statistics are based on what we what was or was I would consider traditional statistics as are based on something called frequentist assistances statistics and we all know that a lot of frequentist statistics have assumptions they might assume normality they might assume something else about the distribution and sometimes those assumptions are reasonable and sometimes they're not reasonable but we but but what the Bayesian say is they say well our assumptions are upfront that we're just gonna admit that we don't know the exact probability that somebody loves soda and we're gonna say this is our guess and we're gonna work with that guess to try to sort of calculate the probability we really know so that's that's that we might talk about that more more in another day but like I said it's is sort of a big topic and it would take a long time to get through it all and it's something I want to do but I really really really I've been meaning to do this for years really want to do this neural networks thing so that's where my heart and my mind are both focused right now someone asked how it's related to maximum likelihood and that's a good question and that's also sort of a lingo issue in that the this if you're looking at this equation on the right side I wish I could get a pointer but I can draw a picture or I can draw a box around this we're talking about this thing and in Bayesian lingo this is often called a likelihood ratio and in that respect it can be associated with maximum likelihood if you guys watched the the stat quest on naive Bayes or or you looked at the study guide you may recognize the numerator of this equation as a big part of how we do naive Bayes and the denominator well I mean what does that denominator do really what that denominator does is it makes sure that for all the different possibilities up here that if we add them together we'll get 1 which means we've got a probability distribution that means that means no matter what the the probability of over here ranges from zero to one and that this this thing over here scales that and make sure we don't go of it I don't know if you guys are familiar with likelihoods but it's likely that it's sometimes possible to go over one and this denominator helps make sure that will never happen and because the dena the the denominator is really just a scaling factor in in a lot of bayesian analyses they omit it and they focus just on a numerator and that's what we did in naive Bayes so there's that I'd love to keep asking questions I'll be available hold on I gotta give a shout-out for a super chat as to Andre Carvalho thank you very much anyways I'd love to go on but we've reached our 30-minute max and I've got a go for today and I got to get started on working on that neural net stat quest so thanks everyone for joining me today I really appreciate it and tune in next time for the next quest so quest on
iiN_J9S0KLM,2020-05-04T16:39:56.000000,Live 2020-05-04!!! Conditional Probability,later okay so stack quest stack quest stat quest livestream hello and welcome to stat land whenever we go to stat land our friend makes a bet I bet you $1 that the next person we meet loves candy and soda so here's a question for the chat and I've already seen a lot of good stuff there someone put smiley stat I think that's good okay so here's the question what's the net guy this guy this this smiley face is gonna be showing up and a lot of our probability stat quest so we got to give him a name and I was thinking stat man stat person Herman or that the guy with nobody but I've seen some some funny things over in this in the chat so just put something there if you have a good idea we could call a mister double bam or mister I'm about to lose a bet but if you have any thoughts on what we should call this guy give him a name put it in the chat and I'll review that afterwards and maybe we'll do a vote on online later I'll do a poll maybe in Twitter or something like that and we can vote on the best name okay moving on this is not the most important part of the video by the way okay anyway since stat land is a small place I asked everyone represented by colorful dog if they loved candy and or soda six people said they loved candy and seven people said they loved soda and of the people that loved candy or soda two people said they loved both and last but not least three people on stat lands said they did not love candy or soda all in all there are 14 people in stat land okay one way to keep track of this data is to put it into something called a contingency table the rows are for keeping lack of people that love soda or do not love soda and the columns are for keeping track of people that love candy or do not love candy we put the two people that love candy and soda here and the four people that love candy but do not love soda here and that means the two plus four equals six so six people in this column are the six people who love candy likewise five people who love soda but do not love candy go here and that means the two plus five equals seven so there's seven people in this row and they are the seven people who love soda lastly the three people that do not love candy or soda go here and that means the four plus three equals seven people in this row are the seven people who do not love soda and the five plus three equals eight people in this column are the eight people who do not love candy so we can use this picture to see who loves and doesn't love candy and popcorn in stat land or we can use the contingency table they both contain the same information someone in the chat just said they love soda too I wonder if they're from stat land anyways well okay now remember remember that bet that our friend that we're still trying to name they wanted to make this bet they said I want to I want you $1 the next personally meet loves candy and soda so using the data from this fancy multicolored diagram or this crazy Technicolor table we can calculate the probability of meeting someone who loves candy and soda the P in this fancy statistics notation is short for probability so this whole thing's the probability that someone loves candy and soda equals and what it equals is the number of people that love candy and soda 2/14 the total number of people in stat land in other words the probability that we will meet someone who loves candy and soda in stat land is the proportion of people that love candy and soda and when we do the math we get 0.14 and that means the probability that next person in we meet in stat land the probability that they like candy or sand soda is pretty small so we can we might take up our friend on his bet and there's a good chance we'll win that $1 so so once we give this guy a name maybe it'll OS a dollar now let's put the probability that someone loves candy and soda in our table so we will remember it note we can put two divided by fourteen or you can put 0.14 in the table since they're both equal I'm gonna put two divided by fourteen in the table because it will help make things clear later on okay so likewise we can calculate the probability that somebody loves candy and does not love soda we put four in the numerator because there are four people that love candy but do not love soda and just like before we put 14 in the denominator because there are 14 people in stat land and that means that probability that the probability of someone loves candy but not soda is the proportion of people that love candy but not soda now we just do the math and we get zero point two nine and we put that probability in this in the contingency table so we can keep track of it likewise the probability that someone love soda but not candy is 5/14 which is 0.36 so we add that to the contingency table and the probability that someone does not like candy or soda is 3 divided by 14 which is 0.2 1 and we add that to the contingency table BAM we've got all the probabilities worked out for the contingency table if we add up the probabilities beep-boop beep-boop we get 1 and that just means that when we run into someone in stat land there's a 100% chance that they will either love candy love soda love candy and soda or not love candy and not love soda so when the probability equals 1 we just know something's going to happen BAM note we can also determine the probability that someone loves soda regardless of how they feel about candy we simply add the two people that love candy and soda to the 5 people that love soda but not candy and divide by the sum 14 to populate or excuse me and divide the sum by 14 the population of stat land this tells us that half the people in stat land love soda likewise we can calculate the probability that someone does not love soda by adding up the counts in the second row and dividing by 14 the population of stat land then we can calculate the probability that someone loves candy by adding up the first row or scuse me first column and we can calculate the probability that someone does not love candy by adding up the second column BAM now what if we heard that that pop that pop sound as someone opened a bottle of soda or a can of soda whatever you have in your region if you have cans or bottles just imagine that sound that sound would tell us that the next person we meet will love soda because they've just popped open a soda now knowing that the next person we meet will love soda what is the probability that the next person also loves candy in other words what is the probability that someone loves candy and soda given that we know they love soda note in fancy statistics notation we use this vertical line to mean given that so this is the probability that someone loves candy and soda given that we know they love soda oh no it's the dreaded terminology alert statisticians call this a conditional probability so they would say that this is the probability someone loves candy and soda given the condition that we know they love soda small bam okay note I hate using abbreviations but let's let's candy equal see beep and soda equal ass beep okay and earlier well note earlier we calculated the probability that someone loved candy and soda but without already knowing they loved soda and that was two divided by fourteen or 0.14 so that's what we did when we had no knowledge about whether or not they of soda or not and because we didn't already know that they loved soda the denominator consisted of the total number of people in stat land 14 in contrast now that we know the person loves soda we can focus our attention on just the two plus five which equals seven so seven people that love soda because that's all we're talking about at this point so just like before there are only two people that love candy and soda so we put that in the numerator but now since there are two plus five people who love soda we put that in the denominator and that means the probability that someone loves candy given that we know they love soda is zero point two nine note the probability is different from the original probability that was calculated without knowing whether or not they liked soda so those two probabilities are not the same in this case knowing that they love soda increased the probability that they would love candy because zero point two nine is greater than zero point one four double bomb okay so now let's see what happens when we calculate the probability that someone doesn't love candy given that we know they love soda using the fancy statistics notation we have the probability that someone does not love candy and they love soda I guess that you could say but they love soda given that we know they love soda since there are five people that love soda and don't love candy we put five in the numerator and since there are two plus five equals seven people who love soda we put that in the denominator and that means the probability that someone does not love candy given that we know they love soda is 0.71 and again knowing that someone loves soda changes the original probability the difference comes from the fact that originally the denominator included all 14 people in stat land but once we knew they love soda or once we know they love soda lets keep my tenses correct we know we only have to put seven people or the seven people that love soda in the denominator okay now to make a little bit more room let me move this up a little bit beep and now let's divide the numerator in the denominator by the total number of people in stat land 14 so if you look at that all we've done is we've divided the the five in the numerator we've divided by that by fourteen and that two plus five in the denominator we divided that by fourteen no big deal now when we do the math we get zero point seven one which is the exact same conditional probability that we got before and that makes total sense right when you when you divide the top and bottom by the exact same number nothing changes because you're essentially dividing by one because that number will cancel itself out so we really have done nothing to the equation so what's the big deal of dividing everything by the total population why would I do that doesn't make any sense or does it tune in for the next slide when we divide the numerator in the denominator by the total people and stat land 14 the numerator is now the original unconditional probability that someone in stat land does not love candy but loves soda so we're gonna move that up and we're going to place it with that probability statement so we've got probability that someone does not love candy and they love soda BAM in the denominator the denominator is the unconditional probability that someone in stat land loves soda and we already calculated that before two plus five divided by fourteen and that means the denominator is just the unconditional I guess I already said that or maybe my slides are out this always happens when I do a stack quest live there's always something anyways that's the something I guess hopefully that's the only something anyways so we can replace that with the with the statistical statement of the probability that somebody loves soda okay olan all the probability that someone does not love candy but love soda given that we know that they love soda is equal to the probability that someone does not love candy but love soda so this is if this is the left side of that conditional probability divided by the probability that someone in stat land loves soda so this is the right side of that conditional probability no big deal and when we do the math we get zero point seven one so now we can calculate conditional probabilities with raw counts or original unconditional probabilities and the reason why are using those original probabilities we're we're not just plugging in numbers is because it's sort of like having an equation where you've got y equals three plus two x that the fact that we have don't already have a value for X and we can plug and future values and X means we've got a flexible system that we can kind of change as we need it and it's not locked down to any specific data set anymore so so when we have it in this notation it's kind of general and can be applied not just a stat land but two stat continent or stat world or stats solar system or stat universe so so when we have this general equation it's just a little bit more flexible and we can change the data set and still calculator are our probabilities okay now that we understand conditional probability the next step is bayes's rule and that's the foundation of Bayesian statistics so we're gonna give that a triple bam and since we're done a little early or about six minutes early what we're gonna do is we're gonna cut back to me hello and I'm gonna go over and I'm gonna look and see if we got any questions that I can answer real quick in the chat room so let's see what we got over here we got pee we got people from all over oh oh we've got different names for our BAM guy or some guy said BAM bud and some guy said mister BAM early we got Batman or stat man someone suggested smirnoff I think that's a good suggestion mister BAM that's a good one status stallion ooh that kind of sounds I like the sound of that Wilson that's a good name okay for anyone who's seen was that Tom Hanks there ain't it on now island that castaway stat man are you from DC or marvel excellent question anyways one question is do I play table and I will say not as well as I used to I used to play table back in the day and and when I say back in the day I mean a long time ago and I and I wasn't even that good back then I actually I I'm I'm a big fan I'm so here's a little history about me when I was in high school my parents well as soon as I graduated from high school my parents moved to India they moved to Chennai back then it was called Madras but the name changed a few years later and now it's Chennai anyways my dad moved my parents moved to Chennai and my dad taught biomedical engineering at the IIT or the Indian Institute of Technology in Chennai and he did that for a couple of years and I was I was here I was in the United States and I was taking you know but for for during the winter vacation I would go to India I go to South India and I would I would spend my days learning how to play Carnatic and I guess Hindustani instruments as well so I learned a little bit of table but I also learned how to play the Carnatic veena which is a big huge instrument made out of a jackfruit tree and it's beautiful and I love it because it has four strings look at my guitar you see that my catarrh only has four strings and I just love instruments that have four strings I also learned how to play an instrument that I do not know how to say in Hindi the English translation I believe is beloved I want to whenever I say it to an Indian person they go what and they never understand what I'm saying so I'm not gonna say it but it's a beautiful instrument and I will show it to you oh this guy this instrument I haven't played this in a while as well I mostly play my guitar I also play the cello this is a sit this is like a sitar it's not a real sitar and you play it with a bow instead of a pick so it's a little it's a little it's related to a sitar so anyway if you guys know the name of that instrument put it in the comments that'd be great or put it in the chat anyways that's the history of me and India I haven't been there for a long time and my goal for the next year is to do a small stat quest tour of maybe the whole world but I'd love to stop off in India while I'm at it I know people all over the world I love stat quest and I want to meet all you guys to be honest I it flatters me and it makes me feel special to know that there's people all over the world watching stat quest and I would love to come and do this in person for your school for your ever and so that's sort of a long-term plan of mine is to is to do the stat tour so anyways enough about me that's my jibber-jabber about India today um just to remind you in the month of May I'm gonna give you a heads up there's gonna be announcement we're going to do our first webinar where we go from start to fit what we call machine learning so I've got we're gonna take a data set we're gonna you were gonna deal with missing data we're gonna normalize or standardize the data we're gonna build a machine learning algorithm we're gonna then optimize the hyper parameters we'd sounds super fancy but it's we're just tweaking some numbers and then we're gonna rebuild the thing and we're gonna print out cool visualizations of what we're doing and at the end you'll have the code and a Jupiter notebook that you can then play with on your own or you can follow along during the webinar and so that's gonna happen in May the other thing that's gonna happen in May is not this the step quest study gods are gonna become available so we got two big things that are coming out in May and I'm super excited about it and I guess that's it for today all right thanks for joining us until next time quest on
VX_M3tIyiYk,2020-05-04T04:00:11.000000,"Power Analysis, Clearly Explained!!!",stat quest is cool [Music] statquest hello i'm josh starmer and welcome to statquest today we're going to talk about power analysis and it's going to be clearly explained note this stat quest assumes that you are already familiar with what power means if not check out the quest it would also be helpful if you understood the difference between population parameters and estimated population parameters if not check out the quest lastly because we do a power analysis to avoid p hacking you should be familiar with that topic as well imagine there was a virus and we had two drugs that we could use to treat it so we see how long it takes for three people using drug a to recover from the virus and we see how long it takes three people using drug b to recover just looking at the data makes us think that drug a might be better since those people tended to recover from the virus more quickly so we calculate the means for both drugs and do a statistical test to compare the means and get a p-value equal to 0.06 because the p-value is greater than 0.05 the threshold that we are using to define a statistically significant difference we can't say that drug a is better than drug b in other words even though we suspect that the measurements for drug a represent this distribution and the measurements for drug b represent this other separate distribution because the p-value 0.06 is greater than 0.05 we cannot reject the idea that maybe all of the measurements represent the same distribution in the middle because we suspect that the measurements represent two different distributions and the p-value 0.06 is just a little bit bigger than 0.05 it is tempting to give one more person drug a and give another person drug b and recalculate the means and then redo the statistical test however we must resist this temptation because that would be p hacking and we don't want to do that instead of p hacking we're going to do the right thing we're going to do a power analysis to determine the sample size for the next time we do this experiment a power analysis determines what sample size will ensure a high probability that we will correctly reject the null hypothesis that there is no difference between the two groups in other words if we use the sample size recommended by the power analysis we will know that regardless of the p-value we used enough data to make a good decision power is affected by several things however there are two main factors one how much overlap there is between the two distributions we want to identify with our study two the sample size the number of measurements we collect from each group for example if i want to have power equal to 0.8 meaning i want to have at least an 80 chance of correctly rejecting the null hypothesis then if there is very little overlap a small sample size will give me power equal to 0.8 however the more overlap there is between the two distributions the larger the sample size needs to be in order to have power equal to 0.8 to understand the relationship between overlap and sample size the first thing we need to realize is that when we do a statistical test we usually don't compare the individual measurements instead we compare summaries of the data for example we often compare the means so let's see what happens when we calculate means with different sample sizes first let's focus on the distribution for drug a the population mean for drug a is represented by this green arrow typically when we do an experiment we don't know the population mean and instead have to estimate it so if we collected one measurement and use that one measurement to estimate the population mean for drug a then the estimated mean represented by this green bar would be the same as the measurement we collected now let's get rid of the measurement but keep the estimated mean and collect a new measurement and use it to estimate the population mean now let's collect more measurements and use each one to estimate the population mean note we occasionally get a wonky point that is really far from the population mean and when that happens the estimated mean is also pretty wonky and really far from the population mean now compared to the population mean for the distribution we see that the estimated means are all over the place in other words there's a lot of variation in the estimated means and all this variation makes it hard to be confident that any single estimated mean is a good estimate of the population mean sure some of the estimated means are close to the population mean but others are pretty far away and since 25 of the area under the curve is relatively far from the mean and off to the left 25 of the measurements should be far from the mean and off to the left and since in this case a single measurement is the same as the estimated mean 25 percent of the estimated means should be pretty far off to the left likewise another 25 percent of the estimated means should be pretty far off to the right in summary when we only use one measurement to estimate the population mean the probability that we'll get something far from it is too high for us to be confident that we have a good estimate now let's do the same thing for drug b collect one measurement and use that to estimate the population mean now let's do that a bunch of times just like before compared to the population mean for the distribution the estimated means are all over the place and just like before fifty percent of the estimated mean should be pretty far to the left or right of the population mean and when we only use one measurement to estimate the population mean the probability that we'll get something far from it is too high for us to be confident in our estimate and because we don't have a lot of confidence in the estimated means we'll end up with a relatively large p-value and that means we will not correctly reject the null hypothesis in other words if this distribution said all data comes from me then due to all of the variation in the estimated means we'd say in a small meek voice dang i can't reject the null hypothesis bam now let's collect two measurements at one time and use the average to estimate the population mean now let's repeat that process a bunch of times collect two measurements and use the average to estimate the population mean note just like before we occasionally get a wonky point that is really far from the population mean however the other point we measured compensates for the wonky point and prevents the estimated mean from being too far from the population mean in other words even though there is a 25 probability that we will get a measurement way out on the left side there is a 75 probability that the next measurement will be in this range and pull the estimated mean back to the population mean in summary when we use more than one measurement to estimate the population mean extreme measurements have less effect on how far the estimated mean is from the population mean and as a result the estimated means are closer to the population mean compared to the means we estimated with a single observation this suggests that we should have more confidence that averages estimated with two observations will be closer to the population mean than averages estimated with one observation now let's collect two measurements each time for drug b and use their average to estimate the population mean again when we use two measurements we can have more confidence that the estimated means are closer to the population mean compared to the means we estimated with a single observation bam now imagine we did the same thing only this time we collected 10 measurements and used them to estimate the population mean then we repeated that process collected 10 measurements and then estimated the population mean a bunch of times then we did the same thing for drug b now we see that the more measurements we use for each estimate the closer they are to the population main and the more confidence we can have that an individual estimated mean will be close to the population mean in this example the estimated means are so close to the population means that they no longer overlap and that suggests there is a high probability that we will correctly reject the null hypothesis that both samples were taken from the same distribution in other words even when the distributions overlap if the sample size is large we can have high power bam note although we used normal distributions in this example the central limit theorem tells us that these results apply to any underlying distribution shameless self-promotion for more details about the central limit theorem check out the quest the link is in the description below now at long last let's talk about how to actually do a power analysis first remember that a power analysis will tell us what sample size we need to have power so the first thing we need to decide is how much power we want although we can pick any value between 0 and 1 for power a common value is 0.8 so let's use that that means we want an 80 probability that we will correctly reject the null hypothesis the second thing we need to do is determine the threshold for significance often called alpha we can use any value between 0 and 1 but a very common threshold is 0.05 so we'll use that lastly we need to estimate the overlap between the two distributions overlap is affected by both the distance between the population means and the standard deviations a common way to combine the distance between the means and the standard deviations into a single metric is to calculate an effect size which is also called d in the numerator we have the estimated difference in the means and in the denominator we have the pooled estimated standard deviations one of the simplest ways to pool the estimated standard deviations is the square root of the sum of the squared standard deviations divided by 2 where the green s represents the estimated standard deviation for the green distribution and the purple s represents the estimated standard deviation for the purple distribution note there are tons of other ways to calculate effect sizes and this is just one of them so when you do a power analysis you may have to do a little research about how to estimate the overlap however in general the mean and standard deviations can be estimated with prior data a literature search or in a worst case scenario an educated guess in this case the original data suggests that the difference between the two means is ten and the estimated standard deviations are seven and six so we plug those into the formula for the pooled standard deviation and we get six point five so the effect size is 1.5 once we know the effect size and the amount of power we want and the threshold for significance we google statistics power calculator pretty much every statistics department in the world has one online then we plug in the numbers i got sample size equals nine this means that if i get nine measurements per group i will have an eighty percent chance that i will correctly reject the null hypothesis double bam in summary when two distributions overlap we need a relatively large sample size to have a lot of power when the sample size is small we have low confidence that the estimated means are close to the population means and that lack of confidence is reflected in a low probability that we will correctly reject the null hypothesis in contrast when we increase the sample size we have more confidence that the estimated means are close to the population means because extreme observations have less effect on the location of the estimated means and the closer the estimated means are to the population means the less the means from the different distributions will overlap and that increases the probability that we will correctly reject the null hypothesis and when you have a high probability that we will correctly reject the null hypothesis you have high power triple bam hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
Rsc5znwR5FA,2020-05-04T04:00:11.000000,"Statistical Power, Clearly Explained!!!",this clouds outside but who cares it's time for stat quest stat quest hello I'm Josh starburns welcome to stat quest today we're gonna talk about statistical power and it's gonna be clearly explained note this stat quest assumes that you are already familiar with p-values if not check out the quests this stat quest also assumes that you are already familiar with the normal distribution if not check out the quest let's start with two distributions the one on the Left represents the weights of mice on a special diet and the one on the right represents the weights of mice eating normal Mouse food since there's only a little bit of overlap it's pretty easy to see the difference between these two diets most of the mice on the special diet weigh less than the mice on the normal diet and if we collect a small set of measurements from the special diet and another small set of measurements from the normal diet and plot these points on a graph and compare their means then in this case we'll get a p-value equal to zero point zero zero zero four and this small p-value less than 0.05 would cause us to correctly reject the null hypothesis that both sets of data came from the same distribution in other words if this distribution which is somewhere between the special and normal diets said all data a tums from me then the small p-value would say in a loud and confident voice I reject your hypothesis if we repeated this experiment a bunch of times there's a high probability that each statistical test will correctly give us a small p-value in other words there is a high probability that the null hypothesis that all of the data came from the same distribution will be correctly rejected but every once in a while we will get something like this where the data overlap and when this happens we will get a large p-value greater than 0.05 and that means that even though we know the data came from two different distributions we cannot correctly reject the null hypothesis that all of the data comes from the same distribution so the large p-value says in a very small and meek voice dang I can't reject the null hypothesis that said because these distributions are so far apart and there is so little overlap the probability of correctly rejecting the null hypothesis is high power is the probability that we will correctly reject the null hypothesis alternatively you could say that power is the probability that we will correctly get a small p-value in this example because we have a high probability of correctly getting a small p-value and rejecting the null hypothesis we have a large amount of power BAM [Music] note if there was no difference between the special diet and the normal diet and they both shared the same distribution and we collected one set of measurements for mice on the special diet and one set of measurements for mice on the normal diet then the null hypothesis that both datasets came from the same distribution would be true in this case there is no such thing as correctly rejecting the null hypothesis so the concept of power the probability that we will correctly reject the null hypothesis doesn't apply in this situation contrast if this special diet wasn't very good at helping mice lose weight but it still made a difference then even though there is a lot of overlap we have two distinct distributions and that means power the probability that we correctly reject the null hypothesis applause if we were to weigh three mice on the special diet and three mice on the normal diet and plot these points on a graph and compare their averages then in this case we will get a p-value equal to 0.34 that means we will fail to reject the null hypothesis that both groups come from the same distribution this is a bummer because in this case we know the data comes from two different distributions and when we repeat this many times most of the time we will get a large p-value and fail to reject the null hypothesis however every once in a while we will get something like this where the data do not overlap and we will correctly get a small p-value when this happens even though the null hypothesis says all day today comes from me the small p-value will say in a loud and confident voice I reject the null hypothesis so out of all these tests this was the only one that gave us a p-value small enough that we correctly rejected the null hypothesis and that means when there is a lot of overlap between the two distributions and we have a small sample size we have a relatively low power medium BAM the good news is that we can always increase power by increasing the number of measurements we collect and a power analysis will tell us how many measurements we need to collect to have a good amount of power shameless self-promotion we'll talk more about how and why we can increase power in the stat quest on power analyses BAM in summary power is the probability that we will correctly reject the null hypothesis when we have two distributions that have very little overlap we will have a lot of power because there is a high probability that we will correctly reject the null hypothesis however when the two distributions overlap a lot and if we have a small sample size we will have a small amount of power however if we want more power we can increase the sample size lastly a power analysis will tell us how many measurements to collect to have a good amount of power double BAM hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
HDCOUXE3HMM,2020-05-04T04:00:05.000000,p-hacking: What it is and how to avoid it!,"P hackin don't do it if you do it it's a shame skin quest yeah hello I'm Josh stormer and welcome to stat quest today we're gonna talk about P hacking what it is and how to avoid it note this stack quest assumes that you are already familiar with p-values if not check out the quests imagine there was a virus and we wanted to develop a drug to reduce the time it took to recover from it so he created a bunch of candidate drugs and we tested each one to find out if any of them worked so we measured how long it took for three people to recover from the virus without any drugs and then we gave three people drug a and measured how long it took them to recover just by looking at the graph it appears that drug a did not shorten recovery very much if at all so we measure how long it takes three more people to recover without any medicine and then we give three people drug B and measure how long it takes them to recover again just by looking at the graph it doesn't look like drug B helped people recover any faster than people without any medicine so we just keep testing drugs until we get one that really looks like it does a good job and at long last it looks like drugs II does a great job reducing the amount of time it takes to recover from the virus so we calculate the means of the two groups and do a statistical test to compare the means and we get a p-value equal to 0.02 and since 0.02 is less than 0.05 we reject the null hypothesis which is that there is no difference between not taking a drug and taking drugs II BAM no no BAM we just pee hacked pee hacking refers to the misuse and abuse of analysis techniques and results in being fooled by false positives however instead of feeling great shame let's learn about pee hacking so we don't do it again imagine we measured recovery times for a whole lot of people who did not take any drugs to fight the virus and then we fit a normal distribution to all of the recovery times the red area under the curve indicates the percentage of people that recovered from the illness within a range of possible values for example 2.5 percent of the area under the curve is for durations less than 5 days indicating a 2.5 percent of the people recovered in less than 5 days in contrast 95 percent of the area under the curve is between 5 and 15 days indicating that 95 percent of the people were covered between 5 to 15 days now if we only asked three people represented by light blue circles how long it took them to recover from the illness there's a good chance all three would say something between five and fifteen days and if we asked a different set of three people represented by dark blue circles there's a good chance all three would also say something between five and 15 days just like before we can plot these two groups of people on a graph and we can calculate the mean values for the two groups and compare those two means and get a p-value equal to zero point eight six and because zero point eight six is greater than 0.05 we would fail to see a significant difference between the two groups of observations in other words the p-value did not convince us that the observations came from two different distributions and that makes sense because both groups of people came from the exact same distribution now imagine we asked another group of three people how long it took them to recover and we plotted their recovery times and mean value on a graph then we asked another group of three people how long it took them to recover and we plotted their recovery times and mean value on the graph again we do a test to compare the two means and we get a p-value equal to zero point six three and since 0.63 is greater than 0.05 we would fail to see a significant difference between the two groups of observations and again this is good because both sets of observations came from the exact same distribution now imagine we just keep taking two groups of three from the same distribution and testing to see if they are different note these two groups almost look like they could be different but the p value equals 0.06 which is greater than the standard threshold for significance 0.05 so we just keep going and sooner or later we will get something like this when we compare the two means the p-value equals zero point zero two and that tells us that there is a statistically significant difference between the two groups suggesting that the data came from two different distributions which is incorrect since we know that both samples came from the same distribution the small p-value is a false positive note you may remember from the stat quest on interpreting p-values that setting the threshold for significance to 0.05 means that approximately 5% of the statistical tests we do on data gathered from the same distribution will result in false positives that means if we did 100 tests we would expect about 5 false positives or 5 percent and if we did 10,000 tests we would expect about 500 false positives in other words the more tests we do the more false positives we have to deal with oh no it's the dreaded terminology alert doing a lot of tests and ending up with false positives is called the multiple testing problem the good news is that there are many ways to compensate for the multiple testing problem and reduce the number of false positives one popular method is called the false discovery rate shameless self-promotion I have a whole stack quest on the false discovery rate the link is in the description below the main idea is that you input the p-values for every single comparison d ppppp boop boop and then the false discovery rate does some surprisingly simple mathematics and outcome adjusted p-values that are usually larger than the original p-values and ultimately some of the tests that were false positives before end up with adjusted p-values greater than 0.05 like I said I have a whole stack quest on this method if you want to know more details the important thing to know now however is that in order for false discovery rates or any other method that compensates for multiple testing to work properly you have to include all of the p-values for all of the tests not just the one that looks like it will give you a small p-value in other words don't cherry-pick your data and only do tests that look good BAM now let's talk about a slightly less obvious form of pee hacking remember these two groups the p-value was 0.06 now we know that both groups came from the same distribution but typically when we are doing experiments we don't know if they both came from the same distribution or different ones and let's be honest we usually hope that the observations come from two different distributions in this example we are looking for a new drug to help people so we want to see an improvement so when we get data like this where the p-value is close to 0.05 but not less than it is very tempting to think hmm I wonder if the p-value will get smaller if I add more data so we add one more measurement to each group and now when we calculate the p-value we get zero point zero two which is less than 0.05 so we can report a statistically significant difference hooray we got what we wanted right no we P hacked again wah wah when a p-value is close to 0.05 like what we had with the original data there's a surprisingly high probability that just adding one new measurement to both groups will result in a false positive in other words even though using a threshold of 0.05 should only result in 5% of the bogus test giving us false positives the theory assumes that we only calculate a single p-value to make a decision in this case we calculated two p-values to make our decision the one at the start which was 0.06 then because the first p-value is close to 0.05 we added more data and calculated a second p-value in this case we know all of the measurements came from the exact same distribution so we know this is a false positive so how do we keep from making this mistake in order to avoid making this mistake we need to determine the proper sample size before doing the experiment and that means we need to do a power analysis a power analysis is performed before doing an experiment and tells us how many replicates we need in order to have a relatively high probability of correctly rejecting the null hypothesis cool where can I learn more about doing a power analysis in the next stack quests we'll talk about power and power analyses to determine the appropriate sample size BAM in summary if you have a bunch of things you want to test out like a bunch of different drugs that might help people recover from a virus don't just collect all the data but only calculate a p-value for the one time things look different instead calculate a p-value for each test and adjust all of the p-values with something like the false discovery rate this will help reduce the probability of reporting a false positive and when you do a test and get a p-value close to 0.05 but not quite less than 0.05 don't just add more observations to the data you already have instead use the data you have for a power analysis to determine the correct sample size this will help prevent you from being fooled by a false positive double bomb hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on"
fU2PuYKsr6M,2020-04-20T16:46:34.000000,Live 2020-04-20!!! Expected Values,stack Questlove Street watching its living in country stack quest live stream now [Applause] whoo hello I'm Josh Darin welcome to stat quest thank you for joining me I see in the chat we've got all kinds of people from all over in place we've got Egypt I just saw Egypt go by we've got lots of people from India we got Poland Colombia Brazil Uruguay anyways lots of excitement a lot of stuff going on I'm just gonna go ahead and get started because I'm super excited about today so BAM alright today we're gonna talk about probability and expected values and I'm gonna take off my guitar so there BAM we're gonna start crazy basic but just hang in there I know this probability this intro probability stuff is gonna be so simple that is you might get a little bored but whatever we're gonna plow through that stuff found and then we're gonna dive in whoo I just lost power so I don't know how long I'm gonna last a transformer just exploded somewhere down the street so so we'll see what happens if I just drop out I will try to do this again tomorrow but for now we'll just keep plowing about through anyways we're gonna start off with simple probability stuff and but by the end you're gonna understand the concept of an expected value and how to calculate one okay just so you know probability and expected values are two of the fundamental building blocks for statistics so if you ever try to like figure out why something works the way it works like why do we divide the variance by n minus 1 instead of in Y is n minus 1 make it unbiased if you want to understand what kind of Y of Statistics you're gonna run into expected values all the time it's sort of like the Gig way to understanding all the all the kind of like mucky things that lurk underneath everything that we do in statistics okay so with that said let's take a trip to the magical place called stat land and if I had chimes I'd have chimes going okay so once we get to stat land this guy he's gonna be our friend our friend in state land he says hey I bet the next person we meet has heard of the movie troll too and we say are you kidding me troll two is one of the worst movies ever made why would anyone know about it by the way I'm talking about the 1990 movie troll 2 that if you look at it on Rotten Tomatoes Tomatoes it is routinely rated as the worst movie ever made so why would anyone know about that movie anyways our friend says I bet you $1 that the next person we meet has heard of the movie troll 2 so we think to ourselves we go huh thinking thinking thinking good thing we have just asked everyone in stat land if they've heard of the movie troll 2 and here's the data BAM there are two types of people in stat land people that have heard of the 1990 terrible movie troll 2 and people who have never heard of troll 2 there isn't anyone who sort of heard of troll 2 or sort of hasn't heard of troll 2 they're either in one group or the other you each person is either heard of troll 2 or they have never heard of troll 2 they may not remember that they've heard of troll 2 but they've heard of it ok so 37 people in stat land have heard of troll 2 and the remaining 176 people in stat land never heard of - and that means there are 213 people living in stat land so stat Lam is not a huge place and looking at the raw numbers is useful because they tell us one stat land is pretty small and that whatever analysis we do only applies to a handful of people so it's always important to kind of know what what size population we're talking about and by looking at the numbers we can get a general sense of the trends and in this case we see that most of the people in stat land have never heard of troll - but at a glance it's not super obvious how the number of people who have never heard of troll - relates to the total population of stat land however we can make this relationship super obvious by calculating percentages so let's calculate the percentage of people in stat land that have and haven't heard of troll - actually I've been I've been nosed ichinose oh I've got something well I was gonna say I've got something to show you but let's let's keep going I'll show it to you at the very end okay so the percentage of people that have heard of troll - is the counts 37th of 37 people have heard of troll - divided by the total which is 213 people so there's 213 people that live in stat land and then we multiply that fraction by 100 all you guys know this everybody knows how to calculate percentages and if you don't wow you just learned so no big deal either way okay and that tells us that 17 percent of the people and stat land have heard of troll - likewise the percentage of people in stat land who have never heard of troll - is the counts 176 divided by the total 213 multiplied by 100 and that gives us 83 percent so we that so we see that 83% of the people and stat land have never heard of troll tube okay lastly we can plug in the total counts 213 into the equation it's just the counts and the total and that tells us that 100% of the people have either heard of troll 2 or never heard of troll 2 alternatively we can just add up the percentage of people that have heard of troll 2 17 percent to the percentage of people who have not heard of troll 2 that's 83 percent and that should give us a hundred percent of the people in stat line because we said people in the set land are - there's just two types you've either heard of troll 2 or you haven't heard of troll 2 there's no other category so that accounts for 100 percent of the population okay percentages give us a good sense of how each sub population in stat lamb and by sub population I mean people that have heard of troll 2 or not relates to the whole however it's important to note that percentages alone don't tell the whole story and that we also need to know the total number of people to know if we were talking about a large population or a small one so it's nice to know sort of the population size or the number of people were talking about okay now if we wanted to know the probability that a randomly selected person in stat land has heard of troll - we simply divide the counts 37 by the total to 13 and that gives us zero point one seven now I told you this is going to be basic so if you're fading out don't worry stick with us it's gonna get kind of cool pretty soon okay 0.17 what does that mean that means that the probability that someone in stat land has heard of troll - is zero point one seven which is pretty low which means we're unlikely to just run into someone randomly and they're gonna say hey I've heard of troll - the probability that a randomly selected person that land has not heard of troll - is 0.83 which is pretty high and that means and that means that there's a good chance if we just randomly bump into someone that they're gonna say no I've never heard of that movie I don't care about that movie so so that's that's a pretty pretty probable outcome okay lastly the probability that randomly select a person instantly and that is either heard of troll - or not heard of troll - is one so they're kind of guaranteed to be in one of those two categories now remember that bet our friend wanted to make our friend said I bet you $1 that the next person we meet has heard of the movie troll - and because we want to know the probability that we will win the bet we're gonna focus on probability so let's move the probabilities to the top so we can focus on them okay now if the next person we meet has heard of troll - then we will lose the bet and that means we will lose $1 so let's put negative 1 here to represent the outcome of losing $1 if we meet someone who has heard of troll - in contrast if the next person has not heard of troll - then we will win the bet hooray and that means we will win $1 so let's put a 1 here to represent the outcome of winning $1 if the next person we meet has not heard of troll - so now we see that the probability we will lose $1 is 0.17 in the probability that we will win $1 is 0.83 in other words the probability we will win $1 is much higher than the probability that we will lose one dollar so this looks like a pretty good bet for us to make oh I just said that so it seems like it would be a good idea to accept this bet however even though there's a high probability that we will win the bet there's still a low probability that we will lose the bet and no one likes to lose money especially me I'm I'm not a betting person so I'm gonna be honest this example is completely abstract betting is one of those things that some people do for fun but it stresses me out so I tend not to bet so we say can we make this bet a hundred times or is this just a one-time offer and there I'll be honest the reason why I'm saying this is if we just make the bet once you know we could win or lose we either win a dollar or we lose a dollar but if we make this bet a bunch of times maybe on average we'll come up you know ahead no matter what make you know there's always a small chance we could lose our honored bets in a row but that's a very small chance and so I'd like to know is like what's our chance of sort of like what are we gonna do on average over a hundred bets and our friend says we can make this bet 100 times so this is great so what we're gonna do is we're gonna we're gonna make this bet a hundred times we're going to ask a hundred people okay and so if we make this bet a hundred times we will win some and we will lose some but we can use this table to figure out how much we will win and lose and that'll give us a sense of like what will happen in the long term how safe is this bet okay if we make the bet a hundred times we can approximate how many times we will lose by multiplying the probability we will lose 0.17 by 100 and if we do the math we get seventeen so that means we expect to lose seventeen times out of 100 bets however just for now humor me and let's not do the math and just know that this this represents the approximate number of times we will lose the bet since each time we lose the bet we lose one dollar we can estimate how much money we will lose by multiplying the X the number of times we expect to lose by negative one so this whole term represents how much money we expect to lose in 100 bets and if we did the math we'd see that we expect to lose about 17 dollars wha-wha now since we can make the bet 100 times we can approximate how many times we will win by multiplying the probability we will win zero point eight three by 100 again if we did the math we'd see that we expect to win eighty three times but we're not doing the math quite yet since each time we win the bet we win $1 we can estimate how much money we will win by multiplying the number of times we expect to win by one so this whole term represents how much money we expect to win in 100 beds and if we did the math we'd see that we expect to win eighty three dollars if we wanted to find out the total of how much we could expect to win or lose we can add these two terms together and now when we do all of the math we see that we expect to win approximately 66 dollars after 100 bets however we can also calculate the average amount of money we win per bet by dividing everything by the number of bets 100 doing the math gives us 66 divided by 100 which is 0.66 so on average we expect to win 66 cents every time we bet note even though I win or lose $1 each time I bet so I don't win or lose 66 cents even though I'm betting $1 and I'm either winning or losing that dollar on average I expect to win 60 cents each time because sometimes I'm gonna lose and sometimes I'm gonna win and so on once you a verge on the wins we get 66 cents each time in statistics lingo 66 cents is the expected value for the bet using statistics notation we would write e of the bat or the expected value of the bet equals zero point six six so that's how you notate that and if we wanted to make it look even more cryptic we would write e of X or the expected value of x equals zero point six six where in this case X represents the bet whether or not someone has heard of troll 2 and I'm using this sort of fancy cryptic terminology because that's what you're gonna see if you pull up the Wikipedia article on any distribution they're gonna say who the expected value they're gonna say e of x equals something and I just want you to know that e of X is really just an average it's a type of average and we're gonna go through a bunch of examples I'm going to see that kind of different ways to do it okay so we're gonna put a of X or the expected value of x over here and we're going to talk about why we left this math so messy because you remember we could have done the math halfway through but I said let's just not do it quite yet and we're gonna we left it messy and now we're gonna talk about why we did that okay since I'm multiplying each probability by the number of bats and I'm dividing by the number of bats 100 all of the values that represent the number of bets cancel out and we are left with the probability that someone in stat land has heard of troll 2 times the outcome negative 1 plus the probability that someone has not heard of troll 2 times the outcome 1 and when we do the math we get the same thing we got before 0.66 because those hundreds just cancel out so of course we're gonna get the same result note because all of the terms for the number of bets cancelled out the expected value simply represents the average of what we would expect if we made this bet a bunch of times now if we made that bet once we will either win $1 or we will lose $1 we won't win or lose 66 cents but if we make it a bunch of times and we just average out the wins and the losses we're gonna get 66 cents every time we do it so that's what this expected value represents note in fancy math notation this expected value is this sum so it's the sum of each outcome X and we'll talk about what X means in a little bit times the probability of observing each outcome X okay so for the first term heard of troll 2 the outcome is negative 1 so x equals negative 1 so that's that's what we're observing is that we've lost some money and the probability of observing the outcome of negative on at the other deserving the loss of money is zero point 1 7 so we multiply those two values together then we add that term to the term for not heard of troll 2 and in this case the outcome is 1 so x equals 1 and the probability of observing the outcome of 1 is zero point 8 3 so we see the probability that x equals 1 is zero point 8 3 okay so now we've seen how the fancy math definition of expected value works so if you if you if you're out there in the wild I usually don't like doing these fancy terminology things but this is one of those things but I will I'm gonna be honest with you expected value for a long time when someone said hey we're just gonna do the expected value I kind of get chills down my spine and I go oh no that's not good and then they would show me this equation I go what does that equation mean I don't know and that would have that would intimidate me so what I'm doing is I'm putting them on the screen to try to like demystify so we're gonna plug in things for X and we're going to plug in stuff for that probability a bunch of times so hopefully by the end of this tech quest you're gonna be real comfortable with this formula you'll be like I get it it's just a way of calculating an average okay BAM okay now imagine our friends saying because it is rare for someone in stat land who have heard of troll 2 I will pay you ten dollars if the next person we meet has heard of the movie troll 2 but if they have not you pay me $1 okay so this is a slightly different bet if they've heard of troll 2 we get a lot we get a bigger payout but it's also rare to find people that have heard of troll 2 so there's a higher likelihood we're gonna have to pay our friend okay so we will win money will we win money or lose money if we can make this bet a bunch of time so what's gonna happen on average say like we could make this bet a hundred times or a thousand times overall are we gonna win money are we gonna lose money so we're gonna calculate the expected value to find out okay this outcome this is the outcome for when someone has heard of troll 2 so that's 10 and the outcome for when someone has not heard of troll 2 holy smokes someone just did a crazy super chat Andre care avail yo holy smokes that's like the superest superjet I've seen in a long time dang ok sorry about that shoutout but I just had to do it totally caught me I see a lot of chat going on in the background and people are using the the triple triple BAM and the special emojis but I had to had to shout that out ok oh where were we and the outcome for when someone has not heard of troll 2 was negative 1 okay when these are the outcomes the expected value is and we're using that fancy notation the sum of each outcome that's x times The Associated probability so the probability that we observe that outcome we'll start by plugging in numbers for heard of troll 2 the outcome is 10 and the probability of observing that outcome is 0 point 1 7 now we add the term for never heard of troll 2 the outcome is negative 1 and the probability of observing that outcome is zero point 8 3 so that's how we plug the numbers into that fancy equation now we just do the math and the expected value is zero point eight seven and that means we expect to make on average eighty seven cents every time we make this bet double BAM okay I've got two more examples I know we've been going pretty long I've got two more examples using a six-sided die this first example is kind of a standard example so if you go to the Wikipedia article on expected values you're gonna see this example but now that you've got a better feel for expected values you're gonna see that example I get it I know how that works okay so the probability so we're imagine we're rolling this die and the probability of landing on any specific side like having five like we have over here in the in the image of the die is 1/6 now if we want to find the expected value or the average value of rolling the die a bunch of times then we need just need to plug in values into the formula for the expected values and I think I skipped a slide or it got deleted somehow but you see that we get out the outcomes are just the the number that length comes up on top of the die so if we roll one the outcome is 1 if we roll a 6 the outcome is 6 so those are the outcomes and we're gonna plug the probabilities and the outcomes into this formula for the expected value so for lands on one the outcome is 1 and the probability of observing a one is 1/6 and that gives us 1 times 1/6 for lands on to the outcome is 2 and the probability of observing a 2 is 1/6 and that gives us 2 times 1/6 so we add that term to our sum and four lands on three we get three times one time 1/6 we add that term to the sum and four lands on four we get four times 1/6 and four lands on five we get five times 1/6 and four lands on six we get six times 1/6 now we just do the math and the expected value for rolling a six-sided die is three point five note there is no side on the die for 3.5 so we will never roll 3.5 exactly but if we take the average of what we get after a bunch of rolls then we expect that average value to be close to 3.5 damn okay one last example and then we're gonna be all done okay here's the new bet this is what our friend says he says I will pay you $100 if you roll a six otherwise you pay me ten dollars this bet means the outcome for rolling a six is 100 because he will pay us $100 and the outcome for rolling anything else is negative 10 so if we roll a 1 we owe this guy our friend nay we owe him 10 dollars so that's the those are our outcomes now what's the expected value all we have to do to find the expected value is plug the outcomes and probabilities of observing the outcomes into the formula so boo boo boo boo boo boo now we just do the math and the expected value for this bet is eight point three in other words if we can bet a bunch of times and not just once if we can bet a bunch of times we will win on average eight dollars and thirty cents each time and that means our friend is the worst gambler ever triple bam um so that is hold on I think that's our last slide we're I try to keep these things to about 30 minutes and we're at 30 minutes however I just want to do a little shout out to smeared you mayn't it she just became a member and that's awesome that's super helpful for me and keeping stat quest alive memberships are doing pretty well we're almost up to 200 the channel members which is really exciting and remember channel members are a way to support me making stack quest videos full-time as a job and the more time I can dedicate to that the more the more time the more videos and more live streams and more stuff you get and I've got some exciting stuff to talk about the last time we talked about how I was working on these Jupiter notebooks for how to do machine learning in Python I've got a bunch of those made and here's the deal what I'm gonna do is I'm gonna start doing umm because everyone's doing zoom these days I'm gonna start doing zoom classes we're gonna do a zoom class where I take you through this Python notebook oh this Jupiter notebook and we're gonna go through how to use XG boost from start to finish we're gonna get some data we're gonna clean that data up we're gonna deal with missing values we're gonna make sure that the data is formatted correctly each type is correct we're gonna do all that then we're gonna do XG boost we're gonna optimize the hyper parameters hyper parameters is just a fun fancy fancy way for things that you just have to tweak by hand but we're gonna call him hyper printer parameters because that makes him sound cool rather than things you tweak by hand and we're gonna tune those and then we're gonna have a finished model and I'm we're gonna start doing those so that's something very exciting to look forward to I'm also working on the study guides PDF study guides and those are gonna be downloadable so lots of exciting things that are going on right now and so I'm excited about stat quest I want to thank all of you guys for tuned in unfortunately I rambled on for way too long today so I'm unable to take questions live but I rest assured I I will save the live stream and I will go through it and I will look at your questions and I'll read through everything and some may end up as a queue for a future stat quest a future live stream or whatever I think we're gonna stick with this probability for a little while I want to do look I'll be honest when I started this out my goal was to do conditional probability and and sort of joint distributions and marginal distributions and kind of talk about these sort of like weird kind of fundamental concepts that have a lot of scary terms associated with them but a really basic and and once we do that we can start heading into bayesian territory then we'll start understanding Bayesian so if that was the goal but when I was just I was just kind of going and all the sudden I was like wham expected values are here let's talk about them because expected values are a big deal and I've been meaning to to cover them because I've got a I've got a stat quest on extraa to explain why the variance equation the way it is why do we divide when we want an unbiased estimate of the variance why do we divide by n minus 1 instead of n where n is the number of observations and it all has to do with expected values and so this is a stepping stone towards answering that question so anyways thank you very much I'm really happy you guys are here it means a ton to me and I'm looking forward to the next stat quest it's gonna be the first Monday in May and I think we're just gonna do them all at noon it's a good time for me and I hope it's a good time for you I know it's not a good time for everybody but these are recorded and so if it's not a good time for you you can always watch it later alright until next time quest on
uHK1-Q8cKAw,2020-04-06T16:49:34.000000,Live 2020-04-06!!! Naive Bayes: Gaussian,stat quest naivebayes stat quest gaussians at quest hello and welcome to stack quest livestream one day I'm gonna get this introduction just right but not today thanks for joining me we've got people coming from all over the world lots from India India's great we've got Nepal Germany Argentina and Brazil Peru holy smokes I don't know if I've ever had South America covered so well Germany Sweden France Indonesia I don't know what time it isn't any Indonesia it's gotta be like someone's that someone is up late at night there anyways I'm really excited you guys are all here this is gonna be a lot of fun and today we're gonna talk about naive Bayes Gaussian all right so let's get started imagine we want to predict if someone would love the movie troll 2 or not so we collected data from people that loved troll 2 and from people that do not love troll 2 we measured the amount of popcorn people eat excuse me each day how much Soda Pop they drank and how much candy they ate the mean for popcorn for the people who love troll 2 is 24 and the standard deviation is 4 and a Gaussian distribution with mean equal to 24 and standard deviation equal to 4 looks like this likewise the mean for people who do not love troll 2 is 4 and the standard deviation is 2 and that corresponds to this Gaussian distribution now we calculate the mean and standard deviation for Soda Pop for people that love troll 2 and draw the corresponding Gaussian distribution and then likewise we do the same thing for people that do not love troll 2 lastly we go we draw the Gaussian distributions for candy okay here's a pop quiz can anyone guess why this version of naivebayes is called a Gaussian naive Bayes I'm not going to give you a hint yes you are correct or at least I hope you are if you're not anyways let's assume you're correct it is because we use Gaussian distributions to represent the data in the training data set okay now remember the X access on this graph represents how much popcorn someone eats in a day so we see that people that love troll to tend to eat more popcorn than people that do not love troll - and by the way I love popcorn I could eat that thing all day okay likewise people that love troll - tend to drink more soda pop than people that do not love troll - this might be because popcorn is usually salty and pairs well with refreshing soda pop so if someone new comes in and says they eat 20 grams of popcorn each day and they drink 500 milliliters of soda pop each day which by the way is a lot more soda pop than I drink I I really drink soda pop and and they eat 25 grams of candy every day I don't need a lot of candy either but I do eat a lot of popcorn then we can use Gaussian naive Bayes to decide if they love troll too or not the first thing we do is make an initial guess that they love Patrol to this gas can be any probability that we want but a common guess is estimated from the training data for example since 8 of the 16 people love to troll - the initial guess will be 0.5 so we put that up here so we don't forget likewise the initial guess for does not love troll - is 0.5 so let's put that here so we don't forget now the score for love's troll - is the initial guess that the person loves troll - times the likelihood that they eat 10 grams of popcorn given that they love troll - and right now we're just assuming that they love troll - we don't actually know yet we're just assuming that if they loved troll - we're going to calculate the likelihood that they will eat 10 grams of popcorn and note the likelihood is the Y access coordinate on the curve that corresponds to the X access coordinate so you can see we we start on the x axis at 10 grams and we go up to the curve and bam that's where we intersect the curve and we just figure out what the Y access coordinate that corresponds to that's the likelihood and if this is kind of confusing or you saying hey why isn't that a probability there is a stat quest this is a blatant self-promotion right here there is a stat quest that explains the difference between likelihood and probability so check that one out it's subtle but useful in times like these when you you need to know what a likelihood is alright continuing our multiplication we include the likelihood that they drink 500 milliliters of soda pop times the likelihood that they eat 24 grams of candy so let's plug in the numbers 0.5 for our initial guess and for popcorn we've got a pretty small number and for soda pop we got all slightly larger number and for candy we've got a really really small number note when we get really really small numbers it's a good idea to take the log of everything to prevent underflow so we take the log and note any log will do but the natural log is the most commonly used one in statistics and machine learning so that's the one I use all the time I rarely if ever think of the other logs unless there's an earthquake or I'm at a rock concert and I'm wondering how loud the sound system is okay when we use the log function we get a sum because the log turns multiplication into addition and the sum is equal to negative one hundred twenty nine point seven BAM okay I went too fast that's our score for the log of the loves troll two or that's the love loves log loves troll to score log BAM okay now let's calculate the score for not loving troll two we'll start with the initial guess that someone does not love troll two times the likelihood that they eat 10 grams of popcorn given that they do not love troll two times the likelihood that they drink 500 milliliters of soda pop times the likelihood that they eat 25 grams of candy so let's plug in the numbers boom zero point five and use the log function to convert the multiplication into a psalm and we get negative 20 one six and since negative twenty point one six is greater than negative one hundred twenty nine point seven we classify the person as someone who does not love troll to double bam so if you I kind of went fast through Gaussian naive Bayes because we talked about normal naive Bayes last what is that called live stream and we're actually gonna do a quick review in just a little bit because I'm gonna talk about why naive Bayes is naive but but I think you guys get the general idea we just for each for each hold on for each thing in our dataset we calculate a Gaussian curve Bam Bam Bam Bam Bam Bam we calculate these Gaussian curves and we use them to calculate likelihoods given that they either love troll two or they don't love troll and we basically just multiply those likelihoods together it's not rocket science all right so now let's move on to some fun fun stuff we're gonna talk about why naive Bayes is naive however it's easier to explain this concept using the normal messages versus spam example that we used in the last live stream so let's do a little review last time we talked about naive Bayes we we walked through an example where we had normal messages from friends and family in spam unwanted messages that are usually scams or unsolicited advertisements and we use naive Bayes to filter message for us we made histograms and we calculated the probabilities that we would see each word in normal messages and the probabilities we would see each word given that it was in spam then we calculated the initial guess that a message was normal based on a number of normal messages and spam in the training dataset and then we calculated the initial guest that a message was spam then we used all of those probabilities to classify this message dear friend hi friends okay to start we multiplied the initial guess that the message was normal by the probability that we would see the word dear in a normal message and the probability we would see the word friend in a normal message and after plugging in the numbers we got 0.09 the score for the message being normal given that it says dear friend is 0.09 then we multiplied the initial guessed that the message was spam by the probability we would see the word dear in spam and the probability we would see the word friend in spam and after plugging in the numbers we got 0.01 so the score for the message being spam given that it says dear friend is 0.01 and since 0.09 is greater than 0.01 we decided that dear friend was a normal message BAM okay now that we're done with the review let's talk about why naive Bayes is naive imagine we got a bunch of normal messages and that this and and this is what each one said just like before we can make a histogram of the words and calculate the probability we will see the word dear given that it was a normal message in the probability we see the word friend given that it was in a normal message now we can calculate the probability of seeing dear friend we start with the probability we see dear times the probability we see friend plugging in the numbers gives us 0.25 however this value 0.25 ignores the fact that in the original messages dear and friend were found together a lot of the time in other words if you tell me that the first word is dear then that means the message is most similar to these four messages in the training dataset and that means the probability of seeing friend should depend on having already seen the word dear in other words if I know the first word is dear these two messages should not affect the probability of seeing the next word so the probability we see the word dear is the same as before since four of the nine messages started with dear but once we know the first word is dear and three of the four dears are followed by friend the probability of seeing friend given that it was proceeding with deer in a normal message is zero excuse me 3/4 and doing the math gives us 0.33 and this result is different from when we ignored the relationship between words in the training data set ignoring the relationships between words in the training data set is what makes naive bayes naive now you may be wondering why naive bayes ignores relationships between words this training set is super simple because I just needed to demonstrate how naive Bayes works however in practice a real training dataset might have thousands if not millions of examples of normal messages and in a large training dataset will get longer messages where there are lots of phrases and word combinations that are commonly used together so in here we've got how are you that's a real common phrase in the English language to say I'm well is a very common thing to say so those words are commonly found together last night I ate pizza a lot of people in english-speaking countries are people that speak English eat pizza so you might you might see the phrase last night I he ate pizza a lot some people eat pizza and salad but some people just eat pizza um anyways accurately modeling every single phrase in a language would be impossible so naivebayes doesn't even try however like we mentioned last time naive bayes tends to work well in practice in other words by ignoring relationships among words naive bayes has high bias but because it works well in practice naive bayes has low variance if you don't know the difference between bias and variance and machine learning well here's another blatant self-promotion check out the stat quest triple bam actually no I blew it my own joke triple spam oh I guess it's a live stream so I can't go back and do it again oh well so here's we've gotten to the end of sort of the prepared content let's I'm gonna go over to the comments and see if we got any good good comments or see if we got in any what is super chats or something like that I don't want to do some shout outs let's see what we got over here oh I also want to say I hope you guys are all doing well and are healthy and you know and if you're not you're getting the care you need I know these are tough times I thank you for visiting me during my livestream I know there's a lot going on and you might be worried about family and friends so it means a lot to me that you're joining me today and I hope you're doing okay I see what else we got Oh Italy Honduras Denmark Romania Israel Portugal New York City Korea some people are up really late at night it looks like Vietnam holy smokes Bangladesh Oh a lot of people ask if I'm gonna post this afterwards I post all of my live streams if you can't just if they're not easy to find I don't know if they're on the stat quest YouTube page but I always put them on the stack quest dot orgy video index it takes me a few hours to get them up I added the live stream record the first like ten minutes where all it is is that that I sort of that screen or just as haystack quest is about to start it records all that I have to edit that off and I can trim that off but it for some reason it takes hours to do I don't know why it's a total mystery but it takes hours to do so the live stream will be posted probably around 4 o'clock p.m. my time I don't live in New York but I know a lot of people know where New York is so I say it's New York time actually so so there's that what else we got we got some channel members that are talking which is nice it's always great to see channel members channel memberships are really important right now I think with the global economy everyone is hurting and that's not good and if you're hurting you've lost a job or your parents lost a job or something like that I my heart goes out to you and do not feel obligated to support me but because the whole economy is tanking right now ad revenue from YouTube is also tanking so my revenues got dropped by 25% in the last couple of weeks so that's a little bit scary but we'll see I'm about to come out with and guess I'm just rambling but I just want to let you guys know that I'm about to come out with some PDF study guides I know a ton a ton of people asked me if they can have the PowerPoint slides or the slides from my stack quests and the answer to that is no because that's my livelihood but what I'm doing is I'm making these PDF study guides which are you know just like six pages of just the s you know it's like everything you need to know in six pages they kind of assume that you've seen the stat quest because it doesn't walk you through everything in very small steps but it's got all the main ideas there and they're beautiful they look beautiful and I'm making these and I'm gonna make those for sale for real cheap and that's I'm gonna be another way that you can help support the channel is by purchasing the stat quest study guides for the subject of your choice so actually that's something you guys might want to comment about if there's something you would particularly like to see a study guide made for let me know um that's something I'm gonna try to dedicate a couple hours each day to making a new study guide and actually making a study guide actually takes because everything I do takes forever making a study guide actually takes a couple of days because I want it to look good and it's also interesting it's a it's a different problem for making a stat quest it's a sort of a different puzzle to solve and so I end up having to think about it like what's the best way to present the information in this smaller format where I can't walk you through things every little step is there a way I can still convey just as much information so I end up like scratching my head a lot going how am I gonna do this I'm having to make it right so they take a little while but if there's something you guys want to study god on put it in a comment or post it later on on them as a comment to the livestream or or some other video that's something I'm doing okay oh there's another thing I'm working on and I'm also working on Jupiter notebooks of how to do machine learning in Python and it's from start to finish so it includes downloading the data it includes massaging the data dealing with missing data dealing with when you import it it's got the wrong data types dealing with all those kind of finicky issues okay but then we make the machine learning math we do that we make the machine learning model and then we optimize it we do cross-validation we do grid search we do all these things and then we draw a picture to visualize what we did and so I've got four of those that I've made already I'm just sort of like trying to figure out a way to put them online that might be just something I give to members and patreon supporters but but but they're pretty cool jupiter notebooks that just go through everything and i've spent a ton of time on those so that's a really exciting thing that we're working on someone asks where they can buy these study guides i'm hoping in the next week i will have an announcement of where you can do that i'm i've got two and a half study guides made i'm gonna make two and a half more and then we're gonna upload them and then we'll have a little store and i'll send out a link there'll be a link from golly i'll send out an announcement on youtube but there'll be a link on my on my store excuse me on the stack quest out o RG and i'll be promoting that so so those will be fun to get logistic regression somebody wants that yes I'm doing logistic regression someone wants neural networks I see lots of people want neural networks and yes that one's coming however I want to do this stat quest first and I've started working on it but it's it's always like a month away it's so frustrating because something always comes up but I've but I have started working on neural networks and so that one is coming someone wants to know how to get started with machine learning after learning all these things and a few algorithms is that like as a career is that like with just started like you've learned the things in abstracting you want to practice if you just want to practice my Jupiter notebooks which will be out soon would be a perfect place to start because it covers every little detail of how to get it going someone else wants structural equation modelling which sounds crazy fancy what else we got easy and linear regression yeah that's Bayesian stuff is is you know it's on the to-do list someone else wants general linear models yes we're working on that one for sure that'll be one of the first that'll be one of the first five that come out in the next week because that is something that's near and dear to my heart yeah someone asked if the Jupiter notebooks could be for purchase for non-members too yeah I'll look into that that sounds great and it sounds like sounds like mohit wants to know as a career by the way before we get to that I would want to do a shout out to Batum Ivan Oskie they just they just signed up for early access they became a member that's awesome that's great yeah so how to get started on a machine learning career other than just apply for jobs internships practice a lot I don't know that's a little embarrassing however to that end one thing that I've requested before and I will request again now as some people in the comments will ask could you address common interview questions and I would be happy to do that I just since I'm not interviewing for these jobs I don't know the questions off the top of my head so if you can post questions in the comments on the chat through the contact me pay on my on the stat questo RG website that would be a great place to to send me some questions and I would be happy happy happy to do a live stream or we just go through these questions and kind of knock them out ah the difference between AI and data science that's an interesting question yeah I I think specifically I mean it used to be you know back in the 60s a I was specifically like trying to find you know programs that could beat everyone in chess and they were truck they were genuinely trying to come up with something that could learn on its own and simulated real intelligence whereas machine learning you know we've know about machine learning machine learning is sort of just like fancy statistical methods but the machine learning thing will never sort of like you know become self-aware and take over the world whereas I think back in the early days artificial intelligence really was trying to come up with a with algorithms that could teach themselves new tricks and I think over time since the 60s they've moved into more machine learning areas and that's fine and so the question to me now is what's the difference from machine learning and data science and machine learning is a part of data science but data science I think is is a little bigger than that data science in my mind involves like database work which is useful on Michigan learning because a lot of your data comes from database but I think if you're a specialist in machine learning you're gonna know more about the algorithms know about more about how to train them you're gonna know more about how to apply them deploy them blah blah blah all that stuff whereas if you're more data science and don't quote me on this or maybe quote me on this and just say less what that's what Josh Dahmer stat quest said but what is he know I think with data science it's it's more general like data analysis as well as like knowing some machine learning things but it's also knowing how to come up with awesome looking graphs how to summarize complicated datasets where you've got thousands of variables how do you condense that to easy to interpret you map graph or a tea sneak graph or a PCA plot so I that's what I kind of think that's what's going on Oh so so some people just asked if I could explain the membership thing for my channel so I've got memberships channel memberships I don't know if there should be maybe somewhere on your screen it says become a member that's you can do that through YouTube I also have a patreon thing where you can become a patreon through and support me through that different people have its you know with through YouTube it's it's it's a little easier to do it through patreon patreon sort of compensates for making it a little harder by making sure a larger percentage of the money you give that comes to me but either way is fine I love the support either way but what it is is you've become if you become a member you can come at different levels if for $1 a month you get early access which means you get whatever the Neustadt Quest is one or you get one usually about two weeks in advance one to two two weeks maybe one and a half weeks on average two weeks in advance one week anyways if you give a little bit more um you get discounts on merch which isn't a huge deal although I am wearing my stack quest t-shirt I love it um get 10% off merch and I think you'll get 10% off the study guides if you give a little bit more you get your name on the on the stack quest website every time I post a new video and if you post you contribute even more and I don't expect a lot of people to be able to contribute at that we're talking at this level we're talking $20 a month I put your name at the end of the stack quest video and for a crazy amount of money $50 no it's 42 dollars a month you get an album of silly songs plus all the other stuff so that's that's sort of the idea behind memberships I'm trying to do special things for those people but to be honest I know not everyone can support me so I try to do special things for everybody but yeah they they all get early access and and some of them get their name recognized because it's it's very toward it for me Oh someone asked how I do I decide to use R or Python in my data analysis and I say it depends on the data and sort of what type of analysis so when I was making my Python Jupiter notebooks on machine learning I wanted to do one on I did I did neural networks I did decision trees I did support vector machines and I did one more hold on let me see let me check what did I do XG boost and I did XG boost which is awesome oh holy smokes we got another early access member mark T thank you for joining anyway so I did those but I wanted to do random forests however the implementation of random forests in Python is terrible it's way way better in R and I don't know why there's no real reason for it but in our random forests have some really cool features for imputing missing data and visualizing crazy complicated datasets which I like and I kind of love and for some reason you can only do that in our you can't do that in Python unless you code it yourself so in that case if I want to do a random forest I'm doing in our hands down however other than that it sort of depends on what kind of environment I'm working with who I'm working with XG boost for example it's has kind of the same interface for Python as are so it's sort of like wherever you are if you're already doing Python then just stay in Python if you're doing are just stay in R if if the data analysis if it's less machine learning and more just data analysis I'll do it in R because R you don't have to import all the you know it's important um PI and and pandas and stuff like you know how to have have like NP dot normal distribution or whatever you can just type give me a normal distribution in are super easy however python is great in that it's got flexibility to do anything if I want to deploy like my machine learning thing in a production environment I'm gonna find a lot more ways to do that with Python so it obviously it always it always depends so yeah I mean both languages have pros and cons both languages have tons of support on Stack Exchange and Stack Overflow so you know and that's you know that's how I code yes I'm like huh I wonder how they do it on the web and I google it anyways thank you guys very very much for joining me for today's livestream I hate to end it I always have to end it and always surprises me when we get to the end of the live stream let me know in the comments what you thought about this time of day I'm still just trying to figure out what the best time of day is for a stat quest and I think I still think it's gonna be two times a day like I'll do a morning one for me it'll be 9:00 a.m. and that will that allows me to get Japan and some of China and places like that and maybe a noon one and that hopefully will get the rest of the world I don't know so that's another thing we're kind of working on and and I'm just sort of looking at the at the statistics and the feedback that YouTube's giving me so I can see whatever gets the most people available is great oh and before we go I want to give a Bamm out to Josh Erickson for joining becoming a member that's great thank you very much all right oh I know I keep saying we're gonna leave and I keep saying there's one more thing to say the next time we're gonna do conditional probability and basic probability that's gonna be the next live stream so get ready for some basic probability and understanding conditional probabilities alright until next time quest on
i4iUvjsGCMc,2020-04-01T04:00:14.000000,Bam!!! Clearly Explained!!!,"When the going gets tough the tough get stuff and when the tough get stuff they get StatQuest! Hello, I'm Josh Starmer welcome to StatQuest. Today we're going to talk about BAM and it's gonna be clearly explained Note this StatQuest assumes that you want to know what BAM!!! means. If not, it assumes you have a sense of humor If you watch StatQuest videos, chances are, you've experienced BAM!!! You may have also experienced Double BAM!!! And if you're really lucky, you might have experienced the rare and coveted TRIPLE BAM!!! Unfortunately, some of you may have also experienced small bam or even worse tiny bam If you've experienced these ""bams"" and wondered what they're all about, the good news is that you are not alone. This guy wants to know what BAM means and so does this dude? When you see a big bam like... BAM!!! Then you know something awesome just happened. And if it didn't seem awesome to you, just rewind and play it over and over again. Sooner or later you will achieve enlightenment and become one with the... BAM!!! NOTE: The size and number of bams are(not is) correlated with the awesomeness. For example, this is just one bam with font size equal to 88 and it means something awesome just happened. In contrast, this is three bams with font size equal to 248 and it means something totally awesome happened. When you see a small bam, you know, you've just survived another Dreaded terminology alert!!! ...or maybe you just saw something else, that is lame but probably important to know. In summary BAM!!! Double BAM and TRIPLE BAM!!! Let you know that you're watching... StatQuest Because StatQuest is totally BAAAAM!!! (It reminded me the kid who yells yeah boi) Hooray. We've made it to the end of another exciting StatQuest I hope you guys are all healthy and well and enjoyed this little bit of light humor in this time of uncertainty Alright until next time quest on"
JQc3yx0-Q9E,2020-03-23T04:15:00.000000,How to calculate p-values,calculating p-values is kind of fun and not just when you're done stat quest hello I'm Josh Starman welcome to stat quest today we're gonna talk about how to calculate p-values note this stat quest assumes that you are already familiar with what p-values are and how to interpret them if not check out the quest also note before we get started I want to mention that there are two types of P values one sided and two sided two-sided p-values are the most common and this quest focuses on calculating them in contrast one-sided p-values are rarely used and to be honest potentially dangerous I won't mention them again until the very end when I give an example of why they should be avoided with that said let's imagine I had a coin and I flipped it once and got heads then I flipped it again and got heads a second time now at this point I might be tempted to think wow my coin is super special because it landed on heads twice in a row this is a hypothesis however in statistics lingo the hypothesis is even though I got two heads in a row my coin is no different from a normal coin note although we want to know if our coin is special statistics lingo version says the opposite that our coin is the same as a normal coin statisticians call this the null hypothesis and a small p-value will tell us to reject it and if we reject this null hypothesis we will know that our coin is special so let's test this hypothesis by calculating a p-value p-values are determined by adding up probabilities so let's start by figuring out the probability of getting two heads in a row when we flip a normal everyday coin there's a 50% chance we'll get heads and a 50% chance we'll get tails now if we got heads on the first flip and flip the coin a second time then just like before there's a 50% chance we'll get heads and a 50% chance we'll get tails likewise if we got tails on the first flip and flip the coin again then just like before there's a 50% chance we'll get heads and a 50% chance we'll get tails ultimately these are the four possible outcomes after flipping a coin two times because each outcome is equally probable we can calculate the probability of getting two heads with the following formula the number of times we got two heads divided by the total number of outcomes in this case we only got two heads one time so we put a 1 in the numerator and since there were four possible outcomes we put a four in the denominator thus the probability of getting two heads is 0.25 likewise the probability of getting two tails is 0.25 finally the probability of getting one heads and one tails regardless of the order is 0.5 now you may be wondering why we don't care about the order of the heads and tails and treat these outcomes as the same in this case the order doesn't matter because getting a heads on the first flip doesn't change the probabilities of getting heads or tails on the second flip likewise getting tails on the first flip doesn't change the probabilities of getting heads or tails on the second flip because order does not affect the probabilities of getting heads and tails we treat these outcomes as the same now let's move the outcomes over to the left unless the probability of each outcome and calculate the p-value for getting two heads a p-value is composed of three parts the first part is the probability random chance would result in the observation in this case the first part is just the probability that a normal coin would give us two heads which is 0.25 the second part is the probability of observing something else that is equally rare in this case getting two tails is as rare as two heads so we add 0.25 the third part is the probability of observing something rare or more extreme in this case the third part is zero because no other outcomes are rarer than two heads or two tails now we just add everything together and the p-value for getting two heads equals 0.5 now remember the reason we calculated the p-value was to test this hypothesis even though I got two heads in a row my coin is no different from a normal coin typically we only reject a hypothesis if the p-value is less than 0.05 and since 0.5 is greater than 0.05 we fail to reject the hypothesis in other words the data getting two heads in a row fail to convince us that our coin is special note the probability of getting two heads 0.25 is different from the p-value for getting two heads 0.5 this is because the p-value is the sum of three parts the first part is the probability random chance would result in the observation the second part is the probability of observing something else that is equally rare and the third part is the probability of observing something rare or more extreme now the question is why do we care about things that are equally rare or more extreme in other words why do we add parts two and three to the p value we add part to the probability of something else that is equally rare because although getting two heads might seem special it doesn't seem as special when we know that other things are just as rare for example imagine giving a loved one a flower and saying this is the rarest flower of this species none are equally as rare chances are your loved one would think that the flower was super special he might even get a kiss on the cheek now imagine saying to your loved one this flower is equally as rare as all of these other flowers in this case your loved one might not think the flower is very special wha-wha note even though these flowers are different colors just knowing that they're equally rare would be a bummer because a lot of equally rare things would make something less special we add part two to the p-value and we add rare things to the p-value for a similar reason going back to our flower example imagine telling your loved one this is the rarest flower of this species none are rarer again there's a good chance your loved one would think that the flower was super special now imagine saying there are a lot of flowers that are rarer than this one in this case your loved one might not think the flower is very special wah-wah and like before even though these flowers are all different colors just knowing they are rare would be a bummer thus because rarer things make something less special we add part three to the p-value okay now that we know that getting two heads in a row is not very special or statistically significant what about getting four heads and one tails would that suggest that our coin is special in other words we can calculate a p-value to test this hypothesis even though I got four heads and one tails my coin is no different from a normal coin again although we want to know if the coin is special the null hypothesis focuses on a normal coin but if we get a small p-value and reject the null hypothesis we will know that our coin is special so let's calculate the p-value for getting four heads and one tails first we know that it is possible to flip a coin five times and get heads each time so let's keep track of that with five blue H's we can also flip a coin five times and get four heads and one tails note there are five different ways to get four heads and one tails but we treat them all the same because the order of heads and tails doesn't matter likewise there are ten ways that we can flip a coin and get three heads and two tails and ten ways to get two heads and three tails and five ways to get one heads and four tails and lastly one way to flip a coin five times and get five tails all in all when we flip a coin five times there are 32 possible outcomes the p-value for getting four heads and one tails is the probability we randomly get four heads and one tails this is 5/32 since five of the 32 outcomes had four heads and one tails plus the probability we randomly get something else that is equally rare this is 5 divided by 32 since 5 of the 32 outcomes had one head and four tails plus the probability we randomly get something rarer or more extreme this is 2 divided by 32 because both 5 heads and 5 tails only occurred once each they are rarer than four heads and one tails thus the p-value for getting four heads and one tails is 0.375 again we typically only reject the null hypothesis if the p-value is less than 0.05 so in this case we will fail to reject the null hypothesis in other words the data getting four heads and one tails did not convince us that our coin was special with coin tosses it's pretty easy to calculate probabilities and p-values because it's pretty easy to list all of the possible outcomes but what if we wanted to calculate probabilities and p-values for how tall or short people are in theory we could try to list every single possible value for height however in practice when we calculate probabilities and p-values for something continuous like height we usually use something called a statistical distribution here we have a distribution of height measurements from Brazilian women between 15 and 49 years old taken in 1996 the red area under the curve indicates the probability that a person's height will be within a range of possible values for example 95 percent of the area under the curve is between 142 and 169 and that means that 95 percent of the Brazilian women were between 142 and 169 centimeters tall in other words there is a 95 percent probability that each time we measure a Brazilian woman their height will be between 142 and 169 centimeters 2.5 percent of the total area under the curve is greater than 169 and that means there is a 2.5 percent probability that each time we measure a Brazilian woman their height will be greater than 169 centimeters likewise 2.5 percent of the total area under the curve is less than 142 thus there is a 2.5 percent probability that each time we measure a Brazilian woman their height will be less than 142 centimeters to calculate p-values with a distribution you add up the percentages of area under the curve for example imagine we measured someone who is 142 centimeters tall if we measured someone who is 142 centimeters tall we might wonder if it came from this distribution of heights which has an average value of 150 5.7 or if it came from another distribution of heights for example this green distribution has an average value of 142 so the question is is this measurement 142 centimeters so far away from the mean of the blue distribution that we can reject the idea that it came from it if so then that would suggest that another distribution like this green one might do a better job explaining the data the p-value for the hypothesis this measurement comes from the blue distribution starts with the 2.5 percent of the area for people less than or equal to 142 centimeters note when we are working with a distribution we are interested in adding more extreme values to the p value rather than rarer values in this case all heights further than 142 centimeters from the mean are considered more extreme than what we observed we also add the 2.5% of the area for people 169 centimeters or taller note just like on the other side of the distribution these values are considered equal to or more extreme because there is far from the mean or further now we just do the math and get 0.05 so the p-value for the hypothesis someone 142 centimeters tall could come from the blue distribution is 0.05 and since the cutoff for significance is usually 0.05 we would say hmm maybe it could come from this distribution maybe not it's hard to tell since the p-value is right on the borderline so maybe they come from this distribution or maybe they come from this distribution the data are inconclusive wah wah note if we had measured someone who is 141 centimeters tall so just a little bit shorter than 142 centimeters then the p value would be 0.01 6 plus 0.01 6 which equals zero point zero three and since 0.03 is less than 0.05 the standard threshold we can reject the hypothesis that given the blue distribution it is normal to measure someone 141 centimeters tall thus we will conclude that it's pretty special to measure someone that short and that suggests that a different distribution of Heights makes more sense now what if we measured someone who is between 155 point four in 156 centimeters tall note the peak of the curve is right at the average height so we were asking is a measurement between 155 point four and 156 so far away from the mean of the blue distribution that we can reject the idea that it came from it if the p-value is small then that suggests that some other distribution would do a better job explaining the data note the probability of someone being between 155 point four and 156 centimeters is only 0.04 the red area is pretty small barely a lime so 0.04 is the first part of calculating the p-value since given this distribution of heights that is the probability that we would randomly measure someone in this range of values now we need to figure out the more extreme parts on the left side all of the heights less than 150 5.4 are further from the mean thus they are more extreme and because 48% of the area under the curve is for Heights less than 150 5.4 we add 0.48 to the p-value on the right side all of the heights greater than 156 are further from the mean thus they are all more extreme and because 48% of the area under the curve is for Heights greater than 156 we add 0.48 to the p-value ultimately we end up adding all of the area under the curve so the p-value equals one so this means that given this distribution of heights we would not find it unusual to measure someone whose height was close to the average even though the probability is small in other words the data does not suggest that another distribution would do a better job explaining the data BAM so far we've only talked about two-sided p-values now I'll give you an example of a one-sided p-value and tell you why it has the potential to be dangerous imagine we measured how long it took a bunch of people to recover from an illness now imagine we created a new drug super drug and wanted to see if it helped people recover in fewer days if we gave super drug to a bunch of people and the average recovery was 4.5 days then a two-sided p-value like the ones we've been computing all along would be the sum of this area under the curve 0.01 6 plus this area under the curve 0.01 6 and the total is 0.03 and since 0.03 is less than 0.05 the two-sided p-value tells us that given this distribution of recovery times super drug did something unusual and that suggests that some other distribution does a better job explaining the data for a one-sided p-value the first thing we do is decide which direction we want to see change in in this case we'd like super drug to shorten the time it takes to recover from the illness so that means we want to see if recovery times are shorter because we want to see change in this direction the only more extreme values are less than 4.5 days all of the values greater than 4.5 days are considered less extreme so when we calculate a one-sided p-value we only use the area that is in the direction we want to see change 0.016 again since 0.01 6 is less than 0.05 the one-sided p-value would tell us that given this distribution super drug did something unusual and that some other distribution makes more sense now imagine that super drug wasn't so super and on average it took fifteen point five days to recover just like before the two-sided p-value would be the sum of this area under the curve zero point zero one six plus this area under the curve zero point zero one six and the total is zero point zero three in other words regardless of whether super drug is super and makes things better or if it is not so super and makes things worse a two-sided p-value will detect something unusual happened for a one-sided p-value the first thing we do is decide which direction we want to see change in and just like before that means we want to see if recovery times are shorter so the one-sided p-value is this huge area 0.98 because it is more extreme in the direction we want to see change and since 0.98 is greater than 0.05 the one-sided p-value would not detect that Superdrug was doing anything unusual in other words the one-sided p-value is only looking to see if a distribution to the left of the original mean makes more sense and since the observation is on the right side of the mean we fail to reject the hypothesis that the original distribution makes sense and since failing to detect that Superdrug is making things worse would be bad one-sided p-values are tricky and should be avoided or only used by experts who really know what they're doing BAM in summary a p-value is composed of three parts the first part is the probability random chance would result in the observation the second part is the probability of observing something else that is equally rare and the third part is the probability of observing something rarer or more extreme BAM hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
vemZtEM63GY,2020-03-23T04:00:17.000000,p-values: What they are and how to interpret them,[Music] gonna talk about p values yeah statquest hello i'm josh starmer and welcome to statquest today we're going to talk about what p-values are and how to interpret them imagine i have two drugs drug a and drug b and i want to know if drug a is different from drug b so i give one person drug a and i give one other person drug b the one person using drug a is cured hooray the one person using drug b is not cured bummer can we conclude that drug a is better than drug b nope drug b may have failed for a lot of different reasons maybe this guy is taking a medication that has a bad interaction with drug b or maybe this guy has a rare allergy to drug b or maybe this guy didn't take drug b properly and missed a dose or maybe drug a doesn't actually work and the placebo effect deserves all of the credit there are a lot of weird random things that can happen when doing a test and this means that we need to try each drug on more than just one person each so we redo the experiment but this time we give each drug to two different people this time both people taking drug a are cured hooray and one person taking drug b is cured and one person is not cured hooray and bummer is drug a better are both drugs the same we can't answer either of those questions because maybe something weird happened to this guy that caused drug b to fail or maybe something weird happened to this guy like maybe the drug was mislabeled and he actually took drug a and that's why he was cured so now we test the drugs on a lot of different people and these are the results drug a cured a whole lot of people 1043 compared to the number of people it didn't cure 3. in other words 99.7 of the 1046 people using drug a were cured in contrast drug b only cured a few people two compared to the number of people it didn't cure one thousand four hundred thirty two in other words only 0.1 percent of the 1434 people using drug b were cured if these were the results then it would be pretty obvious that drug a was better than drug b in other words it would seem unrealistic to suppose that these results were just random chance and that there is no real difference between drug a and drug b it's possible that some of these people were cured by placebo and some of these people were not cured because of some rare allergy but they are just too many people cured by drug a and too few cured by drug b for us to seriously think that these results are just random and that drug a is no better or worse than drug b in contrast what if these were the results now only 37 percent of the people that took drug a were cured compared to 31 percent that took drug b so drug a cured a larger percentage of people but given that no study is perfect and there are always a few random things that happen how confident can we be that drug a is superior that's where the p-value comes in p-values are numbers between 0 and 1 that in this example quantify how confident we should be that drug a is different from drug b the closer a p-value is to zero the more confidence we have that drug a and drug b are different so the question is how small does a p-value have to be before we are sufficiently confident that drug a is different from drug b in other words what threshold can we use to make a good decision in practice a commonly used threshold is 0.05 it means that if there is no difference between drug a and drug b and if we did this exact same experiment a bunch of times then only 5 of those experiments would result in the wrong decision yes this is an awkward sentence so let's go through an example and work this out one step at a time imagine i gave the same drug drug a to two different groups now any differences in the results are 100 percent attributable to weird random things like a rare allergy in one person or a strong placebo effect in another in this case the p-value would be 0.9 which is way larger than 0.05 thus we would say that we fail to see a difference between the two groups if we repeated this same experiment a lot of times most of the time we would get similarly large p values however every once in a while all of the people with rare allergies might end up in the group on the left and all of the people with the strong placebo reactions might end up in the group on the right as a result the p-value for this specific run of the experiment is 0.01 since the results are pretty different thus in this case we would say that the two groups are different even though they both took the same drug oh no it's the dreaded terminology alert getting a small p value when there is no difference is called a false positive a 0.05 threshold for p values means that 5 of the experiments where the only differences come from weird random things we'll generate a p-value smaller than 0.05 in other words if there's no difference between drug a and drug b 5 percent of the time we do the experiment we will get a p-value less than 0.05 aka a false positive note if it is extremely important that we are correct when we say the drugs are different then we can use a smaller threshold like 0.00001 using a threshold of 0.00001 means we would only get a false positive once every 100 000 experiments likewise if it's not that important for example if we're trying to decide if the ice cream truck will arrive on time then we can use a larger threshold like 0.2 using a threshold of 0.2 means we are willing to get a false positive two times out of 10. that said the most common threshold is 0.05 because trying to reduce the number of false positives below 5 often costs more than it's worth so if we calculate a p-value for this experiment and the p-value is less than 0.05 then we will decide that drug a is different from drug b that said the p-value is actually 0.24 so we are not confident that drug a is different from drug b bam okay before we're done let me say two more things about p-values unfortunately the first thing i want to say is just more terminology in fancy statistical lingo the idea of trying to determine if these drugs are the same or not is called hypothesis testing the null hypothesis is that the drugs are the same and the p-value helps us decide if we should reject the null hypothesis or not small bam okay now that we have that fancy terminology out of the way the second thing i want to say is way more interesting while a small p-value helps us decide if drug a is different from drug b it does not tell us how different they are in other words you can have a small p-value regardless of the size of difference between drug a and drug b the difference can be tiny or huge for example this experiment gives us a relatively large p-value 0.24 even though there is a six-point difference between drug a and drug b in contrast this experiment which involves a lot more people gives us a smaller p-value 0.04 even though given the new data there is a one point difference between drug a and drug b in summary a small p-value does not imply that the effect size or difference between drug a and drug b is large double bam hooray we've made it to the end of another exciting stat quest if you liked this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
bTs-QA2oJSE,2020-03-16T13:49:34.000000,Live 2020-03-16!!! Naive Bayes,stat quest naivebayes gonna talk about it clearly explained stat quest hello and welcome to the stat quest a live stream I'm glad you guys are all here I'm really excited about today's stat quest and I hope you guys are excited about it too we're gonna be talking about naive Bayes and it's gonna be clearly explained I see in the chat we've got lots of hellos hello everybody we got someone from Luxembourg some people from India people from Brazil people all over the world this is very exciting [Music] just check-in i someone said no audio is that correct are you guys getting any audio maybe someone post a comment maybe I will I mean I'm gonna write in a comment real quick are you getting audio got someone from Texas Tunisia greetings from Germany Toronto Poland holy smokes Greece yes you have audio it's been confirmed you can hear what I'm saying thank you very much let's talk about naivebayes China holy smokes Russia oh my gosh this is very exciting and India Netherlands wow this is a like this is just blowing my mind I it's not common to talk to everyone in the whole world least have most major countries represented Turkey holy smokes more India alright this is very exciting ok enough of me just getting all excited about the chat going on right now let's talk about naive Bayes I'm gonna be focused on that all right so I have a theory that the simpler the machine learning method the more jargon is used to make it seem complicated naivebayes is a good example of this theory in action it is chalk up lock full of fancy jargon and it's really one of the just most straightforward simplest machine learning methods out there so we're just gonna skip the jargon for now and dive right into an example that shows how it works imagine we received normal messages from friends and family and we also received spam unwanted messages that are usually scams or unsolicited advertisements and we wanted to filter out the spam messages so the first thing we do is make a histogram of all the words that occur in the normal messages from friends and family we can use the histogram to calculate the probabilities of seeing each word given that it was in a normal message for example the probability of seeing we see the word dear given that we saw it in a normal message is 8 the total number of times deer occurred in a normal messages divided by 17 the total number of words in all of the normal messages and that gives us 0.47 so let's put that over the word deer over on the left side so we don't forget it likewise the probability that we see the word friend given that we saw it in a normal message is 5 the total number of times friend occurred in normal messages divided by 17 the total number of words and all of the normal messages and that gives us zero point 2 9 so let's put that over the word friend so we don't forget it and likewise the probability that we see the word line given that it isn't a normal message is 0.18 in the probability we see the word money given that it is in a normal message is 0.06 now we make a histogram of all the words that occur in the spam and we calculate the probability of seeing the word dear given that we saw it in the spam and that is - the number of times we saw deer in the spam divided by 7 the total number of words in the spam and that gives us zero point 2 9 likewise we calculate a probability of seeing the remaining words given they were in the spam I'm gonna go back one slide look at that the word lunch is not seen in any of the spam so the probability of seeing is zero that's gonna come to haunt us later on but then we're gonna come up with a workaround to fix that problem but so I'm kind of giving you a sneak peak of what's gonna happen later on but for now we're just gonna let that be 0 it's not gonna bother us for this first example ok BAM now because these histograms are taking up a lot of space let's get rid of them but keep the probabilities so we just got the probabilities on the left side of the screen instead of the histograms ok now imagine we got a new message that said dear friend it's a very short message but it's it's an example okay if we want it to decide if it's a normal map ant excuse me and we want to decide if it's a normal message or spam we start with an initial guess about the probability that any message regardless of what it says is a normal message the guests can be any probability that we want but a common guess is estimated from the training data for example since 8 of the 12 messages are normal messages our initial guess will be zero point six seven so let's put that under the normal messages so we don't forget it now we multiply that initial guess by the probability the word dear occurs in a normal message and the probability that the word friend occurs in a normal message now we just plug in the values we worked out earlier and do the math and we get zero point zero nine in a simple way and improbably actually more than just simple probably in a proper way we can think of zero point zero nine as the score that dear friend gets if it is a normal message however technically it is proportional to the probability that the message is normal given that it says dear friend so let's put that on top of the normal messages so we don't forget now just like we did before we start with an initial guess about the probability that any message regardless of what it says is spam and just like before we we can guess the guests can be any probability we want but a common guess is estimated from the training data and since four of the twelve messages are spam our initial guess will be 0.33 so let's put that under the spam so we don't forget it now we multiply that initial guess by the probability that the word dear occurs in spam and the probability that the word friend occurs in spam now we just plug in the values that we worked out earlier and do the math and we get 0.01 like before we can think of 0.01 as the score that dear friend gets if it is spam however technically it is proportional to the probability that the message is spam given that it says dear friend and because the score we got for normal message is 0.09 is greater than the score we got for spam 0.01 we will decide that dear friend is a normal message double bam now before we move on to a slightly more complex situation let's review what we've done so far we started with histograms of all the words in the normal messages and all the words in the spam then we calculated the probabilities of seeing each word given that we saw the word in either a normal message or spam by the way this is a terminology alert it's not really part of the stat quest because I don't think it's super necessary to know this because it's pretty intuitive but technically these probabilities are called conditional probabilities and up up on top we have the probability the conditional probability of seeing deer given that it's in a normal message that's sort of the the statistical lingo for how you would say that you'd say the conditional probability of seeing deer given the nuts in a normal message is 0.47 so if you want to sound fancy use the fancy terminology anyways so we put those conditional probabilities over to the left then we made an initial guess about the probability of seeing a normal message this guess can be anything but we based hours on the classifications in the training data set then we made the same sort of guess about the probability of seeing spam then we multiplied our initial guess that the message was normal by the probabilities of seeing the words dear and friend given that the message was normal then we multiplied our initial guessed that the message was spam by the probabilities of seeing the words dear and friend given that the message was spam then we did the math and decided that dear friend was a normal message because 0.09 which is what we got for the normal message is greater than 0.01 which is what we got for spam so there we go we've classified dear friend as a normal message now now that we understand the basics of how naive Bayes classification works let's look at a slightly more complicated example and by slightly I mean this is much more complicated naive Bayes is really really simple even when it gets complicated okay this time let's try to classify this message lunch money money money money note this message contains the word money four times and since the probability of seeing the word money is much higher in spam 0.56 then in normal messages 0.06 then it seems reasonable to predict that this message will end up being spam so let's do the math calculating the score for a normal message works just like before we start with the guests then we multiply it by the probability we see lunch given that it is in a normal message and the probability we see money for x given that it isn't a normal message and you'll notice I don't not really pointing this out in the stat quest explicitly but you'll see a little you'll see that the probability that we see money given that it's a normal message is raised to the fourth power so that's how we're gonna kind of like keep this equation from going right off the edge of the screen we'll just use that exponent okay when we do the math we get this tiny number a bunch of zeros and then finally we get the number 2 it's small no big deal however when we do the same calculation for spam we get 0 and this is because the probability we see lunch in spam is zero since it was not in the training data in other words any message with the word lunch in it will get 0 for spam and that means we will always classify it as normal no matter how many times we see the word money and that's a problem to work around this problem people usually add one count represented by a black box to each item in the histograms um I mean I don't want to get ahead of myself when I start adding like random stuff like I'm just thinking about off the top of my head I almost always say what's coming next okay it's a note the number of counts we add to each word is typically referred to with the Greek letter alpha if you're gonna do a naive Bayes and scikit-learn all with Python that's the parameter you're gonna set you're gonna set alpha equal to something I think the default value is 1 already so you don't have to worry about it but but you can check that at your leisure anyways in this case alpha equals 1 but we could have said it to anything another term for alpha by the way and this is another this is like oh no terminology learned another term for alpha is pseudo count that's sort of the term I was kind of brought up on whenever you add extra numbers it's called a pseudo count but like I said naive Bayes is chock-a-block full of terminology to make it sound fancier than it really is we really haven't done anything that fancy by adding one count to every single word anyway now when we calculate the probabilities of observing each word we never get 0 for example the probability of seeing lunch given that it is in spam is 1 that pseudo count that extra count that we just added divided by 7 the total number of words in spam OOP excuse me plus for the extra counts that we added to each word and that gives us 0.09 so now when we calculate the scores for this message we still get a small number of normal messages excuse me we still get a small number for the normal message a little bigger than before I think but now when we calculate the value for spam we get a value greater than zero and since the value for spam is greater than the one for a normal message we classify the message as spam [Music] spam I've been waiting this whole stack quest to say that okay we're gonna talk about why we do this math this way some other day and what it has to do with base and some of it is however I will I will actually we might talk about it now I think we've got time let me let me just check mine let me put this guitar down check the time we have time so we will in fact address that question of like what's going on technically I don't have great slides for this but but we will talk about it in just a little bit however before we do that I want to mention that I've got these sources I'm gonna put these links in the description of the video below um as soon as we've saved everything I should have should have already done it but I'm running a little bit behind this morning so I'm gonna put those there you don't have to copy these down anyways I've got the scikit-learn information including how to what the alpha value is so you guys can look that up and we're gonna say double spam and now we're gonna do some viewer questions and I saw in this in over in the chat window that someone said I did not get that directly proportional part where you said the probability of giving a word at spam is directly proportional to prog Doba yes so so what's going on let's go back let's go back to where let's go back to here okay this is where we're actually talking about the proportionality you remember this is the unit we start on the left side with the probability at spam times the probability we see a word given that it's in spam times the probability we see another word given that it's in spam so we see the word dear in spam and the probability we see friend in spam and once we plugged in the numbers I said this is proportional to the probability that the message is spam given that it says dear friend and this is sort of the magic of bayes's theorem which is sort of how this transformation works the reason why it's proportional is Bazin bayesian fazes theorem I mean divides typically divides the left side by a scaling factor to make sure that that value is going to be between 0 and 1 and it is actually a probability and in this case the way we're calculating the probabilities and everything we're kind of guaranteed to get a value between 0 and 1 but there's still a scaling factor that bayes's theorem sort of requires you do to get an exact probability and that scaling factor actually plays a much much much bigger role when you're using instead of counts like we used counts we use the number of time we saw the word dear in spam and the number of times we saw the word friend and spam so we were just counting these things but if you're using a Gaussian distribution or a normal distribution to kind of model things like somebody's height you say like we're trying to predict gender based on their height and some other things or not gender sex I guess that's the proper word for it now based on their height and some other things and we're using a normal distribution for those that scaling factor plays a much bigger role because we're actually just using likelihoods in the numerator and and so in this case we're using probabilities because we're using Celts so when we have discrete discrete things that were counting we use probabilities because they're the exact same thing as likelihoods but when we use a normal distribution or something like that to measure what were what we're using to make predictions we're using likelihoods and likelihoods can be much larger than one and that normalization factor the thing that we would divide the this this equation by we would divide this equation the probability of seeing something that's spam times the probability of deer given that it's in spam times the probability of friend given that it's in spam we would divide that by something to make sure that we're getting a probability and scales it so I don't know if that's the best answer to that question but sort of given the slides that I have sort of the best I can do so yeah the proportional thing is something that goes from from bayes's equation however it's all still a stretch because the naive aspect of Bayes is that the probability that we see deer in a spam message tom is considered to be independent of the probability we would see the word friend in a spam message and so that's that's one of the things that makes naive Bayes very naive because often times words or things are correlated and when they're correlated you can't just multiply the probabilities together directly and get get a person get an accurate estimate of the true probability so naive Bayes just gets rid of that complexity of dealing with correlations and simplifies everything by saying hey we're just gonna assume there's no relation whatsoever and that's gonna make me naive but it's gonna make me simple and get this surprisingly accurate anyways that's sort of the weird thing about machine learning is we kind of throw away the you know how good the model is in order to you know intern in a technical way in a theoretical way by making sort of like ridiculous assumptions and and when we throw those things away we actually get a model that works great with actual data and the reason why that is has to do a little bit with the vias variance trade-off that affects all of machine learning by by making this assumption that these words are not correlated or that everything's uncorrelated we add a little bias to our model so we're not like you know super accurately predicting the reality of the situation but in exchange for that little bit of bias we get a little big reduction in variance meaning we're doing a much better job predicting in our testing data set so so that's that's what that's that is what else can we talk about with naive Bayes naive Bayes I'll be honest I don't have I'm just gonna be put this out there I actually don't have a lot of practical experience so what I'm doing is I'm just telling you what I read naive Bayes let's talk about the advantages of naive Bayes I haven't experience these firsthand but this is what people say about naive Bayes they say it's great when you don't have much data and it's also great when you have tons and tons of data because it's relatively easy to to scale up to crazy large because this is such a simple model and it just relies on histograms and if you saw the last XG boost video XG boost part for crazy optimizations you learned about sketch algorithms barely and we talked about the quantile sketch and the quantile sketch is a is related to something called a histogram sketch which is an algorithm that can rapidly approximate a histogram without having to look at any single value more than once and that's kind of a big deal because usually when we do a histogram we sort all of the all the observations and we have to count them down and that can kind of be a mess actually what am I saying a histogram is just counts we can just it's we don't even need a sketch algorithm this is what I get for trying to think and talk at the same time we can do this histogram super fast we can the data is coming in we don't we don't ever have to look at it more than once the data is coming at us we just look at a word and go ahead have we seen this before yes we'll just add a count to that no we'll just say we've seen it once and then we just keep going and at the end we've got our histogram BAM no big deal anyways naive Bayes scales really well to really large data sets and I've been told that it does a great job for spam and sort of like document classification but again I don't have a lot of experience with it myself so oh I want to I want to give a shout out to Krishna Kapoor Roo who just joined as a member um joining as a members awesome joining my patreon is awesome as well those are great you get early access I've got I'm actually doing two p value videos today like as soon as this this live stream is over I'm diving into recording and and I'm gonna post later today two videos on p-values on all the members of the channel or patreon members they're all going to get early access to that so they're gonna get those videos today so that's pretty exciting let's see what else we got over here someone wants to know about deciding to use naive method from several other machining or learning algorithms how do you decide that naive bayes's is a good method when you've got a small amount of data just try a bunch and when you've got a lot of data then your your choices are limited and that makes makes your life a lot easier it also kind of depends on sort of what your modeling naivebayes typically from the examples i've seen our counts or basic distributions I haven't seen it used for image recognition for example or sort of like dealing with sort of I guess complicated data input it's usually just accounts or pretty straightforward distributions of say like height or something like thing guys there's some crazy stuff going on on Chad's Oh someone asked about how correlation affects probability and so say like we've got two things like the probability of work seeing the word dear and the probability of seeing the word friend obviously you know I don't know maybe the work maybe those two words are correlate maybe we often CD dear friend frequently and so so if we if we frequently see them together then if we then I face see the word dear then there's a good chance that friends gonna be there it's not just sort of an independent occurrence and that lack of Independence I mean there's a means there's correlation like I said before but it means that the that the that just multiplying together when when we multiply two things together we can only do that when independent I guess I guess I could do a quick stat quest on that next live stream this is one of those things where you kind of have to see the math and if if I can't think and talk at the same time I can't think in like right math out of the same time that would be a total train wreck but maybe that's something we can talk about in the next live stream I can do a little preparation that's the way I like to do it I like to prepare anyways we've we've hit our 30-minute live stream I know I started a little bit late so I'm gonna let it run a little bit late um but I think we're gonna call it an end of today's live stream and I thank you guys for coming think oh I wanted to say check it out I'm wearing this the stat quest shirt I want to thank everyone who's been supporting me everyone who's getting t-shirts everyone who's becoming members everyone who's doing super Chad donating that means a lot to me now that I'm doing this full-time it means a lot to me and it just reminds me that I do have one one little war story that I want to give you guys so a lot of you guys might know that a little bit over a month ago I started doing stat quest full-time and that was very exciting but very scary because I used to have a full-time job and I had a steady income and I was able to pay all my bills and save a little bit of money for a rainy day and all that kind of stuff you're supposed to do but then I did I started to do in that quest full-time and um and my revenue kind of went down and and that was expected but it was also kind of a little bit of a panic and I got a lot of offers to do some consulting work and I thought oh if I do some consulting work I can make up for the for the income thing and I'll be doing stat quest but I'll be making enough money and I'll be saving saving enough and paying all my bills gonna be great what I did and this was a huge mistake and I learned I'm learning from this so I I took on so much consulting that didn't have any time to do stat quest and I was like wait a minute I quit my job so that I could do stat quest all the time and now that I quit my job all I'm doing is consulting um so what I've been trying to do is like streamline things and kind of thin out that consulting and do just enough that I can I can pay all my bills but then still spend most of my time to in stack quest that is the goal and I'm I'm shifting back I'm shifting back from that crazy consulting back to the stack quest and I'm and I'm so much happier because yeah the whole reason to do in this the way I'm doing it is so I can spend more time doing stack quest more time work on these live streams getting them going so that's just sort of like a state of state of Josh star Moran's Tech quest update and that's how we're gonna end this one today thank you thank you thank you very much for joining me today and we're gonna give that a bonus BAM and say the end thank you so much alright until next time quest on
8ECts3BErqk,2020-03-03T02:40:25.000000,Live 2020-03-02!!! Virus Models and p-hacking,a quest livestream is cool BAM hey everybody I'm Josh Dharma and welcome to a stat quest a live stream I hope everyone's doing okay I'm doing fine and check this out I'm wearing my static quest t-shirt double boom I love it um anyways let's just get started there's not a whole lot of introductory announcements we need to do today so we're just gonna get to work first the first thing I want to do is I want to give a special thanks to the people that use super chat during the last livestream I want to thank you for supporting stack quest in theory I'm supposed to when people do a super chat I'm supposed to shout out or give them a shout-out and I'd love to do that but I'll be honest I've got a lot of things going on the screen including this actual livestream stat quest thing and so I sometimes I miss those things and and apparently I missed him last time so I wanted to say thanks to the people who did the super chat last time about two weeks too late but better late than never that's what I always say when I do something late anyways so let's move on enough of that so we've got question number one this was from Chi yang he's super chat last week and he asked what is the best model for predicting the new corona virus epidemic growth and I think that's a great question very timely and I'm gonna be totally honest with you guys oh I didn't know I did not know the answer off the top of my head so I had to do some research and it appears that the most commonly used models for infection spread are based on differential equations and these models these differential equation models they take all kinds of into account including social networks and we're talking real world social networks and disease transmission characteristics like how contagious it is how it spread stuff like that so these are they're pretty elaborate equations and believe it or not differential equations are fun they and they and they're actually pretty intuitive and get this outside of the the textbook examples they make you do in class pretty much computers do all of the work for you I did when I was in grad school I took some differential equations classes and I thought they were just the most fun I'll be honest I don't do a lot of difference of equation modeling now but I just because now I do statistics and machine learning and those aren't as popular in those areas but I remember at the time just thinking wow these are super fun so if you've not familiar with differential equations don't be afraid and actually you get excited they might be super cool that said however since stat quest mostly focuses on statistics and machine learning I probably won't cover differential equations anytime soon unless a ton of people requested but I've got like I said I've got neural networks coming up I've got a time series I got all kinds of crazy stuff we're gonna cover so it might be a while if we do differential equations maybe we'll do them maybe now we'll just see we'll see what people really are asking for anyways for more details if you want to learn more about the differential equations involved in mathematical models for epidemics this is a article that I was reading that I thought was pretty fascinating it gets kind of technical pretty quick but it's kind of interesting to skim to be honest the introduction the the what do they call that the abstract and maybe the first couple of pages of the results were pretty readable and will get you you know give you a general idea of what the deal is okay so we're just gonna give that bam bam all right you guys ready for set question number two I am here's question number two Josh star Murr he wrote in and he asked what is P hacking yeah in the last livestream we imagined giving four groups of DIF of difference okay in the last a live stream we imagine giving four groups of people four different diets I don't know if you guys remember that we were talking about ANOVA yeah anyways and we measured how much weight each person gained after four weeks on the diet and these were the results and then we mentioned that the first thing we want to test when we do this sort of thing is whether or not any of the diets made a difference and I know this might seem strange because just looking at the data it's pretty obvious that diet C made a pretty big difference relative to diet a but if we just tested the things that looked different we'd be doing something called P hacking and that's a big no-no so today we're gonna follow up on what we did in the last live stream and we're gonna talk about why we can't just test groups that look different and why we have to plan out the tests in advance and why when we just look at things that look different we're P hacking and we're doing a no-no okay imagine we measured how long it took a bunch of people to recover from an illness the red area under the curve indicates the probability that a person will recover from the illness in a range of possible values for example 2.5% of the area under the curve is less than five days indicating that there is a 2.5 percent probability that someone will recover in less than five days and 2.5% of the area under the curve is greater than 15 days indicating that there is a 2.5 percent probability that it will take longer than 15 days to recover from the illness lastly 95 percent of the area under the curve is between 5 and 15 days indicating that there is a 95% probability that someone will recover between 5 and 15 days in other words if we randomly ask someone how long it took them to recover from the illness there's a 95% chance they will say it took them between 5 and 15 days and for example if we asked three people and we're gonna represent those people by green circles how long it took them to recover from the illness there's a good chance all three would say something between 5 and 15 days and if we act and if we asked a different set of 3 people represented by pink circles there's a good chance that all three would say something between 5 and 15 days now we can plot these two groups of people on a graph with days till recovery on the y-axis and we can calculate the mean values for the two groups and if we do a test to compare the two means we get a p-value equal to zero point eight six and because zero point eight six is much greater than 0.05 we would conclude that there's not a significant difference between the two groups of people and this conclusion makes sense because both groups of people came from the exact same distribution now imagine we asked another group of three people how long it took him how long it took them to recover and we plotted their recovery time times and their mean value there's a typo there don't worry about it and the mean value on the graph then we asked another group of three people how long it took them to recover and we plotted their recovery times and their mean value on the graph and again we can do a test to compare the two means and we get a p-value that equals zero point zero zero point six three and again since zero point six three is way greater than 0.05 we would conclude that the two groups are not significantly different from each other okay now imagine we just kept taking two groups of three people from the same distribution and testing to see if they're different so we just did this a bunch of times note these two groups almost look like they could be different but the p-value equals zero point zero six which is greater than the standard threshold for significance 0.05 we're going to come back to this one later but for now we're just gonna press on okay so we just keep going okay and sooner or later we will work there's all kinds of typos in this I apologize sooner or later we are going to see something like this when we measure the two means the p-value equals zero point zero two and that tells us that there's a statistically significant difference between the two groups and thus this is a false positive one way to deal with false positives is to adjust the p-value with the false discovery rate or something like it like the bonferroni correction which by the way isn't a great idea you'd be better off using the false discovery rate just a little editorial comment from from the peanut gallery okay and we know that p-value adjustments like the false discovery rate and even a bomb for Arnie correction only work if we include the people the p-values from every test that we do so with that in mind one thing that is tempting is instead of computing p-values for each pair of data sets we just plot the data and only test these means because they look really different and only calculate one p-value cherry-picking the tests that du makes it easy to be fooled by a false positive because there's no p-value adjustment that just works on a single test so if we only did this one test we would get a small p-value and we'd say hey this is the real deal even though we know it's a false positive thus cherry-picking tests is a form of pee hacking pee hacking refers to the misuse and abuse of analysis techniques and it results in being fooled by false positives I know it seems obvious but you would be surprised how many times people make this mistake and it's and it's and it's not like people are being vicious because you remember when we started and we were just talking about that anova and we drew all the different different diets I think it's a legitimate question to say hey why are we testing to see if they're all different when I can look at that and it's obvious what we just did these examples where we took all these extra tests demonstrates why we can't do that but it's it I can it's a real temptation so so I don't know lots of people make this mistake you however now that you know the deal you're not gonna be one of those people and you may end up saving the day you may realize that hey we need to do all those tests and adjust the p-values and you'll catch those false positives and you'll be the hero and everyone will pat you on the back including me so that'd be a good thing to do okay the nice thing is is that this solution is simple all we have to do is plan out the tests that we want to do before we see the data BAM okay now I'm gonna talk about a slightly less obvious form of pee hacking you remember these two groups the p-value is 0.6 actually the p-value is 0.06 so that's another typo so the p-value is just above that threshold of significance zero 0-5 okay now in this example we know that both groups come from the same distribution but typically when we are doing experiments we don't know if they come from the same or different different or different distributions and to be honest we usually hope that the observations come from two different distributions so when we get data like this where the p-value is close to 0.05 but not less then it is very very tempting to think I wonder if the p-value will get smaller if I get more data so we add one more measurement to each group and now when we calculate the p-value we get 0.02 which is less than 0.05 so we can report a statistically significant difference hooray we got the what we wanted right we got a we got a difference no we didn't get what we wanted because what we got is called a false positive because all the measurements came from the exact same distribution okay so this is a super tempting mistake and I see people make this all the time as well they they collect some data and then they go well that wasn't statistically significant but I'm close so I'm just gonna add a few more replications and then we'll see what happens and it what happens is I actually did some simulations just using a normal distribution so I wasn't I wasn't being like super fancy or anything but I did some some simulations where I as you can see I took a normal distribution just like what we've got and at all the p-values that were between 0.05 and 0.1 so sort of like borderline p-values but not significant I took all of those and I added one measurement to each group of samples or a group of observations and I calculated the p-value and a third of them came out significant so when we when we have a p-value that's close to being significant and we add data a third of the time we could be reporting a false positive so that's a big error rate and we need to avoid that okay that means we need to do something called a power analysis before we even do the first experiment and the word power in power analysis is a specific technical term so we're going to talk about what this means if we have two digit if we have two distributions that hardly overlap each other and we collect a small set of observations from one distribution and a small set of observations from the other distribution and when we plot these points on a graph and compare their means we get a p-value less than 0.05 and since the small p-value causes us to correctly reject the hypothesis that both sets of data are from the same distribution this is called a true positive and this is what we want power is the probability we will correctly reject the null hypothesis in this example where the two distributions have a lot of separation we have a relatively large amount of power in other words there's a relatively large probability we will reject the hypothesis that both groups of data come from the same distribution and if we repeat this experiment most of the time we would reject the hypothesis that the samples come from the same distribution but every once in a while we get data that look like this there would just be the data would be overlap and when this happens we'll get a large p-value then fail to reject the hypothesis that both groups come from the same distribution and because this is rare we have relatively large power because remember power is the probability that we will correctly reject the null hypothesis that both sets of data come from the same distribution and because this is rare this the the the making a mistake is rare we have relatively large power in contrast when the two distributions overlap a lot it becomes a lot harder to reject the idea that both data come from the same distribution so in this example the p-value equals zero point three four so we will incorrectly fail to reject I the idea that both sets come from the same distribution if we were to repeat that experiments most of the time we would fail to reject the hypothesis that both of the samples come from the same distribution but every once in a while we get something like this so where the groups are of data are way far away from each other so there's a small probability that we will correctly reject the idea that both datasets come from the same distribution and that means we have I got ahead of myself and that means we have a small amount of power in this case that said we can always increase power by increasing the sample size and that's why why why we're interested in a power analysis because power analysis will tell us what the sample size needs to be to give us a high probability that we will correctly reject the null hypothesis that there's no difference between the two groups okay before we were just using three measurements per distribution however if we double that to six measurements per distribution the p-value is now 0.01 and that means we're back to correctly rejecting the idea that both sets come from the same distribution okay terminology alert an 80% power analysis tells us what sample size we need in order to correctly reject the hypothesis that both samples came from the same distribution 80% of the time so the next time you do an experiment you can get a p-value that's just a little bit larger than 0.05 don't just do another replicate and cross your fingers instead use the means and standard deviations of your data that you have so far to do a power analysis to determine the proper sample size double bam alright so we've made it oh yeah we didn't quite made it to the end we've got a little extra time so what I'm gonna do is I'm gonna I'm gonna look at some of the questions over here over in the comments and see if we got anything good that I should address someone asked when we will be starting the session that we started at 9 that's my time looks like people are recommending my Channel that's great that's great someone asked about Bayesian statistics they said hi there Josh how about Bayesian statistics and that is a good question how about Bayesian statistics I've wanted to do a Bayesian statistics stat quest for a long long time why have I kept putting it off the concepts of Bayesian statistics are actually pretty straightforward it's the doing of the Bayesian statistics that gets messy really fast and every time I kind of like start but you know boning up and doing research on Bayesian statistics I kind of get stuck in a quagmire like getting your foot stuck in mud reading about all of the little technical nuances that you have to be aware of when you actually do Bayesian statistics and I know there's a lot of people that swear by them and use it all the time and for them it's pretty straightforward but for me I've always kind of gotten stuck on these details but that being said I promise I will do Bayesian statistics sometime but like I said it probably won't be super soon this might be it I mean I know it's it's barely March 2020 but this might be a 2021 project for me because the easiest assistance is like a mountain I've always wanted to climb to the top of and say I have conquered and I'm sure you guys are dying for that too so it's high on my to-do list and it's something I've taken stabs at before but but I just have to keep trying is what it boils down to I can't give up I gotta keep going let's see if we got anything else over here wow we got something from Argentina's tune in that's cool Oh somebody asked how to pass a machine learning technical interview successfully and I'm gonna be honest I don't really have any tips but I have some ideas and what I need and this would be a great thing from the community the stat quest worldwide community would be if people could post if if people have taken a machine learning technical interview if you could post some of the questions that you were asked I think it'd be fun to kind of like do a stack quest where we take a stab at these we you know we could do a live stream and we'll do three of these interview questions and just go through them and and try to answer them to the best of our ability I'm gonna be honest the answer I give may not be the answer that the that the interviewer wants to hear I don't know if you watched the last stack quest but excuse me the last stack quest live I gave an example of sort of an interview that went bust because I just was determined not to tell the interview I knew what they wanted but I was determined not to tell them that because I thought the question was stupid so I may not be the best person but whatever let's let's tackle these questions and I'll tell you what I think if you wanted to work for me this is the answer I'd want to hear so let's do that sometime so yeah so if you've had one of these interviews posting these questions you can put you can email them you can go to my website stack west or RG and you can go through the contact link and you can email them you can contact me on LinkedIn or Twitter or you know in the comments of my videos so so I think that'd be super fun and so let's just plan on doing that Oh someone asked random effects versus fixed effects yes that is also super high on the to-do list that's something that's been it's like an itch that I've had for years that's one of the problems and maybe that but also one of the great things about stat quest is there's there's a lot of material out there and it's all pretty fascinating and I my dream my fantasy is one day I will do it all I am I long I've got this like long term plan which may just be a fantasy but but you know how like Khan Academy it started out with just Sal Khan and but now it's like this big organization with people doing all kinds of stuff I had this fantasy that one day maybe stat quest would be like that and and that would mean we might actually be able to cover a lot of these concepts but boy when it's just me doing it all by myself I can only do so much in a day and I believe me I'm working as hard as I can to crank these things out but maybe one day there'll be there'll be like a team of stat Questers out there cranking out material and we can just cover all kinds of cool topics so that's a that's my maybe my ten year dream what else we got greetings from Chile Wow we've got all kinds of love from South America this is awesome yeah a collab with Khan Academy laughing man just posted that that's a great idea um I think it'd be fantastic I wonder if I wonder how to do any of you guys know Sal I don't know how I'd even get in touch with them although I will say this I don't want to knock Sal Sal has a very specific style where he you know where they they write everything on the on the on the screen I don't know if you guys have watched Khan Academy videos I have and I love them um but he has a very specific style and you'll notice that my style is different partially because we have different ideas on on what the best approaches for communicating ideas so if I collaborated with those guys would they make me write everything down my handwriting is so bad that's one of the reasons why I have a fundamentally different approach is my handwriting is terrible and I'm also left-handed and I don't know if that has anything to do with it but yeah well I don't know who knows maybe it'd be a good idea maybe it wouldn't we'd have to we'd we'd have to try it and then we'd find out for sure and I'm all about trying new things so so that's that Oh what are my tips this is gonna be the last one and then we'll call it an in for this live stream this will be the last question for tonight and then we'll start back two weeks from today but we'll do it my time in the morning anyway so the question is what's your tips to be able to explain complex logic so clearly and a lot of people say things like well if you can explain it to your grandma then you know you're doing a good job I I'm gonna be honest I've never tried to explain any of this to my grandma not not even once a month both of them actually have passed away a long time ago so so how do I do it myself what I do is well is this this is my secret statistics and machine learning is actually very hard for me and I know that sounds weird but it's not easy and all I have to do is teach myself and if I teach myself I usually have to cover all I can't skip big steps I have to do it one little step at a time because otherwise I don't understand it um and so that's my big trick is my only way that I understand it is I just explain it to myself and um and if I can explain it to myself I can usually explain it to other people I also though that being said although I did never explain this to my grandmother I used to explain it to biologists and biologists aren't math people often some of them are some of her great math people but a lot of biologists are not math people and it's one of the reasons why they went into biology to begin with um and I used to do sort of a Friday morning little seminar about once a month for a bunch of biologists and they were great because they would sit in the back of the room they'd all think because they were afraid of the math so they'd sit as far away as possible and they would just look stunned whenever whenever they couldn't follow something and so I would just look at their faces and I would know that I was moving too fast because I could just tell they'd to get this this look of like I don't get it but I'm not gonna say anything and that whenever they got that I don't get it but I'm not gonna say anything I would make a mental note that whatever I was talking about wasn't making sense to anybody alright so that's it for the stat quest this week Stan Quest thank you guys very much for tuning in I'm really looking forward to the next stat quest already and I hope you guys all have a wonderful night or a wonderful day or wherever you are you're just having a wonderful time alright until next time quest on see if I can in this you
oRrKeUCEbq8,2020-03-02T05:00:06.000000,XGBoost Part 4 (of 4): Crazy Cool Optimizations,I want to do things fast want to do things faster uh XG booze got crazy optimizations they're gonna blow your mind you better watch out cuz they're so crazy stack west hello I'm Josh Starman welcome to stack West today we're going to talk about XG boost part 4 optimizations note this stack quest assumes that you are already familiar with how XG Boost creates trees for classification and regression if not check out the quests the links are in the description below this stack west also assumes that you are familiar with quantiles and percentiles if not check out the quest-x G boost is a big algorithm with a lot of parts and since gradient boost and regularization were covered in other stat quest videos so far this series has focused entirely on XG boosts unique regression trees so now it's time to talk about the other parts these parts are what make XG boost relatively efficient with relatively large training data sets in other words the first three parts give us a conceptual idea of how XG boost is fit to training data and how it makes predictions in the last six parts describe optimizations for large data sets so let's start by talking about the approximate greedy algorithm boost part 1xg boost trees for regression we had a super simple training data set and used different drug dosages to predict drug effectiveness the first thing we did was make an initial prediction which could be anything like the mean drug effectiveness but by default is 0.5 then we calculated the residuals fit a tree to the residuals we did this by calculating similarity scores and the gain for each possible threshold and the threshold with the largest gain is the 1xg boost uses note the decision to use the threshold that gives the largest gain is made without worrying about how the leaves will be split later and that means XG boost uses a greedy algorithm to build trees in other words since XG boost uses a greedy algorithm it makes a decision without looking ahead to see if it is the absolute best choice in the long term in contrast if XG boost did not use a greedy algorithm it would postpone making a final decision about this threshold until after trying different thresholds in the leaves to see how things played out in the long run and this same process would be repeated for every single possible threshold for the route in other words by using a greedy algorithm XG boost can build a tree relatively quickly said when we have a lot of measurements then the greedy algorithm becomes slow because it still has to look at every possible threshold and if we had a more interesting training data set that used dosage mass favorite number and a bunch of other stuff to predict drug effectiveness then checking every single threshold in every single variable would take forever wah wah this is where the approximate greedy algorithm comes in going back to our example with a lot of observations instead of testing every single threshold we could divide the data into quantiles and only use the quantiles as candidate thresholds to split the observations for example instead of using the smallest two dosages to define the first threshold the approximate greedy algorithm uses the first quantile to define the first threshold in the second quantile is the second threshold that we will consider cetera etc etc note if we only used one quantile and split the observations in half then since there are no other options that quantile would be the threshold make finding the best threshold very fast since we would not have to calculate gain or similarity to make the decision but since both sides of the threshold represent a lot of people who have positive drug effectiveness values and a negative drug effectiveness values then this threshold would not do a good job predicting drug effectiveness in contrast if we had to quantiles then our predictions would improve because we would do a better job separating observations with positive values for drug effectiveness from observations with negative values for drug effectiveness so for this data to quantiles are better than one if we had five quantiles then our predictions would be more accurate since each threshold represents a smaller cluster of observations however the more quantiles we have the more thresholds we will have to test and that means it will take longer to build the tree she boozed the approximate greedy algorithm means that instead of testing all possible thresholds we only test the quantiles and by default the approximate greedy algorithm uses about 33 quantiles now the question is why do we say about 33 quantiles instead of exactly 33 quantiles to answer this question we need to talk about parallel learning and the weighted quantile sketch when you have tons and tons of data so much data that you can't fit it all into a computer's memory at one time then things that seem simple like sorting a list of numbers and finding quantiles become really slow to get around this problem a class of algorithms called sketches can quickly create approximate solutions unfortunately explaining the details of sketch algorithms is out of the scope of this stat quest but we can discuss the general idea of how XG boost uses them for this example imagine we are just using a ton of dosages to predict drug effectiveness now let's move this data set to the top of the screen and imagine splitting it into small pieces and putting the pieces on different computers on a network the quantile sketch algorithm combines the values from each computer to make an approximate histogram then the approximate histogram is used to calculate approximate quantiles and the approximate greedy algorithm uses approximate quantiles BAM no not yet far we have described the quantile sketch algorithm but XG boost uses a weighted quantile sketch so that means that these quantiles are not normal everyday quantiles instead they are weighted quantiles hey what's a weighted quantile usually quantiles are set up so that the same number of observations are in each one in other words if ten observations were in this quantile then there would be ten observations in this quantile and ten observations in this quantile etc etc contrast with weighted quantiles each observation has a corresponding weight and the sum of the weights are the same in each quantile for example if the sum of the weights in this quantile was 10 then the sum of the weights in this quantile would also be 10 in the sum of the weights in this quantile would be 10 etc etc etc the weights are derived from the cover metric that we discussed in parts two and three in this series specifically the weight for each observation is the second derivative of the loss function what we are referring to as the hessian that means per regression the weights are all equal to one and that means the weighted quantile czar just like the normal quantile and contain an equal number of observations in contrast for classification the weights are the previous probability times one minus the previous probability so let's see how the equation for weights affects the quantiles in classification and we'll do that with this simple data set in this classification example we are using drug dosage predict the probability that the drug is effective the red dots are dosages in the training data set that were not effective and the green dots are dosages in the training data set that were effective these red and green X's correspond to the previously predicted probabilities that these dosages are effective and they start out at the initial prediction 0.5 after running the data down the first tree most of the predictions improve and as we add more trees most of the predictions get better and better okay cool but what does this have to do with calculating the weights for the weighted quantile sketch when using XG boost for classification the weights for the weighted quantile sketch are calculated from the previously predicted probabilities so let's calculate the weights for each observation note I'm just showing one example of calculating weights in practice weights are calculated after building each tree these predicted probabilities are very close to zero indicating a high amount of confidence in classifying these dosages as ineffective since the previously predicted probability for these two points is 0.1 the weight is 0.1 times 1 minus 0.1 which equals 0.09 so let's put 0.09 here so we will remember it these predicted probabilities are very close to 1 indicating we have high confidence in classifying these dosages as effective since the previously predicted probability for these two points is 0.9 the weight is 0.9 times 1 minus 0.9 which equals 0.09 so let's put 0.09 here so we will remember it these two predicted probabilities are very close to 0.5 and that means that we are not very confident in how to classify these observations since the previously predicted probability for this point is 0.6 the weight is 0.24 so let's put 0.2 for here lastly since the previously predicted probability for this point is 0.4 the weight is 0.24 so let's put 0.24 here now we see that when the previously predicted probability is close to 0.5 meaning we don't have much confidence in the classification the weights are relatively large in contrast when the previously predicted probability is very close to zero or one meaning we have a lot of confidence in the classification the weights are relatively small now if we split this data into equal quantiles we would put a quantile here and here but remember we are treating each quantile as a unit and lumping the last two observations together as a unit means they will end up in the same leaf together in the tree and since the positive residual will cancel out the negative residual it will be very difficult to improve the predicted probabilities so instead of using equal quantiles XG boost tries to make quantiles that have a similar sum of weights in order to divide the observations into quantiles where the some of the weights are similar we divide them into these quantiles the sum of the weights in the first quantile is 0.18 some of the weights in the second quantile is 0.18 the third quantile only has one observation in its weight is 0.24 in the last quantile only has a single observation and its weight is 0.24 by dividing the observations into quantiles where the sum of the weights are similar we split the two observations with low confidence predictions in two separate bins in other words the advantage of using the weighted quantile sketch is that we get smaller quantiles when we need them BAM so when we have a huge training data set XG boosts uses an approximate greedy algorithm and that means using parallel learning to split up the data sets so that multiple computers can work on it at the same time and a weighted quantile sketch merges the data into an approximate histogram and the histogram is divided into weighted quantile that put observations with low confidence predictions into quantiles with fewer observations note before we move on I want to mention that XG boost only uses the approximate greedy algorithm parallel learning and the weighted quantile sketch when the training data set is huge when the training data sets are small like the ones in my examples XG boost just uses a normal everyday greedy algorithm BAM now let's talk about sparsity aware split finding so let's return to the example where we were using dosage to predict drug effectiveness only this time we have a few missing values even though we have missing values we can calculate the residuals the differences between the observed drug effectiveness and the predicted drug effectiveness using the initial prediction 0.5 and just like we normally do when we build XG boost trees we can put all of the residuals into a single leaf now we need to determine if splitting the residuals into two leaves will do a better job clustering them so just like we always do for continuous data we need to sort the dosages from low to high unfortunately it's unclear how to sort the dosages with missing values so what we'll do is we'll split the data into two tables one table will contain all of the observations with dosage values and another table will contain all of the observations without dosage values now focusing on the table that has dosage values for every observation we sort rose by dosage from low to high now we test the average of the first two dosages 7.5 as a candidate threshold note if this was a large data set we would be using the first quantile here in this case we test the threshold by putting the residual for the one observation that has a dosage less than seven point five in the leaf on the left and put the remaining residuals which all have dosages greater than seven point five into the leaf on the right now that we have all of the residuals with known dosages in the tree we calculate two separate gain values the first gain value which we will call gain left is calculated by putting all of the residuals with missing dosage values into the leaf on the left ii gain value which we will call gain right is calculated by putting all of the residuals with missing dosage values into the leaf on the right now we do the same thing using the average of the next two dosages 15.5 as a candidate threshold we put the residuals with dosages less than 15.5 in the leaf on the left and the residuals with dosages greater than or equal to fifteen point five in the leaf on the right and then we put all of the residuals with missing dosages into the leaf on the left and calculate gain left then we put all of the residuals with missing dosage values into the leaf on the right and calculate gain right lastly we do the same thing using the average of the last two dosages 23 as a candidate threshold we calculate gain left and we calculate gain right we choose the threshold that gave us the largest value for gained overall in this case that meant picking gain left when the threshold was dosage less than fifteen point five note this path going to the left leaf when dosage is less than fifteen point five will be the default path for all future observations that are missing dosage values for example if this was the XG boost model and we got a new observation without a value for dosage but we still needed to predict drug effectiveness then we would assume that this observation goes to the leaf on the left thus sparsity aware split finding tells us how to build trees with missing data and how to deal with new observations when there is missing data double bam now we need to talk about cache aware access this is where XG boo starts to get super nitty-gritty the basic idea is that inside each computer we have a CPU a central processing unit and that CPU has a small amount of cache memory the CPU can use this memory faster than any other memory in the computer the CPU is also attached to a large amount of main memory while the main memory is larger than the cache it takes longer to use lastly the CPU is also attached to the hard drive the hard drive can store the most stuff but is the slowest of all memory options if you want your program to run really fast the goal is to maximize what you can do with the cache memory so XG boost puts the gradients and Hessians in the cache so that it can rapidly calculate similarity scores and output values BAM that was pretty simple lastly we need to talk about blocks four out of core computation back to the super simple computer schematic when the data set is too large for the cache and main memory then at least some of it must be stored on the hard drive because reading and writing data to the hard drive is super slow XG boost tries to minimize these actions by compressing the data even though the CPU must spend some time decompressing the data that comes from the hard drive it can do this faster than the hard drive can read the data in other words by spending a little bit of CPU time uncompressing the data we can avoid spending a lot of time accessing the hard drive also when there is more than one hard drive available for storage XG Boost uses a database technique called sharding to speed up disk access for example if this is the data set and it is very large then XG boosts splits the data so that each drive gets a unique set of records then when the CPU needs data both drives can be reading data at the same time BAM thus Kasia where access and blocks four out of core computation or optimizations that take the computer hardware into account lastly I need to mention that XG boost can also speed things up by allowing you to build each tree with only a random subset of the data and XG boost can speed up building trees by only looking at a random subset of features when deciding how to split the data BAM summary --xg boost is fast for a lot of reasons some of these reasons like Kasia where access are not even vaguely related to statistics and that makes XG boost something more than just an applied statistical technique and that means machine learning is more than just applied statistics triple bam hooray we've made it to the end of another exciting stat quest if you like this stack quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
iTxzRVLoTQ0,2020-02-17T14:39:56.000000,Live 2020-02-17!!! Imbalanced Data and Post-Hoc Tests,SiC quest livestream hello and welcome to stack quest a quest live I'm really hope HAP excuse me alright I'm really happy you guys are here today we're gonna be talking about primarily we're gonna be talking about imbalanced data and then we're going to talk about post hoc tests for ANOVA and I'm really excited so I'm talking really fast so I'm gonna take a few deep breaths and I'm gonna calm down and then we're gonna talk about imbalance data alright so question number one Alexander Roman chenko he asked what do we do with unbalanced data for example if we're trying to predict if people love troll 2 and 90% of the people in the training data set love troll 2 then what do we do great question all right so we're gonna start by talking about the problem ok so imagine this was our training data set we're using likes romantic comedies rom-coms an astrological sign to predict whether or not someone loves the movie troll 2 the problem is that 90% of the people in the training data set love troll 2 and this means that regardless of the data if we simply classify every single person as someone who loves troll 2 we will be correct in 90% of the time and being correct 90% of the time sounds pretty good right well deciding if someone loves troll 2 is kind of silly let's try a more serious example where the the stakes of being wrong in the minority case are higher so let's talk about that okay what if instead we are trying to decide if someone has a rare but contagious disease now we're using age and exposure to predict whether or not someone has a rare but contagious disease if we simply predicted that no one had the rare but contagious disease we would be correct almost all of the time but we would miss classify every single person that had the rare but contagious disease and that would be terrible because that person with the rare and contagious disease will be out there spreading the disease and that'd be no good uh-huh you don't want that to happen even if we used a more sophisticated classification method like logistic regression or X G boost most are expecting most of the machine learning methods are expecting a relatively equal proportion of yeses and noes in the training data set so regardless of what method you use make sure you look at the output of a confusion matrix because here is where all the secrets lie in this example where everyone is classified as being diseased free so we're correct 90% of the time note since 90% of the observations were correctly classified the accuracy of the method is 90% so we're proud of ourselves we got high accuracy however we can see where we went wrong with about every person who had the disease so we misclassified every single person who had the disease in other words the precision and precision equals the number of true positives divided by the total number predicted to be true so the first row of the row in our confusing matrix so we put zero in the numerator and we put zero plus zero into the denominator and we get zeros so our precision is zero so even though accuracy is pretty high ninety percent precision is terrible zero percent and that tells us that there's something fishy going on so there's a one way we can diagnose that we've got some imbalance in our data thus the confusion makers in general makes it easy to spot the weakness in our method okay now that we have spotted the problem what can we do about it there's a couple of things one thing we can do about it is change the training data set so we have more equal proportion of people with and without the rare but contagious data set or excuse me this is why I typically edit my stat quest videos but when it's doing it live I'll say all kinds of crazy things okay so what we want is an equal proportion of people with and without the rare but contagious disease okay the best thing obviously would be to collect more data but since this is a rare but contagious disease that may not be an option right if not many people have the disease to begin with you're gonna be hard-pressed finding more people with that disease okay so instead if we already have a lot of data we can do something called under sampling or down scaling and all that means is that when we understand Pilar downscale we randomly remove people that are over-represented so in this case that means we random that means randomly removing people without the disease and by removing people that do not have the disease we put more emphasis on correctly classifying the people that have it so yeah but so by removing the people with without the disease we end up with a closer to like a 50/50 representation and that's important okay alternatively we can use over sampling or up scaling over sampling or up scaling duplicates individuals that were under representative so in this case what we're doing is we're adding additional copies of people with disease we're just taking that same entry and we're just duplicating it and by doing that we put more emphasis on correctly classifying the people that have it because now there's a bigger penalty our accuracy is gonna go down note we can combine under sampling with over sampling to get a more balanced data set I think a lot of people do that where they just combine both techniques to get a data that's more closer to 5050 representation of both both types of data both classes I guess is the technical term okay also note there are other methods that try to under and over sample in intelligent ways rather than just randomly picking observations two commonly used methods are called rows and smote and I'm gonna at least for now unless there's a huge demand for it I'm gonna leave those as a reader as excuse me as an exercise for the reader we can clearly talk about these in future stat quests or future live streams just put the request in especially for a live stream because I think it's a pretty short easy topic and we could probably just knock it out in a few minutes so just let me know if those are something you'd be interested in and we'll talk about them in a live stream or a special static quest okay that said regardless of how we under an oversample if we are using cross-validation make sure you apply the you apply it under sampling or or over sampling you apply those things prior to this Kali [Laughter] [Applause] I need to edit this one bad okay let's start over go back one slide okay that said regardless have we under or over sample if you're going to use cross-validation make sure you apply it it being cross-validation prior to using these sampling methods otherwise you will have data linkage and we discuss data language linkage in a previous live stream and and so you can go back and look at that I've got them all sort of indexed on the stack quest website I don't know if you're familiar with it but there's a there's a stack quest website stat questo RG and on it and I have a video index and at the bottom of that video index I have the live streams and I have the each topic indexed with the correct time so you click on the link to the topic and go straight to the live stream at that time so if you want to learn more about data linkage because you missed that live stream it's easy to find but the general idea is when you're doing cross-validation you want each one of your train data sets to be independent of the others and and that means splitting the data up first and then doing sort of modifications to the data on a per sort of sub samples per fold basis rather than transforming the data all at once and then splitting it up into smaller pieces because that initial transformation is going to then sort of corrupt all of our little sub splits because this guy is going to this guy over here is going to know how this guy transformed the data and that's no good so we want to avoid data linkage so if you're gonna use sub sampling or over sampling and cross validation use cross validation first or split up the data into sub groups and then on each sub group apply the over sampling or subsampling okay another thing you can do is assign each observation a weight and that gives observations that have the rare and contagious disease larger weights so that's how we would solve this problem and for example if you used a random forest these weights would be used for finding splits and for voting on the classification in other words this if this observation is in this leaf on the left side of this little tiny tree then it's individual vote will count more towards yes the patient has the rare contagious disease then an individual vote against it and then likewise at that observation also has more weight when deciding how to split the data into the two leaves so that leads to the next thing we should do which is try different methods when you have imbalanced data obviously you can do over sampling and under sampling and you can add weights to your data but different methods may work better with your you know even after you've done that thing you may have some imbalance and different methods may work better so just try different methods this is one thing one reason it's good to be familiar with a bunch of different methods and sort of what they can do and how to use them because yeah just throw them at it and see what the confusion matrix looks like for each method and pick the one that's gonna work best in this case XG boost worked the best so that's what we'll use hooray so in summary when we have imbalanced data one use a confusion matrix to see if rare cases are correctly classified and we can also calculate precision to make sure that's that's correct and if the samples are oops if the samples are bad use up scaling or down sampling oops there we go that looks better if this if the results are bad use up sampling or down sampling of or weights to put more emphasis on the rare cases and then the lastly last thing we want to talk about is we try different methods Oh okay so we get BAM alright so we got the BAM question number two Christina you she asked after getting a significant p-value for ANOVA what kinds of post hoc tests are generally used and can I use a t-test so we got two sort of basic questions here we're gonna answer both of those but first we're gonna give a little bit of background as to what the problem is okay so imagine we have four groups of people on four different diets and we measure how much weight each person gained after four weeks on the diet and these were the results so okay the first thing we want to do or test when we do this sort of thing is whether or not any of the diets made a difference and I'll be honest this may seem strange I remember when I was first learning about ANOVA this seemed very strange to me because if you just look at the data it's pretty obvious that diet C made a pretty big difference relative to diet a why would I need to do a test that looks for differences overall is there one different why would I need to do that and the reason is when you do these type of experiments you always plan the statistical test before you see the data and that's very important if you don't and you what you do is you do a ton of statistical or you don't do the test what you do is you do a you collect a ton of different samples and things like that from a billion different diets or a billion different things and then you plot it and you and you just look and you just test the things that look different you're doing something called P hacking and that is a big no-no you don't want a P heck because all that's doing is asking for false positives and results the can't be replicated by other people and just garbage results and garbage garbage stuff and you don't want garbage stuff coming from you you want the good stuff coming from you we're gonna talk about pee hacking more in the next livestream I've talked about it before in a stat quest but I really want to just get back to it and kind of re rehash that topic because I only talked about one aspect of it and there's a few other things that we need to talk about so we're gonna bring it up in the next live stream but for now let's just assume that before we collected the data we did the right thing and we decided to test if there is any diets if any of the diets made a difference in other words we're gonna do an ANOVA test ANOVA is like a t-test but it compares the mean simultaneously and if you want more information about ANOVA check out the stat quest anyways if the anova gives us a small p-value then we know at least one of the means is different but we don't know which one so if we want to find out which diet is different from the others we have to do what's called post hoc testing again assuming we haven't seen the data if we want to find out if diet a is different from any other diet we have to test the difference between a and B the difference between a and C and the difference between a and C likewise if we want to know if diet B is different from the others then we need to test the difference between diets a and B the difference between B and C and the difference between B and D likewise we have to do the same thing for diet C and D all in all we have to make six different comparisons in this situation and that kind of covers all the bases we get if we do a b a c a d bc b d and c d covers every single combination of tests that we would need to do this is called all pairwise tests and one problem is that when we do more statistical tests that increases the chances of getting a false positive we'll talk more about this when we discuss P hacking in in the beginning of March I think so there's a pop know I'll be here it'll be great we're gonna do the will do the stat quest just like we're we've been doing for the past couple of months the first and third Monday's in March and so we'll talk about P hacking in the first Thursday excuse me first Monday in March I'm kind of all over the place yeah okay uh a confession to make I had family visiting over the weekend and so I've kind of wiped out that's I think that's why I'm all over the place all right okay enough about that okay if we back to the stat quest what I'm gonna say is when we do a lot of tests it's problematic because it increases the likelihood of getting a false positive so in stab quest or it's not stat quest in stats land we call this the multiple testing problem the good news is that there are ways to deal with this problem the multiple testing problem the standard practice for doing pairwise tests after an ANOVA is to use something called the two key Kramer test and I think it's pronounced Tookie I think so yeah Tookie Kramer test and that's the standard practice it's been the standard practice for ever since the 1950s and so statisticians that were trained in the 50s learned about it in the 60s they learned about it in the 70s they learned about it in the eighties they learned about and in the 90s they learned about it but in the late 90s sort of early thousands something called FDR or false discovery rate came on the scene and false discovery rate is a game-changer I have a stat quest on false discovery rate I'd recommend watching it if you don't know about it I I talk about it in terms of an experiment called RNA seek but it basically applies to any time you have lots and lots of tests it's a very clever way of adjusting p-values I don't know I don't know if you guys are familiar with the bone furrow knee technique that's that was invented like a billion years ago and it's terrible FDR may adjust p-values in a really cool way and it does a great job so it's a relatively modern technique compared to the Tookie Kramer test okay if you don't already know about FDR check out the quest anyways most people still recommend the Tookie Kramer test that said there are lots of other options including pairwise t-tests with p-values corrected with false discovery rate FDR aka the benjamine II Hirschberg method so those are that's an option and and I think the difference between what people will recommend is whether they grew up with the Tookie Kramer test or they grew up with FDR um I know a lot of older people don't even know what FDR is and so they're like you got to use the Tookie Kramer test and when they teach doesn't matter they're gonna that's only the only thing they're gonna teach ya in contrast I grew up with false discovery rate I'm not quite that old and so so when I think about multiple testing problems I think about FDR and I think about adjusting p-values and what I like about FDR is it just a general correction for multiple tests and you can use it in all kinds of settings and it does a great job so if you want to use a t-test after you do ANOVA go ahead just be sure you adjust the p-values with FDR false discovery rate double bow all right now it's time for some viewer questions so I'm gonna go over to the chat window and see what we got over here hold on let's see we got we got some funny jokes over there which is pretty funny what else we got Oh someone asked what a percentage - up sample or down sample that's a great question and I mean I'll be honest the standard question and the standard answer to anything in machine learning or when you don't know the answer is to apply cross-validation and just see which works best and try different things however you don't always have that amount of time and I'll also give you a secret for example one thing that random forest does built-in is it and that's actually random force has a very clever way of dealing with this which is pretty impressive like I just learned about it over the weekend so what it does because it builds a bunch of trees for each tree it creates a new I don't know how familiar you guys are with random forest it's cool methods worth knowing about anyways it basically creates a bunch of decision trees and for each decision tree it creates a new bootstrapped database meaning it randomly selects observations from the original data set and it does so was what with replacement so so it so it you start with the original database data set and you randomly select things and you create a new data set only this new guy can have duplicates from the original one and it's the same size which means because it's this and because it's the same size and it contains duplicates that means it doesn't have all of the original observations because the duplicates kind of edged them out so in generally speaking the bootstrap bootstrap data set contains about 60% of the original observations over here okay so what random forest does in this situation you have to tell it to do it this way otherwise it won't do it but if you tell it to do it this way what it'll do is it'll UM this bootstrap data set will consist of all of your minority things so these are all of the people with the rare and contagious disease they go over here and say like there are 20 people over here with the rare and contagious disease well then it'll only select 20 people that don't have that one and that'll be the bootstrap data set for that tree and then it'll and it'll build that tree and then it builds another tree and then it takes all 20 people from that have the disease and then it takes another random 20 people over here and it puts them over here in the new training days it and it does that and when you do it that way because we're using relatively small training data sets on each tree the what you do is then you just make a lot of trees maybe more than you would normally do and I don't know if you've seen my random forests have do it in our video by the way I'm working on a Python version of that so that should be out sometime in March or early April but anyways what you do is you can actually track to see how well your random force is doing per tree so you start with one tree and your Anna force isn't doing great but once you get around to like a hundred trees or 200 trees or 500 trees you can see that it starts doing better and better and you can kind of see where it plateaus and once it plateaus you're kind of done and then you select that number of trees so that's one way to pick it but otherwise try to get it closer to one to one if you can and if you can't like you just don't have enough data use cross-validation to try to figure out a good ratio that's gonna work with the methods that you're using so there's no silver bullet in this situation but there's ways to make things better alright let's see what else we got up here look for another question someone asked and actually this is a really good question someone asked is shouldn't we be checking sensitivity instead of precision here I think that's a good question for a couple of reasons one is I'm gonna be totally honest with you I can never remember the difference between all of those terms I know it sounds terrible I'm a cast at Quest I'm Josh Dahmer I'm an expert in machine learning and I can never remember what any of those terms mean for more than about five seconds however I do know that when we pull up the confusion matrix sure whatever metric works works but I can just look at that confusion matrix and I can tell that things are messed up and there are different terms and different lingo for for how you can parse that confusion matrix but they're really just ways of like taking what I see and giving it a new name that's kind of fancy so so yeah maybe you should be using sensitivity may be using precision I'm gonna be honest I can't remember the names of all those things just look at the confusion matrix um and go with that that being said I do know that when you apply for jobs for some reason employers really care whether or not you know off the top year ahead the difference between precision sensitivity specificity all those little things oh so my recommendation is 30 minutes before your interview cram those little terms and jam them in your head and hold on to them for as long as that interview less and then when you're done you can say whatever I'm just gonna look at the confusion matrix so that's my off-the-record advice for when you have to do a job interview because I know I know they ask those questions and it kind of drives me crazy it it reminds me it to be honest it reminds me of a job interview I know I said I was gonna take questions but I'm gonna tell you about a job interview I had a long time ago I'm not gonna mention that company but it's a very big software company I don't know if it's the biggest it might be the biggest software company in the world but a might not I'm not naming name I had this interview and they they were like I need you to tell us how you would how you would sort this phonebook length list of names you know what's the fastest way to sort this notebook list of name or not notebook I mean it was a huge huge list of names and I said you know I probably google it it's probably on Stack Exchange and I probably just copy and paste the code and they were like oh no no no no we don't we that's not the right answer imagine imagine you don't have the internet you can't just google the answer and I was like well you know this is sorting and sorting as a standardized algorithm so I bet you someone I work with probably has memorized this thing so I'm gonna go down the hall and just knock on everyone's door and see who see if anyone knows this the solution of this a problem because because I don't remember these answers and they were like okay this is a terrible interviews Josh you're you're saying all the wrong things so here's what you need to do you need to imagine you were on a desert island and there's no internet and there's no other people and I looked at them I said then why am i sorting all these names I should be fishing building a shelter building a fire to make smoke signals so I'm rescued the last thing I'm gonna do on a desert island with no internet and no friends is sort of a phone book worth of names and anyways uh they didn't offer me a job so that's my little story and I'm sorry I kind of got distracted and told you my little story instead of answering more questions and I see that we are already at 30 minutes in the
ZVFeW798-2I,2020-02-10T05:00:08.000000,XGBoost Part 3 (of 4): Mathematical Details,XG boost math details there's a lot of them watch out staffed quest hello I'm Josh stormer and welcome to stat quest today we're gonna talk about XG boost part 3 mathematical details this stat quest assumes that you already have a general idea of how XG boost builds trees if not check out the quests the links are in the description below this stat quest also assumes that you know the details of how gradient boost works if not check out the quests lastly it assumes that you are familiar with Ridge regression if not the link is in the description below in XG boost part 1 we saw how XG boost builds XG boost trees for regression and in XG boost part 2 we saw ha XG boost builds XG boost trees for classification in both cases we build the trees using similarity scores and then calculated the output values for the leaves now we will derive the equations for the similarity scores in the output values and show you how the only difference between regression and classification is the loss function to keep the examples manageable we'll start with a simple training data set for regression and this simple training data set for classification for regression we are using drug dosage on the x-axis to predict drug effectiveness on the y-axis for classification we are using drug dosage on the x-axis to predict the probability the drug will be effective for both regression and classification we already know that XG boost starts with an initial prediction that is usually 0.5 and in both cases we can represent this prediction with a thick black line at 0.5 and the residuals the differences between the observed and predicted values show us how good the initial prediction is just like in regular unex treem gradient boost we can quantify how good the prediction is with a loss function in gradient boost part two regression details we learned how to use this loss function 1/2 times the squared residual for regression to review y sub I stands for the Y access value from one of the observed values y sub 1 Y sub 2 and Y sub 3 and P sub I stands for a prediction P sub 1 P sub 2 in P sub 3 that corresponds to one of the observations y sub 1 Y sub 2 and Y sub 3 for example if we applied the loss function to the initial prediction then we would add one term for each observation so N equals three the first term in the summation corresponds to the first observation y sub one second term corresponds to the second observation y sub two in the third term corresponds to the third observation y sub three so we just plug in the numbers BP boopy boopy boopy boopy boopy boopy boopy and do the math and get 100 4.4 later we can apply the loss function to new predictions and compare the results to this one to determine if our predictions are improving or not note if we had in observations then we would add up in terms no big deal in gradient boost part for classification details we learned how to use this loss function the negative log likelihood for classification and just like before whysa by refers to a y-axis value for one of the observed values which is either 0 or 1 and P sub I refers to a predicted value between and including 0 & 1 note if you want more details and examples using this loss function check out the stat quest gradient boost part for classification details now that we have one loss function for regression and another loss function for classification XG boost uses those loss functions to build trees by minimizing this equation note the equation in the original manuscript for XG boost contains an extra term that I'm omitting this term gamma times T where T is the number of terminal nodes or leaves in a tree and gamma is a user definable penalty is meant to encourage pruning I say that it encourages pruning because as we saw an XG boost part 1 XG boost can prune even when gamma equals 0 I'm omitting this term because as we saw in parts 1 & 2 pruning takes place after the full tree is built and it plays no role in deriving the optimal output values or similarity scores so let's talk about this equation the first part is the loss function which we just talked about the second part consists of a regularization term the goal is to find an output value for the leaf that minimizes the whole equation and in a way that is very similar to Ridge regression we square the output value from the Nutri and scale it with lambda later on I will show you that just like Ridge regression if lambda is greater than 0 then we will shrink the output value the 1/2 just makes the math easier note because we are optimizing the output value from the first tree we can replace the prediction P sub I with the initial prediction P of 0 plus the output value from the new tree now that we understand all of the terms in this equation let's use it to build the first tree we start by putting all of the residuals into a single leaf and now we need to find an output value for this leaf that will minimize this equation but before we dive into the math let's simplify things by setting lambda equal to zero and that means removing the regularization term okay now let's plug in different output values for the leaf and see what happens for example if we set the output value equal to zero then we are left with the loss function for the initial prediction zero point five note we already calculated the loss function for the initial prediction when we demonstrated the regression loss function and we got 100 four point four so let's move the equation up here and plot the result on this graph X access on this graph represents different values for the output value and the y axis represents the sum of the loss functions for each observed value now let's see what happens if we set the output value equal to negative one when the output value for the leaf is negative 1 then the new prediction is 0.5 plus negative 1 which equals negative 0.5 and negative 0.5 corresponds to this new thick black line and that shrinks the residual for y sub 1 but it makes the residuals for both y sub 2 and y sub 3 larger when we do the math we get 109 point 4 now we plot the point on the graph and we see that negative one is a worse choice for the output value than zero because it has a larger total loss in contrast if we set the output value to positive one then the new prediction is 0.5 plus 1 which equals 1.5 and that makes the residual for y sub 1 larger but the residuals for y sub 2 and y sub 3 are smaller now when we do the math we get 100 2.4 and we see that we have the lowest totals so far and thus positive 1 is the best output value we have picked so far note we can keep plugging in numbers for the output value to see what we get or we can just plot the function as a curve curve shows us that when lambda is zero then the optimal output value is at the bottom of the parabola where the derivative is zero now let's see what happens when we increase lambda to 4 when lambda equals 4 the lowest point in the parabola shifts closer to 0 and if we increase lambda to 40 then the lowest point in the parabola shifts even closer to 0 in other words the more emphasis we give the regularization penalty by increasing lambda the optimal output value gets closer to zero and this is exactly what regularization is supposed to do so that's super cool BAM now one last thing before we solve for the optimal output value you may remember that when regular on extreme gradient boost found the optimal output value for a leaf it solved an equation very similar to the first part without regularization on extreme gradient boost used two techniques to solve this equation one for regression because the math was easy in a different one for classification because the math was not easy specifically for classification on extreme gradient boost use a second-order Taylor approximation to simplify the math when solving for the optimal output value in contrast XG boost uses the second-order Taylor approximation for both regression and classification unfortunately explaining Taylor series approximation x' is out of the scope of this stat quest so you'll just have to take my word for it that the loss function that includes the output value can be approximated by this mess of sums and derivatives the genius of a Taylor approximation is that it's made of relatively simple parts this part is just the loss function for the previous prediction this is the first derivative of that loss function and this is the second derivative of that loss function note since the derivative of a function is related to something called a gradient XG boost uses G to represent the derivative of the loss function and since the second derivative of a function is related to something called a hessian XG boost uses h to represent the second derivative of the loss function now let's expand the summation bit bit bit bit the regularization term and plug in the second-order Taylor approximation for each loss function bit bit bit bit bit before we move on let's remember that we want to find an output value that minimizes the loss function plus the regularization and that all we have done so far is approximate the equation we want to minimize with a second-order Taylor polynomial now it's worth noting that these terms do not contain the output value and that means they have no effect on the optimal output value so we can omit them from the optimization now all that remains are terms associated with the output value so let's combine all of the unsquare output value terms into a single term in combine all of the squared output value terms into a single term and move the formula to give us some space to work now let's do what we usually do when we want a value that minimizes a function one take the derivative with respect to the output value to set the derivative equal to zero and three solve for the output value of the first term with respect to the output value is just the sum of the G's when we take the derivative of the second term with respect to the output value the exponent two comes down and cancels out the one-half leaving us with the sum of the H's and lambda times the output value now we set the derivative equal to zero and solve for the output value so we subtract the sum of the g's from both sides and divide both sides by the sum of the HS and lambda hooray we have finally solved for the optimal output value for the leaf now we need to plug in the gradients the G's and the Hessians the H's for the loss function so let's move the equation out of the way so we have some room if we're using XG boost for regression then this is the most commonly used loss function shameless self-promotion just to remind you this is the exact same loss function that we described in detail in gradient boost part two regression details so if the next few steps move too quickly just check out the quest the link is in the description below the first derivative aka the gradient G sub I with respect to the predicted value P sub I is negative one times the difference between the observed value y sub I and the predicted value P sub I in other words G sub I is the negative residual that means we plug in the negative residual for each G sub I in the numerator note this negative sign cancels out all of these negative signs so the whole numerator is just the sum of the residuals now we need to figure out what the ages are in the denominator the second derivative aka the Hessian H sub I with respect to the predicted value P sub I is the number one so that means we replace all in H's in the denominator with the number one in other words the denominator is the number of residuals plus lambda so when we are using XG boosts for regression this is the specific formula for the output value for a leaf to summarize what we've done so far we started out with this data then we made an initial prediction 0.5 then we put all of the residuals in this leaf then we asked ourselves what the output value of this leaf should be given a loss function and regularization we then plotted the equation as a function of the output value and solve for the lowest point where the derivative is zero and this is what we got the equation for the output value gives us the X access coordinate for the lowest point in the parabola BAM now if we are using XG boost for classification then this the negative log likelihood is the most commonly used loss function shameless self-promotion this is the exact same loss function that we worked with in gradient boost part for classification details in that stat quest we spent a long time deriving the first and second derivative of this equation calculating the derivatives took a long time because the output values are in terms of the log odds so we converted the probabilities to log-odds one step at a time rather than skipping the fun parts like we are now then we took the derivatives without skipping the fun parts like we're doing here and lastly we converted the log-odds back to probabilities without skipping any steps in the math note if you feel like you just missed out on a lot of fun stuff fear not the link to gradient boost part for classification details is in the description below now that we have the first derivative of the loss function aka the gradient G sub I and the second derivative of the loss function aka the Hessian H sub I we can plug them into the equation for the optimal output value just like for regression G sub I is the negative residual and we know that this negative sign will cancel out this negative sign in the numerator for the output value so we can replace the numerator for the output value with the sum of the residuals in the denominator we can just replace all of the H sub i's with the sum of P sub I times 1 minus P sub I note in the denominator we're using previous probability to specify the previously predicted probability rather than the previously predicted log odds so when we are using XG boost for classification this is the specific formula for the output value for a leaf BAM now regardless of whether we are using XG boosts for regression or classification we can calculate the optimal output value for this leaf and we do it by plugging derivatives of the loss functions into the equation for the output value BAM now we need to derive the equations for the similarity score so we can grow the tree however before we do that remember that we derived the equation for the output value by minimizing the sum of the loss functions plus the regularization and let's also remember that depending on the loss function optimizing this part can be hard so we approximated it with a second-order Taylor polynomial so we expanded the summation bit Allah debt debt debt but at debt added the regularization term and swapped in the second-order Taylor approximation of the loss function then removed the constant terms and lastly did a little algebra to simplify everything now because we removed constants went arriving this equation it's not equal to what we started with however if we plotted both equations on a graph we'd see that the same X access coordinate represented by the optimal value tells us the location of the lowest points in both parabolas I mention this because XG Boost uses the simplified equation to determine the similarity score so the first thing XG boost does is multiply everything by negative one and that makes each term negative and it flips the parabola over the horizontal line y equals zero now the optimal output value represents the X access coordinate for the highest point on the parabola and this Y access coordinate for the highest point on the parabola is the similarity score at least it's the similarity score described in the original XG boost manuscript however the similarity score used in the implementations is actually two times that number the reason for this difference will become clear once we do the algebra so let's do the algebra to convert this into the similarity scores we saw an XG boost parts 1 & 2 first let's plug in the solution for the output value now multiply together the sums of the gradients G's on the Left note these negative signs cancel out and we get the square of the sum now we square the term on the right this sum cancel out this square now we add these two terms together and we end up with this fraction this is the equation for the similarity score as described in the original XG boost manuscript however in the XG boost implementations this 1/2 is omitted because the similarity score is only a relative measure and as long as every similarity score is scaled the same amount the results of the comparisons will be the same this is an example of how extreme extreme gradient boost is it will do anything to reduce the amount of computation now if we're using XG boost for regression and we're using this loss function we plug the first derivative G sub I into the numerator and since G sub I is the negative residual the numerator is simply the sum of the residuals squared now we plug the second derivative H sub I into the denominator and since H sub I equals 1 the denominator is just the number of residuals plus lambda thus this equation is the similarity score that we use for regression double bam now we need to derive the similarity score for classification and that just means plugging in the first derivative for the loss function G sub I and the second derivative H sub I again since G sub I is the negative residual the numerator is simply the sum of the residuals squared and since H sub I is the previously predicted probability times 1 minus the previously predicted probability then the denominator is just the sum of the H sub i's plus lambda thus this equation is the similarity score that we use for classification triple bam now for one little annoying detail in part two of this series we talked about cover cover is related to the minimum number of residuals in a leaf and we said that cover was the denominator of the similarity score minus lambda in other words cover is the sum of the Hessians the h sub i's for regression the hessian AKA the second derivative of the loss function is 1 and since there is one hessian / residual in a leaf cover for regression is simply the number of residuals in a leaf for classification the hessian is P times 1 minus P so cover is equal to the sum of the previously predicted probability times 1 minus the previously predicted probability small bam in summary boost builds trees by finding the output value that minimizes this equation the equation consists of a loss function and a regularization term that is just like Ridge regression we then solve for the optimal output value and once we have the output value we plug it into the simplified equation to get the similarity score we then configure the output value and similarity score equations for regression or classification by plugging in the first derivative G sub I and the second derivative H sub I of the loss function BAM [Music] hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stat quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
BKnM0MCGbVc,2020-02-04T02:45:07.000000,"Live 2020-02-03!!! Statistical Models, Regularization, Best ML Algorithm.",since it's a live stream who raced a quest hello I'm Josh Dahmer and welcome to my stat quest a live stream I'm really excited you guys are here and one of the reasons is I have got a big huge announcement to make the big huge announcement is starting today stat Quest is my full-time job on Friday I went to the lab and that was my last day at my old job and now when I woke up this morning I went to stack west HQ world HQ and I spent the whole day doing stat quest and it was awesome this is awesome for many reasons one is is that you'll get more static quest content like this live stream but I'm also gonna do more applied tutorials and Python and R and also I'm gonna try to just crank out more stat quest videos just gonna do everything I can because I got a little bit more time to spend on stat quest the other thing I'm really excited about is now I have time to travel for stat quest so if you want me to come to your work or school or whatever let's do it I think I'd be super fun I have a special stat cast that I don't put online it's not a video I only do it in person so if you want to see that special stat quest be good way to do it oh I got too excited let's get back to where this why this is awesome okay I also I just love stat quest so I'm excited about that I get to spend more time doing it however doing stat quest is a full-time job it means I need your help one thing you can do to help me out and this would be huge is subscribe and click the like button I know a lot of people ask you to do that and now I'm asking you to do that too I'd really appreciate it you could also share stat quest videos with your friends and loved ones and encourage them to subscribe as well that would be huge it's such a compliment when somebody shares my videos with someone they like you can also support the channel as a member or via patreon and what those are are for as little as a dollar a month you can help support me making stat quest and that's real important for me because it's a it's sort of like a known upfront source of income that I can have so I'll know I'll have enough money to buy groceries at the end of the month and stat quest will then become sustainable you can also contribute through super chat and you can also get merch and t-shirts and whatnot and you can also donate so there's lots of options that you can help subscribing and liking are probably the most important of those right now but but if you can do the other words though that would be great too alright enough of this let's just say BAM about how excited I am that I'm now doing stat quest full-time and move on to the questions so here's question number one Anita Pallenberg she asked do we use statistical models to predict or explain stuff and how can I make the distinction clear in the methods I use okay so in order to talk about this question I answer this question we're gonna do a linear regression and we're gonna predict and explain stuff so we've got weight and height and first we're gonna predict stuff so if you tell me you weigh this much then I can use the linear regression line to predict that you were this tall BAM no big deal we can also use linear regression to explain stuff when we do a linear regression we get an r-squared value and we get a p-value the are this r-squared value is 0.7 tells us that 70% of the variation in height can be explained by weight and the p-value tells us that if we randomly drew four dots on a bunch of graphs and calculated the r-squared value for each data set oops excuse me I'm having a little technical difficulty with the slide show over here so just bear with me I'm sorry for all the problems this is the first time I'm trying to like mix sort of traditional stat quest plus the live stream aspects and we've got a couple of bumps in the road so hopefully we won't have too many more of those throughout I think we're I think we're probably gonna have two more as my guess but you guys are cool you're just gonna roll with it right yeah okay cool so so remember we we we had some random data sets and we calculated our squared values for all those random data sets and if we did that then only then we would only get an R square value greater than or equal to zero point seven two percent of the time that's what that p value is telling us so in other words it's unlikely that the relationship between weight and height is due to chance and even though correlation in this case measured by R squared is not causation we still have some insight into our data so this is how we can predict and explain stuff with linear regression and we can easily make the distinction between predictions and explanations by putting emphasis on the fact that we've made a prediction and we say I predict you're this tall or we can put emphasis on the r-squared value wherever and r-squared is used more for sort of explaining stuff and trying to tease out relationships in the data note the same things can be said about logistic regression if you said you weigh this much then I will predict that there's a low probability that you were obese so I can make a prediction and also with logistic regression a weak calculating R squared and a p-value that'll tell us if there is a relationship between weight and the probability of obesity interestingly enough decision trees can also make predictions and give us insight into our data for example this tree predicts whether or not someone will love the movie troll - based on whether they like rom-coms for romantic comedies and their astrological sign however the actual tree only uses likes rom-coms to make predictions and if those predictions are good then we have an idea that there's a relationship between rom-coms and loving or not loving troll 2 and since astrological sign was not used at all then we have some sense that it's not as important for predicting if someone loves troll 2 so even though we did not calculate R squared or a p-value we still got some insight into our data so even sort of like the non statistical based methods can give us insight into our data and so we can use regression trees or excuse me decision trees regression trees or random for us stuff like that if it gives us insight into our data and a lot of those methods will even without a p-value so and actually I finally I found myself using alternative methods I'll do this I'll do the traditional linear models approach but I've also used some of these machine learning methods to get some insight into my data and not just make predictions all the time so that's a cool thing to do BAM all right now we're ready for question number 2 JM asked can you show the effects of regularization nope regularization is a trick that we use to determine the best parameters for machine learning and if you don't know regularization is this would be good time to go get a snack but come back quick because the last question is for everybody and we're gonna all gonna learn a lot from that one and also if you don't know what regularization is I would highly recommend checking out the stat quests on the subject because it's one of the most important things in machine learning and it's used in all kinds of data analysis not just machine learning but statistical analyses as well it's a general technique for dealing with lots of variables so if you have a lot of things that can explain something like whether or not someone you're gonna like a troll to or not regularization is a standard procedure for dealing with that situation okay so we're gonna go back to our weight and height data hold on my cat once in the room hold on let me let the cat in this is my cat okay back to what we were doing so say like we fit this line to the data I know that's a terrible fit to the data it's a horizontal line it's about as bad a fit as you could possibly have but it's a starting point we're gonna improve on it in just a little bit okay now let's add the ridge regression Ridge regression penalty aka the l2 norm oops okay and if he asked me it should be called the square penalty because that's what we're doing we're squaring the slope the thick blue line represents lambda equals zero so that means there's no extra penalty because we multiply the square of the slope by lambda so when lambda zero all we're back to just having the sum of the squared residuals this thick orange line represents lambda equals ten so we have increased the penalty and we see that the minimum value is closer to zero than before in other words the ridge regression penalty the square penalty Shrunk the slope the thick green line represents lambda equals 20 and we see that the minimum value is closer to zero and the slope is shrunk some more this thick purple line lambda equals 40 excuse me the thick purple line represents I left out the word line and that threw me off anyway so thick purple line represents lambda equals 40 so now the penalty is quite large and we've shrunk the slope even more okay so we see that as we increase lambda the optimal slope gets closer and closer to zero now let's see what happens when we use the lasso penny penalty aka the l1 norm or if you asked me I'd call the absolute value penalty because that's what we do we take the absolute value of the slope okay again the thick blue line represents when lambda equals zero so there's no extra penalty because we we multiply the absolute value of the slope by lambda so the thick orange line represents lambda equals 10 so now we are turning on the penalty and shrinking the slope the thick green line represents lambda equals 20 and the thick purple line represents lambda equals 40 note this is super important and is one of the big differences between Ridge and lasso or squaring the the parameters versus taking the absolute value okay the low point in the purple curve aka the optimal slope given the absolute value penalty when lambda equals 40 is 0 and that means the slope of this line is 0 and that means that when lambda equals 40 then we have then we ignore weight as a variable when predicting height so now let's compare the squared penalty Ridge to the absolute value penalty lasso so on the left side we see that when we increase lambda the lowest point in the parabola shifts over toward zero but the parabola still retains its prayer bollocks shape it just quota for de gets scaled a little bit but it's still a nice sort of like curve it dips down and it goes back up in contrast with the absolute value penalty the lasso penalty on the right side as we increase lambda we shift faster we shift towards zero faster and we start losing that parabolic shape and you see when lambda equals 40 we've still got a little bit of curve but it doesn't we don't actually bottom out we could imagine that curve still going but we don't bottle map and but where we do bottom where we end is at zero so and that's why we ended up ignoring weight and kind of using a horizontal line at that point so those are the two big differences between the squared Ridge penalty and the absolute value lasso penalty and I hope these visualizations help I've seen other visualizations out there I'll be honest they don't make as much sense as these do to me so hopefully that was helpful and we'll give it the double BAM double BAM okay question number three this is from shove ham Borg err and they want to know how I can choose the best machine learning algorithm for my bet for my data so this is something you don't need to know regularization or anything fancy to appreciate the answer to this question so the most important thing is to become familiar with a bunch of different methods so what I mean by that is you go to scikit-learn or you find a repository of machine learning methods for R or whatever programming language you'd like and you just pick one and you start with a simple data set and you get the method working and then you you know you then you just add a more complicated data set and see see if you can get a good feel for how the machine learning method works and what its strengths and weaknesses are and if you're wondering well how do I even know where to start how do I know which machine learning algorithms to even like try I mean because there's a bunch where do you start I'm gonna address that towards the end of the question so we'll get we'll come back to this aspect of how do we even start out when we talk about that when we get to the end of this question so that said aside from just trying about a bunch of methods and seeing what happens here are some things to keep in mind when taking a machine learning algorithm one thing to keep in mind is do you need the predictions do you need to make predictions quickly for example or is your machine learning in a autonomous car and it needs to know if it needs to hit the brakes or not and those decisions need to be sometimes need to be made very very quickly or is it the kind of machine learning algorithm where you can feed it your data and come back after lunch and it gives you a prediction you're like sweet that's our prediction BAM okay so if you're no rush versus you're in a huge rush to get predictions that can change which methods you use some methods like support vector machines can make predictions very quickly since they need relatively few calculations to make a decision in contrast random forests or any of the boost algorithms might have a lot of calculations to do to get to a prediction so so the the speed at which you need a prediction can affect which method you choose another question you want to ask yourself is does learning have to be fast or am I going to update my model on the fly with new data methods like XG boost or highly optimised up to excuse me methods like XG boost are highly optimized to be trained quickly and that's very useful because that means you can use cross-validation to find the hyper parameters in and you can sort of tune X G boost pretty efficiently so that's a that's something to keep in mind if that's something you need to do methods that can be trained with stochastic gradient descent like linear regression or logistic regression or neural networks or any other method there's a bunch of methods they can easily be updated with new data over time without having to start over so those are great when you sort of train early on with a small data set but you're continuing to gather more data and modify or how you predicting so that's something to keep in mind as well because some methods are better at doing that than others okay a question you need to ask yourself with how big is the data set if your data set is too small then you might just need more data rather than a machine learning algorithm and if your data set is very very very large then you need a method like XG boost that is designed to work with huge data sets and I know I've mentioned XG boost a billion times I'm not necessarily endorsing it as my favorite method but I'm just in the midst of working on stat quest videos on XG boost so I'm thinking about it a lot so that's why I keep bringing it up oh okay I think we've I think we've made it through all of our slideshow Fiasco's and hopefully I had this smoothed out for the next live stream okay okay when you have several options to choose from and time to train multiple methods you should use 10-fold cross-validation to find the best one for your data and last but not least scikit-learn has an algorithm cheat sheet and I'm sorry that well yours truly right here is cutting off part of that cheat sheet but basically it's a decision tree where if you know what the what kind of data you have and you know sort of what you want to do with that data and you know how much data you have you just follow this decision tree and it leads you to a machine learning method and this this cheat sheet is also a great way to figure out where to start when you're just trying to learn new machine learning algorithms look at the ones here and learn how they work so if you're really new to machine learning this is a great resource at many stages right at the very beginning you can you can use this as a way to learn and get familiar with new machine learning methods that you might not already know anyway so while I I don't actually have the link for this in the description below yet but I will put that as soon as I can it'll probably be a couple hours actually no it'll be soon I'll get it up soon triple bam all right now it's time for some viewer questions I'm gonna look over the chat over here and oh here's a question the very apropos questions jnt said do I prefer patreon or YouTube memberships or is it exactly the same they are different patreon they are different so I think the big difference is YouTube takes a larger Commission than patreon so with patreon I get more of the money comes to me and less of it goes to patreon itself whereas with YouTube memberships more the money goes to YouTube then it comes to I mean the YouTube basically takes 30% and I think patreon takes a 10 or 15 or something like that it's it's different so but that's being said I know that YouTube memberships are much easier for a lot of people and if that's the easiest thing for you to go for it I treat both YouTube and patreon subscribers the same you get the same perks you get you get early access so so some of you guys probably have already seen the latest XG Boost video it's been out for a little over a week now and it'll probably be in early access for another week before I make it public so that's one of the perks you also can get acknowledgments in the videos acknowledgement on the website I've also got something crazy for people that are incredibly wealthy you get a collection of a lot of silly songs my favorite silly songs so that's kind of a fun perk but but really it's it's it's not really about necessarily the perks well may-maybe do for the perks anyways let's see what else we got someone else is asking about PayPal details I have a link for that in the in the description of all my videos it's you just kind of go to the description and scroll down I'm also gonna try to put a link for that on my website so that should be pretty easy someone asked if I could make a video about gradient a gradient booster and I hate to say it I think I already have that video ready to go so just check my website I've got an index of all my videos and you can just find the one for gradient boost and go for it what else we got up here Oh someone asked do we have to handle outliers before handling null values in a data set or the other way [Music] that's a good question and I'm not necessarily certain how to answer that off the top of my head I will say I think the process is you take care of the null values first if you're removing them and if your imputed them then you need to get rid of the outliers first does that make sense to be honest I would I guess you could always remove the outliers first because if you're just removing null you know missing data with you're just removing those samples that have the missing data it doesn't matter what the outliers say but when you're imputing data it does so in either case get rid of the outliers and then remove the missing data and then or deal with the missing data you can either remove it or you can impute so yeah great I answered that question fantastic what else we got extreme gradient boosting videos yes I already have two and there's a third one out so you were lucky and if you become a member you get early access to that third video and I've got a fourth one coming out for early access in a week or two and I'm really excited about that oh here's a question and my professor in real life I was I was up until last Friday I was a research assistant professor at UNC in genetics and now I'm a stat quest guy I'm a stack West guy that's what I do i do stat quest so let's see if we got a couple other questions maybe I can scroll over here in the chat window got a lot of stuff going over here mm-hmm holy smokes someone who in Portugal was viewing it to a to a 207 a.m. that's crazy what else we got over here Oh videos on neural networks those are coming out in the spring in a couple of months when I finished the XG boost videos I'm gonna do a couple of odds and ends I've got some new p-value videos that are that are coming out I've got one on something called you map which is a way of visualizing really complicated data sets and Oh naive Bayes so I'm gonna have a handful of kind of like short videos naive Bayes is like a super short method simple method so we won't spend along with that and once we've gotten through those little ones we're gonna do neural networks and I'm really excited about that I've been I've been talking about doing it for years and it's really exciting to finally be like right on the edge of doing it so that's exciting what else we got long-term stuff oh yeah someone asked if we're gonna do Bayesian statistics I think at some point we will we're gonna obviously we're gonna do naive Bayes pretty soon and I'm pretty excited about that although I'll be honest even though it's called naive Bayes and whenever if you if you google naive Bayes and you look at any of the instructions they all teach you about the Bayesian I can't even remember what it's called but it's basically the Bayesian formula the foundation of Bayesian statistics and then they say but all the all you know they then they say they make all these crazy assumption you end up with something that's hardly Bayesian at all so it's kind of cheating to say you're doing Bayesian when you're using naive Bayes but we'll get to that one of these days it's something to be honest I've wanted to learn more about as well it's yeah it's something I've wanted to learn about as much as well someone asked is PCA applicable only for unsupervised learning ah and the answer to that is no you can use it all the time and I use it all the time for for supervised things I use it well I used to when I was working in the lab to verify that an experiment worked properly they'd have a bunch of experiments and they'd be labeled based on some feature like these these these samples were taken from cells that were given this drug and these samples were taken from a pool of cells that were given another drug so I've got three of each and I'll do PCA to make sure they cluster appropriately in an even though I know what how they should cost her and so it's so it's kind of supervised I'll turn that off and make sure that they're clustering properly however if I'm going to take advantage of those labels of the fact that these cells have one drug given to them and these this pool of cells had another drug coming down I would use something like linear discriminant analysis to kind of take advantage of the fact that I know the different categories um so I hate to say goodbye but I'm gonna give this a bonus BAM and then I'm gonna say the end because I'm trying to keep these two 30 minutes and my voice is giving out by this second I don't think I can talk much longer so hey I always hate to end these things but we're gonna have to end now I'll stick with the chat for a little bit if you guys want to keep chatting it's always fun to participate in that and until next time quest on
pSWRT9pObX0,2020-01-20T14:34:50.000000,"Live 2020-01-20!!! Favorite ML, Data Leakage, How to Learn ML",hello I'm Josh Dharma and welcome to stat quest today we're gonna do a live stream actually right now we're gonna do a live stream so this is pretty exciting okay the way we've done this before is that I've taken three comments that people have posted on static quest videos or on the community page and I'm gonna go through those comments and then at the end I'm gonna take comments from the chat so we'll take some live comments and questions and we'll just see how it goes all right I'm just really excited thank you for being here for my second live stream ever I think the plan is to do it twice a month at least for now once in the evening my time and once in the morning my time I think that kind of covers the whole world we get Asia we get Europe we get North America so just tune in for the time that works best for you all right let's talked about our first comment okay I'm terrible with people's names so I'm not even gonna try to read this one but you see it on the screen this is a great question and the question is what is your favorite machine learning algorithm now obviously that's a tricky question but what I want to say is my personal use of machine learning is different from a lot of other people's use of machine learning most people when they use machine learning what they really want is they just want to get the right answer all the time and that sounds reasonable right however I'd like to do research and when I do research what I want to do is I want to understand sort of underlying mechanisms an underlying structure in my data and I'm actually willing to sacrifice a little bit of the accuracy in order to get more insight into my data and so one of my all-time favorite machine learning algorithms is actually called random forests and I know these are simple and I know they're basic and I know they're not the best but they give me insight into the underlying variables they're very easy to interpret as to what they're doing and why they're doing it I like that I love the fact that I can take any type of data I can take continuous data I can take discrete data I can take categorical data I can throw it at random forests and random forests will give me clustering of it I can get a visualization as to how things are related and how the samples are related I love that what else is random forests - gives me inside of the data allows me to graph and oh the last thing that I love about it is it gives me an estimate on how how much confidence I should have in the prediction now a lot of the methods will do that well they'll give you some confidence however the random forest one is so easy and so simple and easy to calculate that I just kind of love it for example say like I get some I've got my 3 and enforce it's all trained its ready to go I give it some new data and I want to make a prediction with it well imagine I had a hundred trees just for this example imagine if all 100 trees came up with the same answer and if that was the case I'd have a lot of confidence in that that that random forest kind of like was very confident about how it made its decision in contrast imagine only 51% of the trees came to the same conclusion well in that case I would have a little less confidence in the output from it and I just love how simple it is I can just easily see how many trees were involved in making a decision or a classification and I can let that be a guide as to confidence in the output so for those reasons random forests are my favorite don't get me wrong though I love a lot of machine learning algorithms they're all I mean I mean I wouldn't be making videos about them if I didn't think they were all kind of interesting cool but I'm just saying personally for the kind of work that I do I'm a big fan of random for us all right so that gets a bow alright ready for comment number two so another name I'm not going to try to pronounce all live I know it's embarrassing I think maybe one day we'll just have a contest of names that I just can't pronounce and it'll you know we'll all laugh and it'll be a good time but I think this is a great question the question is what is data leakage in machine learning so let's talk about what data leakage is okay so we start with a data set sort of what all machine learnings do we start with data set now this is going to be example of data leakage and machine learning we use that data for training and then we use that same data for testing using the same data for training and testing as an example of data leakage it's in a crazy over-the-top totally obvious example of data leakage it will cause us to overestimate how well our model will perform with real data because we've trained and tested with the exact same data set so that's a big no-no and we all know that so obviously we didn't learn a lot from that example but now we have at least have a sense of what data leakage is it data leakage is when our training data set is not independent of our testing data set ok so here's an example of what we should be doing we should make a training data as a subset of the original data set and testing data should be a subset of the original data and they should be completely independent of each other now we train the model with the training data and we test the model with the testing data when we train and test using separate data sets that are independent of each other we avoid leakage okay that's all obvious no big deal okay however leakage can be sneaky okay here's an example of super sneaky data leakage we start with the original data set and in this original data set we've got some missing data do not impute the debt the values first and then split the data into training and testing data sets don't do this this is because when we use all of the data to impute values then that means the testing data was involved in filling in this value and the training data was involved in filling in this data so these two data sets are not independent of each other and as a result once we do our machine we train and we test we're gonna be overly optimistic about how well this method is gonna perform so this is kind of a tricky thing instead what we should do is we should leave the data that's missing we just leave it and then split the data into training and deaths testing data sets and now we impute the missing data in the training data set just using the trained us training data set data and we can impute the missing data in the testing data set just using the testing data set data now the training and testing data sets are independent of each other so we can avoid data leakage all right so that gets a double BAM double down super exciting okay so we're making great progress we're moving along pretty fast which is good and now that means we're ready for comment number three give me just a second let me get this ready okay so this is a question of just my process of how I do my work the question is where do you learn these nitty-gritty details from and the answer is the Internet Google what I do is I find so what I'm showing you right now is an example of saw a webpage that I found the described X G boost and what I do is I google and I read everything every single thing and oftentimes when I'm reading something I don't understand it it's very complicated it's mathematical it's got this weird notation it's it's complicated words I've never seen before and I'll read it and I'll go okay that didn't make any sense to me so then I'll move on to the next thing and I'll read that and maybe that makes sense to me maybe it doesn't if it doesn't I just keep going and sooner or later I'll start finding out Google the words I don't understand I'll Google the terms I don't understand sooner or later I'll start getting a picture in my head and once I start understanding little pieces I'll go back to where I started and I'll reread everything and the stuff that didn't make any sense before will make a little bit of sense the second time it'll it'll usually it brings on new questions it makes me you know maybe I understood one thing but now I don't understand another thing so I got to keep reading I've got to keep googling I got to keep looking for new sources of information and then I just go back and I go back and I go back and tell everything it makes sense okay so I do tons and tons of reading and that's the first phase and that can take to be honest that can take anywhere from a day to a month to a year believe it or not my PC a video I spent about two years on that video alone it's not like all I did for two years is work on that video but I was constantly reading and constantly getting more background information yeah just a little bit every day until I finally said okay I got it I figured it out I know how we can visualize it I know how we can tell this story in a way that no one's ever done it before because I've done all this research once I've done the research and I understand what I'm doing I go through all of the materials and I annotate it and I do the math and I redo the math myself and make sure it makes sense I just keep trying to figure out make sure everything works the way I was expecting it to um and I'll just derive everything and so yeah so what I'm showing you right now are notes from how I prepared for the XG boost stack quests that I've been working on we have at least two more coming out in the next couple of weeks I'm really excited about we got the nitty-gritty math details coming out and then we've got a video on optimizations so anyways that's how I do what I do oh one last thing before we move on if it's possible I will run a basic data set through the algorithm and make sure it does everything I'm expecting it to do and I do it one step at a time going to make sure I can do everything on paper as well as and it matches exactly what I'm getting with the computer program or the machine learning algorithm triple bam all right so now we've come to the part of the live stream where I get to take questions from you and I'm super excited about this this is one of the more fun parts of it so let's get down to it I'm just gonna starch I'm gonna I'm gonna put up this screen and this lets me go over to the chat room and see if we got any good questions over here let's see wow we've got a lot of good chat going on a lot of whole hellos what do we got I see someone wants to see power regression step by step that's a stat quest that's not really a good question to answer on the air someone else wants more sessions on RNA seek DNA seek and s generation sequencing Oh interesting so um Uma Shankar shreddy I think I pronounced that name right she's asking about r-squared and why we can't trust it and why sometimes we come across a model yielding an r-squared that's greater than one and also to talk about the adjusted r-squared okay interesting question I'll be honest I've never actually seen an r-squared greater than one I've seen R Squared's less than one that's when your default model is just the worst it's worse than the most simple model and it can happen every now and then a lot of people have pointed that out to me but I will talk about adjusted r-squared so here's the deal with R squared or any correlation metric say like we're doing a linear model for example say like I'm modeling height using a person's weight and using their age and maybe some other variables okay so I'm just trying to predict someone's will someone's height and r-squared will tell us roughly how strong the relationship is between the variables I'm using to make the prediction weight and age and what are the other variables R and it so R squared tells us the relationship between those and the thing that we're actually predicting which is height now here's the thing about this type of model if I added a bunch of random noise to that say like I added someone's astrological sign I added the name of their pet dog I added I don't know what time of year it is stuff like just kind of random stuff that should have nothing to do with height well so when we do a linear regression or something like if the variable is really bad and doesn't help at all the parameter in front of it will be zero and that variable will just go away it'll effectively not exist but if just by random chance that variable helps even a little bit and it's completely unrelated it's completely off it should be uncorrelated but just because of the way random noise sometimes work it helps a little bit well then it'll remain in our linear regression model and as a result our r-squared will increase and that's no good because we don't want the r-squared to increase and that's what adjusted r-squared tries to compensate for so the more variables we add to our model our model will never get worse because of because of the variables really bad the parameter will be zero and it'll be as if we never added it however the model can get better for all the wrong reasons because we've added some sort of random noise that just happens to help out and so because the variable tends to fit the data better even with random variables the adjusted r-squared compensates for that by penalizing for the number of variables we use to predict something so our just at R squared can be super useful okay so that's what what we're going to talk about for R squared we're going to move along and see what else we got oh actually in here's a here's a question not necessarily a technical question but yan how Chen asked for me to classify my videos into different levels of difficulties if that's possible and if you've ever gone to my website stat questo RG i have a video index and generally speaking that indexes within each category's ordered from simple to complicated so at the top so at the very stop top I've got a section called the basics and statistical basics and at the very top it's got the most basic concepts first and they just get more complicated as you go down that list if you go into the machine learning a part of the of the of the page it starts simple and it works its way down to more complicated so it's already pretty pretty ordered oh here's an interesting question Schuyler DIMMs Worth asked is it important to learn all of the formulas and equations even though we already have advanced statistical software to help us do the work and the answer to that is it depends it depends on what you want to do do you just want to use these algorithms and be a user or do you want to be sort of an in-between sort of semi power user someone who knows all the ins and outs or do you want to be that kind of person who's deriving the next generation machine learning algorithms obviously if you want to be deriving the next generation understanding all the formulas is super helpful in between its you know maybe not super important to memorize all them it's sometimes it's good to go through them and get a sensible oftentimes when you go through the math you can kind of see where the strengths and weaknesses are however personally I think as long as you under that stand the main ideas you're good to go as a practitioner it's sort of like the executive summary that's the most important thing are the main ideas and that's typically the way I structure my videos we're all had like a main like 4xg boost I've got some main ideas videos and the longer weeks last the more nitty-gritty the details are that we cover largely because I think the main ideas are the most important in the and the implementation details or the actual mathematics are kind of sort of important but not as much important at least from my perspective unless like I said you want to be that person making the next generation machine learning algorithm yeah you better dive into that math and learn it well all right let's see what else we got oh this is a funny one Shravan Kumar says can you build a model to pronounce all names from different parts of the world that would be great I love that one you know I wish I could I'm gonna spend more time working on stack quest videos maybe someone could come up with a way to help me with this if so posted in the comment that would be awesome Phil Thompson just put a comment he said I purchased an introduction of statistical learning but didn't know that I needed to know calculus in order to truly understand the book yes you need to understand calculus however if you have an opportunity and I don't know if I should be saying this on air return the book you can download it for free don't actually have to buy it the calculate calculus is is in that is in there right there's a lot of calculus in the introduction of statistical learning you can skip it read the introductions to each section you don't have to get into the nitty-gritty details a lot of the examples and the introductory material doesn't have calculus and if you understand that you understand 90% of all you need to know so that's my recommendation so even if you can't return the book it's still a great book to have and it's good to support the authors that wrote the book I mean that's in a great book and I loved it so yeah if you can support them that's great but don't freak out if you can't do all the math you don't have to do all the math just read the introduction to each subject read about the examples that they use and you can still learn a ton it's a good book alright a lot of people are asking for time series and answer to that is yes we're gonna do time series possibly over the summer hopefully hopefully in the summer so here's what the deal is I we've got X G boost for the next month or so I've got a couple of like odds-and-ends housekeeping videos that I just kind of want to put out I've wanted to update my p-value video in four years and I finally have it as a two-part how-to what is the p-value what is a p-value and how do you interpret it and then a second video on how to calculate they're both super short but I want to put those out just because I really need to update my p-value video and then after we've done those sort of odds and ends we're gonna do neural networks and after neural networks we're gonna do time series and so I think that'll be in the summer it could be early fall I'm looking forward to it there's a lot to learn and it's a pretty exciting field some people have been asking about Bayes naive Bayes that may be one of those odds and ends videos that comes out in a month or so we just have to see kind of how the scheduling goes I am gonna have to research neural networks for a while so there may be a little bit of a downtime where I'm just like I'm like I said I'm just reading every single thing I can think of as the topic and I'm trying to come up with a new way a way that nobody else has done before a new way to talk about neural networks because if I just repeat what everybody else says well you might as well just go to everybody else so I'm gonna try to come up with a stat quest way to talk about neural networks what else we got yeah I'm gonna take one more question see let's see what we got ah can I talk about Pearson and versus Spearman correlation and the answer is yes if that's an odds and ends video that comes out in a month or so it that would be great otherwise it'll come out maybe in between neural networks and time series that's something I've wanted to cover for a long time it's been on the to-do list for years different correlation metrics it's important to know all of them at least have a familiarity with them so that's the plan so for all these questions we have a bonus BAM so thank you very much for your questions I love it and thank you very much for joining my live stream it's a real treat to have everyone here and get all the support from everybody so until next time the end
sQ870aTKqiM,2020-01-15T17:27:49.000000,StatQuest: Random Forests Part 2: Missing data and clustering,random force part - yep it hooray it's true stead quest hello I'm Josh stommer and welcome to stack quest today we're doing random forests part two and we're gonna focus on missing data and sample clustering to be honest the sample clustering aspect of random forests is my favorite part so I'm really excited we're gonna cover it here's our data set we've got data for four separate patients however for patient number four we've got some missing data random forests consider two types of missing data one missing data in the original data set used to create the random forest and two missing data in a new sample that we want to categorize we'll start with this one so we want to create a random forest from this data however we don't know if this patient has blocked arteries or their weight the general idea for dealing with missing data in this context is to make an initial guess that could be bad and then gradually refine the guess until it is hopefully a good guess because this person did not have heart disease the initial and possibly bad guests for the blocked arteries value is just the most common value for blocked arteries found in the other samples that do not have heart disease among the people that do not have heart disease know is the most common value for blocked arteries it occurs in two out of two samples so no is our initial guess since weight is numeric our initial guess will be the median value of the patients that did not have heart disease in this case the median value is 160 7.5 here's our new data set with the filled in missing values now we want to refine these guesses we do this by first determining which samples are similar to the one with missing data so let's talk about how to determine similarity step 1 build a random forest step two run all of the data down all of the trees we'll start by running all of the data down the first tree dooba dooba dooba dooba dooba do dooba dooba dooba dooba dooba dooba do Duke do this boo notice that sample three and sample for both ended up at the same leaf node that means they're similar at least that's how similarity is defined in random forests we keep track of similar samples using a proximity matrix the proximity matrix has a row for each sample and it has a column for each sample because sample three and sample four ended up in the same leaf node we put a one here we also put a one here since this position also represents samples three and four because no other pair of samples ended in the same leaf node our proximity matrix looks like this after running the samples down the first tree now we run all of the data down the second tree bet it boop bop bop bop better but but buttered up oh but up up up but I don't bump up but out up buh-buh-buh note samples two three and four all ended up in the same leaf node this is what the proximity matrix looked like after running the data down the first tree and after the second tree we add one to any pair of samples that ended up in the same leaf node samples three and four ended up in the same node together again in sample two also ended up in that same node now we run all of the data down the third tree and here's the updated proximity matrix only samples three and four ended up in the same leaf node ultimately we run the data down all the trees and the proximity matrix fills in then we divide each proximity value by the total number of trees in this example assume we have ten trees now we use the proximity values for sample four to make better guesses about the missing data four blocked arteries we calculate the weighted frequency of yes and no using proximity values as the weights yes occurs in one third of the samples no occurs in two-thirds of the samples the weighted frequency for yes is the frequency of yes times the weight for yes the weight for yes equals the proximity of yes divided by all of the proximities the proximity for yes is the proximity value for sample to the only one with yes and we divide that by the sum of the proximities for sample for so the wait for yes is 0.1 thus the weighted frequency for yes is 0.03 the weighted frequency for no is the frequency of no which is 2/3 times the weight for now samples 1 & 3 both have no with that in mind we can plug in the values for the proximity of no / all proximities thus the weight for no is 0.9 and the weighted frequency for no is 0.6 no has a way higher weighted frequency so we'll go with it in other words our new improved and revised guessed based on the proximities is no for blocked arteries for weight we use the proximities to calculate a weighted average in this case the weighted average equals sample ones weight sample ones weighted average weight sorry if there's any confusion between a patient's weight or a samples weight and the weight used in the weighted average to calculate that weight we start with the proximity for sample 1 divided by the sum of the proximities so sample ones weighted average weight is 0.1 here's the weighted value for sample number two who weighs 180 here's the weighted average value for sample number three who weighs 210 ultimately the weighted average of weight is 198 point five and remember the weights that we used in the weighted average were based on proximity x' now that we've revised our guesses a little bit we do the whole thing over again we build a random forest run the data through the trees recalculate the proximities and recalculate the missing values we do this six or seven times until the missing values converge ie no longer change each time we recalculate BAM now it's time for an interlude of Awesomeness let me show you something super cool we can do with the proximity matrix this is the proximity matrix before we divided each value by 10 the number of trees in the pretend random forest just for the sake of easy math imagine if samples 3 & 4 ended up in the same leaf node in all 10 trees now we have a 10 here and here after dividing by 10 the number of trees in the forest we see that the largest number in the proximity matrix is 1 1 in the proximity matrix means the samples are as close as close can be that means 1 minus the proximity values equals distance closest can be equals no distance between and not close equals far away this is a distance matrix and that means we can draw heat map with it if you don't know what a heat map is check out the stat quest and we can also draw an MVS plot with it and if you don't know what an MDS plot is well check out the stat quest I think this is super cool because it means that no matter what the data are ranks multiple-choice numeric etc if we can use it to make a tree we can draw a heat map or an MDS plot to show how the samples are related to each other this is awesome ok enough fun stuff let's get back to the missing data problem at long last we'll talk about the second method this is when we have missing data in a new sample that we want to categorize imagine we had already built a random forest with existing data and wanted to classify this new patient so we want to know if they have heart disease or not but we don't know if they have blocked arteries so we need to make a guess about blocked arteries so we can run the patient down all the trees in the forest the first thing we do is create two copies of the data one that has heart disease and one that doesn't have heart disease then we use the iterative method we just talked about to make a good guess about the missing values these are the guesses that we came up with then we run the two samples down the trees in the forest and we see which of the two is correctly labeled by the random forest the most times this option was correctly labeled yes in all three trees this option was only correctly labeled no in one tree this option wins because it was correctly labeled more than the other option BAM we filled in the missing data and we've classified our sample hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
8b1JEDvenQU,2020-01-13T12:30:01.000000,XGBoost Part 2 (of 4): Classification,classification it's not a vacation it's not a sensation but it's cool step quest hello I'm Josh stormer and welcome to stack quest today we're gonna talk about XG boost part 2 XG Boost trees for classification note this stack quest assumes that you are already familiar with the main ideas of how XG boost does regression and at least the main ideas of how gradient boost is used for classification if not check out the quests the links are in the description below XG boost is extreme and that means it's a big machine learning algorithm with lots of parts the good news is that each part is pretty simple and easy to understand and we'll go through them one step at a time in part one in this series we provided an overview of how XG boost trees are built for regression in this video part two we'll give an overview of how XG boost trees are built for classification and in part three we'll dive into the mathematical details to show you how regression and classification are related and why creating unique trees makes so much sense note X G boost was designed to be used with large complicated data sets however to keep the examples from getting out of hand we will use this super simple training data consisting of four different drug dosages the green dots indicate the drug was effective and the red dots indicate that the drug was not effective the very first step in fitting XG boosts to training data is to make an initial prediction this prediction can be anything for example the probability of observing an effective dosage in the training data but by default it is 0.5 regardless of whether you are using XG boost for regression or classification in other words regardless of the dosage the default prediction is that there is a 50% chance the drug is effective we can illustrate the initial prediction by adding a y access to our graph to represent the probability that the drug is effective in drawing a thick black line at 0.5 to represent a 50% chance that the drug is effective since these two green dots represent effective dosages we will move them to the top of the graph where the probability that the drug is effective is 1 these two red dots represent ineffective dosages so we will leave them at the bottom of the graph where the probability that the drug is effective is zero the residuals the differences between the observed and predicted values show us how good the initial prediction is now just like we did for regression we fit an XG boost tree to the residuals however since we are using XG boost for classification we have a new formula for the similarity scores note even though the numerator looks fancy it's just the sum of the residuals squared in other words the numerator for classification is the same as the numerator for regression and just like for regression the denominator contains lambda the regularization parameter however the rest of the denominator is different the good news is that we already saw something just like this in regular on extreme gradient boost it's just the sum for each observation of the previously predicted probability times 1 minus the previously predicted probability note although this formula is different from what XG boost uses for regression it is very closely related and we will show you why in part 3 when we get into the nitty-gritty details now let's build a tree just like for regression each tree starts out as a single leaf and all of the residuals go to the leaf now we need to calculate a similarity score for the leaf and that means we plug all four residuals into the numerator note because we do not square the residuals before we add them together they will cancel each other out and we will end up with zero in the numerator and that makes the similarity score equal to zero and that's a little bit of a bummer since it doesn't give us a chance to talk about the denominator which is the interesting part however don't freak out we'll get to it soon for now let's just put similarity equals zero up here so we can keep track of it now we need to decide if we can do a better job clustering similar residuals if we split them into two groups we'll start with this threshold dosage less than 15 note we chose the threshold dosage less than 15 because 15 is the average value between the last two observations thus the three residuals with dosage is less than 15 go to the leaf on the left and the one residual with dosage greater than 15 goes to the leaf on the right to calculate the similarity score for the three residuals that ended up in the leaf on the Left we plug the three residuals into the numerator and since we are building the first tree the previous probability refers to the prediction from the initial leaf so we plug in zero point five for each residual that ended up in the leaf now just to keep things simple we'll let lambda equal zero however you know from part one that lambda reduces the similarity score which ultimately makes leaves easier to prune now notice that these two residuals in the numerator cancel each other out leaving us with just one residual in the numerator and when we do the math we get 0.33 so let's put similarity equals 0.33 under this leaf so we can keep track of it the similarity score for the leaf on the right is B to tutitu tutitu tutitu to to one when lambda equals zero so let's put similarity equals 1 under this leaf to keep track of it now we can calculate the gain just like we did when we used XG boost for regression we plug in the similarity scores pee pee poo pee pee poo poo poo poo and get 1.33 so when we split the observations based on the threshold dosage less than 15 gain equals one point three three since I'm such a nice guy I'm going to tell you that no other threshold gives us a larger gain value and that means dosage less than 15 will be the first branch in our tree now we can focus on splitting these residuals into two leaves note we can tell just by looking at the data that this threshold dosage less than five has a higher gain than this threshold dosage less than ten this is because when the threshold is dosage less than ten these two residuals will cancel each other out in the similarity score for this leaf will be zero so when we calculate the gain we get bit it bit bit bit bit bit bit bit bit bit zero point six six now let's compare that to the gain we get when the threshold is dosage less than five these are the similarity scores and when we plug them into the equation for the game bit lip it bit a bit bit but we get two point six six and since two point six six is greater than zero point six six we we use dosage less than five as the threshold for this branch now since I'm limiting the trees to two levels we will not split this leaf any further and we are done building this tree BAM note we stopped growing this tree because we limited the number of levels to two however --xg boost also has a threshold for the minimum number of residuals in each leaf warning it's time for some tedious detail and terminology the minimum number of residuals in each leaf is determined by calculating something called cover cover is defined as the denominator of the similarity score minus lambda in other words when we are using X G boosts for classification cover is equal to the sum of the previous probability times 1 minus the previous probability for each residual that's in the leaf in contrast when XG boost is used for regression and we are using this formula for the similarity score then cover is equal to the number of residuals in a leaf by default the minimum value for cover is one thus by default when we use XG boost for regression we can have as few as one residual per leaf in other words when we use XG boost for regression and use the default minimum value for cover cover has no effect on how we grow the tree contrast things are way more complicated when we use XG boost for classification because cover depends on the previously predicted probability of each residual in a leaf for example the cover for this leaf is the previously predicted probability for this observation which was 0.5 times 1 minus the previously predicted probability which is 0.25 and since the default value for the minimum cover is 1xg boost would not allow this leaf likewise the cover for this leaf is equal to 0.5 so by default XG boost would not allow this leaf either since these leaves are not allowed let's remove them and go back to this leaf because the previously predicted probability is the same for all three of these residuals cover is just 3 times the cover for one of the residuals and that means cover equals 0.75 so XG boost would not allow this leaf either ultimately if we use the default minimum value for cover 1 then we would be left with the root and XG boost requires trees to be larger than just the root so in order to prevent this from being the worst example ever let's set the minimum value for cover equal to zero and that means setting the men child weight parameter equal to zero small Bamm now we can talk about how to prune the tree just like we did in part one we prune by calculating the difference between the gain associated with the lowest branch and a number we pick for gamma for example if we plugged in the gain and Set gamma equal to two then we would not prune because the difference is a positive number contrast if we set gamma equal to three then we would prune because the difference is a negative number and we would also prune this branch because 1.3 3 minus 3 equals a negative number and all we would be left with is the original prediction small Bale now going back to the original tree remember from part 1 that lambda the regularization parameter reduces the similarity scores and that lower similarity scores result in lower values for gain for example if we set lambda equal to 1 then we would get these lower values for gain and that means a lower value for gamma will result in a negative difference and cause us to prune branches in other words values for lambda greater than zero reduce the sensitivity of the tree to individual observations by pruning and combining them with other observations BAM for now regardless of lambda and gamma let's assume that this is the tree we are working with and determine the output values for the leaves for classification the output value for a leaf is the sum of the residuals divided by the sum of the previous probability times 1 minus the previous probability for each residual in the leaf plus lambda note with the exception of lambda the regularization parameter this is the same formula we used for unex treem gradient boost so for this leaf we plug in the residual negative 0.5 and the previously predicted probability and the value for the regularization parameter lambda if lambda equals zero then there is no regularization and the output value equals negative two on the other hand if lambda equals 1 then the output value equals negative 0.4 which is closer to zero than negative two when lambda equals zero in other words when lambda is greater than zero then it reduces the amount that this single observation adds to the new prediction thus lambda the regularization parameter reduces the prediction sensitivity to isolated observations for now we'll let lambda equal zero because this is the default value and put negative two under the leaf so we will remember it similarly when lambda equals zero the output value for this leaf dudududududu dudududududududududududu boo-boo is to note if lambda equals one then we get 0.67 which is closer to zero but the effect of lambda is smaller this time because there are two observations in this leaf but like I said we'll let lambda equal zero since that is the default value and put two under the leaf so we will remember it lastly when lambda equals zero the output value for this leaf is negative two the first tree is complete double BAM now that we have built the first tree we can make new predictions just like other boosting methods as she boosted makes new predictions by starting with the initial prediction however just like with unex treem gradient boost for classification we need to convert this probability to a log odds value so since this is the formula that converts probabilities to odds we can get a formula that converts probabilities to the log odds by taking the log of both sides note if these equations are freaking you out just watch the stat quest on ODS and log odds the link is in the description below in this case we plug in P equals 0.5 do the math did it did it and we see that when P equals 0.5 the log of the odds equals zero so let's put that under the initial prediction so we don't forget now just like unex treem gradient boost for classification we add the log odds of the initial prediction to the output of the tree scaled by a learning rate XG boost calls the learning rate Etta and the default value is 0.3 so that's what we'll use thus the new predicted value for this observation with dosage equal to is the log of the odds for the original prediction 0 plus the learning rate 0.3 times the output value negative 2 and that gives us a log of the odds value equal to negative 0.6 to convert a log of the odds value into a probability we plug it into the logistic function note if the logistic function makes you feel a little uncomfortable check out the stat quest logistic regression details part 2 fitting a line with maximum likelihood assuming we're cool with this equation let's plug in the log of the odds do the math and the new predicted probability is 0.35 remember the original prediction was 0.5 and this was the original residual now the new predicted probability is 0.35 and the new residual is smaller than before so we have taken a small step in the right direction hooray note you may be wondering why we even bothered adding the log of the odds of the initial prediction since it is zero this is always the case if you use the default value 0.5 for the initial prediction however you can change the initial prediction to any probability and any value other than 0.5 will give you something more interesting to add for example if 75% of the observations in the training data said that the drug was effective we might set the initial prediction to 0.75 and now the initial log of the odds equals 1.1 so we would plug 1.1 into the equation instead of 0 but since the default initial prediction is 0.5 we will use 0 for the initial log of the odds in the remaining examples now let's make a new prediction for this observation with dosage equal to eight we start with the original log of the odds prediction zero plus zero point three times the output value two and the predicted log of the odds equals 0.6 now we convert the log of the odds to a probability and we get 0.65 residual is smaller than before so we've taken another small step in the right direction likewise the new predictions for the remaining observations have smaller residuals than before BAM now that we have new residuals we can build a second tree that is fit to the new residuals note when we build the second tree calculating the similarity scores is a little more interesting because the previous probabilities are no longer the same for all of the observations for example since all of the residuals start in the root of the tree we would plug in the previously predicted probabilities for each observation into the denominator and this time they are not all the same similarly if we had to calculate an output value for the root the denominator would also contain a mixture of previously predicted probabilities small BAM now that we have a new tree we add it to all of the previous predictions and make new predictions that give us even smaller residuals then we build another tree based on the new residuals and we keep building trees until the residuals are super small or we have reached the maximum number of trees triple bam in summary when building x/g boost trees for classification we calculate similarity scores and game to determine how to split the data and we prune the tree by calculating the difference between gain values and a user-defined tree complexity parameter gamma for example if we subtract gamma from this game and get a negative value we will prune otherwise we're done if we prune then we will subtract gamma from the next game value etc etc etc then we calculate the output values for the leaves and lastly lambda is a regularization parameter and when lambda is greater than zero it results in more pruning by shrinking the similarity scores and smaller output values for the leaves oh and I almost forgot when using XG boost for classification we have to be aware that the minimum number of residuals in a leaf is related to a metric called cover which is the denominator of the similarity score minus lambda tune in next time for XG boost part 3 when we dive deep into the nitty gritty details of the math that ties XG boost trees for regression and classification into one elegant equation hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
jaXexgj2ONQ,2020-01-07T02:28:08.000000,"Live 2020-01-06!!! Sample Sizes, ML vs Statistics and a Poem",that quest live-stream pray stat quest hello and welcome to my very first livestream ever the connection that seems to be coming and going so this may be a bust but we're gonna do the best we can oh I see it a lot of people have joined and that's wonderful I'm seeing a bunch of people in the comments that's really exciting oh so thank you for joining me on this historic day and so let's get started so as you guys all know I make videos and as you guys all know people comment on those videos and most of those comments are just awesome and I love them and they say nice things about the videos and me and that makes me feel really good um however there are some comments that really make me think and they're the kind of thing that I think everyone might be interested in hearing about and kind of maybe there's a lesson that we could all learn from some of these comments so so let's just get started here's oh I'm getting on I'm so excited that I'm just getting ahead of myself okay I've got three comments I've selected three comments that I'm really excited about that I want to share with you guys and then we'll have a little bit of time where you guys can post some comments in the in the chat and and there's also if you the other thing is I want to know I want you to know is that if you have thoughts about some of these comments of your own obviously you can put them in the chat I'll try they read them a lots going on right now but you can also put them in the comments and we can talk about them and think about them later all right so here's comment number one okay Jose Lopez by the way I'm terrible with names I'm gonna mispronounce everybody's name all night long so if I do that try not to take it personally anyways Jose Lopez said I don't understand the difference between a single sample of 20 observations and for Sam peoples of five observations so I think the best way to answer this question oh by the way this question comes up a lot a lot of people ask me this question a lot of researchers data scientists buying premonitions all kinds of people so I think the best way to answer this question is just to look at some data that's sort of the answer to every question isn't it okay so here's some data we've got four people Herman Beth Sally and Jim and each one measured five mice from head to tail their lengths and so instead of just getting one person to measure twenty mice all at once so the thing is is if you look on the left side you see Herman Herman is afraid of mice so he always measures the smallest mice that we can find okay now if we gotten Herman to measure all twenty mice this experiment would have been a total disaster by spreading it out by giving each person five mice to measure instead of one person twenty we can we can actually detect that there's something wrong with Herman's measurements that we wouldn't be able to detect if Herman had done everything himself so that's one reason it's a good idea to spread your data or your measurements out among different people holy smokes we just got a super chat cheese Aang wants to know if we can use ml or stats to save this Internet that's a good question we'll talk about that later anyways sorry for that distraction uh I just saw it in the corner of my eye and I've never seen a super chat before so that was the very first one ever so I apologize for the distraction anyways we were back discussing how it's such a good idea that we didn't give this whole job to herman because he's incompetent okay that said um one thing that's nice about the way we've done this experiment here by spreading it out can actually correct for Herman's mistakes what Herman did is called a batch effect that's a technical term and statistics lingo it just means there's something that affects all of Herman's measurements and and we can correct for that because we can do batch correction and we can only do that because we've got measurements from Beth Sallie and Jim who give us a better idea of what's really going on with how long these mice are that said if I got if I had two different diets a normal diet and a special diet for my mice and I had Herman do all of the measurements well if he collected all the smallest mice from the normal diet and then he collected all the novice the smallest mice from the special diet we may still be able to detect a difference because maybe all of the mice on a special diet even the smallest ones got a little proportionally bigger and so that's a so we you know all is not lost depending on how the experiment is done but generally speaking if we can get smaller groups of measurements rather than one huge collection of data we'll be better off because we can detect batch problems and weird things that happen during experiments maybe someone was having a bad day or you know there's something weird going on with cosmos who knows anyways the more date the more places and people and time points that you have when you collect your data the more likely it's going to be reproducible because it's not going to be just tied to one person BAM here's comet number two and this is like a philosophical question someone asked actually not just someone masteren to Gavi I'm doing the best I can with your name he asked or she asked oh this is really embarrassing everybody anyways the question is is machine learning a subset of statistics and I'll be honest I grew up I didn't grow up doing statistics but I have a statistics background that's how I was introduced to machine learning and I was introduced to all this stuff from Statistics land so when I first saw this question the first thing I thought of was of course of course machine learning is a subset of statistics especially as I've read or you know about the book called the introduction to statistical learning they don't even call it machine learning they call it statistical learning so of course it's a subset of statistics by the way if you don't know about that book check it out it's a free download it's fantastic a lot of my videos follow that book I mean yeah it's got some math in it it's kind of its kind of a math heavy at times but it's got a lot of good stuff in there and a lot of my videos follow the content in that book and it's a free download so check that out anyways so like I said I was reading the introduction to statistical learning so of course machine learning is a subset of statistics but the more I thought about it especially now that we're doing XG boost the more I'm realizing that there is something called data science and I'll be honest I don't actually know what data science means it's a kind of a new term and I I call myself a data scientist and I say that my videos are data science related but what does it really mean I don't know however I do think it's a little bit more than statistics and algorithm like XG boost does it's it's not just statistics it does and we're gonna learn about this in the coming weeks with the videos that come out it has a lot of optimization things that allow it to work with massive datasets datasets that are so large that I don't think statistics ever knew these things were possible these things are truly massive and it has a lot of implementation details 'xg boost unlike a lot of the other machine learning algorithms that we've talked about on videos XG boost cares about how large the cash is for your processor and it does this because it wants to optimize everything that's why it's called extreme gradient boost it's so extreme it optimizes every little step and that's what makes it so fast with super large datasets and so in order to learn an algorithm like XG boost you can't just learn the statistics side of things you got to learn the hardware you got to learn the computer science the computer engineering and that's what makes me think that machine learning is a little bit more than statistics maybe it's data science and if any guys know what data science is put it in the comments that'd be a great place to put it I'd love to learn more about what you guys think data science is okay so that gets a double BAM so that leads us to comment number three and comment number three is a poem and you know someone wrote a poem about stat quest and that's just the best thing I've ever seen so let me read you this poem that came from Sri pooja Maha voddy and I said that wrong and I'm never gonna say it again unless they write another beautiful poem me here it goes stat quest stat quest can you note that you are them so that's a triple bam right there the connection I looks like the connection is coming and going I hope that isn't too bad anyways anyways at this point I'd like to see what kind of comments you guys have uh if you have any questions I'd love to kind of try to read some of them I'm looking over at the comments so if it looks like I'm not looking at you it's because I'm scanning the comments and boy there are a ton so let's see if we got over here hello from Washington DC thank you for making this is from Chris ger Miller and I mispronounced that Chris I really apologize but hello from Washington G thank you for making this material always accessible it's definitely a stat quest I'm definitely a stat quest evangelist in all of us thank you so much Chris that's wonderful it looks like Thomas kneeled Thomas you're I'm a big fan of yours and I'm glad to see you here on the chat that's great anyways he was saying that he's been working on trying to understand a definition of data science so that's that's pretty cool I'm not the only one who's mildly confused by this actually that's a good question someone asked did some what did you guys hear the triple BAM I know my connections been kind of coming and going so if you didn't hear the triple BAM that's a big bummer but let me know and I'll make it happen again because he can't have a stat quest without the triple BAM so what else we got over here in the comments make a video on ANOVA and design of experience good thing I already have one so just check the stat quest homepage there's an index there and I've got a video on ANOVA so check that one out it's a good one it's an old one it's a classic but but it's still a good one so what else we got over here in the comments looks like some people stayed up too late someone wants to know if they should do a masters and Minchin learning in six months do you think it's worth doing all that effort putting in all that money that's a great question this is from Calvin I don't really know the answer to that I don't know what your background is if you're if you don't have a background in computing and statistics a crash course like that maybe too much too quick six months is a very short period of time to cover a ton of material and get a lot of programming jobs but if you're already on a spur grammar and you've already got some good analytical chops well we'll go for it but if you're starting from scratch six months is probably too short to get a good good education got another super chat from I can't even say that but I did get a super chat so thank you very much that's very friendly and what else we got Oh someone says they're an Oklahoma state and they have a data science degree based on sass and that's fine sass is fine a lot of people use as I was to be honest I was just looking on the job bulletin boards the other day and I saw a ton of jobs in sass Oh however I'm gonna be honest I'm gonna tell you I'm an AR guy I like are like Python I like the open source languages a little bit more all those asses just 30 miles down the down the road from where I live so I can't badmouth them too much maybe one day they'll support a stat quest and that'd be pretty cool someone else says looks like Daniel Romero Alvarez says time-series would be super cool so do some time series and I will say that here's the plan we've got XG boost at least for another month maybe two we've got a lot to learn about XG boost you didn't mean to burp on a live stream after that we're gonna go into neural networks and deep learning and I'm were excited about that because I've been promising people I would do neural networks and deep learning for a long time and we're just getting to it now and then after that I want to do time series I've also wanted to do time series for a long time so we're probably talking late spring early summer with time series I hate to say that I kind of tend to promise that I'll get to things earlier than I do and that's always a problem but but yeah time series is on the to-do list and it's very high there may be a few interludes there some dimension reduction methods called you map and there's some other ones that are pretty fancy and and I want to do a couple of quick videos on those because to be honest they apply to my work and it's always nice when I have an overlap between stat quest and work because then I can kind of do them at the same time and that's kind of bonus don't tell my boss okay what else do we got looking over at the comments seeing all kinds of stuff scrolling by real fast if we're calling ourselves data scientists and we don't know what data science is how do we get jobs okay so that can I tell you a story about graduate school when I graduated from college I got a job at a hospital doing database work and the job was great I had a lot of fun and I enjoyed that job but I really wanted to get an advanced degree largely because there I was sitting in a biology class at the College of Charleston I was biology 101 I never taken a biology class and I fell in love with biology and I wanted to learn how to apply what I knew computationally to biology and so I found out about something called bioinformatics and so I applied to NC State's program in Raleigh North Carolina and I got in and I had to interview well they didn't I I guess I didn't 100% get in but they said we'll interview you and then we'll tell you if you got in or not so I went to my interview and I talked to the head of my informatics it was a guy named Bruce weir and I said dr. weir I'm gonna be honest I don't know what a bio informatics is and I've applied to this program and I'm about to do this program and Bruce we're said you know bifur Maddox has a hat that I put on and when I put on this hat I'm available for more jobs than I was before but fundamentally I'm a statistician and I think the lesson from that that I'm trying to say about what data science is is is if you're a programmer if you're a coder if you're a statistician if you're learning machine learning if you're doing all those things call yourself a data science apply for the job and see what happens you'll interview or hopefully you'll interview and you'll find out if this is a good fit for you and they'll find out if you're a good fit for them I wouldn't try to sweat it too much if I see something in a job posting it says data science and the job looks interesting to me it's an interesting company it's an interesting location I'm gonna apply for that job and they'll look at my resume and they'll see the things that I've done they accomplished it's the classes I've taken the degrees that I've got and they'll say yeah this is our kind of guy let's have them in for an interview let's talk we'll figure out what data science is when we're talking let's not worry too much about the definition until then so let's just give it a shot so that's my advice there all right people thank you very much for participating in my very first ever stat quest live stream I know we got a lot of stuff going on but actually to be honest I didn't even think it was going to go half this long I've just been rambling here and and this whole thing has exceeded my expectations just seeing all the activity over on the right side is on the right side what I mean I that's where I see all the comments scrolling by and the chats look we're gonna do this again in two weeks and I'll
OtD8wVaFm6E,2019-12-16T14:00:04.000000,XGBoost Part 1 (of 4): Regression,XG boost its extreme and its gradient boost stat quest hello I'm Josh stormer and welcome to stat quest today we're gonna talk about XG boost part 1 we're gonna talk about XG boost trees and how they're used for regression note this stat quest assumes that you are already familiar with at least the main ideas of how gradient boost does regression and you should be familiar with at least the main ideas behind regularization if not check out the quests the links are in the description below XG boost is extreme and that means it's a big machine learning algorithm with lots of parts the good news is that each part is pretty simple and easy to understand and we'll go through them one step at a time actually I'm assuming that you are already familiar with gradient boost and regularization so we'll start by learning about XG boosts unique regression trees because this is a big topic will spend three whole stack quests on it in this stack quest part 1 will build our intuition about how XG boost does regression with its unique trees in part two we'll build our intuition about how XG boost does classification and in part three we'll dive into the mathematical details and show you how our aggression and classification are related and why creating unique trees makes so much sense note 'xg boost was designed to be used with large complicated datasets however to keep the examples from getting out of hand we'll use this super simple training data on the x-axis we have different drug dosages and on the y-axis we've measured drug effectiveness these two observations have relatively large positive values for drug effectiveness and that means that the drug was helpful these two observations have relatively large negative values for drug effectiveness and that means that the drug did more harm than good the very first step in fitting XG boost to the training data is to make an initial prediction this prediction can be anything but by default it is 0.5 regardless of whether you're using XG boost for regression or classification the prediction 0.5 corresponds to this thick black horizontal line and the residuals the differences between the observed and predicted values show us how good the initial prediction is now just like unex treem gradient boost XG boosts fits a regression tree to the residuals however unlike unex treem gradient boost which typically uses regular off-the-shelf regression trees XG boost uses a unique regression tree that I call an XG boost tree so let's talk about how to build an XG boost tree for regression note there are many ways to build XG boost trees this video focuses on the most common way to build them for regression each tree starts out as a single leaf and all of the residuals go to the leaf now we calculate a quality score or similarity score for the residuals similarity score equals the sum of the residuals squared over the number of residuals plus lambda note lambda is a regularization parameter and we'll talk more about that later for now let lambda equals zero now we plug the for residuals into the numerator and since there are four residuals in the leaf we put a four in the denominator note because we do not square the residuals before we add them together in the numerator 7.5 and negative 7.5 cancel each other out in other words when we add this residual to this residual they cancel each other out likewise 6.5 cancels out most of negative 10.5 leaving negative 4 squared in the numerator thus the similarity score for the residuals in the root equals 4 so let's put similarity equals 4 up here so we can keep track of it now the question is whether or not we can do a better job clustering similar residuals if we split them into two groups to answer this we first focus on the two observations with the lowest dosages average dosage is 15 and that corresponds to this dotted red line so we split the observations into two groups based on whether or not the dosage is less than 15 the observation on the far left is the only one with dosage less than 15 so it's residual goes to the leaf on the left all of the other residuals go to the leaf on the right now we calculate the similarity score for the leaf on the left by plugging the 1 residual into the numerator and since only one residual went to the leaf on the Left the number of residuals equals 1 like before we set lambda equal to zero and the similarity score for the leaf on the Left equals 110 0.25 so let's put similarity equals 110 0.25 under the leaf so we can keep track of it and calculate the similarity score for the residuals that go to the leaf on the right we plug in the sum of residuals squared into the numerator and since there are three residuals in the leaf on the right we plug 3 into the denominator like before let's let lambda equal zero note like we saw earlier because we do not square the residuals before we add them together 7.5 and negative 7.5 cancel each other out leaving only one residual 6.5 in the numerator thus the similarity score for the residuals in the leaf on the right equals 14 point zero eight so let's put similarity equals 14 point zero eight under the leaf so we can keep track of it now that we have calculated similarity scores for each node we see that when the residuals in a node are very different they cancel each other out and the similarity score is relatively small in contrast when the residuals are similar or there is just one of them they do not cancel out and the similarity score is relatively large now we need to quantify how much better the leaves cluster similar residuals than the root we do this by calculating the gain of splitting the residuals into two groups gain is equal to the similarity score for the leaf on the Left plus the similarity score for the leaf on the right minus the similarity score for the root plugging in the numbers eep-eep eep-eep eep-eep eep-eep eep-eep gives us 120 point three three small BAM now that we have calculated the gain for the threshold of dosage less than 15 we can compare it to the gain calculated for other thresholds so we shift the threshold over so that it is the average of the next two observations and build a simple tree that divides the observations using the new threshold dosage less than twenty two point five now we calculate the similarity scores for the leaves and calculate the game pppp pppp pppp de boop boop the game for dosage less than twenty two point five is four since the gain for dosage less than twenty two point five is less than the game for dosage less than fifteen dosage less than fifteen is better at splitting the residuals into clusters of similar values now we shift the threshold over so that it is the average of the last two observations and build a simple tree that divides the observations using the new threshold dosage less than 30 then we calculate the similarity scores for the leaves and the gain doot-doot doot-doot doot-doot doot-doot doot-doot doot-doot doot-doot doot-doot the game for dosage less than thirty equals fifty six point three three again since the gain for dosage less than 30 is less than the game for dosage less than 15 dosage less than 15 is better at splitting the observations and since we can't shift the threshold over any further to the right we are done comparing different thresholds and we will use the threshold that gave us the largest gain dosage less than 15 for the first branch in the tree BAM now since there is only one residual in the leaf on the Left we can't split it any further however we can split the three residuals in the leaf on the right so we start with these two observations and their average dosage is twenty-two point five which corresponds to this dotted green line so the first threshold that we try is dosage less than twenty two point five now just like before we calculate the similarity scores for the leaves note we calculated the similarity score for this node when we figured out how to split the root so now we calculate the game dududududu dudududududududu and we get gain equals twenty eight point one seven four when the threshold is dosage less than twenty two point five now we shift the threshold over so that it is the average of the last two observations calculate the similarity scores for the leaves and the gain doo-doo-doo-doo - and we get gain equals one hundred forty point one seven which is much larger than twenty eight point one seven when the threshold was dosage less than twenty two point five so we will use dosage less than 30 as the threshold for this branch note to keep this example from getting out of hand I've limited the tree depth to two levels and this means we will not split this leaf any further and we are done building this tree however the default is to allow up to six levels small BAM now we need to talk about how to prune this tree we prune an XG boost tree based on its gain values we start by picking a number for example 130 oh no it's the dreaded terminology alert XG Boost calls this number gamma we then calculate the difference between the gain associated with the lowest branch in the tree and the value for gamma difference between the gain and gamma is negative we will remove the branch and if the difference between the gain and gamma is positive we will not remove the branch in this case when we plug in the game in the value for gamma 130 we get a positive number so we will not remove this branch and we are done pruning note the gain for the route 120 point three is less than 130 the value for gamma so the difference will be negative however because we did not remove the first branch we will not remove the route in contrast if we set gamma equal to 150 then we would remove this branch because 140 point 1 7 minus 150 equals a negative number so let's remove this branch now we subtract gamma from the gain for the route since 120 point 3 3 minus 150 equals a negative number we will remove the root and all we would be left with is the original prediction which is pretty extreme pruning so while this wasn't the most nuanced example of how an X G boost tree is pruned I hope you get the idea now let's go back to the original residuals and build a tree just like before only this time when we calculate similarity scores we will set lambda equal to one remember lambda is a regularization parameter which means that it is intended to reduce the prediction sensitivity to individual observations now the similarity score for the root is 3.2 which is 8/10 of what we got when lambda equals 0 when we calculate the similarity score for the leaf on the Left we get fifty five point one two which is half of what we got when lambda equals zero and when we calculate the similarity score for the leaf on the right we get ten point five six which is three-quarters of what we got when lambda equals zero so one thing we see is that when lambda is greater than zero the similarity scores are smaller and the amount of decrease is inversely proportional to the number of residuals in the node in other words the leaf on the Left had only one residual and it had the largest decrease in similarity score 50% in contrast the root had all four residuals in the smallest decrease 20% now when we calculate the gain we get 66 which is a lot less than 120 point 3 3 the value weak out when lambda equals 0 similarly when lambda equals one the game for the next branch is smaller than before now just for comparison these were the gain values when lambda equals zero when we first talked about pruning trees we set gamma equal to 130 and because for the lowest branch in the first tree gain minus gamma equaled a positive number so we did not prune at all now with lambda equals 1 the values for gain are both less than 130 so we would prune the whole tree away so when lambda is greater than zero it is easier to prune leaves because the values for gain are smaller note before we move on I want to illustrate one last feature of lambda for this example imagine we split this node into two leaves now let's calculate the similarity scores with lambda equal to one for the branch we get sixty five point three for the left leaf we get twenty one point one two and for the right leaf we get 28.1 to that means the gain is negative sixteen point zero six now when we decide if we should prune this branch we plug in the game and we plug in a value for gamma note if we set gamma equal to zero then we will get a negative number and we will prune this branch even though gamma equals zero in other words setting gamma equal to zero does not turn off pruning - on the other hand by setting lambda equal to 1 lambda did what he was supposed to do it prevented overfitting the training data awesome for now regardless of lambda and gamma let's assume that this is the tree we are working with and determine the output values for the leaves the output value equals the sum of the residuals divided by the number of residuals plus lambda note the output value equation is like the similarity score except we do not square the sum of the residuals so for this leaf we plug in the residual negative 10.5 the number of residuals in the leaf 1 and the value for the regularization parameter lambda if lambda equals zero then there is no regularization in the output value equals negative 10.5 on the other hand if lambda equals one the output value equals negative five point two five in other words when lambda is greater than zero then it will reduce the amount that this individual observation adds to the overall prediction thus lambda the regularization parameter will reduce the prediction sensitivity to this individual observation for now we'll keep things simple and let lambda equal zero because this is the default value and put negative 10.5 under the leaf so we will remember it now let's calculate the output value for this leaf when lambda equals zero the output value is seven in other words when lambda equals zero the output value for a leaf is simply the average of the residuals in that leaf so we'll put the output value under the leaf so we will remember it lastly when lambda equals zero the output value for this leaf is negative seven point five now at long last the first tree is complete double BAM since we have built our first tree we can make new predictions and just like on extreme gradient boost XG boost makes new predictions by starting with the initial prediction and adding the output of the tree scaled by a learning rate oh no it's another dreaded terminology alert XG boost calls the learning rate Etta and the default value is 0.3 so that's what we'll use thus the new predicted value for this observation with dosage equal to 10 is the original prediction 0.5 plus the learning rate Etta 0.3 times the output value negative 10.5 and that gives us negative 2.6 5 so if the original prediction was 0.5 then this was the original residual the new prediction is negative two point six five and we see that the new residual is smaller than before so we've taken a small step in the right direction similarly the new prediction for this observation with dosage equal twenty is beep boop boop boop beep boop beep boop 2.6 and the new residual is smaller than before so we've taken another small step in the right direction likewise the new predictions for the remaining observations have smaller residuals than before suggesting each small step was in the right direction BAM now we build another tree based on the new residuals and make new predictions that give us even smaller residuals and then build another tree based on the newest residuals and we keep building trees until the residuals are super small or we have reached the maximum number triple bam in summary when building XG boost trees for regression we calculate similarity scores gain to determine how to split the data prune the tree by calculating the differences between gain values and a user defined a tree complexity parameter gamma if the difference is positive then we do not prune if it's negative then we prune for example if we subtract gam off from this game and get a negative value we will prune otherwise were done if we prune then we will subtract gamma from the next game value and work our way up the tree then we calculate the output values for the remaining leaves and lastly lambda is a regularization parameter and when lambda is greater than zero it results in more pruning by shrinking the similarity scores and it results in smaller output values for the leaves BAM tune in next time for XG boost part 2 when we give an overview of how XG boost trees are built for classification it's going to be totally awesome hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
vP06aMoz4v8,2019-12-02T14:07:38.000000,Machine Learning Fundamentals: Sensitivity and Specificity,when the well runs dry you might be thirsty but this still StatQuest you can watch it StatQuest hello I'm Josh Starmar and welcome to StatQuest today we're gonna continue our series on machine learning fundamentals and we're going to talk about sensitivity and specificity they're gonna be clearly explained this StatQuest follows up on the one that describes the confusion matrix so if you're not already down with that check out the quest the first half of this video will explain how to calculate and interpret sensitivity and specificity when you have a confusion matrix with two rows and two columns and the second half will show you how to calculate and interpret sensitivity and specificity when you have three or more rows and columns even if you're already down with the confusion matrix let's remember that rows correspond to what was predicted and columns correspond to the known truth when there are only two categories to choose from in this case the two choices were has heart disease or does not have heart disease then the top left-hand corner contains the true positives true positives are patients that had heart disease that were also predicted to have heart disease true negatives are in the bottom right hand corner true negatives are patients that did not have heart disease and were predicted not to have heart disease the bottom left-hand corner contains the false negatives false negatives are when a patient has heart disease but the prediction said they didn't lastly the top right hand corner contains the false positives false positives are patients that do not have heart disease but the prediction says that they do once we filled out the confusion matrix we can calculate two useful metrics sensitivity and specificity in this case sensitivity tells us what percentage of patients with heart disease were correctly identified sensitivity is the true positives divided by the sum of the true positives and the false negatives specificity tells us what percentage of patients without heart disease were correctly identified specificity are the true negatives divided by the sum of the true negatives and the false positives in the StatQuest on the confusion matrix we applied logistic regression to a testing data set and ended up with this confusion matrix let's start by calculating sensitivity for this logistic regression here's the formula for sensitivity and for true positives we plug in 139 and for false negatives we plug in 32 when we do the math we get zero point eight one sensitivity tells us that 81% of the people with heart disease were correctly identified by the logistic regression model now let's calculate the specificity here's the formula for specificity and for true negatives we will plug in 112 and for false positives we will plug in 20 when we do the math we get 0.85 specificity tells us that 85% of the people without heart disease were correctly identified by the logistic regression model now let's calculate sensitivity and specificity for the random forest model that we used in the confusion matrix StatQuest here's the confusion matrix here's the formula for sensitivity and when we plug in the numbers we get zero point eight three here's the formula for specificity and when we plug in the numbers we get zero point eight three again now we can compare the sensitivity and specificity values that we calculated for the logistic regression to the values we calculated for the random forest sensitivity tells us that the random forest is slightly better at correctly identifying positives which in this case are patients with heart disease specificity tells us that logistic regression is slightly better correctly identifying negatives which in this case are patients without heart disease we would choose the logistic regression model if correctly identifying patients without heart disease was more important than correctly identifying patients with heart disease alternatively we would choose the random forest model if correctly identifying patients with heart disease was more important than correctly identifying patients without heart disease BAM in the confusion matrix stat quest we calculated this confusion matrix when we tried to predict someone's favorite movie now let's talk about how to calculate sensitivity and specificity when we have a confusion matrix with three rows and three columns the big difference when calculating sensitivity and specificity for larger confusion matrices is that there are no single values that work for the entire matrix instead we calculate a different sensitivity and specificity for each category so for this confusion matrix we'll need to calculate sensitivity and specificity for the movie troll 2 for the movie Gore police and for the movie cool as ice let's start by calculating sensitivity for troll 2 for troll 2 there were 12 true positives people that were correctly predicted to love troll 2 more than Gore police and cool as ice so for true positives we'll plug in 12 and there were 112 plus 83 which equals 195 false negatives people that love to troll 2 but were predicted to love Gore police or cool as ice so for false negatives will plug in 195 and when we do the math we get 0.06 sensitivity for troll 2 tells us that only 6% of the people that loved the movie troll 2 more than Gore police or cool as ice were correctly identified now let's calculate the specificity for troll 2 there were 23 plus 77 plus 92 plus 17 equals 209 true negatives people that were correctly predicted to like Gore police or cool as ice more than troll 2 so for true negatives will plug in 209 and there were 102 plus 93 equals 195 false positives people that loved gore police or cool as ice the most but were predicted to love troll 2 so for false positives will plug in 195 and when we do the math we get 0.52 specificity for troll 2 tells us that 52% of the people who loved Gore police or cool as ice more than troll 2 were correctly identified calculating sensitivity and specificity for Gore police is very similar let's start by calculating sensitivity there are 23 true positives people that were correctly predicted to love Gore police the most and 102 plus 92 equals 194 false negatives people who loved Gore police the most but were predicted to love troll 2 or cool as ice more when we do the math we get 0.11 sensitivity for Gore police tells us that only 11% of the people that loved Gore police were correctly identified now let's calculate specificity there were 12 plus 93 plus 83 plus 17 equals 205 true negatives people correctly identified as loving troll 2 or cool as ice more than gore police and 112 plus 77 equals 189 false positives people predicted to love gore police even though they didn't and when we do the math we get 0.52 specificity for gore police tells us that 52% of the people that loved troll 2 or cool as ice more than gore police were correctly identified lastly calculating sensitivity and specificity for cool as ice follows the same steps we identify the true positives the false positives the true negatives and the false negatives and then plug in the numbers first for sensitivity then for specificity double bam if we had a confusion matrix with four rows and four columns then we would have to calculate sensitivity and specificity for four different categories little bam in summary sensitivity equals the true positives divided by the sum of the true positives and the false negatives and specificity equals the true negatives divided by the sum of the true negatives and the false positives we can use sensitivity and specificity to help us decide which machine learning method would be best for our data if correctly identifying positives is the most important thing to do with the data we should choose a method with higher sensitivity if correctly identifying negatives is more important then we should put more emphasis on specificity hooray we've made it to the end of another exciting StatQuest if you like this StatQuest and want to see more please subscribe and if you want to support stack quest well consider buying one or two of my original songs alright until next time quest on
D0efHEJsfHo,2019-11-25T10:59:44.000000,"How to Prune Regression Trees, Clearly Explained!!!","smelly stat smelly stat how are they training you I hope they're using stat quest hello I'm Josh stormer and welcome to stat quest today we're going to talk about how to prune regression trees there are several methods for pruning regression trees the one we'll talk about in this quest is called cost complexity pruning aka weakest link pruning we'll start by giving a general overview of how cost complexity pruning works and then will describe how it's used to build regression trees note this stat quest assumes that you are already familiar with regression trees if not check out the quest the link is in the description below also note this stat quest assumes that you are already familiar with cross-validation if not check out the quest in the stack quest on regression trees we had this data given different drug dosages on the x-axis we measured the drug effectiveness on the y-axis when the drug dosage was too low or too high the drug was not effective medium dosages were very effective and moderately high dosages were moderately effective we then fit a regression tree to the data and each leaf correspond to the average drug effectiveness from a different cluster of observations this tree does a pretty good job reflecting the training data because each leaf represents a value that is close to the data however what if these red circles were testing data these three observations are pretty close to the predicted values so their residuals the difference between the observed and predicted values are not very large similarly the residuals for these observations in the testing data are relatively small however the residuals for these observations are larger than before and the residuals for these observations are much larger these four observations from the training data with 100% drug effectiveness now look a little bit like outliers and that means that we over fit the regression tree to the training data one way to prevent over-fitting a regression tree to the training data is to remove some of the leaves and replace the split with a leaf that is the average of a larger number of observations now all of the observations between 14.5 and 29 go to the leaf on the far right the large residuals tell us that the Nutri doesn't fit the training data as well as before but the new subtree does a much better job with the testing data thus the main idea behind pruning a regression tree is to prevent overfitting the training data so that the tree will do a better job with the testing data BAM note if we wanted to prune the tree more we could remove these two leaves and replace the split with a leaf that is the average of a larger number of observations and we could then remove these two leaves and replace the split with a leaf that is the average of all of the observations so the question is how do we decide which tree to use in this stat quest we will answer that question with cost complexity pruning the first step in cost complexity pruning is to calculate the sum of the squared residuals for each tree in this example we'll start with the original full-sized tree here is the original full-sized tree some of the squared residuals for the observations with dosages less than 14.5 is d tu tutitu tutitu tu tu 320 point 8 so we'll save that sum of squared residuals underneath the corresponding leaf some of squared residuals for observations with dosages greater than or equal to 29 is 75 some of squared residuals for observations with dosages greater than or equal to 23 in less than 29 is 140 8.8 in the sum of squared residuals for observations with dosages greater than or equal to 14.5 in less than twenty three point five is zero thus the total sum of squared residuals for the whole tree is 320 plus seventy five plus one hundred forty eight point eight plus zero equals five hundred forty three point eight so let's put SSR equals five hundred forty three point eight on top of the original full-sized tree now let's calculate the sum of squared residuals for the subtree with one fewer leaf going back to the data some of squared residuals for when dosage is less than 14.5 is the same as before and it's the same for when dosage is greater than or equal to 29 but we have to calculate a new sum of squared residuals for when the dosage is between 14.5 and 29 thus the total sum of squared residuals for this tree is three hundred twenty plus seventy five plus five thousand ninety nine point eight which equals five thousand four hundred ninety four point eight so let's put SSR equals five thousand four hundred ninety four point eight on top of the sub tree with three leaves similarly the sum of squared residuals for the subtree with two leaves is nineteen thousand two hundred forty three point seven so we put SSR equals nineteen thousand two hundred forty three point seven on top of the subtree with two leaves lastly the sum of squared residuals for the subtree with only one leaf is twenty eight thousand eight hundred ninety seven point two so let's put SSR equals twenty eight thousand eight hundred ninety seven point two on top of the sub tree with one leaf note the sum of squared residuals is relatively small for the original full-size tree but each time we remove a leaf the sum of squared residuals gets larger and larger however we knew that was going to happen because the whole idea was for the pruned trees to not fit the training data as well as the full sized tree so how do we compare these trees weakest link pruning works by calculating a tree score that is based on the sum of squared residuals for the tree or subtree and a tree complexity penalty that is a function of the number of leaves or terminal nodes in the tree or subtree the tree complexity penalty compensates for the difference in the number of leaves note alpha is a tuning parameter that we find using cross-validation and we'll talk more about it in a bit for now let's let alpha equal 10,000 now let's calculate the tree score for each tree the tree score for the original full-sized tree is the total SSR for the tree which is five hundred forty three point eight plus ten thousand times T the total number of leaves which is four so the tree score for the original full-size tree is forty thousand five hundred forty three point eight now let's save the tree score below the tree and calculate the tree score for the subtree with one fewer leaf the sum of squared residuals for this subtree is five thousand four hundred ninety four point eight and since there are three leaves T equals three and the total tree score equals thirty-five thousand four hundred ninety four point eight the tree score for the subtree with two leaves is be boo-boo-boo-boo-boo-boo-boop thirty-nine thousand two hundred forty three point seven lastly the tree score for the subtree with only one leaf is EP boopy boopy boopy boopy 38 thousand eight hundred ninety seven point two note because alpha equals 10,000 the tree complexity penalty for the tree with one leaf was 10,000 and the tree complexity penalty for the tree with two leaves was 20,000 and the tree complexity penalty for the tree with three leaves was 30,000 and the tree complexity penalty for the original full size tree with four leaves was 40,000 thus the more leaves the larger the penalty of calculated tree scores for all of the trees we pick this subtree because it has the lowest tree score double bam note if we set alpha equals 22,000 and calculate the tree scores we would use the subtree with only one leaf because it has the lowest tree score thus the value for alpha makes a difference in our choice of subtree so let's talk about how to build a pruned regression tree and how to find the best value for alpha first using all of the data build a full sized regression tree note this full size tree is different than before because it was fit to all of the data not just the training data also note this full-size tree has the lowest tree score when alpha equals zero this is because when alpha equals zero the tree complexity penalty becomes zero and the tree score is just the sum of the squared residuals and as we saw earlier all of the sub trees will have larger sum of squared residuals so let's put alpha equals zero here to remind us that this tree has the lowest tree score when alpha equals zero now we will increase alpha until pruning leaves will give us a lower tree score in this case when alpha equals 10,000 we'll get a lower tree score if we remove these leaves and use this sub tree now we increase alpha again until pruning leaves will give us a lower tree score in this case when alpha equals 15,000 we will get a lower tree score if we remove these leaves and use this sub tree instead and when alpha equals 22,000 we will get a lower tree score if we remove these leaves and use this subtree instead in the end different values for alpha give us a sequence of trees from full sized to just a leaf now go back to the full data set and divided into training and testing data sets and just using the training data use the Alpha values we found before to build a full tree and a sequence of sub trees that minimize the tree score in other words when alpha equals zero we build a full-sized tree since it will have the lowest tree score however when alpha equals 10,000 we will get a lower tree score if we prune these leaves and use this tree instead and when alpha equals 15,000 we will get a lower tree score if we prune these leaves and use this tree instead lastly when alpha equals 22,000 we will get a lower tree score if we prune these two leaves and use this tree instead now calculate the sum of squared residuals for each new tree using only the testing data in this case the tree with alpha equals 10,000 had the smallest sum of squared residuals for the testing data now we go back and create new training data and a new testing data just using the new training data build a new sequence of trees from full sized to a leaf using the Alpha values we found before then we calculate the sum of squared residuals using the new testing data this time the tree with alpha equals zero had the lowest sum of squared residuals now we just keep repeating until we have done 10-fold cross-validation and the value for alpha that on average gave us the lowest sum of squared residuals with the testing data is the final value for alpha in this case the optimal trees built with alpha equals 10,000 had on average the lowest sum of squared residuals so alpha equals 10,000 is our final value lastly we go back to the original trees and sub trees made from the full data and pick the tree that corresponds to the value for alpha that we selected sub-tree will be the final pruned tree triple bam hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on"
Qc5IyLW_hns,2019-11-04T13:00:06.000000,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),the radio Colonel works in infinite dimensions I know that sounds kind of crazy but it's actually not that bad stat quest hello I'm Josh Armour and welcome to stat quest today we're gonna talk about support vector machines part 3 the radial kernel specifically we're going to talk about the radial kernels parameters how the radio kernel calculates high dimensional relationships and then show you how the radial kernel works in infinite dimensions note this stack quest assumes that you are already familiar with support vector machines and the polynomial kernel if not check out the quests the links are in the description below in the stack quest on support vector machines we had a training data set based on drug dosages measured in a bunch of patients the red dots represented patients that were not cured and the green dots represented patients that were cured in other words the drug doesn't work if the dosage is too small or too large it only works when the dosage is just right because this training data set had so much overlap we were unable to find a satisfying support vector classifier to separate the patients that were cured from the patients that were not cured one way to deal with overlapping data is to use a support vector machine with a radial kernel aka the radial basis function RBF because the radio Colonel find support vector classifier z' in infinite dimensions it's not possible to visualize what it does however when using it on a new observation like this the radio kernel behaves like a weighted nearest neighbor model in other words the closest observations aka the nearest neighbors have a lot of influence on how we classify the new observation and the observations that are further away have relatively little influence on the classification so since these observations are the closest to the new observation the radio kernel uses their classification for the new observation now let's talk about how the radio Colonel determines how much influence each observation in the training dataset has on classifying new observations just like with the polynomial kernel a and B refer to two different dosage measurements the difference between the measurements is then squared giving us the squared distance between the two observations thus the amount of influence one observation has on another is a function of the squared distance gamma which is determined by cross-validation scales the squared distance and thus it scales the influence for example if we set gamma equal to one plug in the dosages from two observations that are relatively close to each other and do the math peepee poopy poop we get 0.11 when gamma equals 1 so let's put 0.11 here now let's set gamma equal to two and plug in the same two dosages as before and do the math beep boop boop when gamma equals 2 we get 0.01 which is less than when gamma equals 1 so we see that by scaling the distance gamma scales the amount of influence two points have on each other small BAM now let's at gamma equal to 1 again and determine how much influence two observations have when they are relatively far from each other so we plug in the two dosages do the math are relatively far from each other we get a number very close to zero thus the further two observations are from each other the less influence they have on each other note just like with the polynomial kernel when we plug values into the radial kernel we get the high-dimensional relationship thus 0.11 is the high dimensional relationship between these two observations that are relatively close to each other and a number very close to zero is the high dimensional relationship between these two observations that are relatively far from each other medium Bamm now before we move on I want to simplify the training data set to just two observations and use the polynomial kernel to give us intuition into how the radio kernel works in infinite dimensions when R equals zero the polynomial kernel simplifies to a single term and that gives us a dot product with a single coordinate when D equals 2 we get a squared times B squared which is equal to the dot product of a squared and B squared since this dot product only has one coordinate the new coordinate is just the square of the original measurement on the original axis for example if we plug these two dosages into the kernel we get this dot product and that means the new XS coordinate for this point is two point five squared which equals six point two five and the new x-axis coordinate for this point is four squared which equals 16 in other words when R equals zero and D equals two all the polynomial kernel does is shift the data down the original axis when R equals 0 and D equals 3 we still only have one coordinate in the dot product and we shift the data down the X access further because now we are cubing each value lastly when R equals zero and D equals one we still only have one coordinate in the dot product but now the data just stays in its original location so setting R equals zero seems silly because no matter what values we use for D the dot products leave the data in the original dimension and in this example the data stays on the same one-dimensional line however it turns out that setting R equals zero can result in some pretty awesome stuff going back to the original training data set let's talk about what happens if we take a polynomial kernel with R equals 0 and D equals 1 and add another polynomial kernel with R equals 0 and D equals 2 this gives us a dot product with coordinates for two dimensions the first coordinate is the original dosage and the second coordinate is dosage squared now we can plot the transform data on XY axes and find a support vector classifier to separate the data and by now you know that we don't actually do the transformation we just solve for the dot product to get the high dimensional relationships now if we added another polynomial kernel with R equals zero and D equals three then the dot product has coordinates for three dimensions and we can plot the transform data on XYZ axes and find a support vector classifier to separate the data now what if we just kept adding polynomial kernels with our equals zero and increasing D until D equals infinity that would give us a dot product with coordinates for an infinite number of dimensions that would be awesome right well that's exactly what the radio Colonel does so let's talk about it warning this part gets very mathy so feel free to skip to the end if this is not your thing let's start with the radial kernel and multiply out the square beep OOP OOP OOP OOP now because we can set gamma equal to anything let's set it to 1/2 so that this 2 goes away now let's create the Taylor series expansion of this last term wait a minute what's a Taylor series expansion this big thing is a Taylor series although there are exceptions the main idea is that a function f of X can be split into an infinite sum since this is very abstract let's walk through how to convert e to the X into an infinite sum in other words we are setting f of X equal to e to the X and that means F of a equals e to the a now we plug in the derivative of e to the X evaluated at a divided by 1 factorial and multiplied by X minus a note in case you don't already know the derivative of e to the X equals e to the X so taking the derivative of e to the X is super easy now we plug in the second derivative of e to the X evaluated at a divided by 2 factorial and multiplied by X minus a squared then we keep adding terms based on higher derivatives until we get to infinity thus this is the Taylor series expansion of e to the X now the question is what is a the definition of the Taylor series says that a can be any value as long as F of a exists and since e to the zero equals one e to the zero exists so we will set a equal to zero and simplify peepee poopy poopy poop thus if we can accept that the Taylor series expansion does what it says it does e to the X is equal to this infinite sum large BAM going back to the radial kernel we can now create the Taylor series expansion of this last term to create the Taylor series expansion of e to the a B we plug in a B for X now we have the Taylor series expansion of the last part of the radial kernel okay time to take a deep breath we've done a lot but we still have a few more steps before we are done and get to eat snacks before we move on let's remember that when we added up a bunch of polynomial kernels with R equals 0 and D going from 0 to infinity we got a dot product with coordinates for an infinite number of dimensions now observe that a polynomial kernel with R equals 0 and D equals 0 is equal to 1 and a polynomial kernel with R equals 0 and D equals 1 is equal to a times B and a polynomial kernel with R equals 0 and D equals to is equal to a times B squared etc etc etc thus each term in this Taylor series expansion contains a polynomial kernel with R equals 0 and D going from 0 to infinity now just to remind you converting this sum to a dot product was easy because the dot product tells us to multiply each term together and then add up all the terms with that in mind the dot product for e to the a B is this we can verify that the dot product is correct by multiplying each term together and add up the new terms to get the Taylor series expansion of e to the a B double bam back to the original radial kernel we can plug in the dot product for e to the a B now the radial kernel is equal to this term times the dot product to make the radio Colonel all one dot product instead of something times a dot product we just multiply both parts of the dot product by the square root of this term so we can fit everything onto the screen let's let s equal the square root of the first term now we multiply the dot product by s be boo boo boo boo boo boo boo boo boo boo and at long last we see that the radial kernel is equal to a dot product that has coordinates for an infinite number of dimensions that means when we plug numbers into the radial kernel and do the math be boo-boo-boo the value we get at the end is the relationship between the two points in infinite dimensions triple bam now we can go eat snacks hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
Toet3EiSFcM,2019-11-04T12:00:05.000000,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),a once knew a colonel its name was Fred the stat quest isn't about that Colonel stat quest hello I'm Josh stormer and welcome to stat quest today we're going to talk about support vector machines part two the polynomial kernel specifically we're going to talk about the polynomial kernels parameters and how the polynomial kernel calculates high-dimensional relationships note this stat quest assumes that you are already familiar with support vector machines if not check out the quest the link is in the description below in the stack quest on support vector machines we had a training data set based on drug dosages measured in a bunch of patients the red dots represented patients that were not cured and the green dots represented patients that were cured in other words the drug doesn't work if the dosage is too small or too large it only works when the dosage is just right because this training data set had so much overlap we were unable to find a satisfying support vector classifier to separate the patients that were cured from the patients that were not cured however when we gave each point a Y access coordinate by squaring the original dosage measurements we could draw a line that separated the two categories of patients so we used a support vector machine with a polynomial kernel to compute the relationships between the observations in a higher dimension and then found a good support vector classifier based on the high dimensional relationships the polynomial kernel that I used looks like this a and B refer to two different observations in the dataset are determines the coefficient of the polynomial and like I mentioned in the earlier stat quest D sets the degree of the polynomial in my example I set R equals 1/2 and D equals 2 since we are squaring the term we can expand it to be the product of two terms now we just do the multiplication b.p boo peepee poo-poo and combine these two terms and just because it will make things look better later let's flip the order of these two terms finally this polynomial is equal to this dot product a dot product sounds fancy but all it is is the first terms multiplied together plus the second terms multiplied together plus the third terms multiplied together the dot product gives us the high dimensional coordinates for the data the first terms are the x-axis coordinates and the second terms are the y-axis coordinates the third terms are z axis coordinates but since they are the same for both points we can ignore them thus we have X and y-axis coordinates for the data in the higher dimension BAM alternatively we could have set R equals 1 and D equals 2 now when we do the math we get this polynomial and this dot product we can verify that the dot product is correct by multiplying each term together and then add everything up and the result should be equal to the polynomial using this dot product the new x-axis coordinates are the square root of two times the original dosage values so we move the points on the x-axis over by a factor of the square root of two the new y-axis coordinates are the same as before the original dosage values squared and just like before we can ignore the Z access coordinate since it is a constant value now just like before we can use the high dimensional relationships to find a support vector classifier double bam now brace yourself things are about to get a little crazy going back to the polynomial kernel with R equals 1/2 and D equals 2 it turns out that all we need to do to calculate the high-dimensional relationships is calculate the dot products between each pair of points and since this kernel is equal to this dot product all we need to do is plug values into the kernel to get the high-dimensional relationships for example if we wanted to know the high-dimensional relationships between these two observations then we plug the dosages into the kernel do the math and 16000 2.25 is one of the two dimensional relationships that we need to solve for the support vector classifier even though we didn't actually transform the data to two dimensions triple bam fortunately why we only need to compute the dot product is out of the scope of this stat quest wah-wah to review the polynomial kernel computes relationships between pairs of observations a and B refer to the two observations that we want to calculate the high dimensional relationships for our determines the polynomials coefficient and D determines the degree of the polynomial note R and D are determined using cross-validation once we decide on values for R and D we just plug in the observations and do the math to get the high dimensional relationships hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
efR1C6CvhmE,2019-09-30T18:45:50.000000,Support Vector Machines Part 1 (of 3): Main Ideas!!!,support vector machines have a lot of terminology associated with them brace yourself hello I'm Josh stormer and welcome to stat quest today we're going to talk about support vector machines and they're gonna be clearly explained note this stack quest assumes that you are already familiar with the trade-off that plagues all of machine learning the bias-variance tradeoff you should also be familiar with cross-validation if not check out the quests the links are in the description below let's start by imagining we measured the mass of a bunch of mice the red dots represent mice that are not obese and the green dots represent mice that are obese based on these observations we can pick a threshold and when we get a new observation that has less mass than the threshold we can classify it as not obese and when we get a new observation with more mass than the threshold we can classify it as obese however what if we get a new observation here because this observation has more mass than the threshold we classify it as obese but that doesn't make sense because it is much closer to the observations that are not obese so this threshold is pretty lame can we do better yes going back to the original training data set we can focus on the observations on the edges of each cluster and use the midpoint between them as the threshold now when a new observation falls on the left side of the threshold it will be closer to the observations that are not obese than it is to the obese observations so it makes sense to classify this new observation as not obese BAM no it's a terminology alert the shortest distance between the observations and the threshold is called the margin since we put the threshold halfway between these two observations the distances between the observations and the threshold are the same and both reflect the margin when the threshold is halfway between the two observations the margin is as large as it can be for example if we move the threshold to the left a little bit then the distance between the threshold and the observation that is not obese would be smaller and thus the margin would be smaller than it was before and if we move the threshold to the right a little bit then the distance between the obese observation and the threshold would get smaller and again the margin would be smaller when we use the threshold that gives us the largest margin to make classifications heads up terminology alert we are using a maximal margin classifier BAM no no BAM maximal margin classifiers seem pretty cool but what if our training data looked like this and we had an outlier observation that was classified as not obese but was much closer to the obese observations in this case the maximum margin classifier would be super close to the obese observations and really far from the majority of the observations that are not obese now if we got this new observation we would classify it as not obese even though most of the not obese observations are much further away than the obese observations so maximum margin classifiers are super sensitive to outliers in the training data and that makes them pretty lame can we do better yes to make a threshold that is not so sensitive to outliers we must allow misclassifications for example if we put the threshold halfway between these two observations then we will miss classify this observation however now when we get a new observation here we will classify it as obese make sense because it is closer to most of the obese observations choosing a threshold that allows misclassifications is an example of the bias-variance tradeoff that plagues all of machine learning in other words before we allowed misclassifications we picked a threshold that was very sensitive to the training data it had low bias and it performed poorly when we got new data it had high variance contrast when we picked a threshold that was less sensitive to the training data and allowed misclassifications so it had higher bias it performed better when we got new data so it had low variance small BAM oh no it's another terminology alert when we allow misclassifications the distance between the observations and the threshold is called a soft margin so the question is how do we know that this soft margin is better than this soft margin the answer is simple we use cross-validation to determine how many misclassifications and observations to allow inside of the soft margin to get the best classification for example if cross-validation determined that this was the best soft margin then we would allow one miss classification and two observations that are correctly classified to be within the soft margin BAM when we use a soft margin to determine the location of a threshold brace yourself we have another terminology alert then we are using a soft margin classifier aka a support vector classifier to classify observations the names support vector classifier comes from the fact that the observations on the edge and within the soft margin are called support factors super-small bam note if each observation had a mass measurement and a height measurement then the data would be two-dimensional when the data are two-dimensional a support vector classifier is a line and in this case the soft margin is measured from these two points the blue parallel lines give us a sense of where all of the other points are in relation to the soft margin these observations are outside of the soft margin and this observation is inside the soft margin and misclassified just like before we used cross-validation to determine that allowing this miss classification results in better classification in the long run BAM now if each observation has a mass a height and an age then the data would be three-dimensional note the axis that age is on is supposed to represent depth and these circles are larger in order to appear closer and thus younger and these circles are smaller in order to look further away and thus older when the data are three-dimensional the support vector classifier forms a plane instead of a line and we classify new observations by determining which side of the plane they are on for example if this were a new observation we would classify it as not obese since it is above the support vector classifier note if we measured mass height age and blood pressure then the data would be in four dimensions and I don't know how to draw four dimensional graph wah-wah but we know that when the data are one-dimensional the support vector classifier is a single point on a one-dimensional number line just in mathematical jargon a point is a flat affin zero dimensional subspace and when the data are in two dimensions the support vector classifier is a one-dimensional line in a two dimensional space in mathematical jargon a line is a flat affin one-dimensional subspace and when the data are three-dimensional the support vector classifier is a two-dimensional plane in a three-dimensional space in mathematical jargon a plane is a flat affin two dimensional subspace and when the data are in four or more dimensions the support vector classifier is a hyperplane in mathematical jargon a hyperplane is a flat Athene subspace note technically speaking all flat Athene subspaces are called hyperplanes so technically speaking this one-dimensional line is a hyperplane but we generally only use the term when we can't draw it on paper small BAM because this is just more terminology ugh support vector classifier seemed pretty cool because they can handle outliers and because they can allow misclassifications they can handle overlapping classifications but what if this was our training data and we had tons of overlap in this new example with tons of overlap we are now looking at drug dosages and the red dots represent patients that were not cured and the green dots represent patients that were cured in other words the drug doesn't work if the dosage is too small or too large it only works when the dosage is just right now no matter where we put the classifier we will make a lot of misclassifications so support vector classifier czar only semi-cool since they don't perform well with this type of data can we do better than maximal margin classifiers and support vector classifier x' yes since maximal margin classifiers and support vector classifier x' can't handle this data it's high time we talked about support vector machines so let's start by getting an intuitive sense of the main ideas behind support vector machines we start by adding a y-axis so we can draw a graph the x axis coordinates in this graph will be the dosages that we have already observed and the y axis coordinates will be the square of the dosages so for this observation with dosage equals 0.5 on the x axis the y axis value equals dosage squared which equals 0.5 squared which equals 0.25 now we use dosage squared for this y-axis coordinate and then we use dosage squared for the y-axis coordinates for the remaining observations since each observation has X and y-axis coordinates the data are now two-dimensional and now that the data are two-dimensional we can draw a support vector classifier that separates the people who were cured from the people who were not cured and the support vector classifier can be used to classify new observations for example if a new observation had this dosage then we could calculate the y-axis coordinate by squaring the dosage and classify the observation as not cured because it ended up on this side of the support vector classifier on the other hand if we got a new observation with this dosage then we would square the dosage and get a y-axis coordinate and classify this observation as cured because it falls on the other side of the support vector classifier BAM the main ideas behind support vector machines are one start with data in a relatively low dimension in this example the data started in one dimension to move the data into a higher dimension in this example we move the data from one dimension to two dimensions three find a support vector classifier that separates the higher dimensional data into two groups that's all there is to it double BAM going back to the original one dimensional data you may be wondering why we decided to create y-axis coordinates with dosage squared why not dosage cubed or pi divided by four times the square root of dosage in other words how do we decide how to transform the data in order to make the mathematics possible support vector machines use something called kernel functions to systematically find support vector classifier in higher dimensions so let me show you how a kernel function systematically finds support vector classifier in higher dimensions for this example I use the polynomial kernel which has a parameter D which stands for the degree of the polynomial when D equals 1 the polynomial kernel computes the relationships between each pair of observations in one dimension chips are used to find a support vector classifier when D equals two we get a second dimension based on dosages squared and the polynomial kernel computes the two-dimensional relationships between each pair of observations and those relationships are used to find a support vector classifier and when we set D equal three then we would get a third dimension based on dosages cubed and the polynomial kernel computes the three-dimensional relationships between each pair of observations and those relationships are used to find a support vector classifier and when D equals four or more then we get even more dimensions to find a support vector classifier in summary the polynomial kernel systematically increases dimensions by setting D the degree of the polynomial and the relationships between each pair of observations are used to find a support vector classifier last but not least we can find a good value for D with cross validation double bam another very commonly used kernel is the radial kernel also known as the radial basis function kernel unfortunately the radio Colonel finds support vector classifier x' in infinite dimensions so I can't give you an example of what it does exactly however when using it on a new observation like this the radio kernel behaves like a weighted nearest neighbor model in other words the closest observations aka the nearest neighbors have a lot of influence on how we classify the new observation and observations that are further away have relatively little influence on the classification so since these observations are the closest to the new observation the radio kernel uses their classification for the new observation BAM now for the sake of completeness let me mention one last detail about kernels although the examples I've given show the data being transformed from a relatively low dimension to a relatively high dimension kernel functions only calculate the relationships between every pair of points as if they are in the higher dimensions they don't actually do the transformation this trick calculating the high-dimensional relationships without actually transforming the data to the higher dimension is called the kernel trick the kernel trick reduces the amount of computation required for support vector machines by avoiding the math that transforms the data from low to high dimensions and it makes calculating the relationships in the infinite dimensions used by the radial kernel possible however regardless of how the relationships are calculated the concepts are the same when we have two categories but no obvious linear classifier that separates them in a nice way support vector machines work by moving the data into a relatively high dimensional space and finding a relatively high dimensional support vector classifier that can effectively classify the observations triple bam hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to supports that quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or hoodie or just donate the links are in the description below alright until next time quest on
g9c66TUylZ4,2019-08-20T00:30:00.000000,"Regression Trees, Clearly Explained!!!",regression tree is for you and for me stat quest hello I'm Josh Starman welcome to stat quest today we're going to talk about regression trees and they're gonna be clearly explained this stat quest assumes you are already familiar with the trade-off that plagues all of machine learning the bias-variance tradeoff and the basic ideas behind decision trees and the basic ideas behind regression if not check out the quests the links are in the description below now imagine we developed a new drug to cure the common cold however we don't know the optimal dosage to give patients so we do a clinical trial with different dosages and measure how effective each dosage is the data looked like this and in general the higher the dose the more effective the drug then we could easily fit a line to the data and if someone told us they were taking a 27 milligram dose we could use the line to predict that a 27 milligram dose should be 62% effective however what if the data looked like this low dosages are not effective moderate dosages work really well somewhat higher dosages work at about 50% effectiveness and high dosages are not effective at all in this case fitting a straight line to the data will not be very useful for example if someone told us they were taking a 20 milligram dose then we would predict that a 20 milligram dose should be 45% effective even though the observed data says it should be 100% effective so we need to use something other than a straight line to make predictions one option is to use a regression tree regression trees are a type of decision tree in a regression tree each leaf represents a numeric value contrast classification trees have true or false in their leaves or some other discrete category with this regression tree we start by asking if the dosage is less than 14.5 if so then we are talking about these six observations in the training data and the average drug effectiveness for these six observations is 4.2 percent so the tree uses the average value for point two percent as its prediction for people with dosages less than fourteen point five on the other hand if the dosage is greater than or equal to 14.5 and greater than or equal to 29 then we are talking about these four observations in the training data set and the average drug effectiveness for these four observations is 2.5 percent so the tree uses the average value 2.5 percent as its prediction for people with dosages greater than or equal to 29 now if the dosage is greater than or equal to 14.5 and less than 29 and greater than or equal to 23 point 5 then we are talking about these 5 observations in the training data set and the average drug effectiveness for these 5 observations is 52.8% so the tree uses the average value 52.8% as its prediction for people with dosages between 23 point five and 29 lastly if the dosage is greater than or equal to 14.5 and less than 29 and less than 23 point 5 then we are talking about these four observations in the training data set and the average drug effectiveness for these four observations is 100% so the tree uses the average value 100% as its prediction for people with dosages between 14.5 and 23.5 since each leaf corresponds to the average drug effectiveness in a different cluster of observations the tree does a better job reflecting the data than the straight line at this point you might be thinking the regression tree is cool but I can also predict drug effectiveness just by looking at the graph for example if someone said they were taking a 27 milligram dose then just by looking at the graph I can tell that the drug will be about 50% effective so why make a big deal about the regression tree when the data are super simple and we are only using one predictor dosage to predict drug effectiveness making predictions by eye isn't terrible but when we have three or more predictors like dosage age and sex to predict drug effectiveness drawing a graph is very difficult if not impossible in contrast a regression tree easily accommodates the additional predictors for example if we wanted to predict the drug effectiveness for this patient we would start by asking if they are older than 50 and since they are not over 50 we follow the branch on the right and ask if their dosage is greater than or equal to 29 and since their dosage is not greater than or equal to 29 we follow the branch on the right and ask if they are female and since they are female we follow the branch on the left and predict that the dosage will be 100% effective and that's not too far off from the truth 98% okay now that we know that regression trees can easily handle complicated data let's go back to the original data with just one predictor dosage and talk about how to build this regression tree from scratch and since regression trees are built from the top down the first thing we do is figure out why we start by asking if dosage is less than 14.5 going back to the graph of the data let's focus on the two observations with the smallest dosages their average dosage is three and that corresponds to this dotted red line now we can build a very simple tree that splits the observations into two groups based on whether or not dosage is less than three the point on the far left is the only one with dosage less than three and the average drug effectiveness for that one point is zero so we put zero in the leaf on the left side for when dosage is less than three all of the other points have dosages greater than or equal to three and the average drug effectiveness for all of the points with dosages greater than or equal to three is thirty eight point eight so we put 38.8 in the leaf on the right side for when dosage is greater than or equal to three the values in each leaf are the predictions that this simple tree will make for drug effectiveness for example this point on the far left has dosage less than three and the tree predicts that the drug effectiveness will be zero the prediction for this point drug effectiveness equals zero is pretty good since it is the same as the observed value in contrast for this point which has dosage greater than three the tree predicts that the drug effectiveness will be 38.8 and that prediction is not very good since the observed drug effectiveness is 100% note we can visualize how bad the prediction is by drawing a dotted line between the observed and predicted values in other words the dotted line is a residual for each point in the data we can draw its residual the difference between the observed and predicted values and we can use the residuals to quantify the quality of these predictions starting with the only point with dosage less than three we calculate the difference between it's observed drug effectiveness zero and the predicted drug effectiveness zero and then square the difference in other words this is the squared residual for the first point now we add the square residuals for the remaining points with dosages greater than or equal to three in other words for this point we calculate the difference between the observed and predicted values and square it and then add it to the first term then we do the same thing for the next point and the next point and the rest of the points do - ii ii ii ii ii ii ii ii ii ii ii ii ii ii ii ii ii until we have added squared residuals for every point thus to evaluate the predictions made when the threshold is dosage less than three we add up the squared residuals for every point and get twenty seven thousand four hundred sixty eight point five note we can plot the sum of squared residuals on this graph the y axis corresponds to the sum of squared residuals and the x axis corresponds to dosage thresholds in this case the dosage threshold was three but if we focus on the next two points in the graph and calculate their average dosage which is five then we can use dosage less than five as a new threshold and using dosage less than five gives us new predictions and new residuals and that means we can add a new sum of squared residuals to our graph in this case the new threshold dosage less than five results in a smaller sum of squared residuals and that means using dosage less than five as the threshold resulted in better predictions overall BAM now let's focus on the next two points calculate their average which is seven and use dosage less than seven as a new threshold again the new threshold gives us new predictions new residuals and a new sum of squared residuals now shift the threshold over to the average dosage for the next two points and add a new sum of squared residuals to the graph and we repeat until we have calculated the sum of squared residuals for all of the remaining thresholds T 2 T 2 2 2 2 2 2 BAM now we can see the sum of squared residuals for all of the thresholds and dosage less than 14.5 has the smallest sum of squared residuals so dosage less than 14.5 will be the root of the tree in summary we split the data into two groups by finding the threshold that gave us the smallest sum of squared residuals BAM now let's focus on the six observations with dosage less than 14.5 that ended up in the node to the left of the root in theory we could split these six observations into two smaller groups just like we did before by calculating the sum of squared residuals for different thresholds and choosing the threshold with the lowest sum of squared residuals note this observation has dosage less than 14.5 and does not have dosage less than 11.5 so it is the only observation to end up in this node and since we can't split a single observation into two groups we will call this node a leaf however since the remaining five observations go to the other node we can split them once more now we have divided the observations with dosage less than 14.5 into three separate groups these two leaves only contain one observation each and cannot be split into smaller groups in contrast this leaf contains four observations that said those four observations all have the same drug effectiveness so we don't need to split them into smaller groups so we are done splitting the observations with dosage less than 14.5 into smaller groups note the predictions that this tree makes for all observations with dosage less than 14.5 are perfect in other words this observation has 20% drug effectiveness and the tree predicts 20% drug effectiveness so the observed and predicted values are the same this observation has 5% drug effectiveness and that's exactly what the tree predicts these four observations all have 0% drug effectiveness and that's exactly what the tree predicts is that awesome no when a model fits the training data perfectly it probably means it is over fit and will not perform well with new data in machine learning lingo the model has no bias but potentially large variants bummer is there a way to prevent our tree from overfitting the training data yes there are a bunch of techniques the simplest is to only split observations when there are more than some minimum number typically the minimum number of observations to allow for a split is 20 however since this example doesn't have many observations I set the minimum to 7 in other words since there are only six observations with dosage less than 14.5 we will not split the observations in this node instead this node will become a leaf and the output will be the average drug effectiveness for the six observations with dosage less than 14.5 4.2% BAM now we need to figure out what to do with the remaining 13 observations with dosages greater than or equal to 14.5 since we have more than 7 observations on the right side we can split them into two groups and we do that by finding the threshold that gives us the smallest sum of squared residuals note there are only four observations with dosage greater than or equal to 29 thus there are only four observations in this node thus we will make this a leaf because it contains fewer than seven observations and the output will be the average drug effectiveness for these four observations 2.5% now we need to figure out what to do with the nine observations with dosages between 14.5 and 29 since we have more than seven observations we can split them into two groups by finding the threshold that gives us the minimum sum of squared residuals note since there are fewer than seven observations in each of these two groups this is the last split because none of the leaves have more than seven observations in them so we use the average drug effectiveness for the observations with dosages between fourteen point five and twenty-three point five one hundred percent as the output for the leaf on the right and we use the average drug effectiveness for observations with dosages between twenty three point five and twenty nine fifty two point eight percent as the output for the leaf on the Left since no leaf has more than seven observations in it were done building the tree and each leaf corresponds to the average drug effectiveness from a different cluster of observations double BAM so far we have built a tree using a single predictor dosage to predict drug effectiveness now let's talk about how to build a tree to predict drug effectiveness using a bunch of predictors just like before we will start by using dosage to predict drug effectiveness thus just like before we will try different thresholds for dosage and calculate the sum of squared residuals at each step and pick the threshold that gives us the minimum sum of squared residuals the best threshold becomes a candidate for the root now we focus on using age to predict drug effectiveness just like with dosage we tried different thresholds for age and calculate the sum of squared residuals at each step and pick the one that gives us the minimum sum of squared residuals the best threshold becomes another candidate for the route now we focus on using sex to predict drug effectiveness with sex there is only one threshold to try so we use that threshold to calculate the sum of squared residuals and that becomes another candidate for the route now we compare the sum of squared residuals SSRS for each candidate and pick the candidate with the lowest value since age greater than 50 had the lowest sum of squared residuals it becomes the root of the tree then we grow the tree just like before except now we compare the lowest sum of squared residuals from each predictor and just like before when a leaf has less than a minimum number of observations which is usually 20 but we are using 7 we stop trying to divide them triple bow in summary rushon trees are a type of decision tree in a regression tree each leaf represents a numeric value we determine how to divide the observations by trying different thresholds in calculating the sum of squared residuals at each step pp boo boo beep beep boo boo peepee poo-poo the threshold with the smallest sum of squared residuals becomes a candidate for the root of the tree if we have more than one predictor we find the optimal threshold for each one and pick the candidate with the smallest sum of squared residuals to be the root when we have fewer than some minimum number of observations in a node 7 in this example but more commonly 20 then that node becomes a leaf otherwise we repeat the process to split the remaining observations until we can no longer split the observations into smaller groups and then we are done hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
xZ_z8KWkhXE,2019-08-05T15:30:03.000000,"Pearson's Correlation, Clearly Explained!!!",correlation it's the sensation across the nation stack quests hello I'm Josh starburns welcome to stack quest today is part two in our series on covariance and correlation this time we're going to talk about correlation however before we dive deep into correlation I want to talk about relationships not the fun and/or confusing kind we sometimes find ourselves in will you hold my hand um you don't have a hand you're just a stick figure dang instead I want to talk about the relationships between data on the x-axis and data on the y-axis in this example we're looking at mRNA transcripts from gene X in five different cells on the x-axis and from gene Y in the same five different cells on the y-axis however if mRNA transcripts doesn't mean anything to you imagine we went into five different grocery stores and put the number of green apples on the x-axis in the number of red apples on the y-axis each pair of measurements were taken from a single cell or grocery store and can be represented by a blue dot we can see that in general relatively low values for gene X are paired with relatively low values for gene Y and relatively high values for gene X are paired with relatively high values for gene y we can use a straight line with a positive slope to represent this trend and if someone told us that they collected a new measurement for gene X 20 then we can use the line to predict that when gene X equals 20 then the value for gene Y should be somewhere around 27 alternatively if someone gave us a value for gene Y we could use the trend to predict a range of values for gene X both cases we made guesses based on the trend we observed in the data if the data were closer to the trendline then given a gene X value we might guess that the value for gene Y falls in a smaller range in this case the closer the data are to the line the more gene X can tell us about gene y alternatively we could say that the relationship between gene X and gene Y is relatively strong the data were further from the trendline then we might guess that the value for Jean Y falls in a larger range in this case we could say that the values for gene X tell us less about the values for gene y alternatively we could say that the relationship between gene X and gene Y is relatively weak note just to be clear all we are saying is that we observed that low values for gene X tend to be paired with low values for gene Y and that high values for gene X tend to be paired with relatively high values for gene Y and that this observation suggests a trend that we can use to make predictions and inferences aka educated guesses we are not saying that a low value for gene X causes gene Y to have a low value or that a high value for gene Y causes gene X to have a high value in other words we are not ruling out the possibility that something else causes the trend that we observe small bam so far we have looked at a relatively weak relationship and a relatively strong relationship we can quantify the strength of a relationship with correlation in other words these data with a relatively weak relationship have a small correlation value these data with a moderate relationship have a moderate correlation value and these data with a strong relationship have a relatively large correlation value the maximum value for correlation is 1 correlation equals one when a straight line with a positive slope can go through the center of every data point this means that if someone gave us a value for gene X then we could guess that jean y had a value in a very very narrow range note correlation does not depend on the scale of the data in fact I intentionally omitted putting numbers on the axes because they do not affect correlation at all in other words regardless of the scale of the data correlation equals one when a straight line with a positive slope can go through all of the data that means that correlation can equal one when the slope is large and when the slope is small note when a straight line with a positive slope goes through the data correlation equals one regardless of how much data we have for example if we only had two data points then we can draw a straight line with a positive slope by just connecting the two dots and then correlation equals one and that makes the relationship appear strong but we should not have any confidence in predictions made with this line because we have so little data to understand why we should have low confidence in correlations made with small datasets let's start with an empty graph and draw two random points on it then just like before we could draw a straight line that goes through the center of each point just by connecting the dots and that means correlation equals one for these two randomly drawn dots in fact we can always draw a straight line between any two random dots now let's go back to the original data and imagine that instead of two pairs of measurements we had three pairs of measurements now just like before since we can draw a straight line through all three points correlation equals one however now we can have more confidence in the predictions we make with this line this is because if we started with an empty graph and drew three random points on it then even though it's easy to draw a straight line to connect any two points there is a very small chance that we will be able to draw a straight line through all three points ultimately the probability that we can connect three randomly drawn points with a straight line is very small and thus we can have more confidence that the observed correlation isn't just the result of random chance in general the more data we have the more confidence we have in the predictions we make with the line because the probability that we can draw a straight line through the same number of randomly placed points gets smaller and smaller with each additional point note we could draw a squiggly line that connects all of the dots but when we're talking about correlation were only talking about using straight lines oh no it's the dreaded terminology alert for correlation a p-value tells us the probability that randomly drawn dots will result in a similarly strong relationship or stronger thus the smaller the p-value the more confidence we have in the predictions we make with the line in this case the p-value is crazy small 2.2 times 10 to the negative 16 which means that the probability of random data creating a similarly strong or stronger relationship is crazy small to summarize what we've talked about so far the maximum value for correlation one occurs whenever you can draw a straight line with a positive slope that goes through all of the data and our confidence in how useful the relationship is depends on how much data we have of these three examples we should have the least confidence in this relationship since it is supported by the least amount of data and we should have the most confidence in this relationship since it is supported by the most data and has the smallest p-value BAM when a straight line with a negative slope can go through the center of every data point then the correlation equals negative one since a straight line can go through all of the data points correlation equals negative one implies that there is a strong relationship in the data and if someone gives us a value for gene X then we can guess a value for gene Y within a very narrow range just like before our confidence in that guess which we quantify with a p-value depends on how much data we have if we had a lot of data we could have a lot of confidence in the guess because the p-value would be super small and the less data we have the less confidence we have in the guess because the p-value gets larger like before as long as a straight line goes through all of the data and the slope of the line is negative correlation equals negative one when the slope is large and when the slope is small BAM so far we've seen that when the slope of the line is negative the strongest relationship has correlation equal to negative one and when the slope of the line is positive the strongest relationship has correlation equal to one in both cases if a straight line cannot go through all of the data then we will get correlation values closer to zero and the worse the fit the closer the correlation gets to zero and when there is no relationship that we can represent with a straight line correlation equals zero when correlation equals zero a value on the x-axis doesn't tell us anything about what to expect on the y-axis because there is no reason to choose one value over another BAM as long as the correlation value is not zero we can still use the line to make inferences but our guesses become more refined the closer the correlation values get to negative one or one and just like before our confidence in our inferences depends on the amount of data we have collected and the p-value in the left graph we have very little confidence in the trim because we have very little data and the p value equals 0.8 in the middle we have moderate confidence in the trend because we have more data and the p value equals 0.08 on the right we have a lot of confidence in the trend because we have even more data in the p value equals zero point zero zero eight note the correlation equals zero point three in all three examples in this case increasing the sample size did not increase correlation and that means adding data did not refine our guests all it did was increase our confidence in the guess thus our guesses will probably be pretty bad in all three cases however we'll have the most confidence in the bad guest that came from this data in other words just because you have a lot of data and you have a lot of confidence in your guests if the correlation value is small your guests will still be bad double bam if you know how to calculate variance and covariance calculating correlation is a snap note if you're not already familiar with the concepts of variance and covariance check out the quests the links are in the description below if this were the data then the correlation equals the covariance of gene X and gene Y divided by the square root of the variance for gene X times the square root of the variance for gene y as we saw in the stat quest on covariance the numerator can be any value between positive and negative infinity depending on whether the slope of the line that represents the relationship is positive or negative how far the data are spread out around the means and the scale of the data thus when we calculate correlation the denominator squeezes the covariance to be a number from negative 1 to 1 in other words the denominator ensures that the scale of the data does not affect the correlation value and this makes correlations much easier to interpret when the data all fall on a straight line with a positive or negative slope then the covariance and the product of the square root of the variance terms are the same and division gives us 1 or negative 1 depending on the slope when the data do not fall on a straight line with a positive or negative slope then the covariance accounts for less of the variance in the data and the correlation is closer to zero as we saw in the stat quest on covariance the covariance value for this data is 116 so the denominator will squeeze 116 down to a value from negative 1 to 1 the variants in the gene X data is 100 1.8 and the variants in the gene Y data is 160 point 3 and when we do the math we get 0.9 like I mentioned earlier we can quantify our confidence in this relationship with a p-value the smaller the p-value the more confidence we can have in the guesses we make in this case the p-value is 0.03 that means that there is a 3% chance that random data could produce a similarly strong relationship or stronger triple bam before we go there's one last important thing I want to mention about correlation even though correlation values are way easier to interpret then covariance values they are still not super easy to interpret for example it's not super obvious that this relationship where correlation equals zero point nine is twice as good as making predictions as this relationship where correlation equals zero point six four the good news is that R squared which is related to correlation solves this problem the better news is that if you want to learn more about r-squared you can check out these quests the links are in the description below pS another awesome thing about R squared is that it can quantify relationships that are more complicated than simple straight lines in summary correlation quantifies the strengths of relationships if you have a weak relationship then you will have a small correlation value if you have a moderate relationship then you'll have a moderate correlation value and if you have a strong relationship then you will have a large correlation value correlation values go from negative one which is the strongest linear relationship with a negative slope to one which is the strongest linear relationship with a positive slope in both cases if a straight line cannot go through all of the data then we will get correlation values closer to zero and the worse the fit the closer the correlation values get to zero and when there is no relationship that we can represent with a straight line correlation equals zero lastly our confidence in the inferences depends on the amount of data we have collected and the p-value the more data we have the smaller the p-value and the more confidence we have in our inferences BAM hooray we made it to the end of another exciting stat quest if you like this stack quest and want to see more please subscribe and if you want to support stack quest consider buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on
qtaqvPAeEJY,2019-07-29T19:15:01.000000,"Covariance, Clearly Explained!!!",[Music] my cat can't do stats in the window so I'll do stats for her all day long stack Quest hello I'm Josh starmer and welcome to stack Quest today we're going to talk about covariance and this is part one in a two-part series on covariance and correlation note this stack Quest assumes that you are already familiar with the concept of variance if not check out the quest the link is in the description below in the stack Quest on variance we started with the number of mRNA transcripts for Gene X from five different cells and if mRNA transcripts aren't your thing you could think that we counted the number of green apples in five different grocery stores then we estimated the mean xbar and then we estimated the variance bam and that's our review of variant now imagine that in addition to Counting mRNA transcripts for Gene X we also counted Gene y transcripts in the same five cells alternatively you can imagine we counted the number of red apples in the same five grocery stores note if you're wondering why gene Y is perpendicular to Gene X don't don't sweat it the reason will become clear in just a bit anyway just like we did for Gene X we can estimate the mean for Gene Y and since Gene Y is on the Y AIS we will use Y Bar to represent its mean value and we can estimate the variance bam so far we've estimated the mean and variance for two different genes measured in the same five cells alternatively you could imagine we estimated the mean and variance for two different types of apples counted in the same five grocery stores since these measurements were taken from the same cells or the same grocery stores we can look at them in pairs for example this pair of measurements came from the same cell and both measurements are less than the respective mean values this pair of measurements came from another cell and both measurements are greater than their respective mean values since the measurements were taken in pairs the question is do the measurements taken as pairs tell us something that the individual measurements do not covariance is one way to try to answer this question since the measure measurements came from the same cells or grocery stores we can plot each pair as a single dot by combining the values on the X and Y AES now generally speaking we see that cells with relatively low values for Gene X also have relatively low values for Gene Y and cells with relatively High values for Gene X also have relatively High values for Gene y this relationship low measurements for both genes in some cells and high measurements for both genes in other cells can be summarized with this line note the line that represents this particular relationship has a positive slope and it reflects the positive trend where the values for Gene X and Gene y increase together in other words if you told me that there were a lot of transcripts for Gene X in a Cell then the trend suggests that the same cell should have a large number of transcripts for Gene y likewise if you told me that you had a low value for Gene y then the trend suggests that the same cell should have a small number of transcripts for Gene X if the data had looked like this and relatively low values for genan X corresponded with relatively High values for Gene Y and relatively High values for Gene X corresponded with relatively low values for Gene y then the relationship would have a negative slope and reflect the negative Trend that the values for Gene X increase as the values for Gene y decrease if the data had looked like this and every value for Gene X was paired with the same value for Gene y then there would be no Trend positive or negative between Gene X and Gene y this is because if you told me that you got the same measurement for Gene y found in all of the other cells then we would not know if the same cell should have a relatively small value for Gene X or a relatively large value or any other value likewise if every value for Gene y was paired with the same value for Gene X there would be no relationship this is because if you told me that you got the same measurement for Gene X found in all of the other cells then we would not know if the same cell should have a relatively small value for Gene y or a relatively large value or any other value the main idea behind co-variance is that it can classify three types of relationships one relationships with positive Trends two relationships with negative Trends and three times when there is no relationship because there is no Trend okay we just covered the main idea behind covariance it's so important that I'm going to repeat it co-variants can classify these three types of relationships one relationships with positive Trends two relationships with negative Trends and three times when there is no relationship because there is no Trend bam the other main idea behind covariance is kind of a bummer covariance in an of itself is not very interesting what I mean by this is that you will never calculate co-variance and be done for the day instead covariance is a computational stepping stone to something that is interesting like correlation because I like repeating myself let me repeat the second main idea behind covariance covariance is a computational stepping stone to something that is interesting like correlation so let's talk about how co-variance is calculated covariance is calculated with this slightly nasty looking thing to get an intuitive sense for how covariance is calculated let's go back to the mean value for Gene X and extend the green line to the top of the graph and then extend the red line that represents the mean for Gene y to the edge of the graph now let's focus on the leftmost data point since it's to the left of the solid green line we see that it is less than the mean value for Gene X and since it is below the solid red line we see that it is less than the mean value for Gene y now let's plug in the gene X measurement for this cell and the mean value for Gene X and that gives us this difference which is negative since it is to the left of the mean now let's plug in the gene y measurement for this same cell and the mean value for Gene Y and that gives us this difference which is negative because it is below the mean and since both differences are negative mult multiplying them together gives us a positive value now we do the same thing for the next data point again since it is to the left of the solid green line we can see that it is less than the mean for Gene X and since it is below the solid red line we can see that it is less than the mean for Gene y so we plug in the values for genan x and that gives us a negative difference now we plug in the values for Gene Y and that gives us another negative difference again since both differences are negative multiplying them together gives us a positive value so we see that when the values for Gene X and Gene y are both less than their respective averages we end up with positive values bam the remaining three cells are to the right of the solid green line so we see that they are all greater than the average value for Gene X and they are all above the solid red line so we see that they are also greater than the average value for Gene y thus when we plug in the values and do the math we end up with positive values doing the math I'm doing the math hipip hoay I'm doing the math hip hoay hip hoay so we see that when both values are greater than their respective means we end up with positive numbers in summary data in these two quadrants contribute positive values to the total co-variance bam hooray we've calculated these terms terms for each cell the sigma tells us to add up each term and then we divide by the number of measurements n which in this case is 5 minus 1 and ultimately we end up with a co-variance equal to 116 since the co-variance value 116 is positive it means that the slope of the relationship between Gene X and Gene Y is positive in other words when the co-variance value is positive we classify the trend as positive double bam note the co-variance value itself isn't very easy to interpret and depends on the context for example the co-variance value does not tell us if the slope of the line representing the relationship is steep or not steep it just just tells us that the slope is positive more importantly the co-variance value doesn't tell us if the points are relatively close to the dotted line or relatively far from the dotted line again it just tells us that the slope of the relationship is positive note we'll talk about why the covariance value is so hard to interpret later and remember even though Co variance is hard to interpret it is a computational stepping stone to more interesting things bam now let's imagine we got different values for Gene y just like before we can graph the data using pairs of X and Y AIS values the mean value for genan x is the same as before 17.6 and the mean value for Gene Y is 20 20.2 now let's focus on the data point on the far left since the measurement for Gene X is to the left of the solid green line we can see that it is less than the mean value however the measurement for Gene Y is above the solid red line so we see that it is greater than the mean value thus when we plug in the numbers and do the math we end up with a negative number and the same thing happens to the next data point we end up with a negative number so if one value is less than its mean and one value is more than its mean we end up with a negative number bam likewise these cells also result in negative numbers because they are greater than Gene X mean and less than Gene y's mean in summary data in these quadrants contribute negative values to the co-variance now we add up each term and then we divide by the number of measurements n which is 5 - 1 and ultimately we end up with a CO variance equal to - 10515 since the co-variance value 10515 is negative it means that the slope of the relationship between Gene X and Gene Y is negative again covariance doesn't tell us if the slope is steep or not and more importantly covariance doesn't tell us if the points are relatively close to the line or relatively far just that the slope is negative bam now let's calculate the covariance for when there is no Trend in this case every value for Gene X corresponds to the same value for Gene y first we calculate the means then we do the math for the first point the difference from Gene X's average is -4.6 and the difference from Gene y's average is zero and anything times 0 is 0 so the value for the first data point is zero likewise because the differences between the gene y values and the gene y average are all zero all of the remaining terms are zero doing the rest of the math gives us zero in the numerator and the whole thing equals z likewise when every value for Gene y corresponds to the same value for Gene X the co-variance equals zero in this last case we can see that even though there are multiple values for Gene X and Y there is still no Trend because as Gene X increases Gan y increases and decreases in other other words the negative value for this point is canceled out by this positive point and this positive point is canceled out by this negative point and the co-variance equals z so we see that the covariance equals zero when there is no relationship between Gene X and Gene y double bam now let's talk about why the covariance value is hard to interpret to see why the covariance value is difficult to interpret let's go all the way back to looking at just Gene X and calculate the co-variance between Gene X and itself just like before we can plot the data and the mean value for Gene X is the same as before 17 1.6 and we use 17.6 on the Y AIS as well now we are ready to calculate the covariance note in this case the same data are on both axes and that means X = Y and xar = Y Bar so we can replace y with X and Y bar with xar and then we can multiply x - xar * x - xar and we are left with a formula for estimating variance in other words the co-variance for Gene x with itself is the same thing as the estimated variance for Gene X now when we do the math we get 102 since the co-variance value is positive we know that the relationship between Gene X and itself has a positive slope so let's move the graph and the co-variance value over here and see what happens when we multiply the data by two now the X and Y AIS labels on the right are twice what they are on the left and the new mean values are twice what they were before but the relative positions of the data did not change and each dot still falls on the same straight line with positive slope in other words the only thing that changed was the scale that the data is on however when we do the math we get co-variance equals 48 which is four times what we got before thus we see that the co-variance value changes even when the relationship does not in other words co-variance values are sensitive to the scale of the data and this makes them difficult to interpret this sensitivity to scale also prevents the covariance value from telling us if the data are close to the dotted line that represents the relationship or far from it in this example the covariance on the left when each point is on the dotted line is 102 and the co-variance on the right when the data are relatively far from the dotted line is 381 so in this case when the data are far from the line the covariance is larger now let's just change the scale on the right hand side and recalculate the covariance and now the covariance is less for the data that doesn't not fall on the line if you're thinking I sure wish there was something to describe relationships that wasn't sensitive to the scale of the data then you're in luck calculating covariance is the first step in calculating correlation correlation describes relationships and is not sensitive to the scale of the data and we'll talk about correlation more in the next video in this series it's also worth mentioning mentioning that co-variance values are used as stepping stones in a wide variety of analyses for example co-variance values were used for principal component analysis PCA and are still used in other settings as computational stepping stones to other more interesting things bam in summary covariance is a way to classify three types of relationships one when the co Aran is positive it means the relationship has a positive slope two when the covariance is negative it means the relationship has a negative slope and three when co-variance equals zero there is no relationship because there is no Trend the covariance value itself is difficult to interpret however it is useful for calculating correlations and in other computational settings hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more please subscribe and if you want to support stack Quest well consider buying one or two of my original songs or a t-shirt or a hoodie or just donating money the links to do this are in the description below all right until next time Quest on
sHRBg6BhKjI,2019-07-15T18:15:02.000000,Why Dividing By N Underestimates the Variance,if I had to choose between stack quest and watching cool is a start vanilla ice I'd watch stack quest stack quest hello I'm Josh stormer and welcome to stack quest today we're going to talk about why dividing by n underestimates the variance this stack quest assumes you already understand why we want to estimate population parameters if not check out the quest also this stack quest picks up from where we left off when we gave an overview of how to estimate the mean variance and standard deviation if you haven't seen that one you might want to check it out getting up from where we left off in the stat quest on the mean variance and standard deviation we measured gene X in five different liver cells and then we collected data from more liver cells until we had the entire population of liver cells and we used all of that data to draw a histogram and then we fit a normal curve to the histogram that meant we calculated the population mean mu the population mean mu equals the sum of the measurements divided by the number of measurements which equals the average measurement mu and we calculated the population variance in standard deviation the population variance is the average of the squared distances between the data and the population mean and the population standard deviation is just the square root of the variance since the standard deviation is in the same units as the original data we can draw it on the graph however since we rarely if ever have enough time and money to measure every single thing in a population we almost always estimate the population mean the estimated population mean x-bar equals the sum of the measurements divided by the number of measurements and that equals the average measurement x-bar and we estimate the variation and standard deviation the estimated population variance is the sum of the squared differences divided by n minus 1 and the standard deviation is just the square root of the variance and since the standard deviation is in the same units as the original data we can draw it on the graph we also mentioned that dividing by IDI minus one compensates for the fact that we are calculating differences from the sample mean instead of the population mean otherwise we would consistently underestimate the variance around the population mean then I said I'd give more details about why dividing by n underestimates the population variance in a future stat quest the future is now BAM to understand why dividing by n underestimates the population variance we'll start with a few simple examples and then we'll dive into the math and prove it once and for all in this first example I want to replace the sample mean x-bar with zero and see what happens now we square the differences between the measurements and zero calculate the average and that gives us 391 BAM now let's plot that value on a graph the y-axis on this graph corresponds to the value we just calculated it is the variance around a specific point and the x-axis corresponds to that point now let's move the purple line over to five and square the differences between the measurements and five and then calculate the average and that gives us 240 we can plot that point on the graph here's another point and another and this is the variance around the sample mean remember right now we are dividing by n not n minus 1 this is the variance around the population mean and here are a few more points note the point with the smallest variance corresponds to the sample mean x-bar and this point with a slightly larger variance corresponds to the population mean so in this case when we plug in the population mean and divide by n we get a larger variance then when we plug in the sample mean and divide by n in other words when we use the sample mean we underestimated the variance we got with the population mean BAM now let's get 5 new measurements from the same population and see if the same thing happens when we divide by n with a smallest variance be around the sample mean so just like before let's replace the sample mean x-bar with zero and see what happens now square the differences between the measurements and zero calculate the average that gives us 616 before we can plot variances on the graph here's another point and another and another this is the variance around the population mean and this is the variance around the sample mean and here are a few more points before we see that the minimum variance is at the sample main so just like before when we plug in the population mean and divide by n we get a larger variance then when we plug in the sample mean and divide by n so again when we use the sample mean and divided by n we underestimated the variance calculated around the population mean BAM so far we have seen two simple examples where using the sample mean and dividing by n underestimated the variance as we got with the population mean now let's prove that this will always happen first let's go back to the original data in the graph that we drew with it the first thing we do is realize that even though we only calculated variance around a handful of points we can replace X bar with an unknown value V and use this formula to graph all possible values for V note to emphasize the fact that we are plugging in different values for V I've modified the X access label now we can take the derivative of this formula with respect to the unknown value V and use it to determine the slope of the curve at different values for V the slope equals zero then we found the value for V that gives us the smallest variance so let's move this to the upper left hand corner and solve for the derivative the first thing we do is use the chain rule what the chain rule to solve for the derivative of X minus V squared so we bring the square down to the front and then we multiply everything by the derivative of X minus V which is negative one because the derivative of X with respect to V is zero and the derivative of negative V is negative one and that gives us the derivative with respect to V now we simplify by multiplying 2 and negative 1 lastly we can simplify things just a little more by moving the negative 2 and 1 divided by n outside of the summation this is the derivative with respect to the unknown value V BAM just to remind you the derivative corresponds to the slope of the Purple Line and we want to find the value for V such that the slope of the Purple Line equals zero because that is where we will find the minimum variance to make this as clear as possible we will find where the derivative is zero and the variance is minimized three different ways first we'll find where the variance is minimized using the observed data then we'll find where the variance is minimized for any five measurements and then we'll show how to find the minimum variance for any sample regardless of size so let's start by plugging the data into the derivative the first thing we do is plug in 5 for n since we have 5 measurements then we plug in the measurements since we want to find the value for V where the slope equals zero and the derivative is the slope we set the derivative equal to zero and solve for V note this negative 2/5 is the only thing making it hard to solve for V so we multiply both sides by 5 divided by negative 2 to cancel out the negative 2/5 now solving for V is super easy boo boo boo boo boo boo boo boo boo V is the average of the five measurements which is the sample mean x-bar and thus V equals x-bar which equals seventeen point six thus the derivative is zero when V equals x-bar which equals seventeen point six and the variance is minimized when V equals x-bar which equals seventeen point six this is why given this data the value around the sample mean is less than the value around the population mean in other words the differences between the data and the sample mean tend to be smaller than the differences between the data and the population mean thus the differences around the population mean will result in a larger average and the larger average is what we are trying to estimate BAM now let's go back to plugging data into the derivative and replace the data with five unknown values these unknown values represent future measurements we'll call this unknown value X sub one note even though X sub one is on the left side of the graph just know that it could be any possible value and let's call this unknown value X sub 2 X sub 3 of four and X sub 5 now let's plug the unknown data into the derivative just like before we plug in five four in since we have five measurements then we plug in the measurements since we want to find the value for V where the slope equals zero we set the derivative equal to zero and solve for V like before we multiply both sides by 5 divided by negative 2 to cancel out the negative 2/5 now solving for V is super easy boo-boo-boo-boo-boo-boo-boop boom boom V is the average of the five measurements which is the sample mean x-bar so no matter what five measurements we start with the value that gives us the minimum variance is x-bar BAM now let's see what happens when we have any size sample ie a sample with in measurements so let's plug the unknown data into the derivative now instead of replacing n with a number we just leave it since we have n measurements now we plug in all in measurements and set the derivative equal to zero and solve for V first we multiply both sides by n divided by negative 2 to cancel out the negative 2 divided by N now solving for V is super easy v is the average of the end measurements which is the sample mean x-bar so no matter how many measurements we start with the value that gives us the minimum variance is X bar double bail thus when we divide by n the value around the sample mean is always less than the value around the population mean unless the sample mean is the exact same as the population mean and that pretty much never happens triple bam before we go let's talk about why we square the differences instead of using the absolute value remember when we use the derivative to find the minimal value doing the calculus and understanding that this formula underestimated the variance around the population mean was relatively easy in contrast if we use the absolute value instead we'd get a graph that looked like this and since we have this sharp angle at the minimum value and derivatives do not exist at sharp angles like this then finding the minimum value is much harder with the absolute value than with the square BAM in summary when we only divide by n we underestimate the variation in the data around the population main this is because the differences between the data and the sample mean tend to be smaller than the differences between the data and the population mean thus the differences around the population mean will result in a larger average and the larger average is what we are trying to estimate so if you are estimating the population variance divided by n minus one PS if you're wondering why we divide by n minus 1 and not n minus 0.5 or n minus 2 then you'll just have to wait for the stat quest on expected values hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stat quest well consider buying an original song or buying a t-shirt or a hoodie or just donating anything would be great alright until next time quest on
SzZ6GpcfoQY,2019-07-15T18:15:02.000000,"Calculating the Mean, Variance and Standard Deviation, Clearly Explained!!!",I was home last night barians of standard deviation so I estimated them and its goal that quest [Music] hello I'm Josh stormer and welcome to stat quest today we're gonna continue our series on statistics fundamentals this time we're gonna talk about estimating the mean variance and standard deviation note this stat quest assumes you already know about histograms statistical distributions and specifically the normal distribution if not check out the quests the links are in the description below also this stat quest assumes you already understand why we want to estimate population parameters if not check out the quest the stat quest on population parameters we counted the number of mRNA transcripts from gene X in five different liver cells alternatively if mRNA transcripts and liver cells didn't mean anything to you we counted the number of green apples in five different grocery stores or green t-shirts and five different clothing stores or whatever you want to measure in five different units this green dot represented a liver cell that had three mRNA transcripts for gene X and this green dot represented a liver cell that had 13 mRNA transcripts 1924 and 29 now if we had a lot of time and money on our hands we could count the number of mRNA transcripts for gene X in all 240 billion liver cells now we can draw a histogram of the measurements if we wanted to fit a normal curve to this histogram like this then we need to calculate the population mean in the population variance or population standard deviation calculating the population mean is easy we take the average of all 240 billion measurements BP booty P due to baby boo doo bee doo doo boo boo and we get 24 the population mean and we Center the normal curve on the population mean note because we calculated the mean with all 240 billion measurements in the population this is not an estimate of the population mean it is the population mean however since we rarely if ever have enough time and money to measure every single thing in a population we almost always estimate the population mean using a relatively small sample in this example we have the measurements from only five of the 240 billion cells estimating the population mean is super easy we just calculate the average of the measurements we collected doodoo booty boot boot boot boot and in this case the estimated population mean is seventeen point six oh no it's the dreaded terminology alert statisticians often use the symbol x-bar to refer to the estimated mean which is also called the sample mean and they use the Greek symbol mu to refer to the population mean the estimated mean x-bar is different from the population mean mu but with more and more data x-bar should get closer and closer going back to the full set of population data we will now determine how wide to make the curve by calculating not estimating the variance and standard deviation in other words we want to calculate how the data are spread around the population mean this is the formula we use to calculate not estimate the population variance note I'm making a big deal about calculating versus estimating variance because it makes a big difference that we'll talk about later this part X minus mu means we subtract the population mean mu from each measurement X boo-boo-boo-boo-boo the square tells us to square each term character Sigma tells us to add up all the terms lastly we want the average of the squared differences so we divide by the number of measurements n which in this case is 240 billion thus we're just calculating the average of the squared differences between the data and the population mean note squaring each term ensures that each difference is positive otherwise the measurements on the left side of the mean would give negative differences which would cancel out the positive differences from the measurements on the right side of the mean note if you are wondering why we don't take the absolute value of each term great we'll talk about that in the follow-up video that dives deep into these details anyway and now we just do the math and we get 100 for the population variance BAM okay we calculated the population variance and we're all proud of ourselves however there is one thing that is annoying about it because each term is squared the units for the result 100 are mRNA transcripts squared note if the data have been the number of apples and grocery stores then the variance would be 100 apples squared either way we can't plot the variance on the graph since the units on the x axis are not squared to solve this problem we can take the square root of everything and that gives us the population standard deviation so the population standard deviation is the square root of 100 the population variance which is 10 and we can plot that on the graph this shows the main 20 plus and minus the standard deviation 10 mRNA transcripts um note before we move on I want to emphasize the point that we almost never have the population data so we almost never calculate the population mean and population variance in standard deviation instead we estimate the population variance and population standard deviation from the relatively small number of measurements that we have remember the population variance and standard deviation determines how much the curve spreads out and that means the estimated variance and the estimated standard deviation should reflect how the data are spread around the population mean however when we do an experiment we don't see the curve or the population mean we only see the data so we have to use the estimated mean x-bar instead this is the formula we use to estimate the population variance because we almost always work with a relatively small sample and not the entire population this is the formula we will use most of the time the differences between this formula and the one for the calculated population variance are subtle but important first since we don't know the population mean mu we use the sample mean x-bar second we are dividing by n minus 1 instead of n dividing by n minus one compensates for the fact that we are calculating the differences from the sample mean instead of the population mean otherwise we would consistently underestimate the variance around the population mean this is because the differences between the data and the sample mean tend to be smaller than the differences between the data and the population mean thus the differences around the population mean or result in a larger average and the larger average is what we are trying to estimate note if you're like me and want to know more details about why we need to compensate for calculating differences from the sample mean check out the follow-up stat quest the link is in the description below now let's do the math before we calculate the differences between the mean and the data doopa doopa doopa doopa - then we square each term add up each term but now we divide by n minus 1 and the estimated population variance is 100 1.8 now we just take the square root of the estimated variance to get the estimated standard deviation and we get 10.1 we can draw the mean plus and minus the standard deviation on the graph double BAM the estimated population parameters correspond to this purple curve with mean equals seventeen point six and standard deviation equals ten point one which isn't too far off from the true distribution with mean equals 20 and standard deviation equals 10 with more data the estimated parameters would be more accurate and we would have more confidence in them however with just five measurements we still did pretty well and that saved us a ton of time and money hooray in summary if we have all of the data from a population we can calculate the population mean the population mean equals the sum of the measurements divided by the number of measurements and that equals the average measurement mu when we don't have the population data we can estimate the population mean with the same formula the estimated population mean equals the sum of the measurements divided by the number of measurements which equals the average measurement x-bar when we have the population data we can calculate the population variance and standard deviation the population variance is the average of the squared differences between the data and the population mean mu in other words we square these differences to prevent the ones on the left from canceling the ones on the right and then take the average and the population standard deviation is just the square root of the population variance and since the standard deviation is in the original units that we measured we can draw it on the graph however we almost never have the population data so chances are you should not use these formulas said we almost always estimate the variance and standard deviation when we estimate the population variance we divide by n minus one to compensate for measuring distances from the sample mean instead of the population mean and the estimated standard deviation is just the square root of the estimated population variance and since the standard deviation is in the same units that we measured the data we can draw it on the graph and one last shameless plug for the follow up stat quest if you want to know why dividing by n underestimates the variance check out the quest the link is in the description below triple bell [Music] is in this stat quest I made a big deal about how we rarely have the population data and we almost always estimate the population parameters one reason I did this was because while many software packages estimate the variance and standard deviation by default Microsoft Excel does not instead it gives two choices one function VAR p calculates the population variance the other VAR s estimates it since we almost always have a relatively small sample rather than the population data we should almost always use far s hooray we've made it to the end of another exciting stat quest if you like this that quest and want to see more please subscribe and if you want to support stack quest well consider buying an original song or a t-shirt or a hoodie or just donating the links are all in the description below alright until next time quest on you
4jRBRDbJemM,2019-07-11T21:15:02.000000,"ROC and AUC, Clearly Explained!",wait till you see the roc and the a you see they're cool yeah stack quest hello i'm josh starmer and welcome to statquest today we're going to talk about roc and auc and they're going to be clearly explained note this stat quest builds on the confusion matrix and sensitivity and specificity stat quests so if you're not already down with those check out the quests also the example i give in this stat quest is based on logistic regression so even though roc and auc apply to more than just logistic regression make sure you understand those basics let's start with some data the y-axis has two categories obese and not obese the blue dots represent obese mice and the red dots represent mice that are not obese along the x-axis we have weight this mouse is not obese even though it weighs a lot it must be mighty mouse and just full of muscles this mouse doesn't weigh that much but it is still considered obese for its size now let's fit a logistic regression curve to the data when we're doing logistic regression the y-axis is converted to the probability that a mouse is obese now let's just look at the curve if someone told us that they had a heavy mouse that weighs this much then the curve would tell us that there is a high probability that the mouse is obese if someone told us that they had a light mouse that weighs this much then the curve would tell us that there is a low probability that the mouse is obese so this logistic regression tells us the probability that a mouse is obese based on its weight however if we want to classify the mice as obese or not obese then we need a way to turn probabilities into classifications one way to classify mice is to set a threshold at 0.5 and classify all mice with the probability of being obese greater than 0.5 as obese and classify all mice with the probability of being obese less than or equal to 0.5 as not obese using 0.5 as the cut off we would call this mouse obese and this mouse not obese if another mouse weighed this much then we would classify it as obese and if another mouse weighed this much then we would classify it as not obese to evaluate the effectiveness of this logistic regression with the classification threshold set to 0.5 we can test it with mice that we know are obese and not obese here are the weights of four new mice that we know are not obese and here are the weights of four new mice that we know are obese we know that this mouse is not obese and the logistic regression with the classification threshold set to 0.5 correctly classifies it as not obese this mouse is also correctly classified but this mouse is incorrectly classified we know that it is obese but it is classified as not obese the next mouse is correctly classified but this mouse is incorrectly classified the last three mice are correctly classified now we create a confusion matrix to summarize the classifications these three samples were correctly classified as obese and this sample was predicted to be obese but was not obese these three samples were correctly classified as not obese and this sample was predicted to be not obese even though it was obese once the confusion matrix is filled in we can calculate sensitivity and specificity to evaluate this logistic regression when 0.5 is the threshold for obesity little bam because so far this is all review now let's talk about what happens when we use a different threshold for deciding if a sample is obese or not for example if it was super important to correctly classify every ob sample we could set the threshold to 0.1 this would result in correct classifications for all four obese mice but it would also increase the number of false positives the lower threshold would also reduce the number of false negatives because all of the obese mice were correctly classified note if the idea of using a threshold other than 0.5 is blowing your mind imagine that instead of classifying samples as obese or not obese we were classifying samples as infected with ebola and not infected with ebola in this case it's absolutely essential to correctly classify every sample infected with ebola in order to minimize the risk of an outbreak and that means lowering the threshold even if that results in more false positives on the other hand we could set the threshold to 0.9 in this case we would correctly classify the same number of ob samples as when the threshold was set to 0.5 but we wouldn't have any false positives and we would correctly classify one more sample that was not obese and have the same number of false negatives as before with this data the higher threshold does a better job classifying samples as obese or not obese but the threshold could be set to anything between 0 and 1. how do we determine which threshold is the best for starters we don't need to test every single option for example these thresholds result in the exact same confusion matrix but even if we made one confusion matrix for each threshold that mattered it would result in a confusingly large number of confusion matrices so instead of being overwhelmed with confusion matrices receiver operator characteristic roc graphs provide a simple way to summarize all of the information the y-axis shows the true positive rate which is the same thing as sensitivity the true positive rate is the true positives divided by the sum of the true positives and the false negatives in this example the true positives are the samples that were correctly classified as obese and the false negatives are the ob samples that were incorrectly classified as not obese the true positive rate tells you what proportion of ob samples were correctly classified the x-axis shows the false positive rate which is the same thing as one minus specificity the false positive rate is the false positives divided by the sum of the false positives and true negatives the false positives are the non-ob samples that were incorrectly classified as obese and the true negatives are the samples correctly classified as not obese the false positive rate tells you the proportion of not ob samples that were incorrectly classified and are false positives to get a better sense of how the roc works let's draw one from start to finish using our example data we'll start by using a threshold that classifies all of the samples as obese and that gives us this confusion matrix first let's calculate the true positive rate there are four true positives and there were zero false negatives doing the math gives us one the true positive rate when the threshold is so low that every single sample is classified as obese is one this means that every single ob sample was correctly classified now let's calculate the false positive rate there were four false positives in the confusion matrix and there were zero true negatives doing the math gives us one the false positive rate when the threshold is so low that every single sample is classified as obese is also one this means that every single sample that was not obese was incorrectly classified as obese now plot a point at one comma one a point at one comma one means that even though we correctly classified all of the ob samples we incorrectly classified all of the samples that were not obese this green diagonal line shows where the true positive rate equals the false positive rate any point on this line means that the proportion of correctly classified ob samples is the same as the proportion of incorrectly classified samples that are not obese going back to the logistic regression let's increase the threshold so that all but the lightest sample are called obese the new threshold gives us this confusion matrix we then calculate the true positive rate and the false positive rate and plot a point at 0.75 comma 1. since the new point is to the left of the dotted green line we know that the proportion of correctly classified samples that were obese is greater than the proportion of samples that were incorrectly classified as obese in other words the new threshold for deciding if a sample is obese or not is better than the first one now let's increase the threshold so that all but the two lightest samples are called obese the new threshold gives us this confusion matrix we then calculate the true positive rate and the false positive rate and plot a point at 0.5 comma 1. the new point is even further to the left of the dotted green line showing that the new threshold further decreases the proportion of samples that were incorrectly classified as obese in other words the new threshold is the best one so far now we increase the threshold again create a confusion matrix calculate the true positive rate and the false positive rate and plot the point now we increase the threshold again create a confusion matrix calculate the true positive rate and the false positive rate and plot the point the threshold represented by the new point correctly classifies 75 percent of the ob samples and 100 percent of the samples that were not obese in other words this threshold resulted in no false positives now we increase the threshold again and plot the point now we increase the threshold again and plot the point lastly we choose a threshold that classifies all of the samples as not obese and plot the point the point at zero comma zero represents a threshold that results in zero true positives and zero false positives if we want we can connect the dots and that gives us an roc graph the roc graph summarizes all of the confusion matrices that each threshold produced without having to sort through the confusion matrices i can tell that this threshold is better than this threshold and depending on how many false positives i'm willing to accept the optimal threshold is either this one or this one bam now that we know what an roc graph is let's talk about the area under the curve or auc the auc is 0.9 bam the auc makes it easy to compare one roc curve to another the auc for the red roc curve is greater than the auc for the blue roc curve suggesting that the red curve is better so if the red roc curve represented logistic regression and the blue roc curve represented a random forest you would use the logistic regression double bam now one last thing before we're all done although roc graphs are drawn using true positive rays and false positive rates to summarize confusion matrices there are other metrics that attempt to do the same thing for example people often replace the false positive rate with precision precision is the true positives divided by the sum of the true positives and false positives precision is the proportion of positive results that were correctly classified if there were lots of samples that were not obese relative to the number of ob samples then precision might be more useful than the false positive rate this is because precision does not include the number of true negatives in its calculation and is not affected by the imbalance in practice this sort of imbalance occurs when studying a rare disease in this case the study will contain many more people without the disease than with the disease bam in summary roc curves make it easy to identify the best threshold for making a decision this threshold is better than this one and the auc can help you decide which categorization method is better the red method is better than the blue method hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest well consider getting a t-shirt or a hoodie or buying one or two of my original songs the links for doing that are below alright until next time quest on
vikkiwjQqfU,2019-07-01T17:30:00.000000,"Population and Estimated Parameters, Clearly Explained!!!",even if your ukulele is out of tune don't sweat it you can still watch stat quest all day long hooray hooray stat quest hello I'm Josh stormer and welcome to stat quest today we're going to talk about some statistics fundamentals specifically we're gonna talk about population parameters note this stat quest assumes you already know about histograms statistical distributions and specifically the normal distribution if not check out the quests the links are in the description below now imagine we counted the number of mRNA transcripts from gene X in five different liver cells note if mRNA transcripts in liver cells doesn't mean anything to you instead imagine we counted the number of green apples in five different grocery stores or you could imagine counting green t-shirts in five different clothing stores or imagine whatever you want to measure in five different units since I work in a genetics lab I'll stick with mRNA transcripts and liver cells this green dot represents a liver cell that had three mRNA transcripts for gene X and this green dot represents a liver cell that had 13 mRNA transcripts 19 transcripts 24 and 29 now if we had a lot of time and money on our hands we could count the number of mRNA transcripts for gene X in every single liver cell however for the sake of this example you'll just have to imagine 240 billion green dots on this line representing the 240 billion cells in a human liver because I don't have time to draw them all wah wah now we can draw a histogram of the measurements the histogram tells us that most of the cells had between 20 and 30 mRNA transcripts and relatively few cells had less than 10 transcripts and relatively few cells had more than 30 transcripts we can use the histogram to calculate probabilities and statistics for example if we wanted to know the probability of observing a liver cell with 30 or more mRNA transcripts for gene X then we would figure out how many liver cells had 30 or more mRNA transcripts for gene X and divided by the total number of liver cells in this case there are 38 billion cells with 30 or more transcripts and we divide that by 240 billion do the math and the probability of observing a cell with 30 or more transcripts is 0.16 BAM note this histogram made from mRNA counts in all 240 billion liver cells corresponds to a normal distribution with mean equals 20 and standard deviation equals 10 the mean 20 is right in the middle and the standard deviation 10 corresponds to how wide the curve is around the mean in other words the standard deviation tells us how the data are spread around the mean just like with the histogram we can use the distribution to calculate probabilities and statistics for example if we wanted to know the probability of observing a liver cell with 30 or more mRNA transcripts for gene X then we would calculate the area under the curve for all values equal to or greater than 30 and divide by the total area under the curve in this case the area under the curve greater than 30 equals 0.16 and the total area is 1 now we do the math and that tells us that the probability of observing a cell with 30 or more transcripts is 0.16 since we got the same value with the histogram it means the normal curve is a good approximation of the real data BAM note if we had counted green apples in a single chain of grocery stores then the distribution would represent the number of green apples in every single store in that chain and that means we could use the distribution to calculate statistics about apples in that grocery store chain BAM oh no it's a terminology alert watch out because this histogram represents every liver cell or all the grocery stores in a specific chain a statistician would say that it represents a population thus the mean and standard deviation of the normal curve which represents the population are called population parameters and we call the mean the population mean and we call the standard deviation the population standard deviation or the population SD for short note if the histogram had looked like this then we could fit an exponential distribution to the data the shape of an exponential distribution is determined by the rate which in this case equals 0.1 and even though the exponential distribution looks different from the normal distribution it would still represent the population of liver cells and that makes the rate the population rate and we could use the exponential distribution to calculate probabilities and statistics just like when we had a normal distribution alternatively if the shape of the histogram had looked like this then we would fit a gamma distribution to the data and since the shape of the gamma distribution is determined by two parameters shape and rate then shape and rate or population parameters note the concepts that we discuss in the rest of the stack quest apply to almost every statistical distribution however we'll just focus on the normal distribution in these examples so returning to the original normal curve since we rarely if ever have enough time and money to measure every single thing in a population we almost always estimate the population parameters using a relatively small sample in this case we have measurements from only five of the 240 billion cells so we will use these five measurements to estimate the population parameters the reason why we want to know the population parameters is to ensure that the results drawn from our experiment are reproducible in other words if someone else measured gene X in five different liver cells then they would get five different measurements however the new measurements will come from the same population and insights derived from the population like the probability of observing more than 30 mRNA transcripts in a single cell will apply to both experiments and future experiments so instead of just describing the five measurements that we made we want to estimate the population parameters and use those as the basis for the results double bam note if you're coming from a machine-learning background it might be helpful to think of these five measurements as the training data set in the curve that represents the population is what we want to predict with our machine learning method BAM going back to our five measurements I can tell you that the estimated population mean is seventeen point six and the estimated population standard deviation is ten point one note we'll talk about how to estimate the population mean and standard deviation in a follow-up stat quest for now just know that it's not hard now when we repeat the experiment the estimated population mean is nineteen point two and the estimated population standard deviation is twelve point seven thus each time we do the experiment we get different estimates of the population parameters and both sets of estimates are different from the true population values now if you've been paying attention what I just said should be a little disturbing earlier we said the whole idea behind population parameters was to give us reproducible results so how does getting different estimates each time give us reproducible results to answer this question let's start by assuming we only have two measurements when we just have these two measurements the estimated population mean equals 11 and the estimated population standard deviation equals eleven point three compared to the actual values the estimated mean 11 is way off from the true population mean 20 and the estimated standard deviation eleven point three is a little larger than the actual standard deviation 10 however if we had three measurements then the estimated mean equals fifteen point three which is closer to the true value than before and the estimated standard deviation equals 11 which is slightly closer to the true value than before however like we saw earlier when we have all five measurements then the estimated mean equals seventeen point six which is even closer to the true value and the estimated standard deviation equals ten point one which is even closer to the true value than before and if we had ten measurements then our estimates would be even better that means that the more data that we have the more confidence we can have in the accuracy of the estimates one of the main goals and statistics is quantifying how much confidence we can have in population estimates specifically statisticians often calculate p-values and confidence intervals to quantify the confidence in estimated parameters and like we just saw generally speaking the more data the more confidence we have in the estimates going back to the to replicate experiments even though these experiments resulted in different estimates for the population mean and standard deviation we can use statistics to quantify our confidence in how different they are in this case a p-value or alternatively a confidence interval would tell us that while the estimates are different they are not significantly different and that means the results generated from the first experiment should not be significantly different from the results generated from the second experiment and that means we should be able to replicate the results triple bam in summary a population represents every single liver cell or every grocery store in a chain of grocery stores or whatever unit it is you are measuring something awesome and the parameters that determine how a distribution fits the population data are called population parameters we rarely if ever have population data so we always estimate population parameters along with that we also calculate how much confidence we should have in those estimates generally speaking the more data we have the more confidence we have in the estimates by estimating the population parameters and quantifying our confidence in them we can generate results that are reproducible in future experiments PS if you'd like to learn more about how we can quantify our confidence in estimated population parameters check out the quest on confidence intervals the link is in the description below hooray we've made it to the end of another exciting stat quest if you liked this stack quest please subscribe and if you want to support stack quest well consider buying one or two of my original songs or buying a t-shirt or a hoodie or something like that alright until next time quest on
vMh0zPT0tLI,2019-05-13T20:45:00.000000,"Stochastic Gradient Descent, Clearly Explained!!!","you're gonna do something crazy I'm gonna do something random he's gotta be stochastic he's gotta be stat quest hello I'm Josh stormer and welcome to stat quest today we're going to talk about stochastic gradient descent and it's gonna be clearly explained note this stat quest assumes that you are already familiar with gradient descent if not check out the quest the link is in the description below this video picks up where the original leaves off providing more details about how stochastic gradient descent works in some of its more subtle advantages now even though I just said you need to watch the stat quest on gradient descent let's do a little review to demonstrate the problem that stochastic gradient descent solves in the stat quest on gradient descent we took this simple data set height and weight measurements from three different people and we want it to fit a line to it using gradient descent however at first we started out with this generic equation for a line and the goal was to find the optimal values for the intercept and the slope for example if we started with the intercept equals 0 and the slope equals 1 then we could use wait to predict height then we would use the sum of the squared residuals as the loss function to determine how well the initial line fit the data note the sum of the squared residuals is just one of many different loss functions that can evaluate how well something fits the data in this case that something is a lime to find the optimal values for the intercept and slope we plug the equation for the predicted height into the sum of the squared residuals then we took the derivative of the sum of the squared residuals with respect to the intercept and with respect to the slope then we plugged in the values from the observed data into the derivative with respect to the intercept and then we did the same thing for the derivative with respect to the slope then we plugged in the initial guess for the intercept 0 and the initial guess for the slope 1 we did the math plug the slopes into the step size formulas and multiplied by the learning rate which we set to 0.01 then we did the math calculated the new intercept and new slope by plugging in the old intercept an old slope and the step sizes and we did the math and we ended up with a new intercept and a new slope then we went back to the derivatives and repeated the process a lot of times until we took the maximum number of steps or the steps became very very small in this super simple example we were just fitting a line with two parameters the intercept and the slope and we only had three data points so we only had three terms to compute each step for the intercept and we only had three terms to compute each step for the slope so each step didn't require much math what if we had a more complicated model like a logistic regression that used 23,000 genes to predict if someone will have a disease then we will have 23,000 derivatives to plug the data into and what if we have data from 1 million samples then we would have to calculate 1 million terms for each of the 23,000 derivatives in other words we'd have to calculate 23 billion terms for each step and since it's common to take at least 1,000 steps we would calculate at least two point three trillion terms so for big data gradient descent is slow this is where stochastic gradient descent comes in handy going back to our super simple example stochastic gradient descent would randomly pick one sample for each step and just use that one sample to calculate the derivatives thus in this super simple example stochastic gradient descent reduced the number of terms computed by a factor of three if we had 1 million samples then stochastic gradient descent would reduce the amount of terms computed by a factor of 1 million so that's pretty cool stochastic gradient descent is especially useful when there are redundancies in the data for example we have 12 data points but there is a lot of redundancy that forms three clusters so we start with a line with the intercept equals 0 and the slope equals 1 then we randomly pick this point so we plug in the wait three and height three point three do the math in the slopes then multiply by the learning rate note just like with regular gradient descent stochastic gradient descent is sensitive to the value you choose for the learning rate and just like for regular gradient descent the general strategy is to start with a relatively large learning rate and make it smaller with each step and lastly just like for regular gradient descent many implementations of stochastic gradient descent will take care of this for you by default oh no it's a terminology alert the way the learning rate changes from relatively large to relatively small is called the schedule so if you fail to converge on parameter estimates try futzing with this setting in this simple example however we're just setting the learning rate to 0.01 now we do the math intercept and the new slope BAM the new parameters give us this new line then we randomly pick another point and calculate the intercept and slope for another line then we just repeat everything a bunch of times and ultimately we end up with a line where the intercept equals 0.85 and the slope equals zero point six eight and the least squares estimates aka the gold standard gives a line where the intercept equals 0.87 and the slope equals zero point six eight BAM note the strict definition of stochastic gradient descent is to only use one sample per step however it is much more common to select a small subset of data or mini-batch for each step for example we could use three samples per step instead of just one using a mini batch for each step takes the best of both worlds between using just one sample and all of the data at each step similar to using all of the data using a mini batch can result in more stable estimates for the parameters in fewer steps and like using just one sample per step using a mini batch is much faster than using all of the data in this example using three samples per step we ended up with an intercept equals zero point eight six and the slope equals zero point six eight which means that the estimate for the intercept was just a little closer to the gold standard 0.87 then when we used one sample and got zero point eight five double bam one cool thing about stochastic gradient descent is that when we get new data we can easily use it to take another step for the parameter estimates without having to start from scratch in other words we don't have to go all the way back to the initial guesses for the slope and intercept and redo everything instead we pick up right where we left off and take one more step using the new sample so we plug in the weight from the new sample 1.1 and the height too do the math plug in the slopes then multiply by the learning rate 0.01 do the math intercept not from the initial guess but from the most recent estimate and calculate the new slope from the most recent estimate in the new line has intercept equals zero point eight seven eight and slope equals zero point seven triple bam we updated the parameters for the line with just the new data in summary stochastic gradient descent is just like regular gradient descent except it only looks at one sample per step or a small subset or mini batch for each step stochastic gradient descent is great when we have tons of data and lots of parameters in these situations regular gradient descent may not be computationally feasible and it's cool that we can easily update the parameters when new data shows up hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest well consider buying one or two of my original songs or getting a t-shirt the links to do this are in the description below alright until next time quest on"
StWY5QWMXCw,2019-04-22T15:30:00.000000,Gradient Boost Part 4 (of 4): Classification Details,gradient boost for classification seems scary but it's not stat quest hello I'm Josh stormer and welcome to stat quest today we're gonna do gradient boost part 4 we're gonna talk about how gradient boost is used for classification now we're gonna dive deep into the details note this stat quest assumes you've already watched the first three parts in this series if not check out the quests also it's important that you have a pretty good understanding of the roles that the log odds and the log likelihood play in logistic regression so if you haven't already check out these quests in this stat quest we will walk through the original gradient boost algorithm for classification step by step just like in part two of this series we will use an incredibly small training set for the examples the small size will help us focus on the algorithms details but it will mean using stumps instead of trees however by now you know that in practice gradient boost usually uses trees with between 8 and 32 leaves now we'll describe our training data set we have whether or not three people like popcorn their age their favorite color and whether or not they loved the movie troll 2 great now that we know all about the training data set let's go through the algorithm step by step we'll start from the top from part two in this series we know that this refers to the training data set just to remind you X sub I refers to a row of measurements that we will use to predict if someone loves troll 2 and y sub I refers to whether or not someone loves troll 2 now we need a differentiable loss function that will work for classification I think the easiest way to understand the most commonly used loss function for classification is to show how it works on a graph the y axis is the probability of loving troll to the red dot with the probability of loving troll 2 equal to zero represents the one person that does not love troll 2 and the blue dots with the probability of loving troll 2 equal to 1 represent the two people that love troll 2 in other words the red and blue dots are the observed values and we can draw a dotted line to represent the predicted probability that someone loves troll 2 in this example I've set the predicted probability to 0.67 now just like we do for logistic regression we can calculate the log likelihood of the data given the predicted probability the log likelihood of the observed data given the prediction is this nasty-looking summation the peas refer to the predicted probability which is 0.67 in this example and the y sub i's refer to the observed values for loves troll 2 for the two people who love troll 2 y sub I equals 1 which means that this term will be 0 leaving just the log of P in contrast for the one person who does not love troll 2 y sub I equals 0 which means that this term will be 0 leaving just the log of 1 minus P now let's use the summation to calculate the log likelihood of all 3 observed values we'll start by calculating the log likelihood for the first person because this person loves to roll 2 y sub 1 equals 1 then we plug in 0.67 for the predicted probability P one minus one equals zero and now we do the multiplication the log-likelihood for the first person given the predicted probability is the log of 0.67 now let's calculate the log likelihood for the second person we plug in the observed value one for y sub two and plug in 0.67 for the predicted probability p1 minus one equals zero and now we do the multiplication log of 0.67 since the predicted probability was the same now let's calculate the log likelihood for the third person we plug in the observed value 0 for y sub 3 since this person does not love the movie plug in 0.67 for the predicted probability p1 minus 0 equals 1 and now we do the multiplication and we get the log of 1 minus 0.67 note the better the prediction the larger the log-likelihood and this is why when doing logistic regression the goal is to maximize the log likelihood that means that if we want to use the log likelihood as a loss function where smaller values represent better fitting models then we need to multiply the log likelihood by negative one so we'll put this subtle but very important minus sign in front of everything and since a loss function sometimes only deals with one sample at a time we can get rid of the summation and to make it easier to read we'll replace Y with observed now we need to transform this equation the negative log likelihood so that it is a function of the predicted log odds instead of the predicted probability P and we need to simplify it since this will require a lot of steps we'll move the negative log likelihood to the top left-hand corner to give a space to work the first thing we do is distribute the minus sign through the equation now we multiply the negative 1 minus observed by the log of 1 minus P now we combine this with this and get this and since the log of P minus the log of 1 minus P equals the log of P divided by the log of 1 minus P and that equals the log of P divided by 1 minus P and that equals the log of the odds so we replace the log of P minus the log of 1 minus P with the log of the odds in other words we converted the log of P minus the log of one minus P into a function of the log of the odds note the relationship between probability P and the log of the odds is derived in the stack quest on odds and the log of the odds so check that out if you want more details now we need to convert the log of 1 minus P into a function of the log of the odds the log of 1 minus P equals the log of 1 minus e to the log odds divided by 1 plus e to the log odds note this relationship between probability P and the log of the odds is derived in the logistic regression stack quest on estimating parameters with maximum likelihood now we replace 1 with this fraction do the subtraction and we get the log of 1 divided by 1 plus e to the log odds now convert the division into subtraction and since the log of 1 equals 0 we can remove it and that leaves us with the negative log of 1 plus e to the log odds thus the log of 1 minus P which is a function of the predicted probability P can be transformed into a function of the predicted log odds so let's plug that in note this sign changed from negative to positive because we replace the log of 1 minus P with a negative log of 1 plus e to the log odds hooray we converted the negative log likelihood of the data which is a function of the predicted probability P into a function of the predicted log odds BAM so this will be the loss function now we just need to show that it is differentiable so let's take the derivative of the loss function with respect to the predicted log odds the derivative of the first part with respect to the predicted log odds is super easy it's just the negative observed value the derivative of the second part is also super easy if you know how to use the chain rule the derivative of the log of something is 1 over that something times the derivative of that something in this multiplication can be re-written as a single fraction note earlier we saw that we can substitute the predicted probability P with this fraction but we can also swap the predicted probability P back in and that means that the derivative of the loss function can be a function of the predicted log odds or a function of the predicted probability P as we will soon see sometimes it's easier to use the function of the log odds and sometimes it's easier to use the function of the predicted probability P in summary the input data is the training data set in this fairly nasty looking thing which is just a transformation of the negative log likelihood is the differentiable loss function and the derivative can be a function of the predicted log odds or a function of the predicted probability P hooray now we're ready for step 1 like when we used gradient boost for regression we need to come up with the initial prediction and just like before we'll use this funky-looking equation to find the optimal initial prediction remember this is just the loss function the why I refers to the observed values and that funky simple called gamma refers to a log-odds value in theory we could go ahead and replace the log odds with gamma but it's actually easier to see what's going on if we leave the log odds in and remember that it represents gamma the summation means that we add up 1 loss function for each observed value and the Arg men over gamma means we need to find a log odds value that minimizes this sum the first thing we do is take the derivative of each term with respect to the log odds now to make the next step super-easy let's replace the log odds with the predicted probability P and set the sum of the derivatives equal to zero and solve beep boop beep boop beep boop boop and we end up with 2/3 for the initial predicted probability P because two people love troll 2 and there are three people in the training data set now we can convert the predicted probability into the predicted log odds so we plug in the predicted probability and solve p-people dicted log-odds is the log of 2/1 which makes sense because two people love troll two and one person does not so given this loss function the log odds value for gamma that minimizes this sum is the predicted log odds of loving troll two based on the observed yes/no values hooray we've created the initial predicted log odds F sub zero of X it equals zero point six nine so this is the initial leaf F sub zero of X BAM we finish step one we initialize the model with a constant value F sub zero of x equals the log of 2 divided by 1 which equals zero point six nine in other words we created a leaf that predicts the log odds that someone will love troll two and it equals zero point six nine now we can work on step two just like when we used gradient boost for regression this is when we'll build the trees so we'll start by setting little m equal to one and go from here in Part A we calculate pseudo residuals with this nasty looking thing this is just the derivative of the loss function with respect to the predicted log odds and we've already calculated this this big minus sign tells us to multiply the derivative by negative one and that leaves us with this equation for calculating pseudo residuals note as we have seen before we can replace this term with the predicted probability P so we can think of the pseudo residuals as the observed probability minus the predicted probability and the observed minus the predicted results in a pseudo residual this part says to plug in the most recent predicted log odds so we plug in F sub zero of X the most recent predicted log odds then do the math to convert the predicted log odds into the predicted probability P now we can compute the pseudo residuals for each sample R sub I comma M where I is the sample number and M is the tree that we're building so we'll start with our sub 1 comma 1 the residual for the first sample and the first tree so we plug in the observed weight for the first sample and we get 0.33 we'll keep track of our sub 1 comma 1 by adding it to the data set now we'll calculate the other two residuals pppppp hooray we finished Part A of step 2 by calculating a residual for each sample now we're ready for Part B where we will build a regression tree we will build a regression tree using likes popcorn age and favorite color to predict the residuals here's the new tree so we have a regression tree fit to the residuals now we need to create terminal regions R sub J comma M so in this example we'll name this leaf R sub 1 comma 1 and name this leaf R sub 2 comma 1 hooray we finished part b of step 2 by fitting a regression tree to the residuals and labeling the leaves now let's do Part C this is when we calculate the output values for the new tree so for each leaf in the new tree we compute an output value gamma sub J comma M the output value for each leaf is the value for gamma that minimizes this summation the X sub I and R sub I comma J means that since only the first row of data X sub 1 goes to leaf R sub 1 comma 1 then only X sub 1 is used to calculate the output value for R sub 1 comma 1 and since only two samples X sub 2 in X sub 3 go to leaf R sub 2 comma 1 then only X sub 2 and X sub 3 are used to calculate the output value for R sub 2 comma 1 let's start by calculating the output value for the leaf on the Left R sub 1 comma 1 that means that J equals one since this is the first leaf and M equals one since this is the first tree remember this is just the loss function so we can replace the generic form with the actual loss function that we are using note to keep the length of the formula from getting out of hand I'm returning to using y sub I to refer to the observed values since only X sub one goes to R sub 1 comma 1 we can remove the big Sigma and swap the eyes with ones now let's solve for the optimal value for gamma in theory we could take the derivative of this function with respect to gamma and then solve for gamma but that would turn into a huge mess so we will take a different approach than when we used gradient boost for regression let's start by moving the equation up here to give us room to work and even though we want to minimize gamma let's remember that we are working with the loss function since taking the derivative of the loss function with respect to gamma and then solving for gamma is hard we can approximate the loss function with a second-order Taylor polynomial why this second-order Taylor polynomial is a good approximation is something we can talk about in a future stat quest for now just take my word for it now we can take the derivative of this function with respect to gamma since gamma is not part of this term the derivative with respect to gamma is 0 so we can omit it the derivative of this with respect to gamma is super easy we can treat this the derivative of the loss function with respect to the predicted log-odds as a constant and the derivative of a constant times gamma is the constant similarly the derivative of this with respect to gamma is super easy all this stuff one half times the second derivative of the loss function with respect to the predicted log odds can be treated like a constant and we get this because the square on the gamma comes down and cancels the 1/2 using the approximation of the loss function made taking the derivative with respect to gamma super easy now let's move it up to give ourselves a little more room set it equal to zero and solve for gamma first we'll subtract this term from both sides bip then we'll divide both sides by this term boo BAM we solve for gamma and now just because I'm into moving equations to the top of the screen let's move this one up gamma equals negative 1 times the derivative of the loss function divided by the second derivative of the loss function since we have already solved for this we can just plug it in and we can replace this with the predicted probability P and the observed value minus the predicted probability P is just the residual now we need to take the second derivative of the loss function to figure out what goes in the denominator the second derivative of the loss function equals the derivative of the first derivative of the loss function so we can plug in the first derivative of the loss function now to make taking the derivative a little more obvious let's rewrite this fraction as multiplication now we need to take the derivative of this with respect to the log-odds the derivative of negative observed is zero so we can ignore this part this part however is a little more involved we'll need to use the product rule the product rule says that the derivative of a times B equals the derivative of a times B plus a times the derivative of B so we start with the derivative of the first part by using the chain and that gives us this derivative then we multiply by the second part then we add the first part times the derivative of the second part BAM this long thing is the second derivative of the loss function technically we could leave this as it is and move on but with a little bit of algebra we can get something way way easier to work with so let's move this to the top of the screen so that we have room to work the first thing we do is rewrite this as a fraction then we rewrite this as a fraction then we multiply the top and bottom of the second term by 1 plus e to the log odds and now we can add these terms together these two parts in the numerator cancel each other out and that leaves us with this note I also split the denominator into two terms so that the next steps make more sense now we'll multiply the numerator by wand which seems silly but is the key to reducing the whole thing into two super simple terms by multiplying the numerator by one we can easily see how the single term separates into two terms multiplied together at this point you may recognize the first term it converts the predicted log odds to the predicted probability P the second term should also remind you of something we have seen earlier in this stat quest earlier we saw that the log of 1 minus P equals this and that means that 1 minus P equals this which means the second term is just 1 minus the predicted probability P so at long last we see that the second derivative of the loss function is equal to P times 1 minus P and that brings us back to gamma earlier we saw that gamma equals the residual divided by the second derivative of the loss function and now we can replace the second derivative of the loss function with P times 1 minus P BAM I know it seems like a long time ago but remember we were trying to find out the output value for this leaf in other words we were trying to find the value for gamma that when added to the most recent predicted log odds minimized the loss function then we did a ton of math and discovered the gamma sub 1 comma 1 is the residual divided by P times 1 minus P now we can plug in the residual and the most recent predicted probability P for this sample in this case the predicted probability for this sample is derived from F sub 0 of X the most recent log-odds prediction now we just do the math and the output value for leaf R sub 1 comma 1 is 1.5 now let's calculate the output value for the other leaf R sub 2 comma 1 that means we're calculating gamma sub 2 comma 1 since samples x 2 in X 3 go to leaf R sub 2 comma 1 then we will need a loss function for X sub 2 and a loss function for X sub 3 now just like before we can approximate the loss function with second-order Taylor polynomials here's the second-order Taylor polynomial approximation of the loss function for sample X sub 2 and here's the second-order Taylor polynomial approximation of the loss function for X sub 3 if we add these two loss functions together we get this and we can approximate that sum by adding together the two Taylor polynomials we'll start by adding the first terms into Taylor polynomials then add the second terms at the third terms note we can pull these two gammas out of the second terms and we can pull the one halfs and the game of Squared's out of the third terms now let's move everything to the top so we have some space to determine the optimal value for gamma the first step in finding the optimal value for gamma is to take the derivative of the sum of the two approximate loss functions with respect to gamma the derivative of this part is zero since gamma is not involved at all for the second term since everything between the square brackets is like a constant with respect to gamma the derivative is everything between the square brackets and for the third term the derivative of this with respect to gamma means the square comes down and cancels out the one half and that leaves us with everything between the square brackets times gamma so this is the derivative of the sum of the approximate loss functions with respect to gamma move it to the top so we have some room to work set it equal to zero now solve for gamma first we subtract this square bracket stuff from both sides now we divide both sides by this square bracket stuff and this is the solution for gamma now all we need to do is simplify it so we move it to the top to give us a little room to work in the numerator we have two separate derivatives of the loss function 1/4 X 2 and 1/4 X 3 and since we already know that the derivative of the loss function is this we can plug it in for X 2 + 4 X 3 and since e to the log odds divided by one plus e to the log odds equals P we can plug in P sub to the predicted probability for X sub 2 and we can plug in P sub 3 the predicted probability for X sub 3 now we multiply the negative 1 through the numerator and that leaves us with the observed minus the predicted probabilities for X sub 2 and X sub 3 in other words we are left with the sum of the residuals in the numerator similarly we have the sum of two second derivatives in the denominator 1 for X sub 2 and 1 for X sub 3 and since we already know that the second derivative of the loss function equals P times 1 minus P we can plug in P sub 2 for the predicted probability for X sub 2 and we can plug in P sub 3 the predicted probability for X sub 3 now we just tidy everything up Oh BAM last we see that gamma is equal to the sum of the residuals divided by the sum of P times 1 minus P for each sample in the leaf okay I know it seems like a long time ago but we were trying to find the output value for this leaf so we did a ton of math and discovered that gamma sub 2 comma 1 is the sum of the residuals divided by the sum of P times 1 minus P for each sample that ended up in the leaf now we can plug in the residuals now we need to plug in the most recent predicted probabilities P 2 and P 3 for X 2 and X 3 just like before since we are building the first tree the predicted probability for these samples is derived from F sub 0 of X the most recent log-odds prediction note since we are just starting out the predicted probabilities are the same for all of the samples however after we build the first tree they can be different now just do the math in the output value for leaf R sub 2 comma 1 is negative 0.77 hooray we made it through step 2 Part C we calculated output values for each leaf in the tree now let's do Part D in Part D we make a new prediction for each sample since this is our first pass through step 2 and M equals 1 this new prediction will be called F sub 1 of X the new prediction F sub 1 of X is based on the last prediction we made F sub 0 of X plus the learning rate new times the output values from the first tree we made note this summation is there just in case a single sample ends up in multiple leaves also note we've set the learning rate new to 0.8 which is relatively large for more details about this check out the stat quest on the main ideas hooray we've created F sub 1 of X now we will use F sub 1 of X to make new predictions for each sample we'll start with the first sample X sub 1 the new prediction for X sub 1 starts with the last prediction F sub 0 of X which is 0.6 9 plus 0.8 times the output value from the Nutri which is 1.5 because X sub 1 likes popcorn by the way I love popcorn too now just do the math the new log-odds prediction for the first sample is 1 point 8 9 which is a better prediction than before because the odds are more in favor that this person will love troll 2 now we'll calculate the new predicted log odds for the second sample X sub 2 the new log odds prediction for the second sample is 0.07 which is worse than before but that's also why we build more than one tree now we'll calculate the new predicted log odds for the third sample X sub 3 the new log odds prediction for the third sample is 0.07 which is better than before hooray we made it through one iteration of step two we started by setting little m equal to one then we calculated pseudo residuals by plugging in the observed values and the latest predictions and that gave us residuals then we fit a regression tree to the residuals and computed the output values gamma sub J comma M for each leaf lastly we've made new predictions for each sample F sub 1 of X based on the previous prediction F sub 0 of X the learning rate nu and the output values gamma sub J comma M from the new tree now we set little m equal to two and do everything over again a calculate new residuals for the new predictions B create a new regression tree C calculate output values and D make new predictions at the end of the second round little M equals two and the new predictions F sub two of X are based on the predictions made by F sub 1 of X and the learning rate times the output values from the newest tree now in the interest of time let's assume Big M equals two so that we are done with step two note in practice m equals 100 or more now we are ready for gradient boosts 3 and final step if Big M equals 2 then F sub 2 of X is the output from the gradient boost algorithm now if we receive some new data we would use F sub 2 of X to predict whether this person loves troll 2 predicted log-odds that this person will love troll 2 equals the boop-boop boop-boop 3.4 the predicted probability that this person will love troll 2 equals 0.97 if we use a threshold of 0.5 for deciding if someone loves troll 2 then since 0.97 is greater than 0.5 this person loves troll 2 triple bam holy frickin smokes we made it through this whole algorithm I can't believe it we've done it for regression and now we've done it for classification hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stat quest well consider buying one or two of my original songs or getting a t-shirt or a hoodie the links to do this are in the description below alright until next time quest on you
HUzQIZLQxA0,2019-04-06T02:00:00.000000,Saturday,[Music] Monday morning time to turn on the light start the water say goodbye to the night a pack for work this is how I began and I do it all over all over all over all over [Music] [Music] it's what [Music] ooh [Music] Saturday with you who's that morning it's just more of the same paper cuts and coffee stains who's to blame I dragged myself home is this how it is and I do it all over all over all over all [Music] so it cancer it's what [Music] Saturday with you each day passes bringing me closer closer to you each day passes bringing me closer closer to you [Music] [Music] [Music] Saturday [Music] [Music] with you
2xudPOBz-vs,2019-04-01T23:00:02.000000,Gradient Boost Part 2 (of 4): Regression Details,gradient boost is awesome gradient boost is cool now we're going to dive into some regression details stack Quest hello I'm Josh starmer and welcome to stack Quest today we're going to continue our series on gradient boost specifically we're going to dive into the algorithmic details of how gradient boost is used for regression note this stat Quest assumes who have already watched the first video in this series gradient boost part one regression main ideas if not check out the quest also although not required it might be helpful if you understand gradient descent so check out that Quest if you haven't already in this stat Quest we are going to walk through the original gradient boost algorithm step by step in order to keep the example from getting out of hand and we are going to use gradient boost to fit a model to a super simple training data set which contains height measurements from three people their favorite color their gender and their weight great now that we know all about this super simple training data set let's walk through the original gradient descent algorithm step by step we'll start from the top this line describes in an abstract way the training data set and the method we will use to evaluate how well the model fits the training data set this first part is just a fancy way to say that we have some data the x sub I refer to each row of measurements that we will use to predict weight and the Y sub eyes refer to the weights measured for each person in the data set this part just says that the I's in x sub I and Y sub by go from 1 to n where n is the number of people in our data set in this case n equal three since we only have three rows of data Al together this means that we will refer to this row of measurements as x sub one and we will refer to this weight as y sub one this row of measurements is x sub 2 and this weight is y sub 2 this row of measurements is X sub3 and this weight is y sub3 now we need a differentiable loss function in this case a loss function is just something that evaluates how well we can predict weight the loss function that is most commonly used doing regression with gradient boost is 12 time the observed value minus the predicted value squared now just for a few minutes humor me and ignore the one/ half in front when we remove the 1/2 we end up with the same loss function we use for linear regression for example if we had height and weight measurements from three people then we could fit a line to the data residuals are the differences between the observed weights and the weights that are predicted by the line we can evaluate how well this greenish line fits the data with the sum of the squared residuals thus the loss function is just a squared residual if we wanted to compare how well this greenish line fit the data to this pink line then we would calculate the residuals the difference between the observed and the predicted values for the greenish line Square them and add them up and get 0.26 for the sum of the squared residuals for the greenish line then we would do the same thing for the pink line and I would see that 0.26 for the greenish line is smaller than 6.43 for the pink line so the greenish line is a better fit because it's some of the squared residuals is smaller note if I multiplied both sides of the formulas by 1/2 and did the math then we would get smaller numbers but we would still know that the greenish line fits the data better since its number is still relatively smaller in other words it doesn't matter if the loss function is the observed value minus the predicted value squared or if it's 12 * The observed value minus the predicted value squar red since both will tell us that the greenish line has the best fit the reason why people choose this loss function for gradient boost is that when we differentiate it with respect to predicted and use the chain Rule and bring the square down to the front and multiply it by the derivative of minus predicted which is -1 then the 2 / 2 cancels out and that leaves you with the observed minus the predicted multiplied by -1 in other words we are left with the negative residual and that makes the math easier since gradient boost uses the derivative a lot okay we've got a loss function the Y sub i's are the observed values and F ofx is a function that gives us the predicted values note we'll talk more about F ofx later we also know that the loss function is differentiable since we have already taken the derivative bam we figured out what the input is for the gradient boost algorithm we've got data and we've got a differentiable loss function note there are other loss functions to choose from but this is the most popular one for regression hooray now we're ready for step one we start by initializing the model with a constant value and that constant value is determined by this funky looking thing this funky looking thing is easiest to understand if we start on the right side and work our way left this is just the loss function the Y subi refers to the observed values and that funky looking Sy called gamma refers to the predicted values the summation means that we add up one loss function for each observed value and the ARG Min over gamma means we need to find a predicted value that minimizes this sum in other words if we plot The observed weights on a number line then we want to find the point on the line that minimizes the sum of the squared residuals / 2 here's a plot of 1/2 of the sum of the squared residuals for potential predicted values note we could use gradient descent to find the optimal value for predicted but we can also just solve for it because the math isn't that hard the first thing we do is take the derivative of each term with respect to predicted and since we've already shown how to take the derivative of our loss function we can just plug it in then we set the sum of the derivatives equal to zero and solve and we end up with the average of The observed weights so given this loss function the value for gamma that minimizes this sum is the average of The observed weights we have now created the initial predicted value fb0 of x and it equals 73.3 that means that the initial predicted value fb0 of x is just a leaf the leaf predicts that all samples will weigh 73.3 bam we finished step one we initialized the model with a constant value F sub Sub 0 of X = 73.3 in other words we created a leaf that predicts all samples will weigh 73.3 now we can work on step two step two is huge but we'll take it one step at a time step two is a loop where we make all of the trees in generic terms we will make M trees but in practice most people said M to be 100 and make 100 trees Little M refers to an individual tree so when little m equals 1 then we're talking about the first tree when little m equals Big M then we're talking about the last tree and when Little M is somewhere between one and Big M then we're talking about a tree somewhere between one and Big M since we are just starting step two we will set Little M equal to 1 part A of step two looks nasty but it's not this part is just the derivative of the loss function with respect to the predicted value and we've already calculated this this big minus sign tells us to multiply the derivative by -1 and that leaves us with the observed value minus the predicted value in other words this nasty looking thing is just a residual now we plug f subm minus1 of x in for predicted and since m equal 1 that means we plug in F Sub 0 ofx for f submus one of X and since F Sub 0 of X is just the leaf set to 73.3 we plug in 73.3 now we can compute R sub I comma M where R is short for residual I is the sample number and M is the tree that we're trying to build this tells us to calculate residuals for all three samples in the data set so we'll start with r sub1 comma 1 the residual for the first sample and the first tree so we plug in the observed weight for the first sample and get 14.7 we'll keep track of R sub 1 comma 1 by adding it to the data set now we calculate R sub2 comma 1 and we calculate R sub3 comma 1 hooray we finished part A of step two by calculating a residual for each sample example note before we move on I just want to point out that this derivative is the gradient that gradient boost is named after little bam I also want to point out that the r subi comma M values are technically called pseudo residuals when we use this loss function we end up calculating normal residuals but if we used another LW function this time we're not multiplying by 1/2 then we would end up with something similar to a residual but not quite in other words we'd end up with a pseudo residual and that's why the r sub I comma M's are called pseudo residuals okay now let's do Part B all this is saying is that we will build a regression tree to predict the residuals instead of the weights so we will use height favorite color and gender to predict the residuals here's the new tree yes I know this is just a stump and gradient boost almost always uses larger trees however in order to demonstrate the details of the gradient boost algorithm we need at least one leaf with more than one sample in it and when you only have three samples then you can't have more than two leaves so we're stuck using stumps even though they are not typically used with gradient boost the residual for the third sample X sub3 goes to the leaf on the left and the residual for samples x sub1 and x sub2 go to the leaf on the right so we have a regression tree fit to the residuals now we need to create terminal regions R subj comma M this part is super easy because the leaves are the terminal regions R subj comma M note this little m is the index for the tree we just made since this is the first tree m equal 1 and this little J is the index for each Leaf in the tree since this tree has two leaves J subm equals 2 so in this example we'll let this Leaf be R sub 1 comma 1 and this Leaf be R sub 2 comma 1 note it doesn't matter which Leaf gets which label however once we give a leaf a label we need to keep track of it hooray we finished Part B of step two by fitting a regression tree to the residuals and labeling the leaves now let's do part C in this part we determine the output values for each Leaf specific specifically since two residuals ended up in this Leaf it's unclear what its output value should be so for each Leaf in the new tree we compute an output value gamma subj comma M the output value for each Leaf is the value for gamma that minimizes this summation note this minimization is like what we did in step one one small difference is that now we are taking the previous prediction into account while before since we were just starting out there was no previous prediction the other difference is that this summation is picky about which samples it includes while before the summation included all of the samples specifically the x sub I in R sub I comma J means that since only one sample X sub3 3 goes to Leaf R sub 1A 1 then only X sub3 is used to calculate the output value for R sub 1 comma 1 and since only two samples x sub 1 and xub 2 go to Leaf R sub 1 comma 2 then only x sub1 and x sub2 are used to calculate the output value for R sub1a 2 let's start by calculating the output value for the leaf on the left R sub 1A 1 that means Jal 1 since this is the first leaf and m equal 1 since this is the first tree now let's replace the generic loss function with the actual loss function that we decided to use and let's expand the summation into individual terms since only X sub3 goes to R sub 1 comma 1 expanding means we remove the big Sigma and swap the eyes with threes now we plug in the value for y sub3 The observed value and the most recent predicted value for x sub3 since m equal 1 the most recent prediction was F Sub 0 of X which predicted that all samples weighed 73.3 so we plug in 73.3 for f subm minus1 of x sub3 and simplify what's inside the parentheses now we need to find the value for gamma that minimizes this equation just like step one we can try different values for Gamma or Solve IT analytically since the math is easy we'll solve for it first we take the derivative of the loss function with respect to gamma just like we did at the very start we use the chain R and get this now we set the derivative equal to zero and solve the value for gamma that minimizes this equation is -7.3 and that means gamma sub 1A 1 = -7.3 and ultimately the leaf R sub 1A 1 has an output value of -7.3 now let's solve for the output value for R sub 2 comma 1 that means J equal 2 since this is the second leaf and M equals 1 since this is still the first tree now plug in the loss function expand the summation plug in the observed weights and plug in 73.3 for f subm minus1 of x sub 1 and f subn minus1 of x sub 2 now simplify what's inside the parentheses and take the derivative with respect to gamma using the chain Rule and that gives us this derivative which we set equal to zero and then we solveo we end up with the average average of the residuals that ended up in leaf R sub 2 comma 1 and that equals 8.7 so the value for gamma that minimizes this equation is 8.7 and that means gamma sub 2 comma 1 = 8.7 and ultimately the leaf R sub2 comma 1 has an output value of 8.7 one last little observation before we move on we just saw that the output value for this Leaf R sub2 comma 1 is the average of the residuals that ended up here given our choice of loss function the output values are always the average of the residuals that end up in the same Leaf even if only one residual ends up in a leaf the output value is still the average since -7.3 / 1 = -7.3 hooray we finished part C of step two by Computing gamma values or output values for each Leaf now let's do Part D in part D we make a new prediction for each sample since this is our first pass through step two and M equals 1 this new prediction will be called f sub1 of x the new prediction f sub1 of x is based on the last prediction we made F Sub 0 ofx and the tree we just finished making note this summation is there just in case a single sample ends up in multiple leaves the summation says we should add up the output values gamma subj comma M's for all the leaves R subj comma M that a sample X can be found in the last thing in this equation is this Greek character new new is the learning rate and is a value between 1 and zero a small learning rate reduces the effect each tree has on the final prediction and this improves accuracy in the long run in this example we'll set new to 0.1 hooray we've created f sub1 of x now we will use f sub1 of x to make new predictions for each sample we'll start with the first sample X sub1 the new prediction for X sub1 starts with the last prediction F Sub 0 of X which is 73.3 plus 0.1 times the output value from the new tree which is 8.7 because xub 1's height is greater than 1.55 now just do the math the new prediction for the first sample is 74.2 which is slightly closer to the observed weight 88 than the first prediction 73.3 bam now let's make a new prediction for the second sample x sub 2 the new prediction for X sub2 is also 74.2 which is an improvement over the first prediction 73. 3 now let's make a new prediction for the third sample X sub3 the new prediction is 71.6 which is an improvement over 73.3 double bam hooray we made it through one iteration of step two we started by setting M to one then we we solved for the negative gradient Plugged In The observed values plugged in the latest predictions and that gave us residuals then we fit a regression tree to the residuals and computed the output values gamma subj comma M for each Leaf lastly we made new predictions for each sample F sub one of X based on the previous prediction f F Sub 0 of X the learning rate new and the output values gamma subj comma M from the new tree now we set m equals 2 and do everything over again a calculate new residuals for the new predictions B create a new tree C calculate output values and D make new predictions at the end of the second round m equal 2 and the new predictions f sub2 ofx are based on the predictions made by f sub1 of x and the learning rate times the output values from the newest tree now in the interest of time let's assume Big M equals 2 so that we are done with step two note in practice Big M equals 100 or more more now we are ready for gradient Boost's third and final step if Big M equals 2 then f sub2 of x is the output from the gradient boost algorithm holy smokes we made it through this whole thing that's crazy now if we receive some new data we could use f sub2 of x to predict the weight the predicted weight equals 73.3 + 0.1 * -7.3 plus 0.1 * -15.6 which equals 70 gradient boost predicts this person weighs 70 kg triple bam note before we go I want to remind you that gradient boost usually uses trees larger than stumps I only use stumps in this tutorial because our training data set was so darn small also be sure to watch part three of this exciting series on gradient boost next time we'll talk about classification hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more please subscribe and if you want to support stack Quest well consider getting one or two of my original songs or buying a t-shirt or a hoodie or a onesie or something anyways the links are below all right until next time Quest on
3CC4N4z3GJc,2019-03-25T16:00:01.000000,Gradient Boost Part 1 (of 4): Regression Main Ideas,[Music] gradiant buspar Tuan regression main ideas stat quest hello I'm Josh stormer and welcome to stat quest today we're going to talk about the gradient boost machine learning algorithm specifically we're going to focus on how gradient boost is used for regression note this stat quest assumes you already understand decision trees so if you're not already down with those check out the quest this stat quest also assumes that you are familiar with adaboost and the trade-off between bias and variance if not check out the quests the links are in the description below this stat quest is the first part in a series that explains how the gradient boost machine learning algorithm works specifically we'll use this data where we have the height measurements from six people their favorite colors their genders and their weights and we'll walk through step by step the most common way that gradient boost fits a model to this training data note when gradient boost is used to predict a continuous value like weight we say that we are using gradient boost for regression using gradient boosts for regression is different from doing a linear regression so while the two methods are related don't get them confused with each other part two in this series we'll dive deep into the math behind the gradient boost algorithm for regression walking through it step-by-step and proving that what we cover today is correct part three in this series shows how gradient boost can be used for classification specifically we'll walk through step by step the most common way gradient boost can classify someone is either loving the movie troll 2 or not loving troll 2 part 4 we'll return to the math behind gradient boost this time focusing on classification walking through it step-by-step note the gradient boost algorithm looks complicated because it was designed to be configured in a wide variety of ways but the reality is that 99% of the time only one configuration is used to predict continuous values like weight and one configuration is used to classify samples into different categories this stack quest focuses on showing you the most common way gradient boost is used to predict a continuous value like weight if you are familiar with adaboost then a lot of gradient boost will seem very similar so let's briefly compare and contrast at a boost and gradient boost if we want to use these measurements to predict weight then adaboost starts by building a very short tree called a stump from the training data and then the amount of say that the new stump has on the final output is based on how well it compensated for those previous errors then adaboost builds the next stomp based on errors that the previous stump made in this example the new stump did a poor job compensating for the previous stumps errors and its size reflects its reduced amount of say then adaboost builds another stump based on the errors made by the previous stump and this stump did a little better than the last stop so it's a little larger then adaboost continues to make stumps in this fashion until it is made the number of stumps you asked for or it has a perfect fit in contrast gradient boost starts by making a single leaf instead of a tree or stump this leaf represents an initial guess for the weights for all of the samples when trying to predict a continuous value like weight the first guess is the average value then gradient boost builds a tree like adaboost this tree is based on the errors made by the previous tree but unlike adaboost this tree is usually larger than a stump that said gradient boost still restricts the size of the tree in the simple example that we will go through in this stat quest we will build trees with up to 4 leaves but no larger however in practice people often set the maximum number of leaves to be between 8 and 32 thus like adaboost gradient boost builds fixed size trees based on the previous trees errors but unlike adaboost each tree can be larger than a stump also like adaboost gradient boost scales the trees however gradient boost scales all trees by the same amount then gradient boost builds another tree based on the errors made by the previous tree and then it scales the tree and gradient boost continues to build trees in this fashion until it has made the number of trees you asked for or additional trees fail to improve the fit now that we know the main similarities and differences between gradient boost and adaboost let's see how the most common gradient boost configuration would use this training data to predict weight the first thing we do is calculate the average weight this is the first attempt at predicting everyone's weight in other words if we stopped right now we would predict that everyone weighed 70 1.2 kilograms however gradient boost doesn't stop here the next thing we do is build a tree based on the errors from the first tree the errors that the previous tree made are the differences between the observed weights and the predicted weight 70 1.2 so let's start by plugging in seventy one point two for the predicted weight and then plug in the first observed wait and do the math and save the difference which is called a pseudo residual in a new column note the term pseudo residual is based on linear regression where the difference between the observed values and the predicted values results in residuals the pseudo part of pseudo residual is a reminder that we are doing gradient boost not linear regression and is something I'll talk more about in part 2 of this series when we go through the math now we do the same thing for the remaining weights now we will build a tree using height favorite color and gender to predict the residuals if it seems strange to predict the residuals instead of the original weights just bear with me and soon all will become clear so setting aside the reason why we are building a tree to predict the residuals for the time being here's the tree remember in this example we are only allowing up to four leaves but when using a larger data set it is common to allow anywhere from 8 to 32 by restricting the total number of leaves we get fewer leaves than residuals as a result these two rows of data go to the same leaf so we replace these residuals with their average and these two rows of data go to the same leaf so we replace these residuals with their average now we can combine the original leaf with the new tree to make a new prediction of an individual's weight from the training data we start with the initial prediction seventy one point two then we run the data down the tree and we get sixteen point eight so the predicted weight equals seventy one point two plus sixteen point eight which equals eighty eight which is the same as the observed weight is this awesome no the model fits the training data too well in other words we have low bias but probably very high variance gradient boost deals with this problem by using a learning rate to scale the contribution from the new tree the learning rate is a value between zero and one in this case we'll set the learning rate to 0.1 now the predicted weight equals seventy one point two plus zero point one times sixteen point eight which equals seventy two point nine with the learning rate set to 0.1 the new prediction isn't as good as it was before but it's a little better than the prediction made with just the original leaf which predicted that all samples would weigh seventy one point two in other words scaling the tree by the learning rate results in a small step in the right direction according to the dude that invented gradient boost Jerome Freedman empirical evidence shows that taking lots of small steps in the right direction results in better predictions with a testing data set ie lower variance BAM so let's build another tree so we can take another small step in the right direction just like before we calculate the pseudo residuals the difference between the observed weights and our latest predictions so we plug in the observed weight and the new predicted weight and we get 15.1 and we save that in the column for pseudo residuals then we repeat for all of the other individuals in the training data set small BAM note these are the original residuals from when our prediction was simply the average overall weight and these are the residuals after adding the Nutri scaled by the learning rate the new residuals are all smaller than before so we've taken a small step in the right direction double bam now let's build a new tree to predict the new residuals and here's the Nutri note in this simple example the branches are the same as before however in practice the trees can be different each time just like before since multiple samples ended up in these leaves we just replace the residuals with their averages now we combine the new tree with the previous tree and the initial leaf note we scale all of the trees by the learning rate which we set to 0.1 and add everything together now we're ready to make a new prediction from the training data just like before we start with the initial prediction then add the scaled amount from the first tree in the scaled amount from the second tree that gives us seventy one point two plus zero point one times sixteen point eight plus zero point one times fifteen point one which equals seventy four point four which is another small step closer to the observed weight now we use the initial leaf plus the scaled values from the first tree plus the scaled values from the second tree to calculate new residuals remember these were the residuals from when we just use a single leaf to predict weight and these were the residuals after we added the first tree to the prediction and these are the residuals after we added the second tree to the prediction each time we add a tree to the Bur diction the residuals get smaller so we've taken another small step towards making good predictions now we build another tree to predict the new residuals and add it to the chain of trees that we have already created and we keep making trees until we reach the maximum specified or adding additional trees does not significantly reduce the size of the residuals BAM then when we get some new measurements we can predict weight by starting with the initial prediction then adding the scaled value from the first tree and the second tree and the third tree etc etc etc once the math is all done we are left with the predicted weight in this case we predicted that this person weighed 70 kilograms triple bomb in summary when gradient boost is used for regression we start with a leaf that is the average value of the variable we want to predict in this case we want it to predict wait then we add a tree based on the residuals the difference between the observed values and the predicted values and we scale the trees contribution to the final prediction with a learning rate then we add another tree based on the new residuals adding trees based on the errors made by the previous tree that's all there is to it BAM tune in for part 2 in this series when we dive deep into the math behind the gradient boost algorithm for regression walking through it's step by step and proving that it really is this simple hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stat quest consider buying one of my original songs or buying a stat quest t-shirt or hoodie the links are in the description below alright until next time quest on
xOOrxD92Wt4,2019-03-02T22:30:01.000000,Last Night,[Music] last night pretended you were with me last night I pretended we were together last night pretended you were with me last night pretended we were [Music] [Music] [Music] [Music] [Music] last night I remembered your arms around me last night I remember the sound of your size last night I remembered real arms around me last night I remember the sound of your size it made me feel so good [Music] [Music] [Music] [Music] Tom keeps own person but your memory still haunts me time keeps on passing but your memory still haunts me it made me feel so and low it made me so good and alone [Music] feels so good and lonely [Music] feels so good and [Music] and alone you
sDv4f4s2SB8,2019-02-05T16:15:01.000000,"Gradient Descent, Step-by-Step","Gradient Descent is decent at estimating parameters. StatQuest! Hello! I'm Josh Starmer and welcome to StatQuest. Today we're going to learn about Gradient Descent and we're going to go through the algorithm step by step. Note: this StatQuest assumes you already understand the basics of least squares and linear regression, so if you're not already down with that, check out the Quest. In statistics, machine learning, and other data science fields, we optimize a lot of stuff. When we fit a line with linear regression, we optimize the intercept and the slope. When we use logistic regression, we optimize a squiggle. And when we use t-SNE, we optimize clusters. These are just a few examples of the stuff we optimize, there are tons more. The cool thing is that Gradient Descent can optimize all these things, and much more. So, if we learn how to optimize this line using Gradient Descent, then we'll have learned the strategy that optimizes this squiggle, and these clusters, and many more of the optimization problems we have in statistics, machine learning, and data science. So let's start with a simple data set. On the x-axis we have weight. On the y-axis we have height. If we fit a line to the data and someone tells us that they weigh 1.5, we can use the line to predict that they will be 1.9 tall. So let's learn how Gradient Descent can fit a line to data by finding the optimal values for the intercept and the slope. Actually, we'll start by using Gradient Descent to find the intercept. Then, once we understand how Gradient Descent works, we'll use it to solve for the intercept and the slope. So, for now, let's just plug in the Least Squares estimate for the slope, 0.64, and we'll use Gradient Descent to find the optimal value for the intercept. The first thing we do is pick a random value for the intercept. This is just an initial guess that gives Gradient Descent something to improve upon. In this case, we'll use 0, but any number will do. And that gives us the equation for this line. In this example, we will evaluate how well this line fits the data with the sum of the squared residuals. Note: in machine learning lingo, the sum of the squared residuals is a type of Loss Function. We'll talk more about Loss Functions towards the end of the video. We'll start by calculating this residual. This data point represents a person with weight 0.5 and height 1.4. We get the predicted height, the point on the line, by plugging weight equals 0.5 into the equation for the line. And the predicted height is 0.32. The residual is the difference between the observed height and the predicted height, so we calculate the difference between 1.4 and 0.32, and that gives us 1.1 for the residual. Here's the square of the first residual. The second residual is 0.4. and the third residual is 1.3. In the end, 3.1 is the sum of the squared residuals. Now, just for fun, we can plot that value on a graph. This graph has the sum of squared residuals on the y-axis, and different values for the intercept on the x-axis. This point represents the sum of the squared residuals when the intercept equals zero. However, if the intercept equals 0.25, then we would get this point on the graph. And if the intercept equals 0.5, then we would get this point. And for increasing values for the intercept we get these points. Of the points that we calculated for the graph, this one has the lowest sum of squared residuals. But is it the best we can do? What if the best value for the intercept is somewhere between these values? A slow and painful method for finding the minimal sum of the squared residuals is to plug and chug a bunch more values for the intercept. Don't despair! Gradient Descent is way more efficient. Gradient Descent only does a few calculations far from the optimal solution, and increases the number of calculations closer to the optimal value. In other words, gradient descent identifies the optimal value by taking big steps when it is far away, and baby steps when it is close. So let's get back to using gradient ascent to find the optimal value for the intercept, starting from a random value. In this case, the random value was zero. When we calculated the sum of the squared residuals, the first residual was the difference between the observed height, which was 1.4, and the predicted height, which came from the equation for this line. So we replace predicted height with the equation for the line. Since the individual weighs 0.5 we replace weight with 0.5. So, for this individual, this is their observed height and this is their predicted height. Note: we can now plug in any value for the intercept and get a new predicted height. Now let's focus on the second data point. Just like before, the residual is the difference between the observed height, which is 1.9, and the predicted height, which comes from the equation for the line. Snd since this individual weighs 2.3, we replace weight with 2.3. Now let's focus on the last person. Again, the residual is the difference between the observed height, which is 3.2, and the predicted height, which comes from the equation for the line. And since this person weighs 2.9, we'll replace weight with 2.9. Now we can easily plug in any value for the intercept and get the sum of the squared residuals. Thus, we now have an equation for this curve, and we can take the derivative of this function and determine the slope at any value for the intercept. So let's take the derivative of the sum of the squared residuals with respect to the intercept. The derivative of the sum of the squared residuals with respect to the intercept equals the derivative of the first part, plus the derivative of the second part, plus the derivative of the third part. Let's start by taking the derivative of the first part. First, we'll move this part of the equation up here, so that we have room to work. To take the derivative of this we need to apply the chain rule. So we start by moving the square to the front and multiply that by the derivative of the stuff inside the parentheses. These parts don't contain a term for the intercept, so they go away. Then we simplify by multiplying two by negative one. And this is the derivative of the first part, so we plug it in. Now we need to take the derivative of the next two parts. I'll leave that as an exercise for the viewer. Bam! Let's move the derivative up here, so that it's not taking up half the screen. Now that we have the derivative, Gradient Descent will use it to find where the sum of squared residuals is lowest. Note: if we were using least squares to solve for the optimal value for the intercept, we would simply find where the slope of the curve equals zero. In contrast gradient descent finds the minimum value by taking steps from an initial guess until it reaches the best value. This makes Gradient Descent very useful when it is not possible to solve for where the derivative equals zero. And this is why Gradient Descent can be used in so many different situations. Bam! Remember, we started by setting the intercept to a random number. In this case that was zero. So we plug zero into the derivative and we get negative 5.7. So, when the intercept equals 0, the slope of the curve equals negative 5.7. Note: the closer we get to the optimal value for the intercept, the closer the slope of the curve gets to zero. This means that when the slope of the curve is close to zero, then we should take baby steps, because we are close to the optimal value. And when the slope is far from zero, then we should take big steps because we are far from the optimal value. However, if we take a super, huge step, then we would increase the sum of the squared residuals. So the size of the step should be related to the slope, since it tells us if we should take a baby step or a big step. But we need to make sure the big step is not too big. Gradient Descent determines the step size by multiplying the slope by a small number called the learning rate. Note: we'll talk more about learning rates later. When the intercept equals 0, the step size equals negative 5.7. With the step size, we can calculate a new intercept. The new intercept is the old intercept minus the step size. So we plug in the numbers, and the new intercept equals 0.57. Bam! In one big step, we moved much closer to the optimal value for the intercept. Going back to the original data and the original line, with the intercept equals 0, we can see how much the residuals shrink when the intercept equals 0.57. Now let's take another step closer to the optimal value for the intercept. To take another step, we go back to the derivative and plug in the new intercept, and that tells us the slope of the curve equals negative 2.3. Now let's calculate the step size. By plugging in negative 2.3 for the slope, and 0.1 for the learning rate, ultimately the step size is negative 0.23. And the new intercept equals 0.8. Now we can compare the residuals when the intercept equals 0.57 to when the intercept equals 0.8. Overall the sum of the squared residuals is getting smaller. Notice that the first step was relatively large, compared to the second step. Now let's calculate the derivative at the new intercept: and we get negative 0.9. The step size equals negative 0.09, and the new intercept equals 0.89. Now we increase the intercept from 0.8 to 0.89, then we take another step and the new intercept equals 0.92. And then we take another step, and the new intercept equals 0.94. And then we take another step, and the new intercept equals 0.95. Notice how each step gets smaller and smaller the closer we get to the bottom of the curve. After six steps, the gradient ascent estimate for the intercept is 0.95. Note: the least squares estimate for the intercept is also 0.95. so we know that gradient descent has done its job, but without comparing its solution to a gold standard, how does gradient descent know to stop taking steps? Gradient Descent stops when the step size is very close to zero. The step size will be very close to zero when the slope is very close to zero. In practice, the minimum step size equals 0.001 or smaller. So if this slope equals 0.009, then we would plug in 0.009 for the slope and 0.1 for the learning rate and get 0.0009, which is smaller than 0.001, so gradient descent would stop. That said, gradient descent also includes a limit on the number of steps it will take before giving up. In practice, the maximum number of steps equals 1000 or greater. So, even if the step size is large, if there have been more than the maximum number of steps, gradient descent will stop. Okay, let's review what we've learned so far. The first thing we did is decide to use the sum of the squared residuals as the loss function to evaluate how well a line fits the data. Then, we took the derivative of the sum of the squared residuals. In other words, we took the derivative of the loss function. Then, we picked a random value for the intercept, in this case we set the intercept to be equal to zero. Then, we calculated the derivative when the intercept equals zero, plugged that slope into the step size calculation, and then calculated the new intercept, the difference between the old intercept and the step size. Lastly, we plugged the new intercept into the derivative and repeated everything until step size was close to zero. Double bam! Now that we understand how gradient descent can calculate the intercept, let's talk about how to estimate the intercept and the slope. Just like before, we'll use the sum of the squared residuals as the loss function. This is a 3D graph of the loss function for the different values for the intercept and the slope. This axis is the sum of the squared residuals, this axis represents different values for the slope, and this axis represents different values for the intercept. We want to find the values for the intercept and slope that give us the minimum sum of the squared residuals. So, just like before, we need to take the derivative of this function. And just like before, we'll take the derivative with respect to the intercept. But, unlike before, we'll also take the derivative with respect to the slope. We'll start by taking the derivative with respect to the intercept. Just like before, we'll take the derivative of each part. And, just like before, we'll use the chain rule and move the square to the front, and multiply that by the derivative of the stuff inside the parentheses. [Music] Since we are taking the derivative with respect to the intercept, we treat the slope like a constant, and the derivative of a constant is zero. So, we end up with negative one, just like before. Then we simplify by multiplying two by negative one. And this is the derivative of the first part. So we plug it in. Likewise, we replace these terms with their derivatives. So this whole thing is the derivative of the sum of squared residuals with respect to the intercept. Now let's take the derivative of the sum of the squared residuals with respect to the slope. Just like before, we take the derivative of each part and, just like before, we'll use the chain rule to move the square to the front and multiply that by the derivative of the stuff inside the parentheses. Since we are taking the derivative with respect to the slope, we treat the intercept like a constant and the derivative of a constant is zero. So we end up with negative 0.5. Then we simplify by moving the negative 0.5 to the front. Note: I left the 0.5 in bold instead of multiplying it by 2 to remind us that 0.5 is the weight for the first sample. And this is the derivative of the first part. So we plug it in. Likewise, we replace these terms with their derivatives. Again, 2.3 and 2.9 are in bold to remind us that they are the weights of the second and third samples. Here's the derivative of the sum of the squared residuals with respect to the intercept, and here's the derivative with respect to the slope. Note: when you have two or more derivatives of the same function they are called a gradient. We will use this gradient to descend to the lowest point in the loss function, which, in this case, is the sum of the squared residuals. Thus, this is why the algorithm is called Gradient Descent. Bam! Just like before, we'll start by picking a random number for the intercept. In this case, we'll set the intercept to be equal to zero, and we'll pick a random number for the slope. In this case we'll set the slope to be 1. Thus, this line, with intercept equals 0 and slope equals 1, is where we will start. Now, let's plug in 0 for the intercept and 1 for the slope. And that gives us two slopes. Now, we plug the slopes into the step size formulas, and multiply by the learning rate, which this time we set to 0.01. Note: The larger learning rate that we used in the first example doesn't work this time. Even after a bunch of steps, Gradient Descent doesn't arrive at the correct answer. This means that Gradient Descent can be very sensitive to the learning rate. The good news is that, in practice, a reasonable learning rate can be determined automatically by starting large and getting smaller with each step. So, in theory, you shouldn't have to worry too much about the learning rate. Anyway, we do the math and get two step sizes. Now we calculate the new intercept and new slope by plugging in the old intercept and the old slope, and the step sizes. And we end up with a new intercept and a new slope. This is the line we started with and this is the new line after the first step. Now we just repeat what we did until all of the step sizes are very small, or we reach the maximum number of steps. This is the best fitting line, with intercept equals 0.95 and slope equals 0.64,  the same values we get from least squares. Double bam! We now know how Gradient Descent optimizes two parameters, the slope and the intercept. If we had more parameters then we just take more derivatives and everything else stays the same. Triple bam! Note: the sum of the squared residuals is just one type of Loss Function. However, there are tons of other loss functions that work with other types of data. Regardless of which Loss Function you use, Gradient Descent works the same way. Step 1: take the derivative of the loss function for each parameter in it. In fancy machine learning lingo, take the gradient of the loss function. Step 2: pick random values for the parameters. Step 3: plug the parameter values into the derivatives (ahem, the gradient). Step 4: calculate the step sizes. Step 5: calculate the new parameters. Now go back to step 3 and repeatuntil step size is very small or you reach the maximum number of steps. One last thing before we're done. In our example we only had three data points, so the math didn't take very long. But when you have millions of data points it can take a long time. So there is a thing called Stochastic Gradient Descent that uses a randomly selected subset of the data at every step rather than the full data set. This reduces the time spent calculating the derivatives of the loss function. That's all. Stochastic Gradient Descent sounds fancy, but it's no big deal. Hooray! We've made it to the end of another exciting StatQuest. If you like this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, well, consider buying one or two of my original songs, or buying a StatQuest t-shirt or hoodie. The linksare in the description below. Alright, until next time. Quest on!"
Dr2aj85tcUI,2019-02-01T17:30:00.000000,A Drink From The Well,[Music] you take this cup the drink from the world that lend Maori rub story to tell not long ago there was a castle and a prince and the maiden so feral surrounded by fence [Music] you [Music] inside those walls the gardens grow lush and green but the walls themselves were a sight so up see ahead on each post they were awarding to a song to keep to us unknown to question [Music] a night for most sites all the maiden from a farm and he was drawn to the fence like a moth to a star so we calmed up made his way across the lawn but the prince acted fast and stabbed the night before the dawn look over there you see his head upon a post and this is what he said before he gave up the ghost alas sweet revenge won't be a story I can tell but I saw put poison in the world [Music] [Music]
LsK-xG1cLYA,2019-01-14T20:00:04.000000,"AdaBoost, Clearly Explained",mocchi but it's not so complicated stack quest hello I'm Josh stormer and welcome to stack quest today we're gonna cover adaboost and it's gonna be clearly explained note this stack quest shows how to combine adaboost with decision trees because that is the most common way to use adaboost so if you're not familiar with decision trees check out the quest we will also mention random forests so if you don't know about them check out the quest we'll start by using decision trees and random forests to explain the three concepts behind adaboost then we'll get into the nitty-gritty details of how adaboost creates a forest of trees from scratch and how it's used to make classifications so let's start by using decision trees and random forests to explain the three main concepts behind adaboost in a random forest each time you make a tree you make a full-sized tree some trees might be bigger than others but there's no predetermined maximum depth in contrast in a forest of trees made with adaboost the trees are usually just a node and two leaves oh no it's the dreaded terminology alert a tree with just one node and two leaves is called a stump so this is really a forest of stumps rather than trees stumps are not great at making accurate classifications for example if we were using this data to determine if someone had heart disease or not then a full-size decision tree would take advantage of all four variables that we measured chest pain blood circulation blocked arteries and weight to make a decision but a stump can only use one variable to make a decision thus stumps are technically weak learners however that's the way adaboost likes it and it's one of the reasons why they are so commonly combined now back to the random forest in a random forest each tree has an equal vote on the final classification this trees vote is worth just as much as this trees vote or this trees vote in contrast in a forest of stumps made with adaboost some stumps get more say in the final classification than others in this illustration the larger stumps get more say in the final classification than the smaller stumps lastly in a random forest each decision tree is made independently of the others in other words it doesn't matter if this tree was made first or this one in contrast in a forest of stumps made with adaboost order is important the errors that the first stump makes influence how the second stump is made and the air is that the second stone makes influence how the third stump is made etc etc etc to review the three ideas behind adaboost our one adaboost combines a lot of week learners to make classifications the week learners are almost always stumps to some stumps get more say in the classification than others three each stump is made by taking the previous stumps mistakes into account BAM into the nitty-gritty detail of how to create a forest of stumps using adaboost first we'll start with some data we create a forest of stumps with adaboost to predict if a patient has heart disease we will make these predictions based on a patient's chest pain and blocked artery status and their weight the first thing we do is give each sample a weight that indicates how important it is to be correctly classified note the sample weight is different from the patient weight and I'll do the best I can to be clear about which of the two I'm talking about at the start all samples get the same weight one divided by the total number of samples in this case that's 1/8 and that makes the samples all equally important however after we make the first stump these weights will change in order to guide how the next stump is created in other words we'll talk more about the sample weights later now we need to make the first stump in the forest this is done by finding the variable chest pain blocked arteries or patient weight that does the best job classifying the samples note because all of the weights are the same we can ignore them right now we start by seeing how well chest pain classifies the samples of the five samples with chest pain three were correctly classified as having heart disease and two were incorrectly classified of the three samples without chest pain two were correctly classified as not having heart disease and one was incorrectly classified now we do the same thing for blocked arteries and for patient wait note we used the techniques described in the decision tree stat quest to determine that 176 was the best weight to separate the patients now we calculate the Gini index for the three stumps the Gini index for patient weight is the lowest so this will be the first stump in the forest now we need to determine how much say this stump will have in the final classification remember some stumps get more say in the final classification than others we determine how much say a stump has in the final classification based on how well it classified the samples this stump made one error patient who weighs less than 176 has heart disease but the stump says they do not the total error for a stump is the sum of the weights associated with the incorrectly classified samples thus in this case the total error is 1/8 note because all of the sample weights add up to one total air will always be between zero for a perfect stump and one for a horrible stump we use the total error to determine the amount of say this stump has in the final classification with the following formula amount of say equals 1/2 times the log of 1 minus the total error divided by the total error we can draw a graph of the amount of say by plugging in a bunch of numbers between 0 & 1 for total error the blue line tells us the amount of safe for total error values between 0 & 1 when a stump does a good job and the total error is small then the amount of say is a relatively large positive value when a stump is no better at classification than flipping a coin ie half the stumps are correctly classified and half are incorrectly classified and the total error equals 0.5 then the amount of say will be zero and when a stump does a terrible job and the total error is close to one in other words if the stump consistently gives you the opposite classification then the amount of say will be a large negative value so if a stump votes for heart disease the negative amount of say will turn that vote into not heart disease note if total error is 1 or 0 then this equation will freak out in practice a small error term is added to prevent this from happening with patient weight greater than 176 the total error is 1/8 so we just plug and chug booty today voodoo to pooh-poohed and the amount of say that this stump has in the final classification is zero point nine seven BAM now that we've worked out how much say this stump gets when classifying a sample let's work out how much say the chest pain stump would have if it had been the best stump note we don't need to do this but I think it helps illustrate the concepts we've covered so far chest pain made three errors and the total error equals the sum of the weights for the incorrectly classified samples be do do booty so the total error for chest pain is 3/8 we can get a sense of what the amount of say will be by looking at the graph when total error equals 3/8 so we are expecting the amount of say to be between 0 and 0.5 now we plug 3/8 into the formula for the amount of say and do the math do t do pooty to do D pooh-coo pooh-coo and the amount of say that the chest pain stump would have had on the final classification is 0.42 I'll leave the blocked artery stump as an exercise for the viewer now we know how the sample weights for the incorrectly classified samples are used to determine the amount of say each stump gets BAM now we need to learn how to modify the weights so that the next stump will take the errors that the current stump made into account let's go back to the first stump that we made when we created this stump all of the sample weights were the same and that meant we did not emphasize the importance of correctly classifying any particular sample but since this stumped incorrectly classified this sample we will emphasize the need for the next stump to correctly classify it by increasing its sample weight and decreasing all of the other sample weights let's start by increasing the sample wait for the incorrectly classified sample this is the formula we will use to increase the sample weight for the sample that was incorrectly classified we plug in the sample weight from the last stump and we scale one eighth with this term to get a better understanding of how this part will scale the previous sample wait let's draw a graph the blue line is equal to e raised to the amount of say when the amount of say is relatively large ie the last stump did a good job classifying samples then we will scale the previous sample weight with a large number this means that the new sample weight will be much larger than the old one and when the amount of say is relatively low ie the last stump did not do a very good job classifying samples then the previous sample weight is scaled by a relatively small number this means that the new sample weight will only be a little larger than the old one in this example the amount of say was zero point nine seven and E raised to the zero point nine seven equals two point six four that means the new sample weight is zero point three three which is more than the old one BAM now we need to decrease the sample weights for all of the correctly classified samples this is the formula we will use to decrease the sample weights the big difference is the negative sign in front of amount of say just like before we plug in the sample wait and just like before we can get a better understanding of how this will scale the sample weight by plotting a graph using different values for amount of say the blue line represents e raised to the negative amount of say when the amount of se is relatively large then we scale the sample weight by a value close to zero this will make the new sample weight very small if the amount of say for the last stump is relatively small then we will scale the sample weight by a value close to 1 this means that the new sample weight will be just a little smaller than the old one in this example the amount of say was 0.97 and e raised to the negative zero point nine seven equals zero point three eight the new sample weight is zero point zero five which is less than the old one BAM we will keep track of the new sample weights in this column we plug in zero point three three for the sample that was incorrectly classified all of the other samples gets zero point zero five now we need to normalize the new sample weights so that they will add up to one right now if you add up the new sample weights you get zero point six eight so we divide each new sample weight by 0.68 to get the normalized values now when we add up the new sample weights we get 1 plus or minus a little rounding error now we just transfer the normalized sample weights to the sample weights column since those are what we will use for the next stomp now we can use the modified sample weights to make the second stump in the forest BAM in theory we could use the sample weights to calculate weighted genie indexes to determine which variable should split the next stump the weighted genie index would put more emphasis on correctly classifying this sample the one that was miss classified by the last stump since this sample has the largest sample weight alternatively instead of using a weighted Gini index we can make a new collection of samples that contains duplicate copies of the samples with the largest sample weights so we start by making a new but empty data set that is the same size as the original then we pick a random number between 0 and 1 and we see where that number falls when you use the sample weights like a distribution if the number is between 0 and 0.7 then we would put this sample into the new collection of samples and if the number is between 0.7 and 0.14 then we would put this sample into the new collection of samples and if the number is between 0.14 and 0.2 1 then we would put this sample into the new collection of samples and if the number is between zero point two one and zero point seven zero then we would put this sample into the new collection of samples etc etc for example imagine the first number I picked was 0.72 then I would put this sample into my new collection of samples then I pick another random number and get 0.42 and I would put this sample into my new collection of samples then I pick 0.83 and I would put this sample into my new collection of samples then I pick 0.51 and I would put this sample into my new collection of samples note this is the second time we have added this particular sample to the new collection of samples we then continue to pick random numbers and add samples to the new collection until the new collection is the same size as the original ultimately this sample was added to the new collection of samples four times reflecting its larger sample weight now we get rid of the original samples and use the new collection of samples lastly we give all of the samples equal sample ways just like before however that doesn't mean the next stump will not emphasize the need to correctly classify these samples because these samples are all the same they will be treated as a block creating a large penalty for being misclassified now we go back to the beginning and try to find the stump that does the best job classifying the new collection of samples so that is how the errors that the first tree makes influence how the second tree is made and how the errors that the second tree makes influence how the third tree is made etc etc etc double bam now we need to talk about how a forest of stumps created by adaboost makes classifications imagine that these stumps classified a patient as has heart disease and these stumps classified the patient as does not have heart disease these are the amounts of say for these stumps and these are the amounts of say for these stumps now we add up the amounts of say for this group of stumps and for this group of stumps ultimately the patient is classified as has heart disease because this is the larger sum triple bam to review the three ideas behind adaboost our one adaboost combines a lot of weak learners to make classifications the weak learners are almost always stumps to some stumps get more say in the classification than others and three each stump is made by taking the previous stumps mistakes into account if we have a way to Gini function then we use it with the sample weights otherwise we use the sample weights to make a new dataset that reflects those weights hooray we've made it to the end of another exciting stack quest if you like this stack quest and want to see more please subscribe and if you want to support stack quest well consider buying a t-shirt or a hoodie or buying one or two of my original songs the links to do this are in the description below alright until next time quest on
CqLGvwi-5Pc,2019-01-08T02:00:00.000000,"Design Matrices For Linear Models, Clearly Explained!!!",stat Quest is getting bigger watch out hello and welcome to stat quest stat quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about general linear models and this is part 3 of a series that we're doing on this this time we're gonna focus on design matrices this stat quest picks up exactly where part two leaves off so if you haven't seen that one yet I'd recommend doing it right now in part two of this series we ended by saying that this was not the standard design matrix for a t-test it was kind of a cliffhanger and then I showed you that this is the standard design matrix for a t-test it corresponds to a slightly different equation let's focus on what this new design matrix and equation are all about in this version all measurements both control and mutant turn on the term for the mean control value but only the mutant measurements turn on the term for the difference between the mean of the mutant data and the mean of the control data this term serves as an offset that we can use for the mutant data for example this one turns the term for the mean of the control data on for this data point and this zero turns the term for the difference between the mean of the mutant data and the mean of the control data off for this data point this one turns the term for the mean of the control data on for this data point and this one turns the term for the difference between the mean of the mutant data and the control data on for this data point the residuals are the same for both equations and design matrices the equations also have the same number of parameters too so P fit is the same so in our equation for f we plug in the exact same value for the sum of squares around the fit we also plug in the exact same value for the P fit parameter this results in the exact same value for F and that means we're going to get the exact same P value if they both do the same thing and the result is the same P value Y is the one on the right more common I'll be honest I don't know the answer for sure but I think it has something to do with regression so far we've looked at design matrices in the context of using ones and zeros to turn parts of the equation on or off so let's take a step back and remember how it works remember that the numbers in the first column are multiplied by the term for the mean of the control values and the numbers in the second column are multiplied by the term that represents the difference between the mean of the mutant values and the mean of the control values multiplying the mean of the control data by one turns it on by just letting it be multiplying the difference between the mean of the mutant data and the mean of the control data by zero makes it zero and that turns it off a design matrix full of ones and zeros is perfect for doing T tests or ANOVA's anytime we have different categories of data but we can use other numbers for example here's a design matrix for linear regression it pairs with this equation we've got a bunch of ones in the first column and in the second column we've got the x-axis position for each point let's focus on the first row in the design matrix for now it corresponds to this point just like before the numbers in the first column multiply the first term in the formula in this case multiplying the y-intercept by one turns it on and just like before the numbers in the second column multiply the second term in the formula in this case we're scaling the term for the slope to make this more concrete let's see what happens when we use real numbers for the y-intercept and slope the y-intercept is super small N equals 0.01 so that's the number we plug in here the slope equals zero point eight and we plug that in right here and now we do the math and you get a point on the least squares fit line that corresponds with the first data point now let's focus on the second row it corresponds to this point the number in the first column multiplies the y-intercept and the number in the second column scales the slope plug in the y-intercept and the slope and do the math and you get a point on the line that corresponds to the second data point plugging each row into the equation gives us a bunch of points on the least squares fit line once we have all the points on the line we can calculate the residuals and that means we can calculate a p-value this example shows that a design matrix isn't always just a bunch of zeros and ones but can be any set up numbers that we want to plug into an equation one row at a time one note before we move on since this style of design matrix with one's all the way down the first column is more common all of the examples from here on out will be consistent with this format now that we know we can put any number into the design matrix let's do something fancy let's combine a t-test and a regression holy smokes that's totally crazy okay we're back to the relationship between mouse weight and mouse size however we have two types of mice these measurements are from normal control mice these measurements are from mutant mice that make them tall and skinny by eye we can see that mutant mice tend to be larger even if they weigh the same in other words the mutant mice seem to follow this trend in the control mice seem to follow this trend can we use statistics to test if there is a significant difference between the two types of mice if we just did a regression we'd get a nice looking line but it wouldn't tell us if the mutant mice were significantly larger than the normal mice on the other hand a normal t-test would ignore the relationship between weight and size and in this case the p-value is greater than 0.05 since Mouse type and the relationship between weight and size are both important we need to combine them into a single test in other words instead of comparing this mean to this mean which is what a t-test would do we want to compare this line to this line to do this we need an equation that has a term for the y-intercept for the normal mice we also need a term for the mutant mouse offset and lastly a term for the slope which in this case is the same for both types of mice this means we need to design matrix where the first column is ones this means that both lines intercept the y-axis at some point the second column indicates whether the mutant offset is on or off the mutant offset is off for the control mice and on for the mutant mice this allows the mutants to have their own y-intercept and the last column has the weight data the first four values are the x coordinates for the control mice and the last four values are the x coordinates for the mutant mice let's focus on the first row plug in the numbers and we get a value on the red line now let's focus on the second row again we get a value on the red line and from here we just plug in the values and we get coordinates on either the red or the green line we get coordinates on the red line when the mutant offset is off and we get coordinates on the green line when the mutant offset is on once we have the locations on the lines we can calculate the residuals which are hard to see since they are so small in this example now we can compare the fancy model to a simpler model in the simpler model we model Mouse size by using just the average size of the mouse we ignore Mouse weight and we ignore Mouse type this is the default model that we use when we do the t-test now we plug in the sum of squares of the residuals for the fancy model and we plug in 3 for the P fancy term since there are 3 parameters in our fancy equation and we plug in the sum of squares of the residuals for the simple model and we plug in one for the P simple term since there is only one parameter in the simple equation this gives us 20 1.88 and that gives us a p-value of 0.003 BAM the small p-value says that taking weight and Mouse type into account is significantly better at predicting size than just using the average size note the simple model can be any simpler model if we did a super simple linear regression we'd have a model that takes weight into account but ignores the fact that some mice are normal and others are mutants then we plug in the sum of squares of the residuals just like before the simple regression equation has two parameters so P simple equals 2 we then plug in the numbers and we get a p-value equal to zero point zero zero two three double bail this small p-value suggests that using both weight and mouse type is better at predicting Mouse size than weight alone here's another simple model it's just a normal t-test this model ignores Mouse weight again plug in the sum of squares of the residuals in the equation has two parameters so P simple equals two and that gives us a p-value equal to zero point zero zero two five oh my gosh it's the coveted triple bam this small p-value suggests that using mouse weight and type is better at predicting Mouse size than Mouse type alone so you can see that the question you want to ask determines what type of simple model you want to use to compare your fancy model to okay one last example of a design matrix lab a dozen experiment then lab B replicates it however their measurements tended to be smaller overall this is a batch effect we would like to combine these two datasets to see if mutants are different from controls but we need to compensate for the batch effect here's how to do it first add a term for the mean control value from lab a second add a term for the lab B offset this takes care of the batch effect third add a term for the differences between the mutant and the control measurements here's the design matrix essentially we want to know if this last term in the equation is important or not alternatively is this last column important so we compare the fit of this fancy equation to this simpler one that ignores the control mutant difference a small p-value will tell us that the equation that keeps track of the mutant control differences predicts the gene expression better than one that does not this will mean that the difference between controls and mutants is significant hurray we've made it to the end of another exciting stat quest if you like this stat quest and would like to see more please subscribe and if you have any other ideas of things you'd like me to do stat quests on or you've got a certain design matrix you'd like to see an example of just let me know in the comments below until then quest on
F7Gqjbg3bWI,2018-12-31T14:00:04.000000,Wildest Dreams,[Music] he said let's get out of this town drive out of the city away from the crowds she thought heaven count helper now nothing lasts forever but this is gonna take her down he's so tall and handsome as hell he's so bad but he does it so well she can see the end as it begins oh one condition is say you'll remember me standing in a nice dress staring at the sunset babe red lips and rosy cheeks say you'll see me again even if it's just and you two streams [Music] strange [Music] she said no one has to know what we do his hands are in her head his clothes are in her room and his voice a familiar sound nothing lasts forever but this is getting good now he's so tall and handsome as hell he's so bad but he does it so well when they've had the very last kiss the last request is say you'll remember me standing in a nice dress staring at the sunset babe red lips and rosy cheeks say you'll see me again if it's just in your the strings [Music] the streams he'll see her and hindsight tangled up this doll she said someday when you leave these memories follow [Music] you'll see her and hindsight tangled up night burning it's down she said someday when you leave me [Music] these memories follow you around [Music] she said say you'll remember me standing in a nice dress staring at the sunset babe red lips and rosy cheeks say you'll see me again even if it's just pretend say you'll remember me standing in a nice dress staring at the sunset cheeks say you'll see me [Music] it's just proved [Music] you
qcvAqAH60Yw,2018-12-18T00:00:03.000000,ROC and AUC in R,let's make a graph let's make it look cool thank goodness Stack quest is here just step quest rules stack quest hello I'm Josh Starla and welcome to stack quest today we're gonna talk about drawing ROC graphs and calculating the AUC in R if you're interested in doing this at home there's a link to the example code in the description below in this stack quest we'll draw simple ROC graph extract thresholds for a specific region in the ROC draw and compute a partial area under the curve and end by layering two ROC graphs so that they can be easily compared note this stack quest builds on the example used in the ROC and AUC stack quest so you might want to watch the first few minutes of that one if you haven't already the first thing we need to do is load in PR OC the library that will draw ROC graphs for us if you don't have P ROC installed just use installed out packages PR OC to install it we're also going to use the random forests package as part of the example for the purposes of this stack quest you just need to know that a random forest is a way to classify samples and we can change the threshold that we use to make those decisions however if you want more details check out the quests don't have random forests installed just use install dot packages random forests to install it since we are going to generate an example dataset let's set the seed for the random number generator so that we can reproduce our results note I usually set the random number seed to 42 but 420 made a nicer looking set of random data this example dataset will be just like the one we used in the ROC and AOC stat quest only this one will have 100 samples instead of just eight so let's start by setting num dots ampuls to 100 now we'll create 100 measurements and store them in a variable called weight we do this by using the R norm function to generate 100 random values from a normal distribution with the mean set to 172 and the standard deviation set to 29 just just in case you're interested the internet told me that the average man weighs 172 pounds with a standard deviation of 29 and then we use the sort function to sort the numbers from low to high this next line of code classifies an individual as obese or not obese the way we are going to classify a sample as obese is to start by using the rank function to rank the weights from lightest to heaviest the lightest sample will have rank equals 1 and the heaviest sample will have rank equals 100 then we scale the ranks by 100 this means that the lightest sample will equal 1 divided by 100 which equals 0.01 and the heaviest sample will equal 100 divided by 100 which equals 1 then we compare the scaled ranks to random numbers between 0 & 1 and if the random number is smaller than the scaled rank then the individual is classified as obese otherwise it is classified as not obese the if smaller than obese otherwise not obese is performed by the if-else function and the results are stored in a variable called obese to see what that fancy line of code just did we can print out the contents of obese the zeroes stand for not obese and the ones stand for obese the lighter samples are mostly zeros not obese and the heavier samples are mostly wands obese now let's plot the data to see what it looks like BAM these samples are obese and these samples are not obese now we will use the GLM function to fit a logistic regression curve to the data if you want to learn more about how the GLM function works there's a stat quest that can fill you in on the details otherwise just know that we can store the results of the GLM function in a variable called GL m dot fit and pass weight and the fitted values stored in GL m dot fit into the lines function to draw a curve that tells us the predicted probability that an individual is obese or not obese GL m dot fit dollar sign fitted out values contains the y axis coordinates along the curve for each sample in other words GL m dot fit dollar sign fitted dot values contains estimated probabilities that each sample is obese we will use the known classifications and the estimated probabilities to draw an ROC curve we use the ROC function from the PR OC library to draw the ROC graph we pass in the known classifications obese or not obese for each sample and the estimated probabilities that each sample is obese and we tell the ROC function to draw the graph not just calculate all of the numbers used to draw the graph when you use the ROC function it prints out a bunch of stuff the first part just echoes what you typed in and isn't very interesting the second part is a little more interesting it tells us how many samples were not obese remember not obese equals zero and how many samples were obese obese equals one the third part is the most interesting of all it tells us the area under the curve or the AUC tada here's the graph here's the ROC curve and here's the diagonal line that shows where the true positive rate is the same as the false positive rate but if you draw this graph in our studio then you'll also get this ugly padding on each side to get rid of the ugly padding we have to use the par function and muck around with the graphics parameters in this case we set PT y aka the plot type to s which is short for square then we use the up arrow key to bring back the call to the ROC function and we get a much nicer ROC graph by default the ROC function plots specificity on the x-axis instead of one - specificity as a result the x-axis goes from one on the left side to zero on the right side if using a backwards X access gives you a headache don't freak out instead said legacy taxis to true and the ROC function who will use one - specificity on the x-axis and all will be right in the world again now that we're on the subject of changing the axes I have a confession to make I have a very hard time remembering what sensitivity and specificity mean so I set percent to true so that the axes are in percentages rather than values between zero and one and I label the x-axis false positive percentage and label the y-axis true positive percentage BAM now we have a nice-looking x-axis and a nice-looking y-axis we can also change the color of the ROC curve and make it thicker you can change the color to anything you want by setting the call parameter I set it to RGB values that I found on the color Brewer website and you can change the line thickness to whatever you want by setting the lwd parameter now imagine were interested in the range of thresholds that resulted in this part of the ROC curve we can access those thresholds by saving the calculations that the ROC function does in a variable and then make a data frame that contains all of the true positive percentages by multiplying the sensitivities by 100 and the false positive percentages by multiplying 1 minus specificities by 100 and last but not least the thresholds we can then use the head function to look at the first six rows of the new data frame and we see that when the threshold is set to negative infinity so that every single sample is called obese then the TPP the true positive percentage is 100 because all of the obese samples were correctly classified and the fpp the false positive percentage is also 100 because all of the samples that were not obese were incorrectly classified so the first row in roc DF corresponds to the upper right hand corner of the ROC curve we can use the tail function to look at the last six rows of the data frame and we see that when the threshold is set to positive infinity so that every single sample is classified not obese then the TPP and fpp are both zero because none of the samples were classified either correctly or incorrectly obese so the last row in our OCD f corresponds to the bottom left-hand corner of the ROC curve now we can isolate the TPP the fpp and the thresholds used when the true positive rate is between 60 and 80 if we were interested in choosing a threshold in this range we could pick one that had the optimal balance of true positives and false positives now let's go back to talking about customizing with the ROC function drawers if we want to print the AUC directly on the graph then we set the print auc parameter to true you can also draw and calculate a partial area under the curve these are useful when you want to focus on the part of the ROC curve that only allows for a small number of false positives to print and draw the partial a you see we start by setting the print a you see parameter to true we then specify where along the x-axis you want the AUC to be printed otherwise the text might overlap something important note I settled on 45 after trying a bunch of different locations then we set partial AUC to the range of specificity values that we want to focus on note the range of values is in terms of specificity not one - specificity so one hundred percent specificity corresponds to zero percent on our one - specificity axis and ninety percent specificity corresponds to ten percent on our one - specificity axis then we draw the partial area under the curve by setting a UC polygon to true and we set a UC polygon call to specify the polygons color note I use the same RGB numbers that I used to make the line blue however I added two digits to the end to make the color semi-transparent BAM for those of you keeping track were up to three exclamation points on the BAM lastly let's talk about how to overlap two ROC curves so that they are easy to compare we'll start by making a random forest classifier with the same data set now we draw the original ROC curve for the logistic regression then we add the ROC curve for the random forest we do this with the plot ROC function almost everything in the call to plot ROC is the same as in the call to the ROC function however since we are using a random forest for the second ROC we pass in the number of trees in the forest that voted correctly we also set the color to green instead of blue again I got these RGB values from the color Brewer website we also set ad to true so that this ROC curve is added to an existing graph and we set print AUC dot why 240 so that the AUC for the random forest is printed below the AUC for the logistic regression lastly we draw legend in the bottom right hand corner um once we're all done drawing ROC graphs we need to reset the PTY graphical parameter back to its default value M which is short for maximum as in use the maximum amount of space provided to draw graphs hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest well consider getting a t-shirt or a hoodie or buying one or two of my original songs the links to do that are all below alright until next time quest on
Gv9_4yMHFhI,2018-11-26T17:00:01.000000,A Gentle Introduction to Machine Learning,gonna start this tech quest with silly song but if you don't like silly songs that's okay stack quests hello I'm Josh stormer and welcome to stack quest today we're going to do a gentle introduction to machine learning note this stack quest was originally prepared for and presented at the Society for scientific advancements annual conference one of the things that Sosa does is promote science and technology in Jamaica let's start with a silly example do you like silly songs if you like silly songs are you interested in machine learning if you like silly songs and machine learning then you'll love stack quest if you like silly songs but not machine learning are you interested in statistics if you like silly songs and statistics but not machine learning then you'll still love stack quest otherwise you might not like stack quest won't Wang if you don't like silly songs are you interested in machine learning if you don't like silly songs but you like machine learning then you'll love stack quest if you don't like silly songs or machine learning are you interested in statistics if you don't like silly songs or machine learning but you're interested in statistics then you will love stack quest otherwise you might not like stack quest wah wah this is a silly example but it illustrates a decision tree a simple machine learning method the purpose of this particular decision tree is to predict whether or not someone will love stack quest alternatively we could say that this decision tree classifies a person as either someone who loves stack quest or someone who doesn't since decision trees are a type of machine learning then if you understand how we use this tree to predict or classify if someone would love stack quest you are well on your way to understanding machine learning BAM here's another silly example of machine learning imagine we measured how quickly someone could run 100 meters and how much yam they ate this is me I'm not very fast and I don't eat much yam these are some other people and this is Shane bolt hold is very fast Andy eats a lot of yam given this pretend data we see that the more yam someone eats the faster they run the 100-meter dash we can fit a black line to the data to show the trend but we can also use the black line to make predictions for example if someone told us they ate this much yam then we could use the black line to predict how fast that person might run this is the predicted speed the black line is a type of machine learning because we can use it to make predictions in general machine learning is all about making predictions and classifications BAM now that we can make predictions and classifications let's talk about some of the main ideas in machine learning first of all in machine learning lingo the original data is called training data so the black line is fit to training data alternatively we could have fit a green squiggle to the training data the green squiggle fits the training data better than the black line but remember the goal of machine learning is to make predictions so we need a way to decide if the green squiggle is better or worse than the black line at making predictions so we find a new person and measure how fast they run and how much ham they eat and then we find another and another and another altogether the blue dots represent testing data we use the testing data to compare the predictions made by the black line to the predictions made by the green squiggle let's start by seeing how well the black line predicts the speed of each person in the testing data here's the first person in the testing data they ate this much yam and they ran this fast however the black line predicts that someone who ate this much yam should run a little slower so let's measure the distance between the actual speed and the predicted speed and save the distance on the right while we focus on the other people in the testing data here's the second person in the testing data they ate this much yam and they ran this fast but the black line predicts that they will run a little faster so we measure the distance between the actual speed and the predicted speed and add it to the one we measured for the first person in the testing data then we measure the distance between the real and the predicted speed for the third person in the testing data and add it to our running total of distances between the real and predicted speeds for the black line then we do the same thing for the fourth person in the testing data and add that distance to our running total for the black line this is the sum of all the distances between the real and predicted speeds for the black line now let's calculate the distances between the real and predicted speeds using the green squiggle remember the green squiggle did a great job fitting the training data but when we are doing machine learning we are more interested in how well the green squiggle can make predictions with new data so just like before we determine this person's real speed and their predicted speed and measure the distance between them and just like we did for the black line we'll keep track of the distances for the green squiggle over here then we do the same thing for the second person in the testing data and the third person and the fourth person this is the sum of the distances between the real and predicted speeds for the green squiggle the sum of the distances is larger for the green squiggle than the black line in other words even though the green squiggle fit the training data way better than the black line the black line did a better job predicting speeds with the testing data so if we had to choose between using the black line or the green squiggle to make predictions we would choose the black line BAM this example teaches two main ideas about machine learning first we use testing data to evaluate machine learning methods second don't be fooled by how well a machine learning method fits the training data note fitting the training data well but making poor predictions is called the bias-variance tradeoff Ohno a shameless self-promotion if you want to learn more about the bias-variance tradeoff there's a stat quest that will walk you through it one step at a time before we move on you may be wondering why we used a simple black line in a silly green squiggle instead of a deep learning or convolutional neural network or insert sir who is with bestest most fancy machine learning method here here there are tons of fancy sounding machine learning methods and each year something new and exciting comes on the scene but regardless of what you use the most important thing isn't how fancy it is but how it performs with testing data double BAM now let's go back to the decision tree that we started with remember we wanted to classify if someone loved stat quest based on a few questions to create the decision tree we collected data from people who loved stat quest and from people who did not love stat quest altogether this was the training data and we used it to build the decision tree got data from a few more people who love stat quest and a few more people who did not love stat quest altogether this forms the testing data we can use the testing data to see how well our decision tree predicts if someone will love stat quest the first person in the testing data did not like silly songs so we go to the right side of the decision tree they didn't like machine learning either so we just keep on going down the right side of the decision tree they didn't like statistics either so the decision tree predicts that this person will not love stat quest however this person loves stat quest so the decision tree made a mistake want wall the second person in the testing data liked silly songs and that takes us down the left side of the decision tree they were also interested in machine learning so we predict that that person loves stat quest and since this person actually loves stat quest the decision tree did a good job hooray now we just run all of the other people in the testing data down the decision tree and compare the predictions to reality then we can compare this decision tree to the latest greatest machine learning method ultimately we pick the method that does the best job predicting if someone will love stat quest or not triple bam in summary machine learning is all about making predictions and classifications there are tons of fancy machine learning methods but the most important thing to know about them isn't what makes them so fancy it's that we decide which method fits our needs the best by using testing data one last thing before we go you may be wondering how we decide which data go into the training set and which data go into the testing set earlier we just arbitrarily decided that these red dots were the training data but the blue dots could have just as easily been the training data the good news is that there are ways to determine which samples should be used for training data and which samples should be used for testing data and if you're interested in learning more about this check out the stat quest and there are lots more stat quests that walk you through machine learning concepts step-by-step so check them out hooray we've made it to the end of another exciting stack quest if you like this stack quest and want to see more please subscribe and if you want to support stack quest well consider buying one or two of my original songs or getting a t-shirt or a hoodie or some other slick merchandise there's links on the screen and there's links in the description below alright until next time quest on
Kdsp6soqA7o,2018-10-29T14:00:01.000000,Machine Learning Fundamentals: The Confusion Matrix,if you feel confused don't sweat it Stan Quest is here stack quest hello I'm Josh stormer and welcome to stack quest today we're going to cover another machine learning fundamental the confusion matrix and it's going to be clearly explained imagine that we have this medical data we've got some clinical measurements like chest pain good blood circulation blocked arteries and weight and we want to apply a machine learning method to them to predict whether or not someone will develop heart disease to do this we could use logistic regression or K nearest neighbors or random forest or some other method there are tons to choose from how do we decide which one works best with our data we start by dividing the data into training and testing sets note this would be an excellent opportunity to use cross-validation and if you're not familiar with that we'll check out the stat quest then we train all of the methods were interested in with the training data then test each method on the testing set now we need to summarize how each method performed on the testing data one way to do this is by creating a confusion matrix for each method the rows in a confusion matrix correspond to what the machine learning algorithm predicted and the columns correspond to the known truth since there are only two categories to choose from heart disease or does not have heart disease then the top-left corner contains true positives these are patients that had heart disease that were correctly identified by the algorithm the true negatives are in the bottom right hand corner these are patients that did not have heart disease that were correctly identified by the algorithm the bottom left-hand corner contains false negatives false negatives are when a patient has heart disease but the algorithm said they didn't lastly the top right hand corner contains false positives false positives are patients that do not have heart disease but the algorithm says they do for example when we applied the random forest to the testing data there were 142 true positives patients with heart disease that were correctly classified and 110 true negatives patients without heart disease that were correctly classified however the algorithm misclassified 29 patients that did have heart disease by saying they did not these are false negatives in the algorithm misclassified 22 patients that did not have heart disease by saying that they did these are false positives the numbers along the diagonal the green boxes tell us how many times the samples were correctly classified the numbers not on the diagonal the red boxes are samples that the algorithm messed up now we can compare the random force confusion matrix to the confusion matrix we get when we use K nearest neighbors k-nearest neighbors was worse than the random forest at predicting patience with the heart disease 107 versus 142 and worse at predicting patients without heart disease 79 versus 110 so if we had to choose between using the random forest and K nearest neighbors we would choose the random forest BAM lastly we can apply logistic regression to the testing data set and create a confusion matrix these two confusion matrices are very similar and make it hard to choose which machine learning method is a better fit for this data we'll talk about more sophisticated metrics like sensitivity specificity ROC and AOC that can help us make a decision in the next stack quests now that we have the basic confusion matrix figured out let's look at a more complicated one here's a new data set now the question is based on what people think of these movies Jurassic Park 3 run for your wife out cold spelled with a k and Howard the Duck can we use a machine learning method to predict their favorite movie if the only options for favorite movie were troll to Gore police or cool as ice then the confusion matrix would have three rows and three columns but just like before the diagonal the green boxes are where the machine learning algorithm did the right thing and everything else is where the algorithm messed up in this case the machine learning algorithm didn't do very well but can you blame it these are all terrible movies BAM ultimately the size of the confusion matrix is determined by the number of things we want to predict in the first example we were only trying to predict two things if someone had heart disease or if they didn't and that gave us a confusion matrix with two rows and two columns in the second example we had three things to choose from in a confusion matrix with three rows and three columns if we had four things to choose from we get a confusion matrix with four rows and four columns and if we had 40 things to choose from we get a confusion matrix with 40 rows and 40 columns double bam in summary a confusion matrix tells you what your machine learning algorithm did right and what it did wrong hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stat quest well consider buying one or two of my original songs alright until next time quest on
ctmNq7FgbvI,2018-10-23T13:30:00.000000,"Ridge, Lasso and Elastic-Net Regression in R","elastic regression won't help you to sing in tune but if you do it and ah it's not that hard stat quest hello I'm Josh stormer and welcome to stat quest today we're going to talk about Ridge lasso and elastic net regression in our note this stack quest assumes you're already familiar with the concepts behind Ridge lasso in elastic net regression if not check out the quests to do Ridge lasso an elastic net regression and our we will use the GLM net library the GLM part of GLM net stands for generalized linear models which means that this tool can be applied to linear regression and logistic regression as well as a few other models the net part of GLM net is from elastic net at least I'm pretty sure that's where it's from if you know otherwise put it in the comments below in the stat quest on elastic net regression we talked about how the elastic net combines the lasso regression penalty Yi how with the ridge regression penalty BER and that you can control how much either penalty is included by adjusting lambda sub 1 or lambda sub 2 however GLM net does things just a little differently instead of two different lambdas GLM net has a single lambda and another parameter called alpha alpha can be any value from zero to one when alpha equals zero then the whole lasso penalty goes to zero and goes away and we're left with just the ridge regression penalty and the whole thing reduces to Ridge regression burr when alpha equals 1 then the whole Ridge penalty goes to 0 and goes away and were left with just the lasso regression penalty and the whole thing reduces to lasso regression yeehaw and when alpha is between 0 & 1 we get a mixture of the two penalties that does a better job shrinking correlated variables than either lasso or Ridge does on their own lambda controls how much of the penalty to apply to their regression when lambda equals zero then the whole penalty goes away and we're just doing standard least squares for linear regression or maximum likelihood for logistic regression and when lambda is greater than zero then the elastic net penalty kicks in and we start shrinking parameter estimates thus when we use the GLM net package to do elastic net regression we will test different values for lambda and alpha now that we know how GLM networks let's do ridge lasso an elastic net regression in our note the following code is based on an example written by Josh day however the modified code that I'm about to present can be found by following the link in the description below first load the GLM netlibrary then set a seed for the random number generator so that you'll get the same results as me now let's make up a data set that we can use to test out ridge lasso and elastic net regression the made-up data set will have N equals 1,000 samples and P equals 5000 parameters to estimate however only 15 of those parameters will help us predict the outcome the remaining 4985 parameters will just be random noise now we create a matrix called X that is full of randomly generated data the matrix has 1,000 rows since n equals 1000 and 5000 columns since P equals 5,000 and the values in the matrix come from a standard normal distribution with mean equals zero and standard deviation equals one and we'll need n times P or one thousand times five thousand equals five million values since our matrix has n rows and P columns now we create a vector of values called Y that we will try to predict with the data in X this call to apply will return a vector of 1000 values that are the sums of the first 15 columns and X since X has 1000 rows this is what isolates columns 1 through 15 from X and this one specifies that we want to perform a function on each row of the data that we've isolated from X and some is the function we want to apply to each row to summarize this call to apply will return a vector of values that depend on the first 15 columns in X once we have that vector of Psalms we add a little bit of noise using the R norm function which in this case returns 1000 random values from a standard normal distribution so this whole thing creates a vector called Y that is dependent on the first 15 columns in X plus a little noise to make things interesting thus X is a matrix of data that we will use ridge lasso an elastic net regression to predict the values in y BAM now we need to divide the data into training and testing sets so we make a vector of indexes called Train underscore rows that contains the row numbers of the rows that will be in the training set the sample function randomly selects numbers between 1 and n the number of rows in our data set and it will select 0.66 x n row numbers in other words two-thirds of the data will be in the training set now that we have the indexes for the rows in the training set in train underscore rows we can make a new matrix X dot train that just contains the training data and we can make a testing set X dot test that contains the remaining rows this is done by putting a negative sign in front of train underscore rows when we select rows from X now we select the training values in Y and save them in Y dot train and we select the testing values in Y and save them in Y dot test hooray we've created our training and testing datasets will apply Ridge lasso an elastic net regression separately to these datasets so that we can see how it's done and see which method works best we'll start with Ridge regression the first thing we need to do is fit a model to the training data we do this with the CV GLM net function the CV part means we want to use cross-validation to obtain the optimal values for lambda by default CV GLM net uses 10-fold cross-validation the first two parameters specified the training sets in this case we want to use X train to predict y train note unlike the LM or g LM functions cv g LM net does not accept formula notation x and y must be passed in separately type dot measure is how the cross-validation will be evaluated and it is set to MSE which stands for mean squared error mean squared error is just the sum of the squared residuals divided by the sample size note if we are applying elastic net regression to logistic regression then we would set this to deviance since we are starting with Ridge regression we set alpha to zero lastly we set family to Gaussian this tells gl and net that we are doing linear regression note if we were doing logistic regression we would set this to binomial altogether this called a CV GL and net will fit a linear regression with a Ridge regression penalty using 10-fold cross-validation to find optimal values for lambda in the fit model along with optimal values for lambda is saved as alpha 0 dot fit which will help us remember that we set alpha to 0 for ridge regression now we will use the predict function to apply alpha zero da fit to the testing data the first parameter is a fitted model in this case it's alpha 0 dot fit s which I think stands for size as in the size of the penalty is set to one of the optimal values for lambda stored an alpha 0 dot fit in this example we're setting s to lambda dot 1 se lambda dot 1 se is the value for lambda stored an alpha 0 dot fit that resulted in the simplest model ie the model with the fewest nonzero parameters and was within 1 standard error of the lambda that had the smallest sum note alternatively we could set s to lambda dot min which would be the lambda that resulted in the smallest sum however in this example we would use lambda dot 1 s e because in a statistical sense it is indistinguishable from lambda dot min but it results in a model with fewer parameters wait a minute I thought only last so an elastic net regression could eliminate parameters what's going on since we will compare Ridge to lasso and elastic net regression we will use lambda dot one se for all three cases to be consistent lastly we set new X to the testing data set X dot test and we save the predicted values as alpha 0 dot predicted now we calculate the mean squared error of the difference between the true values stored in a wideout test and the predicted values stored in alpha 0 to predicted and we get fourteen point four seven something something BAM [Music] now let's try lasso regression with the same training and testing datasets just like before we call CVG L and net to fit a linear regression using 10-fold cross-validation to determine optimal values for lambda only this time we set alpha to 1 and we will store the model and the optimal values for lambda in alpha one fit to remind us that we set alpha to 1 then we call the predict function just like before only this time we pass an alpha 1 dot fit and save the results as alpha 1 dot predicted then we calculate the mean squared error and we get one point one nine something something one point one nine something something is way smaller than fourteen point four seven something something so Lasser regression is much better with this data than Ridge regression double bam now let's see how well elastic net regression which combines both Ridge and lasso penalties performs just like before we call CV GL and net to determine optimal values for lambda only this time we set alpha to 0.5 and we store the model and the optimal values for lambda in alpha 0.5 dot fit to remind us that we set alpha to 0.5 then we call the predict function with alpha 0.5 dot fit and calculate the mean squared error and we get 1.2 for something something this is slightly larger than the 1.19 something something we got with lasso regression so so far lasso wins yeehaw but to really know if lasso wins we need to try a lot of different values for alpha to try a bunch of values for alpha we'll start by making an empty list called list of fits that will store a bunch of elastic net regression fits then we use a for loop to try different values for alpha in this for loop I will be integer values from 0 to 10 first we paste together a name for the elastic net fit that we are going to create for example when I equals 0 then fit dot name will be alpha 0 because alpha will be pasted to 0 divided by 10 which equals 0 when I equals one then fit name will be alpha 0.1 because alpha will be pasted to 1/10 which equals 0.1 here's where we create the elastic net fit using the CV GLM net function everything is the same as before except where we set alpha when I equals 0 then alpha will be zero and result in Ridge regression and when I equals 1 then alpha will be 0.1 etc etc etc until I equals 10 and alpha equals 1 resulting in lasso regression each fit will be stored in a list of fits under the name we stored in fit dot name now we are ready to calculate the mean squared errors for each fit with the testing data set we'll start by creating an empty data frame called results that will store the mean squared errors and a few other things then we'll use another for loop to predict values using the testing data set to calculate the mean squared errors just like before the for loop goes from zero to ten and just like before we'll create a variable called fit name that contains the name of the elastic net regression fit however this time we'll use the predict function to predict values with X dot test the testing data we use the list of fits list and fit dot name to pass a specific fit to the predict function and to pass the lambda dot one se value for S the predicted values are stored in a variable called predicted and then used to calculate the mean squared error for the fit then we store the value for alpha the mean squared error and the name of the fit in a temporary data frame called temp and we use the our bind function to append temp to the bottom row of the results data frame after we run the for-loop we can print out the results the first column has the values for alpha ranging from zero to one the second column has the mean squared errors note these are slightly different from what we got before because the parameter values prior to regularization and optimization are randomly initialized thus this is another good reason to use set seed to ensure consistent results and in the last column we have the name of the fit v where alpha equals 1 is still the best so Lasser regression is the best method to use with this data triple bam hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest well consider buying one or two of my original songs alright until next time quest on"
1dKRdX9bfIo,2018-10-08T13:06:13.000000,Regularization Part 3: Elastic Net Regression,elastic net regression sounds so crazy fancy but it's way way simpler than you might expect stat quest hello I'm Josh Dahmer and welcome to stat quest today we're gonna do part 3 of our series on regularization we're gonna cover elastic net regression and it's going to be clearly explained this stat quest follows up on the stat quests on Ridge regression and lasso regression so if you aren't already familiar with them check them out we ended the stat quest on lasso regression by saying that it works best when your model contains a lot of useless variables so if this was the model we were using to predict size then lasso regression would keep the terms for weight and high fat diet and it would eliminate the terms for astrological sign and the airspeed of a swallow African or European creating a simpler model that is easier to interpret we also said that Ridge regression works best when most of the variables in your model are useful so if we were trying to predict size using a model where most of the variables were useful then Ridge regression will shrink the parameters but will not remove any of them great when we know a lot about all of the parameters in our model it's easy to choose if we want to use lasso regression or Ridge regression but what do we do when we have a model that includes tons more variables last week I went to a deep learning conference and people there were using models that included millions of parameters far too many to know everything about and when you have millions of parameters then you will almost certainly need to use some sort of regularization to estimate them however the variables in those models might be useful or useless we don't know in advance so how do you choose if you should use lasso or Ridge regression the good news is that you don't have to choose instead use elastic net regression elastic net regression sounds super fancy but if you already know about lasso and Ridge regression it's super simple just like lasso and Ridge regression elastic net regression starts with least squares then it combines the lasso regression penalty yeehaw with the ridge regression penalty BRR altogether elastic net combines the strengths of lasso and Ridge regression note the lasso regression penalty and the ridge regression penalty get their own Landis lambda sub 1 for lasso and lambda sub 2 for Ridge we use cross-validation on different combinations of lambda sub 1 and lambda sub 2 to find the best values when both lambda sub 1 and lambda sub 2 equals 0 then we get the original least-squares parameter estimates when lambda sub 1 is greater than zero and lambda sub two equals zero then we get lasso regression when lambda sub 1 equals zero and lambda sub two is greater than zero then we get ridge regression and when both lambda sub 1 is greater than zero and lambda sub two is greater than zero then we get a hybrid of the two the hybrid elastic net regression is especially good at dealing with situations when there are correlations between parameters this is because on its own lasso regression tends to pick just one of the correlated terms and eliminate the others whereas Ridge regression tends to shrink all of the parameters for the correlated variables together by combining lasso and Ridge regression elastic net regression groups and shrinks the parameters associated with the correlated variables and leaves them in the equation or removes them all at once BAM in summary elastic net regression combines the lasso regression penalty yeehaw with the ridge regression penalty birth and by doing so gets the best of both worlds plus it does a better job dealing with correlated parameters hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stat quest well consider buying one or two of my original songs alright until next time quest on you
NGf0voTMlcs,2018-10-01T13:20:40.000000,Regularization Part 2: Lasso (L1) Regression,laso and ridge regression of similar but there's a big important difference we'll talk about it stat quest hello I'm Josh stormer and welcome to stat quest today we're gonna do part two of our series on regularization we're gonna talk about lasso regression and it's going to be clearly explained this stat quest follows up on the one on Ridge regression so if you aren't already familiar with that check it out even if you are familiar with Ridge regression you should seriously consider watching or at least skimming that stat quest because the examples in this video are based on the ones in that video last so regression is very very similar to Ridge regression but it has some very very important differences to understand those similarities and differences let's first do a super quick review of Ridge regression in the stat quest on Ridge regression we started out with weight and size measurements from a bunch of mice and we split the data into two sets the red dots were training data and the green dots were testing data then we fit a line to the training data using least squares in other words we minimize the sum of the squared residuals when we did this we saw that even though the line fit the training data really well that is to say it had low bias it did not fit the testing data very well at all that is to say it had high variance then we fit a line to the data using Ridge regression we minimized the sum of the squared residuals plus lambda times the slope squared Ridge regression is just least squares plus the ridge regression penalty the Blue Ridge regression line did not fit the training data as well as the red least squares line in other words Ridge regression had more bias than least squares but in return for that small amount of bias the ridge regression line had a significant drop in variance the main idea was that by starting with a slightly worse fit Richard rushman provided better long-term predictions BAM now let's go back to the equation that Ridge regression minimizes and focus on the ridge regression penalty if instead of squaring the slope we take the absolute value then we have lasso regression note just like with Ridge regression lambda can be any value from 0 to positive infinity and is determined using cross-validation like Ridge regression lasso regression the orange line results in a line with a little bit of bias but less variance than least squares BAM reach regression and lasso regression look very similar and they do similar things in this case they make our predictions of size less sensitive to this tiny training data set both Ridge and lasso regression can be applied in the same context like this situation where we are using two different diets to predict size or in a logistic regression setting where we use weight to predict obesity and both Ridge and lasso regression can be applied to complicated models that combine different types of data in this case we've combined the data from the first two examples weight which is continuous and high fat diet which is discrete just like the ridge regression penalty the lasso regression penalty contains all of the estimated parameters except for the y-intercept it's also worth mentioning that when Ridge and lasso regression shrink parameters they don't have to shrink them all equally for example if these were the training data and these were the testing data then when lambda equals zero we would start with these least squares estimates for the slope and the offset for diet difference but as we increase the value for lambda rage and lasso regression may shrink diet difference a lot more than they shrink the slope okay we've seen how Ridge and lasso regression are similar now let's talk about the big difference between them to see what makes lasso regression different from Ridge regression let's go back to the two-sample training data and let's focus on what happens when we increase the value for lambda when lambda equals zero then the lasso regression line will be the same as the least squares line as lambda increases in value the slope gets smaller until the slope equals zero BAM the big difference between Ridge and lasso regression is that Ridge regression can only shrink a slope asymptotically close to zero while lasso regression can shrink the slope all the way to 0 to appreciate this difference let's look at a big huge crazy equation the goal of this equation is to predict size the terms for weight and high fat diet are both reasonable things to use to predict size but the astrological sign and the airspeed of a swallow African or European are terrible ways to predict size when we apply Richard Russian to this equation we find the minimal sum of the squared residuals plus the ridge regression penalty and the larger we make lambda these parameters might shrink a little bit and these parameters might shrink a lot but they will never be equal to 0 in contrast with lasso regression when we increase the value for lambda then these parameters will shrink a little bit and these parameters will go all the way to 0 and these terms go away and we're left with a way to predict size that only includes weight and diet and excludes all of the silly stuff since lasso regression can exclude useless variables from equations it is a little better than Ridge aggression at reducing the variance and models that contain a lot of useless variables in contrast Ridge regression tends to do a little better when most variables are useful double bam in summary Ridge regression is very similar to lasso regression and the superficial difference is that Ridge regression squares the variables and lasso regression takes the absolute value but the big difference is that Lasser aggression can exclude useless variables from equations this makes the final equation simpler and easier to interpret hooray we've made it to the end of another exciting stack quest if you like this stack quest and want to see more please subscribe and if you want to support stack quest well please consider buying one or two of my original songs alright until next time quest on
G9POY1cHn50,2018-10-01T01:02:41.000000,Little Red Fiat,[Music] driving around in your ragtop you take me to school it's the best part of my day combing my hair out the window your big blue brush it's the best part of my day little red Fiats on the passenger side [Music] little red Fiats thanks for the ride but you know the best part was always the driver the wobbly stick what Levi's the wind all around [Music] it's the best part of my day [Music] flooring the gas [Music] on the unwrap I can't hear a thing [Music] it's the best part of my day little red Fiats on the passenger side little red fee thanks for the ride but you know the best part was always the tribe but you know the best part was always the drive [Music] it's the best part my turn [Music] you [Music] you
Q81RR3yKn30,2018-09-24T16:59:00.000000,Regularization Part 1: Ridge (L2) Regression,"regularization it's just another way to save desensitization let's check it out with a new regression stat quest hello I'm Josh stormer and welcome to stat quest today we're going to do part 1 of a series of video on regularization techniques in this video we're gonna cover Ridge regression and it's going to be clearly explained note this stat cuesta seems you understand the concepts of bias and variance in the context of machine learning if not check out machine learning fundamentals bias and variance it also assumes that you are familiar with linear models if not check out the following stat quests the links are in the description below lastly if you're not already familiar with the concept of cross-validation check out the stack west on cross-validation in this stack quest we will one look at a simple example that shows the main ideas behind Ridge regression to go into details about how Ridge regression works three show how Ridge regression works in a variety of situations and four lastly we'll talk about how Ridge regression can solve the unsolvable BAM let's start by collecting weight and size measurements from a bunch of mice since these data look relatively linear we will use linear regression aka least squares to model the relationship between weight and size so we'll fit a line to the data using least squares in other words we find the line that results in the minimum sum of squared residuals ultimately we end up with this equation for the line the line has two parameters a y-axis intercept and a slope we can plug in a value for weight for example two point five and do the math and get a value for size together the value for weight two point five and the value for size two point eight give us a point on the line when we have a lot of measurements we can be fairly confident that the least squares line accurately reflects the relationship between size and weight but what if we only have two measurements we fit a new line with least squares since the new line overlaps the two data points the minimum sum of squared residuals equals zero ultimately we end up with this equation for the new line note here are the original data in the original line for comparison let's call the two red dots the training data in the remaining green dots that testing data the sum of the squared residuals for just the two red points the training data is small in this case it is zero but the sum of the squared residuals for the green points the testing data is large and that means that the new line has high variance in machine learning lingo we'd say that the new line is over fit to the training data now let's go back to just the training data we just saw that least squares results in a line that is over fit and has high variance the main idea behind Ridge regression is to find a new line that doesn't fit the training data as well in other words we introduce a small amount of bias into how the new line is fit to the data but in return for that small amount of bias we get a significant drop in variance in other words by starting with a slightly worse fit Ridge regression can provide better long-term predictions BAM now let's dive into the nitty-gritty and learn how Ridge regression works let's go back to just the training data when least-squares determines values for the parameters in this equation it minimizes the sum of the squared residuals in contrast when Ridge regression determines the values for the parameters in this equation it minimizes the sum of the squared residuals plus lambda times the slope squared note I usually try to avoid using Greek characters as much as possible but if you are ever going to do Ridge regression in practice you have to know that this term is called lambda this part of the equation adds a penalty to the traditional least squares method and lambda determines how severe that penalty is to get a better idea of what's going on let's plug in some numbers let's start by plugging in the numbers that correspond to the least squares fit the sum of the squared residuals for the least squares fit is zero because the line overlaps the data points and the slope is one point three we'll talk more about lambda later but for now let lambda equal one all together we have zero plus one times one point three squared and when we do the math we get one point six nine now let's see what happens when we plug in numbers for the ridge regression line the sum of the squared residuals is zero point three squared for this residual plus zero point one squared for this residual the slope is 0.8 and just like before we'll let lambda equal 1 altogether we have 0.3 squared plus 0.1 squared plus 1 times 0.8 squared and when we do the math we get 0.7 for for the least squares line the sum of squared residuals plus the ridge regression penalty is one point six nine for the ridge regression line the sum of squared residuals plus the ridge regression penalty is 0.74 thus if we wanted to minimize the sum of the squared residuals plus the ridge regression penalty we would choose the ridge regression line over the least squares line without the small amount of bias that the penalty creates the least squares fit has a large amount of variance in contrast the ridge regression line which has a small amount of bias due to the penalty has less variance now before we talk about lambda let's talk a little bit more about the effect that the ridge regression penalty has on how the line is fit to the data to keep things simple imagine we only have one line this line suggests that for every one unit increase in weight there is a one unit increase in predicted size if the slope of the line is steeper than for every one unit increase in weight the prediction for size increases by over two units in other words when the slope of the line is steep then the prediction for size is very sensitive to relatively small changes in weight when the slope is small then for every one unit increase in weight the prediction for size barely increases in other words when the slope of the line is small then predictions for size are much less sensitive to changes in weight now let's go back to the least squares and Ridge regression lines fit to the two data points the ridge regression penalty resulted in a line that has a smaller slope which means that predictions made with the ridge regression line are less sensitive to weight than the least squares line BAM now let's go back to the equation that Ridge regression tries to minimize and talk about lambda lambda can be any value from 0 to positive infinity when lambda equals zero then the ridge regression penalty is also zero and that means that the ridge regression line will only minimize the sum of squared residuals and the ridge regression line will be the same as the least squares line because they are both minimizing the exact same thing now let's see what happens as we increase the value for lambda in the example we just looked at we said lambda equals 1 and the ridge regression line ended up with a smaller slope than the least squares line when we set lambda equals 2 the slope gets even smaller and when we set lambda equals 3 the slope is even smaller and the larger we make lambda the slope gets asymptotically close to 0 so the larger lambda gets our prediction for size become less and less sensitive to weight so how do we decide what value to give lambda we just try a bunch of values for lambda and use cross-validation typically 10-fold cross-validation to determine which one results in the lowest variance double bail in the previous example we showed how ridge regression would work when we want to predict size which is a continuous variable using weight which is also a continuous variable however Ridge regression also works when we use a discrete variable like normal diet versus high fat diet to predict size in this case the data might look like this in the least squares fitted equation might look like this where 1.5 the equivalent of a y-intercept corresponds to the average size of the mice on the normal diet and 0.7 the equivalent of a slope corresponds to the difference between the average size for the mice on the normal diet compared to the mice on the high-fat diet note from here on out we'll refer to this distance as diet difference high-fat diet is either zero for mice on a normal diet or one for mice on the high-fat diet in other words this term alone predicts the size of mice on the normal diet in the sum of these two terms is the prediction for the size of mice on the high-fat diet for the mice on the normal diet the residuals are the distances between the mice and the normal diet mean and for mice on the high-fat diet the residuals are the distances between the mice and the high fat diet mean when Lee squares determines the values for the parameters in this equation it minimizes the sum of the squared residuals in other words these distances between the data and the means are minimized when Ridge regression determines values for the parameters in this equation it minimizes the sum of the squared residuals plus lambda times diet difference squared remember diet difference simply refers to the distance between the mice on the normal diet and the mice on the high-fat diet when lambda equals zero this whole term ends up being zero and we get the same equation that we got with least squares but when lambda gets large the only way to minimize the whole equation is to shrink diet distance down in other words as lambda gets larger our prediction for the size of mice on the high-fat diet becomes less sensitive to the difference between the normal diet and the high-fat diet and remember the whole point of doing rich regression is because small sample sizes like these can lead to poor least squares estimates that result in terrible machine learning predictions BAM Ridge regression can also be applied to logistic regression in this example we are using weight to predict if a mouse's obese or not this is the equation for this logistic regression and Ridge regression would shrink the estimate for the slope making our prediction about whether and out of mouse is obese less sensitive to weight note when applied to logistic regression Ridge regression optimizes the sum of the likelihoods instead of the squared residuals because logistic regression is solved using maximum likelihood so far we've seen simple examples of how Ridge regression helps reduce variance by shrinking parameters and making our predictions less sensitive to them but we can apply Ridge regression to complicated models as well in this model we've combined the weight measurement data from the first example with the two diets from the second example combining these two datasets gives us this equation and Ridge regression tries to minimize this now the ridge regression penalty contains the parameters for the slope and the difference between diets in general the ridge regression penalty contains all of the parameters except for the y intercept if we had a big huge crazy equation with terms for astrological sign the airspeed of a swallow and other stuff then the ridge regression penalty would have all those parameters squared except for the y intercept every parameter except for the y intercept is scaled by the measurements and that's why the y intercept is not included in the ridge regression penalty double bam okay now the next thing we're going to talk about is going to sound totally random but trust me it will lead to the coolest thing about Ridge regression it's so cool it's almost like magic we all know that this is the equation for a line in an order for least squares to solve for the parameters the y-intercept and slope we need at least two data points these data points result in these parameters in this specific line if we only have one data point then we wouldn't be able to solve for these parameters because there would be no way to tell if this line is better than this line or this line or any old line that goes through the one data point all of these lines have zero residuals and thus all minimize the sum of the squared residuals it's not until we have two data points that it becomes clear that this is the least square solution now let's look at an equation that has three parameters to estimate we need to estimate a y-intercept a slope that reflects how weight contributes to the prediction of size and a slope that reflects how age contributes to the prediction of size when we have three parameters to estimate then just two data points isn't going to cut it that's because in three dimensions which is what we get when we add another access to our graph for age we have to fit a plane to the data instead of just a line and with only two data points there's no reason why this plane fits the data any better than this plane or this plane but as soon as we have three data points we can solve for these parameters if we have an equation with four parameters then least squares needs at least four data points to estimate all four parameters and if we have an equation with 10,001 parameters then we need at least 10,000 and one data points to estimate all of the parameters an equation with 10,001 parameters might sound bonkers but it's more common than you might expect for example we might use gene expression measurements from 10,000 genes to predict size and that would mean we would need gene expression measurements from ten thousand and one mice unfortunately collecting gene expression measurements from ten thousand and one mice is crazy expensive and time-consuming right now in practice a huge data set might have measurements from 500 mice so what do we do if we have an equation with ten thousand and one parameters in only 500 data points we use Ridge regression it turns out that by adding the ridge regression penalty we can solve for all 10,000 won parameters with only 500 or even fewer samples one way to think about how Ridge regression can solve for parameters when there isn't enough data is to go back to our original size versus weight example only this time there is only one data point in the training set least squares can't find a single optimal solution since any line that goes through the dot will minimize the sum of the squared residuals but Ridge regression can find a solution with cross-validation and the Ridge regression penalty that favors smaller parameter values since this stack quest is already super long we'll save a more thorough discussion of how this works for a future stat quest triple bam in summary when the sample sizes are relatively small then Ridge regression can improve predictions made from new data ie reduce variance by making the predictions less sensitive to the training data this is done by adding the ridge regression penalty to the thing that must be minimized the ridge regression penalty itself is lambda times the sum of all squared parameters except for the y-intercept and lambda is determined using cross-validation lastly even when there isn't enough data to find the least squares parameter estimates Ridge regression can still find a solution using cross-validation and the ridge regression penalty hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest well consider buying one or two of my original songs alright until next time quest on"
EuBBz3bI-aA,2018-09-17T16:00:42.000000,Machine Learning Fundamentals: Bias and Variance,Hurricane Florence came by while I was working on stat quest dark clouds filled the sky but that didn't stop stat quest stand quest hello I'm Josh stormer and welcome to stat quest today we're going to be talking about some machine learning fundamentals bias and variance and they're gonna be clearly explained imagine we measured the weight and height of a bunch of mice and plotted the data on a graph light mice tend to be short and heavier mice tend to be taller but after a certain weight mice don't get any taller just more obese given this data we would like to predict Mouse height given its weight for example if you told me your mouse weighed this much then we might predict that the mouse is this tall ideally we would know the exact mathematical formula that describes the relationship between weight and height but in this case we don't know the formula so we're going to use two machine learning methods to approximate this relationship however I'll leave the true relationship curve in the figure for reference the first thing we do is split the data into two sets one for training the machine learning algorithms and one for testing them the blue dots are the training set and the green dots are the testing set here's just the training set the first machine learning algorithm that we will use is linear regression aka least squares linear regression it's a straight line to the training set note the straight line doesn't have the flexibility to accurately replicate the arc in the true relationship no matter how we try to fit the line it will never curve thus the straight line will never capture the true relationship between weight and height no matter how well we fit it to the training set the inability for a machine learning method like linear regression to capture the true relationship is called bias because the straight line can't be curved like the true relationship it has a relatively large amount of bias another machine learning method might fit a squiggly line to the training set the squiggly line is super flexible and hugs the training set along the arc of the true relationship because the squiggly line can handle the arc in the true relationship between weight and height it has very little bias we can compare how well the straight line and the squiggly line fit the training set by calculating their sums of squares in other words we measure the distances from the fit lines to the data square them and add them up just they are squared so that negative distances do not cancel out positive distances notice how the squiggly line fits the data so well that the distances between the line and the data are all 0 in the contest to see whether the straight line fits the training set better than the squiggly line the squiggly line wins but remember so far we've only calculated the sums of squares for the training set we also have a testing set now let's calculate the sums of squares for the testing set in the contest to see whether the straight line fits the testing set better than the squiggly line the straight line wins even though the squiggly line did a great job fitting the training set it did a terrible job fitting the testing set in machine learning lingo the difference in fits between data sets is called variance the squiggly line has low bias since it is flexible and can adapt to the curve in the relationship between weight and height but the squiggly line has high variability because it results in vastly different sums of squares for different data sets in other words it's hard to predict how well the squiggly line will perform with future data sets it might do well sometimes and other times it might do terribly in contrast the straight line has relatively high bias since it cannot capture the curve in the relationship between weight and height but the straight line has relatively low variance because the sums of squares are very similar for different data sets in other words the straight line might only give good predictions and not great predictions but they will be consistently good predictions BAM Oh No terminology alert because the squiggly line fits the training set really well but not the testing set we say that the squiggly line is over fit in machine learning the ideal algorithm has low bias and can accurately model the true relationship and it has low variability by producing consistent predictions across different data sets this is done by finding the sweet spot between a simple model and a complex model oh no another terminology alert 3 commonly used methods for finding the sweet spot between simple and complicated models our regularization boosting and bagging the stat quest on a random forest show an example of bagging in action and we'll talk about regularization and boosting in future stat quests double bam hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest well please consider buying one or two of my original songs alright until next time quest arm
YAlJCEDH2uY,2018-09-03T17:04:06.000000,"The Central Limit Theorem, Clearly Explained!!!",even if you're not normal the average is normal hello I'm Josh starmer and welcome to stat Quest today we're going to talk about the central limit theorem and it's going to be clearly explained note for this stat quest to make any sense at all you should be familiar with the normal distribution if not check out the normal distribution clearly explained it would also be helpful if you were familiar with the concept of sampling from a statistical distribution if not check out sampling from a statistical distribution clearly explained the central limit theorem is the basis for a lot of statistics and the good news is that it's a pretty simple concept in this stat Quest I'll explain what the central limit theorem is and why it's important like most things in statistics I think the central limit theorem is easiest to understand if we look at some examples so let's start with a uniform distribution this one goes from zero to one it's called The Uniform distribution because there is an equal probability of selecting values between 0 and 1. the probabilities are all equal and thus are uniform we can collect 20 random samples from this uniform distribution and then calculate the mean of the samples and on the right we can draw a histogram of the mean value since we only have one mean value the histogram isn't very interesting but after we collect 10 more samples and collect 10 more means the histogram starts to look a little more interesting here's the histogram after collecting 20 samples and calculating 20 means 30 mains 40 means 50 means 60 means 70 means 80 means 90 means and 100 means after adding 100 means to the histogram it's pretty easy to see that these means are normally distributed however to make it easy to see that the means are normally distributed we can overlay a normal distribution you might have noticed that in the last two slides I put means are normally distributed in bold I did this because this is what the central limit theorem is all about even though these means were calculated using data from a uniform distribution the means themselves are not uniformly distributed instead the means are normally distributed bam here's another example this time we'll start with an exponential distribution just like before we can collect 20 random samples from this exponential distribution and just like before we can calculate the mean of the 20 samples and lastly we can draw a histogram of that mean over here on the right after we collect 10 samples and Calculate 10 means the histogram starts to look a little more interesting here's the histogram after 20 means 30 means 40 means 50 means 60 means 70 means 80 means 90 means and 100 means after adding 100 means to the histogram we can see that they are normally distributed even though these means were calculated using data from an exponential distribution the means themselves are not exponentially distributed instead the means are normally distributed [Music] so far we have seen that the means calculated from samples taken from a uniform distribution are normally distributed and means calculated from samples taken from an exponential distribution are also normally distributed well it turns out that it doesn't matter what distribution you start with if you collect samples from those distributions the means will be normally distributed yes there's a little asterisk here that means there's some fine print that will come later for now just know it's really fine print and not worth spending too much time worrying about double bam cool but what are the practical implications of knowing that the means are normally distributed when we do an experiment we don't always know what distribution our data comes from to this the central limit theorem says who cares the sample means will be normally distributed because we know that the sample means are normally distributed we don't need to worry too much about the distribution that the samples came from we can use the means normal distribution to make confidence intervals do t-tests where we ask if there's a difference between the means from two samples and Anova where we ask if there is a difference among the means from three or more samples in pretty much any statistical test that uses the sample mean triple bam note out there in the wild some folks say that in order for the central limit theorem to be true the sample size must be at least 30. this is just a rule of thumb and generally considered safe however as you can see in the examples here where I use a sample size of 20 the rule was meant to be broken here's the fine print in order for the central limit theorem to work at all you have to be able to calculate a mean from your sample off the top of my head I can think of only one distribution the Koshi distribution that doesn't have a sample mean and after doing biostatistics for 20 years I've never come across it in practice that said if you know of distributions that don't have means put them in the comments below and tell us what they're used for I'm curious about how common this occurs hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more of them please subscribe and if you want to support stack Quest well consider buying one or two of my original songs alright until next time Quest on
OxA6i3iJd7M,2018-09-01T00:51:18.000000,Miss Carolina,[Music] you [Music] how does your garden grow it's choking full of weeds you thought you'd run Oh while anger in Tabriz hey miss Carolina when are you gonna come back home now you left your foot Cynthia in the yard now it Sam I see up trails a parabola but it swings you far but it's now what you need a mess when are you coming home [Music] [Laughter] the Pretty Maids all in a row each one to supercede [Music] the ground beneath is dark and moist tilled by a centipede hey miss Carolina when are you gonna come back oh now I saw your Fassett the in the yard and it called [Music] Fred thing your trails are parabola swung around by the changes in the wind hey miss Carolina when are you coming home you
4KKV9yZCoM4,2018-08-13T15:17:38.000000,"Maximum Likelihood for the Binomial Distribution, Clearly Explained!!!",maximum likelihood the binomial distribution that's what we'll talk about today stat quest hello I'm Josh stormer and welcome to stat quest today we're going to talk about maximum likelihood for the binomial distribution and it's gonna be clearly explained note this stat quest follows up on the stat quest maximum likelihood clearly explained as well as the stat quest probability versus likelihood and lastly this stat quest assumes you are already familiar with the binomial distribution if not check out the stat quest the binomial distribution and test clearly explained in the stat quest on the binomial distribution and test we use the binomial distribution aka this nasty-looking thing to determine if in general people like orange Fanta more than grape Fanta in the context of this problem X is the number of people who preferred orange Fanta in this case x equals 4 n is the total number of people we asked about whether they preferred orange Fanta or grape Fanta in this case in equals 7 and P is the probability somebody would randomly choose orange Fanta over grape Fanta in this case P equals 0.5 all together the left side of the equation reads the probability of X the number of people who say they prefer orange Fanta given in the number of people we asked and P the probability of picking orange Fanta then we just plug the numbers in and chugged away at the math and the probability that 4 out of 7 people would randomly prefer orange Fanta is zero point 2 7 3 now if we want to calculate the likelihood of P equals 0.5 then all we need to do is rearrange the left side of the equation that is to say we change this to this now the left side of the equation reads the likelihood of P the probability of picking orange Fanta given in the number of people we asked and X the number of people who say they prefer orange Fanta the right side of the equation however stays the same and this is now the likelihood of P equals 0.5 given that four out of seven people would randomly prefer orange Fanta just a reminder when we calculate likelihoods for P we can plug in different values for it while the observed data in equals 7 and x equals four remains fixed in other words we can calculate the likelihoods for different values of P given that four out of seven people said they preferred orange Fanta for example the likelihood of P equals 0.25 given that four out of seven people said they prefer two orange Fanta is plug and chug plug and chug plug and chug 0.058 the likelihood of P equals 0.25 given that four out of seven people would randomly prefer orange Fanta is less than 0.2 7/3 the likelihood when P equals 0.5 we can also calculate the likelihood of P equals 0.57 given that four out of seven people said they preferred orange Fanta plug and chug plug and chug plug and chug and we get zero point two nine four the likelihood of P equals zero point five seven given that four out of seven people would randomly prefer orange Fanta is greater than zero point two seven three the likelihood when P equals zero point five we can plot the likelihood with a bunch of different values for P between zero and one tada this peak is the maximum likelihood the slope of the curve at the peak is zero that means we can solve for the value for P that results in the maximum likelihood by finding where the derivative I II the slope is equal to 0 so let's do it here's the original likelihood function with n equals 7 and x equals 4 the first thing we do is take the log of the likelihood function we do this because the original likelihood function and its log will both reach the maximum using the same value for P and it's way easier to take the derivative of the log of the likelihood function compared to the original function to see this here is a plot of the likelihood function and here is the log of the likelihood function both have peaks at the same value for P the log function turns the multiplication into addition and it turns the exponents into multiplication if this log stuff is freaking you out well don't freak out just watch the stat quest on logs now we're ready to take the derivative but first because we are running out of room we'll move this to the top of the screen okay now we take the derivative with respect to P this first part doesn't contain P at all so it's derivative equals zero the derivative of the second part is just four times one over P the derivative of this last part is a little tricky since we need to apply the chain rule so we start with 7 minus 4 times the derivative of the log of 1 minus P and we multiply that by the derivative of 1 minus P then we simplify and plug it in BAM [Music] now we set the derivative to zero because we want to find the peak where the slope of the curve equals zero and that will tell us which value for P gives the maximum likelihood now multiply both sides by P times 1 minus P now multiply out 4 times 1 minus P now combine negative 4 P and negative 3p then just solve for P the maximum likelihood estimate for P is for the number of people who preferred orange Fanta divided by 7 the total number of people we asked double bam okay we just solved for the maximum likelihood estimate for P when we have data for X and n however we don't actually need data to determine a general formula for the maximum likelihood for P this formula will give us the maximum likelihood estimate for P when there are X successes in n trials that's how you say it using fancy statistics lingo we'll start with the original likelihood function however this time all of the variables P X and n are unknown just like before we take the log of the likelihood function because it will make solving for the derivative way easier and just like before the log function turns the multiplication into addition and the exponents into multiplication now we're ready to take the derivative and just like before because we are running out of room we will move this to the top of the screen okay now we take the derivative with respect to P the first part does not contain P so it's derivative is zero the derivative of the second part is just x times 1 over P and just like before we have to use the chain rule to figure out the derivative of this last part so we start with n minus x times the derivative of the log of 1 minus P and we multiply that by the derivative of 1 minus P then we simplify and plug it in then just like before we set the derivative to zero note different values for n and X will result in different curves but the slope is still zero at the maximum likelihood now multiply both sides by P times 1 minus P now multiply out x times 1 minus P and now negative XP and positive XP cancel each other out then just solve for P in this case the maximum likelihood estimate for P is X the number of successes divided by n the total number of trials BAM some of you may be saying to yourself duh what's the big deal the maximum likelihood estimate for P is just the average that's obvious well I agree once you know the solution it's pretty obvious but now we also have a mathematical proof that backs up our intuition and to me that's a very comforting thing triple bail hooray we've made it to the end of another exciting stat quest if you liked this tech quest and want to see more of them please subscribe and if you want to support stat quest well click the like button below and consider buying one or two of my original songs alright until next time quest on
J8jNoF-K8E8,2018-08-06T19:58:36.000000,"The Binomial Distribution and Test, Clearly Explained!!!",stat quest is cool that's my opinion if you don't think so then your opinion is inversely correlated with mine stat quest hello and welcome to stat quest today we're going to talk about the binomial distribution in the binomial test and they're going to be clearly explained usually when people talk about the binomial distribution they talk about flipping a coin a coin usually has heads in at least one tail for example you can use the binomial distribution to find out the probability of getting six heads in six tosses but who really cares about flipping coins what folks really want to know is whether or not people like orange Fanta more than grape Fanta which flavor reigned supreme or are they both equally loved to answer this question we can ask a bunch of people which flavor they prefer if everybody but one person said they liked orange Fanta more than grape Fanta then it would be pretty obvious what people liked most but what if four people say they like orange Fanta and three people say they'd like grape Fanta is that enough to be confident than most people like orange Fanta or could it be that people in general don't have a preference and these results are just due to random chance and a small sample size maybe if we surveyed another seven people we might only get three people who like orange Fanta and for people who like grape Fanta to get to the bottom of this mystery we need to get a sense of what to expect if there is no preference then we determine if our survey results fit those expectations if not we can reject the idea that both Fantas are loved equally the binomial distribution will tell us what to expect if there is no preference to say the same thing using statistics lingo we will use the binomial distribution aka this nasty looking thing to model what to expect when there is no preference then we'll see how well this model fits the data if the model is a poor fit we will reject the idea that both flavors are loved equally so let's start with a super simple example and assume that I asked three people if they liked orange Fanta more than grape Fanta the first person we asked said they preferred orange Fanta the second person we asked also said they preferred orange Fanta in the third person we asked said they preferred grape Fanta if people really didn't prefer one flavor over the other then we will assume that there is a 50% chance they will pick orange and a 50% chance they will pick grape we can then calculate the probability of the first two people randomly choosing orange in the third person randomly choosing grape assuming that there is no real preference the probability of the first person preferring orange Fanta is 0.5 in the probability of the first two people preferring orange Fanta is 0.5 times 0.5 which equals 0.25 and the probability of the first two people preferring orange Fanta and the third person performing grape is 0.5 times 0.5 times 0.5 which equals 0.125 note 0.125 is the probability of the first two people saying they prefer orange and the third person saying they prefer grape it is not the probability that any two out of three people would prefer orange let me explain it could have just as easily been that the first person said they preferred grape in this case the probability would still be 0.125 but we'd multiply the numbers together in a different order likewise if the second person said they preferred grape we just multiply the numbers together in a different order so all three of these combinations are equally likely and this means that the probability that any two out of three people prefer orange Fanta is the sum of the three possible orders so we just add the three probabilities together in the probability that any two out of three people would randomly say they prefer orange Fanta is 0.375 alternatively we could have done the math using this nasty-looking formula X is the number of people who preferred orange Fanta in this case X equals 2 n is the total number of people we asked in this case n equals 3 note n minus X the total number of people we asked minus the number of people who preferred orange Fanta equals the number of people who said they prefer grape Fanta P is the probability that someone will pick orange Fanta in this case P equals 0.5 note the probability that someone might prefer grape Fanta is 1 minus P together this says the probability of X the number of people who say to prefer orange Fanta given in the number of people we asked and P the probability of picking orange Fanta equals this nasty looking thing ooh it's got factorials don't freak out it looks fancy but it just boils down to the number of different ways two of three people could say they prefer orange Fanta when we did everything by hand we saw that there were three ways for two of three people to say they prefer orange Fanta and if we plug in N equals 3 and x equals 2 and then just do the math we get three three ways that two out of three people could prefer orange Fanta just like when we did it by hand so this fancy thing is really no big deal the next part of the formula P to the X corresponds to the probability that orange Fanta was chosen two of the three times in other words P to the X just consolidates 0.5 times 0.5 into 0.5 squared the last part of the equation corresponds to the probability that someone preferred grape Fanta remember that one minus P is the probability that someone prefers grape fine too and n minus X is the number of people that said they preferred grape Fanta if we plug in N equals three x equals two and P equals zero point five and then do the math we get zero point five so this term corresponds to the one person who liked grape Fanta thus these two parts of the equation correspond to zero point five times zero point five times zero point five and the nasty part just multiplies it by three now we can put all the parts together and plug in x equals to the number of people that preferred orange Fanta in equals three the number of people we asked and P equals 0.5 the probability someone would randomly pick orange Fanta and we get the same probability that two out of three people would randomly prefer orange Fanta that we got when we did everything by hand 0.375 in other words the binomial distribution tells us that the probability that two of three people will prefer orange Fanta due to random chance is 0.375 BAM calculating the probability of three of three people saying they prefer orange Fanta by hand is pretty easy since there is only one combination but we can just as easily use the fancy formula by plugging in x equals three and then we just do the math this term equals one since we are dividing three factorial by three factorial and this term is also equal one because anything raised to the zero power equals one and then we just keep doing the math and this means that the probability of three of three people randomly preferring orange Fanta is 0.125 which is exactly what we got when we did the calculations by hand now that we've seen that we can calculate probabilities with the binomial distribution let's go back to our original question if four people say they'd like orange Fanta and three people say they'd like grape Fanta can we conclude that people in general prefer orange Fanta now we plug in x equals for the number of people that preferred orange Fanta n equals 7 the number of people we asked and P equals 0.5 the probability someone would randomly pick orange Fanta and then just do the math and we get zero point two seven three the probability that four of seven people would randomly prefer orange Fanta double bam when you use a binomial distribution to calculate a p-value it's called a binomial test so what's the p-value for four out of seven people preferring orange Fanta the p-value is the probability of the observed data four of seven people prefer orange Fanta plus the probabilities of all other possibilities that are equally likely or rare this means we need to calculate these probabilities these are the observed results of our poll and these are rare possibilities and we also need to calculate the probabilities of these combinations these two possibilities for verses three and three versus four are equally rare if you don't believe me plug in the numbers and see the remaining possibilities are rare in other words by including possibilities when grape Fanta is preferred equally or more often we are calculating a two-sided p-value if this is blowing your mind don't freak out just watch the stat quests on p-values clearly explained and one and two sided p-values the links are in the description below we've already calculated the probability that four out of seven people prefer orange Fanta it's zero point two seven three for this we just set X to five and plug and chug and we get zero point one six four then we get zero point zero five five and then we get zero point zero zero eight adding the probabilities together gives us 0.5 the probability that orange Fanta is preferred now we just plug and chug the numbers for when grape Fanta is preferred adding the probabilities together gives us 0.5 the probability that orange Fanta is not preferred the sum of the probabilities of all combinations of events that have an equal probability or a rarer equals 0.5 plus 0.5 which equals 1 which means the p-value for 4 out of 7 people saying that prefer orange Fanta is 1 which means that the model the binomial distribution with P equals 0.5 ie orange Fanta and grape Fanta are both equally loved is a good fit for the observed data thus we conclude that given the sample size seven we cannot rule out the possibility that both orange Fanta and grape Fanta are equally loved think about that the next time you watch the World Series of baseball triple bam one last thing before we're done the binomial distribution only works when the probability that someone likes orange Fanta does not change if someone else already said they liked orange Fanta in other words if we asked a bunch of people if they liked orange Fanta and they all say yes then that should not affect the probability that the next person also likes orange Fanta hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more of them please subscribe and if you want to support stat quest well please click the like button below and consider buying one or two of my original songs alright until next time quest on you
p3T-_LMrvBc,2018-07-30T15:18:07.000000,"Maximum Likelihood for the Exponential Distribution, Clearly Explained!!!",it's raining outside good I've got a stat quest inside yeah hello I'm Josh Starla and welcome to stat quest today we're going to be talking about the exponential distribution and its maximum likelihood estimate we'll start with a brief introduction of the distribution and what it's used for and then we'll dive into the math and nitty-gritty of how maximum likelihood is applied to it so what is the exponential distribution it's a statistical distribution that models the time between events for example how long will you wait before you get another text message or how much time will pass before the next person views this video here's what an exponential distribution looks like the x-axis is the amount of time between events the y-axis is scaled so that the total area under the curve equals 1 if we were interested in the probability of an event like someone viewing this video happening within 0 to 5 seconds we solved for the area under the curve from x equals 0 to x equals 5 seconds here's the equation for an exponential distribution you plug in some value for X and out comes a value for y lambda is called the rate parameter and it is proportional to how quickly things happen in this graph lambda equals 1 and this models an event happening like someone watching this video on average every second here lambda equals two and this models someone watching the video on average twice every second here lambda equals 0.5 and this models someone watching this video on average once every two seconds the goal of maximum likelihood is given a set of measurements to find an optimal value for lambda so assume I collected a lot of data about how much time passed between views of this video X sub 1 equals the amount of time that passed between the first and second views X sub 2 equals the amount of time that passed between the second and third views X sub 3 equals the amount of time that passed between the third and fourth views etc etc etc here are the n measurements for now let's assume we already have a good value for lambda what's the likelihood of lambda given our first measurement X sub 1 here's the likelihood function we have the likelihood of lambda given that we have this measurement X sub 1 and the likelihood equals this equation this equation is just the equation for the curve with X sub 1 plugged into it if X sub 1 was here the likelihood of lambda would be this value on the y-axis similarly the likelihood of lambda given the second measurement X sub 2 is just the equation for the curve with X sub 2 plugged into it if X sub 2 was here the likelihood of lambda would be this value on the y-axis what is the likelihood of lambda given both X sub 1 and X sub 2 well here are the individual likelihood functions and here's the combined likelihood function because we are interested in X sub 1 and X sub 2 we multiply the 2 likelihood functions together here I've just plugged in the 2 likelihood equations and now I've pulled the two lambdas out and lastly I can add the exponents together thus this equation is the likelihood of lambda given X sub 1 and X sub 2 bam what is the likelihood of lambda given all of the data X sub 1 X sub 2 all the way to X sub n here's the likelihood function that includes all of the data we have collected it is equal to the product of all the individual likelihoods here I've just plugged in all the individual likelihood equations and now I've just pulled out all of the lambdas and lastly I've added all of the exponents together thus this equation is the likelihood of lambda given all of the data X sub 1 X sub 2 all the way to X sub M BAM [Music] what if we don't have a good value for lambda well we try different values for lambda to find a good one to find the maximum likelihood we one take the derivative of this and to solve for lambda when the derivative is set to be equal to zero here's a graph of the likelihood using different values for lambda at the maximum likelihood the slope and thus the derivative will be equal to zero to find the maximum likelihood estimate for lambda step 1 we take the derivative of the likelihood function anytime you have a function with an exponential in it it's almost always easier to take the derivative of the log of that function so that's what we're going to do we can do this because the derivative of a function and the derivative of the log of a function equals zero at the same place so for the purposes of finding where the derivative equals zero the original function and its log are interchangeable since we are now working with the log of the function we can split up the multiplication into addition next we move lambdas exponent in to be in front of the log in the log of the exponential function reduces to just its exponent if any of this fancy log stuff is confusing you check out the stat quest on logs finally we take the derivative of both parts of the equation with respect to lambda now we move on to step two set the derivative to be zero and solve for lambda here I added X sub 1 plus X sub 2 plus all the way to X sub n to both sides causing the term to cancel out on the right side of the equal sign then I just multiplied both sides by lambda and lastly I divided both sides by X sub 1 plus X sub 2 plus all the way to X sub n this is the maximum likelihood estimate for lambda double BAM now whenever we collect a lot of data about how much time takes place between events we just plug those values into this equation and we'll get the maximum likelihood estimate for lambda and then we can fit an exponential distribution to our data for example if two seconds pass between the first and second times this video was watched then X sub 1 would equal 2 and of 2.5 seconds passed between the second and third times this video was watched then X sub 2 would equal 2.5 in of 1.5 seconds passed between the third and fourth times this video was watched then X sub 3 would equal 1.5 if that's all our data then we just plug in to 2.5 and 1.5 into the formula for the maximum likelihood estimate for lambda and solve for lambda thus given the data the maximum likelihood estimate for lambda is 0.5 and this is what an exponential distribution looks like when lambda equals 0.5 hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more well please subscribe and if you want to support stat quest please click the like button below and consider buying one or two of my original songs alright until next time quest on
C4N3_XJJ-jU,2018-07-26T10:58:24.000000,"Logistic Regression in R, Clearly Explained!!!!",step quest gettin freaky step quest kind of sneaky step quests hello I'm Josh Starla and welcome to stat quest today at long last we're gonna cover logistic regression in our note a link to the code which is chock full of comments and should be easy to follow is in the description below for this example we're going to get a real data set from the UCI machine learning repository specifically we want the heart disease data set note this is the same data set we used when we made random forests in our if you're familiar with that data you can skip ahead to about 3 minutes and 44 seconds in this video we start by making a variable called URL and set it to the location of the data we want this is how we read the data set into our from the URL the head function shows us the first six rows of data unfortunately none of the columns are labeled wah-wah so we named the columns after the names that were listed on the UCI website hooray now when we look at the first six rows with the head function things look a lot better however the stirrer function which describes the structure of the data tell us that some of the columns are messed up right now sex is a number but it's supposed to be a factor where zero represents female and one represents male CP aka chest pain is also supposed to be a factor where levels 1 through 3 represent different types of pain and 4 represents no chest pain CA and Thal are correctly called factors but one of the levels is question mark when we need it to be in a so we've got some cleaning up to do the first thing we do is change the question marks to n A's then just to make the data easier on the eyes we convert the zeros in sex to F for female and the ones to M for male lastly we convert the column into a factor then we convert a bunch of other columns into factors since that's what they're supposed to be see the UCI website or the sample code on the stat quest blog for more details since the CA column originally had a question mark in it rather than in a are thinks it's a column of strings we correct that assumption by telling our that it's a column of integers and then we convert it to a factor then we do the same thing for Thal the last thing we need to do to the data is make HD aka heart disease a factor that is easy on the eyes here I'm using a fancy trick with the if-else function to convert the zeros to healthy and the ones to unhealthy we could have done a similar trick for sex but I wanted to show you both ways to convert numbers to words once we're done fixing up the data we check that we have made the appropriate changes with the stir function hooray it worked now we see how many samples rows of data have na values later we will decide if we can just toss these samples out or if we should impute values for the NA s6 samples rows of data have n A's in them we can view the samples within a z' by selecting those rows from the data frame and there they are here are the NA values five of the six samples are male and two of the six have heart disease if we wanted to we can impute values for the NA s using a random forest or some other method however for this example we'll just remove these samples including the six samples within A's there are three hundred three samples then we remove the six samples that have n A's and after removing those samples there are two hundred ninety seven samples remaining Scapa now we need to make sure that healthy and diseased samples come from each gender female and male if only male samples have heart disease we should probably remove all females from the model we do this with the X tabs function we pass X tabs the data and use model syntax to select the columns in the data we want to build a table from in this case we want a table with heart disease and sex and bam healthy and unhealthy patients are both represented by a lot of female and male samples now let's verify that all four levels of chest pain CP for short were reported by a bunch of patients yes and then we do the same thing for all of the boolean and categorical variables that we are using to predict heart disease here's something that could cause trouble for the resting electrocardiographic results only for patients represent level 1 this could potentially get in the way of finding the best fitting line however for now we'll just leave it in and see what happens and then we just keep looking at the remaining variables to make sure that they're all represented by a number of patients okay we've done all the boring stuff now let's do logistic regression let's start with a super simple model we'll try to predict heart disease using only the gender of each patient here's our call to the GLM function the function that performs generalized linear models first we use formula syntax to specify that we want to use sex to predict heart disease then we specify the data that we are using for the model lastly we specify that we want the binomial family of generalized linear models this makes the GLM function do logistic regression as opposed to some other type of generalized linear model oh I almost forgot to mention that we are storing the output from the GLM function in a variable called logistic we then use the summary function to get details about the logistic regression BAM [Music] the first line has the original call to the GLM function then it gives you a summary of the deviance residuals they look good since they are close to being centered on zero and are roughly symmetrical if you want to know more about deviance residuals check out the stat quest deviance residuals clearly explained then we have the coefficients they correspond to the following model heart disease equals negative one point zero four three eight plus one point two seven three seven times the patient is male the variable the patient is male is equal to zero when the patient is female and one when the patient is male thus if we are predicting heart disease for our female patient we get the following equation heart disease equals negative one point zero four three eight plus one point two seven three seven times zero this reduces to heart disease equals negative one point zero four three eight thus the log odds that a female has heart disease equals negative one point zero four three eight if we were predicting heart disease for a male patient we get the following equation heart disease equals negative one point zero four three eight plus one point two seven three seven times one and that reduces to heart disease equals negative one point zero four three eight plus one point two seven three seven since this first term is the log odds of a female having heart disease the second term indicates the increase in the log of the odds that a male has of having heart disease in other words the second term is the log of the odds ratio of the odds that a male will have heart disease over the odds that a female will have heart disease this part of the logistic regression output shows how the Wald's was computed for both coefficients and here are the P values both P values are well below 0.05 and thus the log of the odds and the log of the odds ratios are both statistically significant but remember a small p value alone isn't interesting we also want large effect sizes and that's what the log of the odds and the log of the odds ratio tells us if you want to know more details on the coefficients and the Wold test check out the following stat quests odds and the log odds clearly explained odds ratios and log odds ratios clearly explained in logistic regression details part 1 coefficients next we see the default dispersion parameter used for this logistic regression when we do normal linear regression we estimate both the mean and the variance from the data in contrast with logistic regression we estimate the mean of the data in the variance is derived from the mean since we are not estimating the variance from the data and instead just deriving it from the mean it is possible that the variance is underestimated if so you can adjust the dispersion parameter in the summary command then we have the null deviance and the residual deviance these can be used to compare models compute R squared in an overall p value for more details check out the stat quests logistic regression details part 3 R squared and its p-value in saturated models and deviance statistics clearly explained then we have the AIC the Chi key information criterion which in this context is just the residual deviance adjusted for the number of parameters in the model the AIC can be used to compare one model to another lastly we have the number of Fisher scoring iterations which just tells us how quickly the GLM function converged on the maximum likelihood estimates for the coefficients if you want more details on how the coefficients were estimated check out the stat quest logistic regression details part two fitting a line with maximum likelihood double bam now that we've done a simple logistic regression using just one of the variables sects to predict heart disease we can create a fancy model that uses all of the variables to predict heart disease this formula syntax HD tilde dot means that we want to model heart disease HD using all of the remaining variables in our data frame called data we can then see what our model looks like with the summary function dang the summary goes off the screen no worries we'll just talk about a few of the coefficients we see that age isn't a useful predictor because it has a large p-value however the median age in our dataset was 56 so most of the folks were pretty old and that explains why it wasn't very useful gender is still a good predictor though if we scroll down to the bottom of the output we see that the residual deviance and the AIC are both much smaller for this fancy model than they work for the simple model when we only use gender to predict heart disease if we want to calculate McFadden's pseudo r-squared we can pull the log likelihood of the null model out of the logistic variable by getting the value for the null deviance and dividing by negative two and we can pull the log-likelihood for the fancy model out of the logistic variable by getting the value for the residual deviance and dividing by negative 2 then we just do the math and we end up with a pseudo R squared equals 0.55 this can be interpreted as the overall effect size and we can use those same log likelihoods to calculate a p-value for that r-squared using a chi-squared distribution in this case the p-value is tiny so the r-squared value isn't due to dumb luck one last shameless self-promotion more details on the r-squared and p-value can be found in the following stat quest logistic regression details part 3 R squared and it's p-value lastly we can draw a graph that shows the predicted probabilities that each patient has heart disease along with their actual heart disease status I'll show you the code in a bit most of the patients with heart disease the ones in turquoise are predicted to have a high probability of having heart disease and most of the patients without heart disease the ones in salmon are predicted to have a low probability of having heart disease thus our logistic regression has done a pretty good job however we could use cross-validation to get a better idea of how well it might perform with new data but we'll save that for another day to draw the graph we start by creating a new data frame that contains the probabilities of having heart disease along with the actual heart disease status then we sort the data frame from low probabilities to high probabilities then we add a new column to the data frame that has the rank of each sample from low probability to high probability then we load the ggplot2 library so we can draw a fancy graph then we load the Cal plot library so that ggplot has nice-looking to false then we call GG plot and use G on point to draw the data and lastly we call GG Save to save the graph as a PDF file triple bam hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more of them please subscribe and if you want to support stat quest well please click the like button below and consider buying one or two of my original songs alright until next time quest on
JC56jS2gVUE,2018-07-16T16:26:47.000000,Deviance Residuals,if I was a cat I'd be sleeping on the couch but I'm not a cat so I'm watching stat quest stat quest hello I'm Josh stormer and welcome to stat quest today we're going to talk about deviance residuals and they're gonna be clearly explained this stat quest follows up on the stat quest on saturated models and deviant statistics so watch that video first if you're not familiar with those topics we already know that the residual deviance is defined as two times the difference between the log likelihood of the saturated model and the log likelihood of the proposed model let's start by reviewing how the residual deviance is calculated the likelihood of the data given the saturated model is equal to the likelihood of the first data point times the likelihood of the second data point times the likelihood of the third data point note we're just using three data points in this example however if you've got more data you just keep adding them to the multiplication and the log likelihood is just duh the log of the likelihood and that's what we plug in to the first part of the equation the likelihood of the data given the proposed model equals the likelihood of the first data point times the likelihood of the second data point times the likelihood of the third data point then we just take the log of the likelihood and plug that into the last part of the equation putting the log likelihoods together gives us the difference in the likelihoods these terms represent the difference in likelihood between the saturated and proposed model for the first data point these terms represent the difference for the second data point and these terms represent the difference for the third data point we can rearrange the equation for residual deviance to reflect the difference in likelihoods for the individual data points again the first term represents the difference for the first data point the second term represents the difference for the second data point and the third term represents the difference for the third data point deviance residuals are just the square roots of the individual terms in other words the deviance residuals represent the square root of the contribution that each data point has to the overall residual deviance the first deviance residual represents the square root of the difference in log likelihoods for the first data point the second deviance residual represents the square root of the difference and log likelihoods for the second data point this last deviance residual represents the square root of the difference in log likelihoods for the third data point if we then Square the individual deviance residuals and then add them up we will end up with the residual deviance thus the deviance residuals are analogous to the residuals in ordinary least squares when we square these residuals we get the sums of squares that we use to assess how well the model fits the data similarly when we square the deviance residuals we get the residual deviance that we use to assess how well a model fits the data BAM [Music] when doing logistic regression the equation for deviance residuals can be rewritten to be based on the distances between the data and the best fitting line the blue deviance residuals calculated with the blue distances are positive because they are above the squiggly line and the deviance residuals calculated with the red distances are negative because they are below the squiggly line making the residuals above the squiggle positive and the residuals below the squiggle negative helps us identify outliers if you plot them on an XY graph they should be centered and relatively close to zero here's the first deviance residual it's close to zero because the squiggly line is very close to it here's the second deviance residual it's a lot further from zero since the squiggly line doesn't fit it well here's the third deviance residual here's the fourth deviance residual here's the fifth deviance residual and here's the last deviance residual the second and fifth deviance residuals are relatively far from zero and maybe outliers we should make sure we didn't make a mistake when we originally labeled them double bail in summary deviance residuals represent the square root of the contribution that each data point has to the overall residual deviance and we use them to identify outliers hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stat quest well please click the like button below and consider buying one or two of my original songs alright until next time quest on
9T0wlKdew6I,2018-07-09T18:48:49.000000,Saturated Models and Deviance,stack question and elevator stack quest now end of long step question a wheelbarrow stack quest is everywhere hello I'm Josh stormer and welcome to stack quest today we're gonna be talking about saturated models and deviance statistics note there's a jackhammer blasting away at concrete right across the street from me so if you hear a low rumbling noise that's not my stomach that's a jackhammer if you're watching this video chances are it is because you've already stumbled over something called the saturated model you might have seen it in the context of the log likelihood based R squared formula aka McFadden's pseudo R squared or maybe you saw the saturated model in the formula for residual deviance or maybe you're just here for the song regardless in this video I'll demystify the saturated model and a related concept deviant statistics lastly I'll explain why you can ignore the saturated model when you're doing logistic regression note the saturated model and deviance statistics are part of generalized linear models and are intended to be used in a wide variety of situations including but not limited to logistic regression linear models and other methods so let's dive in imagine we weighed some mice now just for the sake of this example assume that we already know the standard deviation of the data that means all we need to do to fit a normal curve to the data is estimate its mean BAM here's our normal curve centered on the estimated mean value the normal curve is a model of the data we can use it instead of the original data to estimate probabilities and do statistical tests since we only had to estimate the mean for this model we can say that the model has one parameter and since a one parameter model is as simple as it gets we'll call it the null model the likelihood of the data given the null model equals 0.03 in the log likelihood of the data given the null model equals the log of 0.03 which equals negative 3.5 1 we could also create a slightly fancier model by fitting two normal curves to the data this model would have two parameters one for each mean that we estimated missed we're still assuming we do not have to estimate standard deviations the likelihood of the data given the fancier model equals three point five seven and the log likelihood of the data given the fancier model equals the log of three point five seven which equals one point two seven we could also create a super fancy model for our data the super fancy model has one parameter per data point the super fancy model is called the saturated model because it maxes out the number of parameters we can estimate the likelihood of the data given the saturated model equals 1291 point five in the log likelihood of the data given the saturated model is the log of 1291 point five which equals seven point one six so far we've looked at the likelihood of the data using three different models we looked at the likelihood of the data given the null model which has only one parameter we looked at the likelihood of the data given a model with two parameters let's call this the proposed model imagine it's the one were really interested in using and we looked at the likelihood of the data given the saturated model which has one parameter per data point by calculating the likelihood of the null model we have a sense of the worst case scenario and by calculating the likelihood of the saturated model we have a sense of the best-case scenario the likelihood of the saturated model is as high as it can be ideally we want the likelihood of the data given our proposed model to be larger than the null model and close to the saturated model when using likelihoods we use the null and saturated models to determine whether the proposed model does a good job with the data in other words we use the null and saturated models to calculate R squared and its p-value for the proposed model let's start by explaining the role that the saturated model plays in the log-likelihood paste r-squared but before we get started let me review one concept from the normal r-squared the one from linear regression calculated with the sums of squares of the residuals with the normal r-squared this sums of squares for the null model determines the boundary for a bad fit and when the model fits the data perfectly the residuals are all 0 in the sums of squares for the proposed model equals 0 in other words there is a fixed value 0 that represents a boundary for the best a model can do and when we plug in 0 we get R squared equals 1 to summarize the normal r-squared has a boundary for how poorly a model can perform the sums of squares for the null model and a fixed value zero that represents the best possible model now let's go back to talking about the log likelihood based R squared just like before the null model provides a boundary for how poor a model can perform however unlike the traditional R squared there is no fixed value for an ideal fit that works in every situation and that's where the saturated model comes in the saturated model provides an upper bound for what an ideal fit is thus if the proposed model is just as good as the saturated model then the log-likelihood of the proposed model will equal the log likelihood of the saturated model and that makes the numerator the same as the denominator and r-squared equals one BAM but not a big BAM just a little BAM for now to see the problem that the saturated model solves in action let's see what happens when we exclude it from the equation and plug in the log likelihood values that we've calculated so far first we plug in the log likelihood for the null model that's negative three point five one then we plug in the log likelihood for the proposed model that equals one point two seven now just do the math when we leave out the saturated model the r-squared equals one point three six whoa our squared is only supposed to go from zero to one we must be missing a scaling factor now let's redo the calculation but this time let's plug in the value for the log likelihood of the saturated model the other values are the same as before and now we just do the math this time we got 0.45 a reasonable value that is between 0 & 1 this deserves a big Bam Bam in summary the log-likelihood of the saturated model ensures that the r-squared value ranges between zero and one now let's talk about deviance a concept related to the saturated model that will ultimately lead us to a p-value for the r-squared we're going to talk about two types of deviants residual deviance and null deviance residual deviance is defined as two times the difference between the log-likelihood of the saturated model and the log-likelihood of the proposed model the two times part makes the difference in these log likelihoods have a chi-squared distribution with degrees freedom equal to the difference in the number of parameters in this case the degrees of freedom is for the number of parameters in the saturated model 6 - the number of parameters in the proposed model - to see the residual deviance in action let's plug in the numbers boo-boo-boo-boo-boo PPP boo now we just do the math and we get eleven point seven eight here's a chi-square distribution with four degrees of freedom and here's where the residual deviance 11.7 8 is on the x-axis in the area under the curve from eleven point seven eight to infinity is zero point zero two so the p value equals zero point zero two and this means that the saturated model is significantly different from the proposed model to be honest I'm not a huge fan of this statistic in and of itself but it is very common and as we'll see in a bit can be used as a stepping stone for calculating the p-value for r-squared note for this chi-square test to work correctly the proposed model and the saturated model have to be nested in other words the proposed model has to be a simpler version of the saturated model and not just any old model now let's talk about the null deviance the null deviance is defined as two times the difference between the log-likelihood of the saturated model and the log-likelihood of the null model again the two times part makes the difference in these log likelihoods have a chi-squared distribution with degrees freedom equal to the difference in the number of parameters in this case the degrees of freedom is 5 the number of parameters in the saturated model 6 minus the number of parameters in the null model 1 to see the null deviance in action let's plug in the numbers bbb-big boo boo boo boo boo boo now we just do the math and that gives us a value of 21 point 3 4 here's a chi-square distribution with 5 degrees of freedom and here's where the null deviance 21 point 3 4 is on the x-axis in the area under the curve from 21 point 3 4 to infinity is close to 0 so the p value equals close to 0 thus the null unsaturated models are significantly different just like for the residual deviance I'm not a huge fan of this statistic in and of itself however it's very common and you can calculate the p-value for the log-likelihood r-squared from the difference in the deviances this difference is a chi-square value with degrees freedom equal to the difference in the number of parameters for the proposed model in the null model in this case the degrees of freedom is 1 the number of parameters in the proposed model 2 - the number of parameters in the null model 1 plugging in the values for the null and residual deviances gives us Oh baby baby baby boo boo boo boo nine point five six here's a chi-square distribution with one degree of freedom and here's where nine point five six is on the x-axis in the area under the curve from nine point five six to infinity is zero point zero zero two thus the p-value for the log-likelihood r-squared value we calculated before is zero point zero zero two this means that not only does our model have a reasonable effect size the r-squared value we know that that value is not by chance double bam note there is more than one way to calculate the chi-squared value for the r-squared alternatively we can calculate the significance of the r-squared directly by comparing the proposed model to the null model since the null model is nested within the proposed model the formula is simply two times the difference between the log-likelihood of the proposed model and the log-likelihood of the null model again the two times part makes the difference in these log likelihoods have a chi-squared distribution with degrees freedom equal to the difference in the number of parameters however I mentioned the method that uses deviances because the null and residual deviances are very frequently reported in contrast the log-likelihood of the proposed model and the log-likelihood of the null model are not always reported little Bingham okay we talked about the saturated model residual deviance and null deviance now let's talk about why you can ignore the saturated model entirely when you do logistic regression when you do logistic regression you're fitting a squiggle to your data and calculating the log likelihood of the data given that squiggle however with the saturated model the squiggle fits the data perfectly in the log likelihood of the data is zero because the log of one equals zero so the log likelihood is just a big sum of zeroes this means that the equation for the likelihood based R squared is this which boils down to this simpler equation similarly the equations for residual deviance and null deviance reduced to just two times the negative log likelihoods of the proposed model and the null model note these deviances will ultimately be positive numbers because the likelihoods will be numbers between 0 & 1 and the log of a number between 0 & 1 is a negative number and multiplying a negative number by negative 1 results in a positive number double BAM in summary we talked about how the saturated model provides an upper bound on the log-likelihood for a model and that upper bound is useful for calculating a likelihood based r-squared however with logistic regression the upper bound is always going to be zero so we can ignore it we also talked about how residual and null deviance can be used to calculate the p-value for a likelihood based r-squared the difference between the null deviance and the residual deviance equals a chi-squared value with degrees of freedom equal to the number of parameters in the proposed model minus the number of parameters in the null model hooray we've made it to the end of another exciting step quest if you like this stat quest and want to see more of them please subscribe and if you want to support stat quest please click the like button below who consider buying one or two of my original songs alright until next time quest on
8nm0G-1uJzA,2018-06-21T21:07:23.000000,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!","static quest is on my mind all night long static quest hello I'm Josh stormer and welcome to stack quest today we're going to talk about odds ratios and log odds ratios and they're gonna be clearly explained the stack quest on odds and the log of the odds ended in a cliffhanger we talked about how the odds are just the ratio of something happening ie my team winning to something not happening ie my team not winning we Illustrated this with circles blue circles represented my team winning red circles represented my team losing and the odds of my team winning were just the blue circles over the red circles alternatively we could just use numbers to represent the odds and when we do the math we see the odds of winning are 0.5 the cliffhanger came when I said that even though the odds our ratio it's not what people mean when they say odds ratio so let's clear this up once and for all when people say odds ratio they are talking about a ratio of odds so we've got a ratio of these odds to these odds doing the math gives us 2 divided by 4 over 3 divided by 1 and that gives us zero point 1 7 just like when we calculate the odds of something if the denominator is larger than the numerator the odds ratio will go from 0 to 1 and if the numerator is larger than the denominator then the odds ratio will go from 1 to infinity and beyond and just like the odds taking the log of the odds ratio makes things nice and symmetrical for example if the odds ratio is two divided by four over three divided by one then the log of the odds ratio equals negative one point seven nine and if the odds ratio is the opposite it's three to one over two to four then the log of the odds ratio is the positive version it equals one point seven nine great now that we've got that cleared up what can we do with odds ratios here's an example of the odds ratio in action we've got a bunch of people 356 to be exact 29 of these people have cancer and 327 do not we also know that 140 of these people have the mutated gene I'm just going to let you imagine which gene I'm talking about here and 216 people do not have the mutated gene we can use an odds ratio to determine if there's a relationship between the mutated gene and cancer if someone has the mutated gene or the odds higher that they will get cancer given that a person has the mutated gene the odds that they have cancer are 23 to 117 so we'll put that on top of the odds ratio and given that a person does not have the mutated gene the odds that they have cancer are 6 to 210 so we'll put that on the bottom of the odds ratio here's our odds ratio we do the math and the odds ratio tells us that the odds are six point eight eight times greater that someone with the mutated gene will also have cancer and the log of the odds ratio is 1.93 small bam what does all this mean the odds ratio and the log of the odds ratio are like r-squared they indicate a relationship between two things in this case a relationship between the mutated gene and cancer and just like r-squared the values correspond to effect size larger values mean the mutated gene is a good predictor of cancer smaller values mean that the mutated gene is not a good predictor of cancer BAM [Music] however just like r-squared we need to know if this relationship is statistically significant so let's do it there are three ways to determine if an odds ratio or log of an odds ratio is statistically significant one Fisher's exact test to a chi-square test and three the Wold test one super annoying thing is that there is no general consensus on which method is best and people often mix and match some people we use Fisher's exact test or the chi-square test to calculate a p-value and use the Wald test to calculate a confidence interval and some people are happy to let Wall do all the work calculate the p-value and the confidence interval the last method ensures that the p-value and confidence interval will always be consistent but check and see what other folks do in your field to find out what is most acceptable so let's start with Fisher's exact test step 1 watch the stat quest on enrichment analysis using Fisher's exact test and the hyper geometric distribution BAM now think of the people as a bag of tasty M&Ms people with cancer are represented by 23 plus 6 equals 29 red M&Ms people without cancer are represented by 117 plus 210 equals 320 7 blue M&Ms now just like in that stat quest we work out the p-value for grabbing a handful of 23 red M&Ms and 117 blue M&Ms and just like in that stat quest we use a computer and it says the p-value equals zero point zero zero zero zero one another small BAM now let's talk about how to calculate the p-value using a chi-square test the chi-square test compares the observed values to expected values that assume there is no relationship between the mutated gene and cancer to do this we calculate the probability of having cancer as the total number of people with cancer that's 29 divided by the total number of people that's 356 so the probability of having cancer is 0.08 so if the gene is not associated with the 23 plus 117 equals 140 people with the mutated gene then the probability of having cancer times the number of people with the mutated gene equals 11 point 2 thus the expected number of people with the mutated gene and cancer is 11 point 2 in the remaining 100 28.8 people with the mutated gene are expected not to have cancer likewise if the gene is not associated with the six plus 200 ten equals 216 people without the mutated gene then the probability of having cancer times the number of people without the mutated gene equals seventeen point three thus the expected number of people with cancer that do not have the mutated gene is seventeen point three and the remaining 100 98.7 people with the mutated gene are expected not to have cancer now we do a chi-square test to compare the observed and expected values and the p-value is zero point zero zero zero zero one with the continuity correction and zero point zero zero zero zero zero four without the continuity correction if you're not familiar with the chi-square test don't panic we'll do a stat quest on it one last small BAM lastly let's talk about the Wold test this test is commonly used to determine the significance of odds ratios in logistic regression and to calculate confidence intervals the wold test takes advantage of the fact that the log of the odds ratios just like the log of the odds are normally distributed this is a histogram of 10,000 randomly generated log of the odds ratios that tells us what to expect if there is no relationship between the mutated gene and cancer if you want to draw this histogram at home randomly pick a total number of people between 300 and 400 we do this to simulate the fact that if we repeated this experiment we might not get the exact same sample size each time then for each sample select a random number between 0 and 1 if the number was less than 0.08 the proportion of people with cancer then the sample has cancer then pick another random number between zero and one if the number was less than 0.3 nine the proportion of people with the mutated gene then it has the mutated gene this gives you a matrix of random values that did not depend on a relationship between the mutated gene and cancer lastly calculate the log of the odds ratio do this 10,000 times and draw a histogram a normal curve fits pretty well notice that the histogram and curve are centered on zero when there is no difference in the odds the log of the odds ratio equals zero the standard deviation of the 10,000 log of the odds ratios is zero point four three however it is more common to estimate the standard deviation from the observed values you do this by taking the square root of the sum of one over each of the observed values if we do the math we get 0.47 the two different standard deviations that we calculated are very similar all that the wold test does is look to see how many standard deviations the observed log of the odds ratio is from zero and since the Wold test typically uses the estimated standard deviation we'll replace the histogram with a normal curve centered on 0 that has a standard deviation of 0.47 the log of the odds ratio is the same one that we already calculated here's where the log of the odds ratio goes on the curve to find out how many standard deviations the log of the odds ratio is away from 0 we simply divide by the standard deviation thus we divide 1.93 by 0.47 and that gives us 4 point 1 1 so our log of the odds ratio is 4 point 1 1 standard deviations away from the mean of the distribution a general rule of thumb with normal distributions is that anything further than two standard deviations from the mean will have a p-value less than 0.05 so we know our log of the odds ratio is statistically significant however to get a precise two-sided p-value we can add up the areas under the curve for points greater than one point nine three and four points less than negative one point nine three however this is traditionally done using a standard normal curve ie a normal curve with mean equals zero and standard deviation equal to one and that means adding up the areas under the curve for points that are greater than four point 1 1 and for points that are less than negative 4 point 1 1 where for point 1 1 is the number of standard deviations that the log of the odds ratio is away from the mean ultimately the p-value that the mutated gene does not have a relationship with cancer is 0.00005 double bam before we go here are some final thoughts about the three different statistical tests we can use with the log of the odds ratio when I generated the 10,000 random log of the odds ratios I performed all three tests on them if the tests worked as expected 5 percent should have p-values less than 0.5 here's what I got for Fisher's exact test four percent of the p-values were less than 0.05 for the chi-square test with the continuity correction three percent of the p-values were less than 0.05 without the continuity correction five percent of the p-values were less than 0.05 for the Wold test 5% of the p-values were less than 0.05 so all of the tests did a good job limiting significant p-values so just find out what method is most commonly used in your field personally I'd be more comfortable with a borderline p-value if I knew it passed all of the tests in summary an odds ratio is just a ratio of odds and a log of the odds ratio is just wait for it the log of an odds ratio the odds ratio and the log of the odds ratio tells us if there is a strong or weak relationship between two things like whether or not having a mutated gene increases the odds of having cancer and depending on the field you work in people use Fisher's exact test chi squared or the wold test to determine p-values for the significance of that relationship hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more of them please subscribe and if you want to support stat quest well click the like button below and consider buying one or two of my original songs alright until next time quest on"
xxFYro8QuXA,2018-06-18T18:38:47.000000,Logistic Regression Details Pt 3: R-squared and p-value,ah square p-values effects sons significance let's talk about it stat quest hello I'm Josh Starla and welcome to stat quest in this video we're going to continue our series on logistic regression and we're going to talk about R squared and P values in the stat quest logistic regression details part 2 fitting a line with maximum likelihood we had weight measurements for obese mice and weight measurements for some mice that were not obese and we converted the y-axis from probability to the log odds of obesity and then fit a line to that data using maximum likelihood well technically we maximize the log likelihood but either way you do it you get the same best fitting line however we ended with a bit of a cliffhanger we know that the line is best fit but how do we know if it is useful in other words how do we calculate R squared and a p-value for the relationship between weight and obesity the answer is it's complicated although the idea behind generalized linear models a common framework for solving all kinds of problems is nice the result is a bit of a hot mess even though pretty much everyone agrees on how to calculate R squared and The Associated p-value for linear models there is no consensus on how to calculate R squared for logistic regression there are more than 10 different ways to do it so before you settle on a way to calculate R squared for logistic regression look and see what other people are already doing in your field that will give you a good starting point for this stat quest rather than describe every single r-squared for logistic regression I'm focusing on one that is commonly used and is easily calculated from the output that R gives you just so you know this R squared is called McFadden's pseudo R squared another bonus is that this method is very similar to how R squared is calculated for regular old linear models so let's do a super quick review of R squared for regular old linear models using size and weight measurements as an example so that the concepts are fresh in your mind wow that was a long sentence in linear regression and other linear models R squared and the related p-value are calculated using the residuals in brief we square the residuals and then add them up I call this SS fit for sum of squares of the residuals around the best fitting line and we compare that to the sum of squared residuals around the worst fitting line the mean of the y axis values this is called SS mean R squared compares a measure of a good fit SS fit to a measure of a bad fit SS mean R squared is the percentage of variation around the mean that goes away when you fit a line to the data also because I want to refer to this later I'm going to point out another thing you already know R squared goes from 0 to 1 if there wasn't a relationship between weight and size the data might look like this and the best fitting line might look like this in this case SS fit equals SS mean and when we plug in the values for SS fit and SS mean we get 0 in the numerator and then R squared equals 0 on the other hand when the line fits the data perfectly ss fit equals zero and that means when we plug in the values for SS fit and SS mean we get SS mean minus zero in the numerator and then R squared equals one duh I told you this was something you already knew now let's talk about R squared in terms of logistic regression like linear regression we need to find a measure of a good fit to compare to a measure of a bad fit unfortunately the residuals for logistic regression are all infinite so we can't use them but we can project the data onto the best fitting line and then we translate the log odds back to probabilities and lastly calculate the log likelihood of the data given the best fitting squiggle in this case that gives us negative three point seven seven we can call this ll fit for the log likelihood of the fitted line and use it as a substitute for SS fit now we need a measure of a poorly fitted line that is analogous to SS mean we do this by calculating the log odds of obesity without taking weight into account the overall log odds of obesity is just the total number of obese mice divided by the total number of mice that are not obese then we just take the log of the whole thing and do the math in this case we get a horizontal line at zero point two two then project the data onto this line and then we translate the log odds back to probabilities that gives us a horizontal line at P equals zero point five six note the overall log odds zero point two two translates to the overall probability of being obese zero point five six in other words we can arrive at the same solution by calculating the overall probability of obesity hooray they are the same so we have two different ways to calculate the exact same number now calculate the log likelihood of the data given the overall probability of obesity this gives us negative six point one eight we'll call this ll overall probability and use it as a substitute for SS mean so we have LL overall probability a measure of a bad fit and ll fit hopefully a measure of a good fit and it makes intuitive sense that we could combine them just like we combined SS mean and SS fit to calculate R squared plugging in the numbers gives us an r-squared value equals zero point three nine BAM however before we get too excited let's verify that calculating R squared with these log likelihoods will result in a value between zero when the fit is bad and one when the fit is good let's start by looking at the r-squared we'll get when weight is not a good predictor of obesity there are light mice that are obese and light mice that are not obese and there are heavy mice that are obese and heavy mice that are not obese intuitively we can see that with this data weight makes a poor predictor of obesity the maximum likelihood best fitting line for this data has an intercept of negative zero point two two and a slope of practically zero 5.10 to the negative 17 to be exact on the log-odds scale and this translates to a horizontal line at 0.45 on the probability scale ll fit is the log likelihood of the data projected onto the best fitting line and that value is negative six point one eight now let's calculate ll overall probability the first step is simply to calculate the overall probability of obesity that gives us a line at zero point four four ll overall probability is the log likelihood of the data projected onto the overall probability in this case ll overall probability equals negative six point one eight now let's just plug in the values for ll overall probability and ll fit when we do the math we get R squared equals zero BAM now let's look at the R squared we'll get when weight is an awesome predictor of obesity the maximum-likelihood best fitting line for this data has an intercept of negative sixty three point seven two and a slope of twenty two point four two and this translates to a squiggly line on the probability scale lfit is the log likelihood of the data projected onto the best fitting line in this case ll fit equals zero that's because the log of one equals zero so ll fit is just the sum of a bunch of zeros now let's calculate the overall probability of obesity and that gives us a line at 0.44 ll overall probability is the log likelihood of the data projected onto the overall probability in this case that value is negative six point one eight at this point you may have noticed something when the model is a poor fit the log likelihood for logistic regression is a relatively large negative value and when the model is a good fit the log likelihood for logistic regression is a value close to zero log likelihoods for logistic regression will always be between zero and negative infinity because we are taking logs of values between 0 and 1 and good fits result in log likelihoods close to 0 and bad fits result in larger negative log likelihoods okay back to calculating r squared let's plug in the values for LL overall probability and ll fit this gives us an R squared equals 1 double bail so we see that at least on an intuitive level the r-squared calculated with log likelihoods behaves like the r-squared calculated from sums of squares the log likelihood r-squared values go from 0 for poor models to 1 for good models now we need a p-value the good news is that calculating the p-value is pretty straightforward two times the difference between ll fit and ll overall probability equals a chi-squared value with degrees freedom equal to the difference in the number of parameters in the two models ll fit has two parameters since it needs estimates for a y-axis intercept and a slope ll overall probability has one parameter since it only needs an estimate for a y-axis intercept so in this case the degrees of freedom equals 1 here's a graph of a chi-squared distribution with one degree of freedom in the worst case scenario ll fit equals ll overall probability and the whole thing equals zero in this case the p value equals one since the area under the curve from 0 to infinity equals 1 however most of the time ll fit will be closer to zero than ll overall probability and since the log likelihoods are negative this will be a positive value on the x-axis and the p value will get smaller in the example we've worked out in this stat quest we end up with four point eight two for the chi-squared distribution and the p value equals zero point zero three thus the relationship between weight and obesity is not due to chance and the R squared value zero point three nine tells us the effect size of this relationship triple bam before we leave there are a few things I need to point out when you see these formulas for r-squared and the associated p-value out in the wild they will look more like this this is because the formulas usually include terms for the saturated model I'll talk more about the saturated model in another stat quest for now however just know that when doing logistic regression the log-likelihood of the saturated model equals zero so we can omit it and when we omit the term for the saturated model we get the simple equations are presented in this stat quest however the log likelihood of the saturated model isn't always zero when it is used with other generalized linear models so people included when talking about the log likelihood r-squared and associated p-value so that it'll work in other situations but don't worry we'll talk about these topics in a stat quest on the saturated model and deviance statistics hooray we've made it to the end of another exciting stat quest if you liked this static quest and want to see more please subscribe and if you want to support stack quest well please click the like button below and consider buying one or two of my original songs alright until next time quest on
BfKanl1aSG0,2018-06-11T17:48:12.000000,Logistic Regression Details Pt 2: Maximum Likelihood,maximum likelihood in your neighborhood I think that you should learn about it stack west hello I'm Josh star and welcome to stack quest today we're gonna follow up on our series of videos on logistic regression this time we're going to talk about fitting a line using maximum likelihood that is to say we're going to talk about how this squiggle is optimized to fit the data the best in the first video in this series logistic regression details part 1 coefficients we saw that logistic regression is very similar to regular old linear models like linear regression t-tests and fancy stuff like multiple regression and ANOVA the big difference is that logistic regression uses the log odds on the y-axis however you may recall that in that stat quest I said just take my word for it that this is the best fitting line well the time for blindly trusting me is over let's see how this line is fit to the data however before we talk about how lines are fit in logistic regression let's do a super quick review of how lines are fit in linear regression we start with some data and we fit a line to it using least squares in other words we measured the residuals the distances between the data and the line then square them so that negative values do not cancel out positive values and then add them all up then we rotate the line a little bit and do the same thing measure the residuals square them and add them up in the line with the smallest sum of squared residuals the least squares is the line chosen to fit best okay now that we remember how to fit a line in linear regression let's talk about logistic regression in this example we are using logistic regression to determine the effect of weight on obesity these mice are obese and these mice are not obese our goal is to draw the best fitting squiggle for this data as we know in logistic regression we transform the y axis from the probability of obesity to the log odds of obesity we can draw a candidate best fitting line on the graph the only problem is that the transformation pushes the raw data to positive and negative infinity and this means that the residuals the distance from the data points to the line are also equal to positive and negative infinity and this means we can't use least squares to find the best fitting line instead we use maximum likelihood the first thing we do is project the original data points onto the candidate line this gives each sample a candidate log-odds value in other words the log odds of this point is two point one and the log odds of this point is one point four then we transform the candidate log odds to candidate probabilities using this fancy looking formula which is just a reordering of the transformation from probability to log odds for those at home keeping score here's how to convert the equation that takes probability as input and outputs log odds into an equation that takes log odds as input and outputs probability first we exponentiate both sides then we multiply both sides by one minus P and then multiply 1 minus P and E to the log odds then we add P times e to the log odds to both sides then we pull P out on the left side of the equation lastly we divide both sides by 1 plus e to the log odds BAM now let's see this fancy equation in action for example for this point we substitute negative 2.1 for the log odds and that gives us P equals 0.1 and that gives us a y-coordinate on the squiggle and we do the same thing for all of the points now we use the observed status obese or not obese to calculate their likelihood given the shape of the squiggly line we'll start by calculating the likelihood of the obese mice given the shape of the squiggle the likelihood that this Mouse is obese given the shape of the squiggle is the value on the y-axis where the point intersects the squiggle 0.49 in other words the likelihood that this Mouse is obese given the shape of the squiggle is the same as the predicted probability in this case the probability is not calculated as the area under a curve but instead is the y-axis value and that's why it's the same as the likelihood the likelihood that this Mouse's obese is 0.9 the likelihoods that these mice are obese are 0.9 10.9 1 and 0.92 the likelihood for all of the obese mice is just the product of the individual likelihoods now we'll figure out the likelihoods for the mice that are not obese note the lower the probability of being obese the higher the probability of not being obese thus for these mice the likelihood equals one minus the probability the mouse is obese the probability that this Mouse is obese is 0.9 so the probability and likelihood that it is not obese is 1 minus 0.9 the probability that this Mouse is obese is 0.3 so the probability and likelihood that it is not obese is 1 minus 0.3 the probabilities that these mice are obese are both 0.01 so the probability and the likelihood that they are not obese is 1 minus 0.01 now we can include the individual likelihoods for the mice that are not obese to the equation for the overall likelihood note although it is possible to calculate the likelihood as the product of the individual likelihoods statisticians prefer to calculate the log of the likelihood instead either way works because the squiggle that maximizes the likelihood is the same one that maximizes the log of the likelihood with the log of the likelihood or log-likelihood to those in the know we add the logs of the individual likelihoods instead of multiplying the individual likelihoods thus the log likelihood of the data given the squiggle is negative three point seven seven and this means that the log-likelihood of the original line is negative three point seven seven now we rotate the line in calculate its log likelihood by projecting the data onto it transforming the log odds to probabilities and then calculating the log likelihood and the final value for the log likelihood is negative four point one five so this one is not as good as the first line and we just keep rotating the log odds line and projecting the data onto it and then transforming it to probabilities in calculating the log-likelihood note the algorithm that finds the line with the maximum likelihood is pretty smart each time it rotates the line it does so in a way that increases the log likelihood thus the algorithm can find the optimal fit after a few rotations ultimately we get a line that maximizes the likelihood and that's the one chosen to have the best fit BAM that's all there is to it but just like with linear regression there's more to logistic regression than just fitting a line we want to know if that line represents a useful model and that means we need an r-squared value and a p-value however in logistic regression we have to do that without the usual residuals so we'll learn about r-squared and P values for logistic regression in the next stack west cool hooray we've made it to the end of another exciting stat quest if you liked this stack west and want to see more of them please subscribe and if you want to support stack quest well click the like button below and consider buying one or two of my original songs alright until next time quest on
vN5cNN2-HWE,2018-06-04T14:13:50.000000,Logistic Regression Details Pt1: Coefficients,"If I were in Hawaii I'd be sitting on a beach In the shade of a tree Watching snack quest Hello, I'm Josh Starmer and welcome to StatQuest today We're gonna cover logistic regression and we're gonna dive deep into the details. This is part one of a series of videos I'm gonna do on logistic regression this time we're talking about coefficients This stat quest follows up on logistic regression clearly explained Which provides the big picture of what logistic regression is and how it works In this video. I want to dive deeper into how logistic regression works Specifically we'll talk about the coefficients that are the results of any logistic regression We'll talk about how they are determined and interpreted we'll talk about the coefficients in the context of using a continuous variable like weight to predict obesity and We'll talk about the coefficients in the context of testing if a discrete variable Like whether or not a mutated gene is related to obesity Before we dive into the details Let's do a quick review of some of logistic regressions main ideas to make sure we're all on the same page in This example the y-axis is the probability a mouse is obese It goes from 0 the mouse is not obese to 1 the mouse is obese The dotted line is fit to the data to predict the probability a mouse is obese given its weight If we wait a heavyweight Mouse Then the corresponding point on the line indicates that there is a high probability close to one that it is obese and if we weight a middleweight Mouse Then there's an intermediate probability close to 0.5 that it is obese Lastly a lightweight Mouse has a low probability close to 0 of being obese Ok, those are all the basics we need for this stat quest One last thing before we get started I want to mention that logistic regression is a specific type of generalized a linear model often abbreviated GLM Generalized linear models are a generalization of the concepts and abilities of regular linear models Which we've already talked about in many stat quests That means that if you're familiar with linear models, then you are well on your way to understanding logistic regression We'll start by talking about logistic regression when we use a continuous variable like weight to predict obesity This type of logistic regression is closely related to linear regression a type of linear model So let's do a super quick review of linear regression Shameless self-promotion if you're not already familiar with linear regression Check out the stat quests on linear regression and linear models part one Ok, we start with some data and we fit a line to it and this is the equation for the line It has a y-axis intercept and a slope and We plug in values for wait to get predicted values for size So even though we didn't measure a mouse that weighed 1.5 we can plug 1.5 into the equation and predict that Mouse to have size 1.9 1 And a mouse with weight 1.5 and size 1.9 one would end up on the line at this point Now even though it's silly the equation can predict the size of mice with weight equals zero that's just the y axis intercept zero point eight six and We can even predict the size of mice with negative weights I'm pointing this out because the fact that we are not limiting the equation to a specific domain and range makes it easier to solve and This has a big effect on how logistic regression is done So over on the left side we have the linear regression and on the right side we have the logistic regression with linear regression the values on the y-axis Can in theory be any number? Unfortunately with logistic regression the y axis is confined to probability values between 0 and 1 To solve this problem the y axis in logistic regression is Transformed from the probability of obesity to the log odds of obesity. So just like the y axis and linear regression It can go from negative infinity to positive infinity So we can see what we're doing let's move the logistic regression to the left side Now let's transform the y-axis from a probability of obesity scale to a log odds of obesity scale We do that with the logit function that we talked about in the odds slash log odds stat quest P in this case is the probability of a mouse being obese and Corresponds to a value on the old y axis between 0 and 1 The midpoint on the old y axis corresponds to P equals 0.5 and When we plug P equals 0.5 into the logit formula and do the math We get 0 the center of the new y axis here Is P equals zero point 7 31 on the old y axis if? We plug P equals zero point seven three one into the logit function and do the math we get one on the new y axis if We plug P equals zero point eight eight into the logit function and do the math we get two on the new y axis if We plug P equals zero point nine five into the logit function and do the math We get three on the new y axis Lastly these blue points from the original data are at P equals one If we plug P equals one into the logit function and do the math Well, technically you can't divide by zero. However the log of one divided by zero equals the log of one minus the log of zero and the log of zero is defined as negative infinity and Since something minus negative infinity equals positive infinity this whole thing is equal to positive infinity This means the original samples that were labeled obese are at positive infinity on the new y-axis as a result the original y-axis from 0.5 to 1 is Stretched out from 0 to positive infinity on the new y-axis Similarly 0.5 to 0 on the old y-axis is stretched out from 0 to negative infinity on the new y-axis Ultimately we end up with the log of the odds of obesity on the new y-axis And the new y-axis transforms the squiggly line into a straight line The important thing to know is that even though the graph with the squiggly line is what we associate with logistic regression The coefficients are presented in terms of the log odds graph In the stack quest logistic regression details part 2 fitting a line with maximum likelihood We'll talk more about how this line is fit to the data But for now just take my word for it that this is the best fitting line Just like with linear regression the best fitting line has a y-axis intercept and a slope The coefficients for the line are what you get when you do logistic regression The first coefficient is the y-axis intercept when weight equals zero It means that when weight equals zero the log of the odds of obesity are negative three point four seven six in Other words if you don't weigh anything the odds are against you being obese, duh here's the standard error for the estimated intercept and The Z value is the estimated intercept divided by the standard error In other words, it's the number of standard deviations. The estimated intercept is away from zero on the standard normal curve That means that this is the Wald's test that we talked about in the odds ratio stat quest Since the estimate is less than two standard deviations away from zero. We know it is not statistically significant and This is confirmed by the large p-value the area under the standard normal curve That is further than one point four seven standard deviations away from zero The second coefficient is the slope It means that for every one unit of weight gained the log of the odds of obesity increases by one point eight to five Here's the standard error for the slope Again, the Z value is the number of standard deviations. The estimate is from zero on the standard normal curve and Again, the estimate is less than two standard deviations from zero. So it is not statistically significant, this is no surprise with such a small sample size and This is confirmed with the large p-value Double bam Now we know all about the logistic regression coefficients when we use a continuous variable like weight to predict obesity now let's talk about logistic regression coefficients in the context of testing if a discrete variable like whether or not a mouse has a mutated gene is related to obesity on the left side we have mice that have a normal gene and On the right side. We have mice with a mutated gene Just like before some of the mice are obese in some of the mice are not obese This type of logistic regression is very similar to how a t-test is done using linear models So let's do a quick review of how a t-test is done using linear models Shameless self-promotion If you're not already familiar with how a t-test can be performed using a linear model then check out the stat quest linear models part two t-tests and anova Okay, we start with some data in this case we've measured the size of mice that have a normal gene and the size of mice with a mutated version of that gene Then we fit two lines to the data the first line represents the mean size for the mice with the normal copy of the gene the Second line represents the mean size of the mice with the mutated copy of the gene These two lines come together to form the coefficients in this equation the mean value for size for the mice with the normal copy of the gene goes here and The difference between the mean size of the mice with a mutated gene and the mean size of the mice with the normal gene goes here We then pair this equation with a design majors to predict the size of a mouse Given that it has the normal or mutated version of the gene This is the design matrix for the observed data The first column corresponds to values for b1 and it turns on the first coefficient the mean of the normal mice The second column corresponds to values for b2 and turns the second coefficient the mean of the mutant mice minus the mean of the normal mice off or on Depending on whether it is a zero or a one For example the first row in the design matrix corresponds to a mouse with a normal copy of the gene We predict its size by replacing b1 with 1 in replacing b2 with 0 and then we just do the math and We see that this mouse is associated with the mean of the normal mice This row in the design matrix corresponds to a mouse with the mutated gene We plug in 1 for b1 and Plug in 1 for b2 and we see that the mean of the mice with the normal gene plus the difference between the two means Associates this mouse with the mean for the mice with a mutated gene When we do a t-test this way we are basically testing to see if this coefficient The mean of the mutant mice minus the mean of the normal mice is equal to zero But you already know all this t-test stuff what you really want to know is how it applies to logistic regression The first thing we do is transform the y-axis from the probability of being obese To the log of the odds of obesity Now we fit two lines to the data For the first line we take the normal gene data and Use it to calculate the log of the odds of obesity for mice with the normal gene Thus the first line represents the log of the odds of obesity for the mice with the normal gene Let's call this the log of the odds for gene normal We then calculate the log of the odds of obesity for the mice with the mutated gene Thus the second line represents the log of the odds of obesity for a mouse with the mutant gene Let's call this the log of the odds gene mutated These two lines come together to form the coefficients in this equation the log of the odds gene normal goes here in the difference between the log of the odds gene mutated in the log of the odds gene normal goes here and since subtracting one log from another Can be converted into division this term is a log of the odds ratio It tells us on a log scale how much having the mutated gene Increases or decreases the odds of a mouse being obese Okay, now that we know what the equation is all about let's substitute in the numbers the log of the odds for gene normal is just the log of 2 divided by 9 and The log of the odds for gene mutated is just the log of 7/3 Now we just do the math and that gives us these coefficients and Those are what you get when you do logistic regression the intercept is the log of the odds for gene normal and The gene mutant term is the log of the odds ratio That tells you on a log scale how much having the mutated gene increases or decreases the odds of being obese and here are the standard errors for those estimated coefficients and Here are the z values aka the Wald's test values that tell you how many standard deviations The estimated coefficients are away from 0 on a standard normal curve The Z value for the intercept negative 1.9 tells us that the estimated value for the intercept negative 1.5 is less than 2 standard deviations from 0 and thus not significantly different from 0 and this is confirmed by a p-value greater than 0.05 The z-value for gene mutant the log odds ratio that describes how having the mutated gene Increases the odds of being obese is greater than two Suggesting it is statistically significant. And this is confirmed by a p-value less than 0.05 Double bam now we have seen how some of the linear model concepts for regression apply to logistic regression and We have seen how some of the linear model concepts for T tests apply to logistic regression in short in terms of the coefficients Logistic regression is the exact same as good old linear models, except the coefficients are in terms of the log odds This means that all those fancy things we can do with linear models like multiple regression and ANOVA Can be done using logistic regression all we have to remember is that the scale for the coefficients is log odds Triple bam hooray We've made it to the end of another exciting stat quest if you like this stat quest and want to see more of them Please subscribe and if you want to support stat quest well Click the like button below and consider buying one or two of my original songs Alright until next time quest on"
4b5d3muPQmA,2018-05-23T20:18:55.000000,StatQuest: K-means clustering,statcast [Music] stat quest stat quest stat quest hello I'm Josh stormer and welcome to stat quest today we're going to be talking about k-means clustering we're gonna learn how to cluster samples that can be put on a line on an XY graph and even on a heat map and lastly we'll also talk about how to pick the best value for K imagine you had some data that you could plot on a line and you knew you needed to put it into three clusters maybe they are measurements from three different types of tumors or other cell types in this case the data make three relatively obvious clusters but rather than rely on our eye let's see if we can get a computer to identify the same three clusters to do this we'll use k-means clustering we'll start with raw data that we haven't yet clustered step one select the number of clusters you want to identify in your data this is the K in k-means clustering in this case we'll select K equals three that is to say we want to identify three clusters there is a fancier way to select a value for K but we'll talk about that later step two randomly select three distinct data points these are the initial clusters step 3 measure the distance between the first point and the three initial clusters this is the distance from the first point to the blue cluster this is the distance from the first point to the green cluster and this is the distance from the first point to the orange cluster well it's kind of yellow but we'll just call it orange for now step 4 assign the first point to the nearest cluster in this case the nearest cluster is the blue cluster now we do the same thing for the next point we measure the distances and then assign the point to the nearest cluster now we figure out which cluster the third point belongs to we measure the distances and then assign the point to the nearest cluster the rest of these points are closest to the orange cluster so they'll go in that one two now that all the points are in clusters we go on to step 5 calculate the mean of each cluster then we repeat what we just did measure and cluster using the mean values since the clustering did not change at all during the last iteration were done BAM the k-means clustering is pretty terrible compared to what we did by eye we can assess the quality of the clustering by adding up the variation within each cluster here's the total variation within the clusters since k-means clustering can't see the best clustering it's only option is to keep track of these clusters and their total variance and do the whole thing over again with different starting points so here we are again back at the beginning k-means clustering picks three initial clusters and then clusters all the remaining points calculates the mean of each cluster and then re clusters based on the new means it repeats until the cluster is no longer change bit bit bit of bit of boop boop boop now that the data are clustered we sum the variation within each cluster and then we do it all again at this point k-means clustering knows that the second clustering is the best clustering so far but it doesn't know if it's the best overall so it will do a few more clusters it does as many as you tell it to do and then come back and return that one if it is still the best question how do you figure out what value to use for K with this data it's obvious that we should set K to three but other times it is not so clear one way to decide is to just try different values for K we'll start with k equals 1 k equals 1 is the worst case scenario we can quantify its badness with the total variation now try K equals 2 K equals 2 is better and we can quantify how much better by comparing the total variation within the two clusters to K equals 1 now try K equals 3 k equals 3 is even better we can quantify how much better by comparing the total variation within the three clusters to k equals 2 now try k equals 4 the total variation within each cluster is less than when K equals 3 each time we add a new cluster the total variation within each cluster is smaller than before and when there is only one point per cluster the variation equals 0 however if we plot the reduction in variance per value for K there is a huge reduction in variation with K equals three but after that the variation doesn't go down as quickly this is called an elbow plot and you can pick K by finding the elbow in the plot question how is k-means clustering different from hierarchical clustering k-means clustering specifically tries to put the data into the number of clusters you tell it to hierarchical clustering just tells you pairwise what two things are most similar question what if our data isn't plotted on a number line just like before you pick three random points and we use the Euclidean distance in two dimensions the Euclidean distance is the same thing as the Pythagorean theorem then just like before we assign the point to the nearest cluster and just like before we then calculate the center of each cluster and re cluster BAM although this looks good the computer doesn't know that until it does the clustering a few more times question what if my data is a heatmap well if we just have two samples we can rename them x and y and we can then plot the data in an XY graph then we can cluster just like before note we don't actually need to plot the data in order to cluster it we just need to calculate the distances between things when we have two samples or two axes the Euclidean distance is the square root of x squared plus y squared when we have three samples or three axes the Euclidean distance is the square root of x squared plus y squared plus Z squared and when we have four samples or four axes the Euclidean distance is the square root of x squared plus y squared plus Z squared plus a squared etc etc etc hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stack quest well click the like button down below and consider buying one or two of my original songs alright tune in next time for another exciting stat quest
ARfXDSkQf1Y,2018-05-07T17:30:36.000000,"Odds and Log(Odds), Clearly Explained!!!",the stat quest is odd so I'm gonna talk about the odds and the log of the odds stacked quest hello I'm Josh stommer and welcome to stack quest today we're gonna be talking about odds and log odds and they're gonna be clearly explained the odds are in favor that you're already familiar with odds for example you might say that the odds in favor of my team winning the game are 1 to 4 visually we have 5 games total one of which my team will win and four of which my team will lose so the odds are one to four alternatively we can write this as a fraction visually we have one game my team wins divided by the four games that my team loses if we do the math we will see that the odds are 0.25 that my team will win the game here's another example you might say that the odds in favor of my team winning the game are five to three visually we have eight games total five of which my team will win and three of which my team will lose so the odds are five to three alternatively we can write this as a fraction 5/3 visually we have the five games my team wins divided by the three games my team loses if we do the math we see that the odds are one point seven that my team will win the game note ads are not probabilities the odds are the ratio of something happening ie my team winning to something not happening ie my team not winning probability is the ratio of something happening I eat my team winning to everything that could happen ie my team winning and losing in the previous example the odds in favor of my team winning the game are five to three however the probability of my team winning is the number of games they win five divided by the total number of games they play eight here's the math for the odds we have the ratio of 5 to 3 and for the probability we have the ratio of 5 to 8 thus we see that the odds in favor of my team winning are different from the probability of my team winning now that we know that odds are different from probabilities let's talk about how odds can be calculated from probabilities in the last example we saw that the odds of winning are 1 point 7 and the probability of winning is 0.625 we can also calculate the probability of losing the probability of losing is 0.375 note we could also calculate the probability of losing as 1 minus the probability of winning that equals 1 minus 5/8 and that gives us 8 divided by 8 minus 5/8 and ultimately we get 3/8 this is equal to 0.375 so either way we get the same probability now let's take the ratio of the probability of winning to the probability of losing alternatively we can put 1 minus the probability of winning into the denominator either way we get the same ratio 5/8 divided by 3/8 the eights cancel out since they scale the numerator and the denominator by the exact same amount thus the ratio of the probability ends up being the same thing as the ratio of the raw counts and so either way we get the same odds 1.7 I mentioned this because about 50% of the time you see odds calculated from counts and the other 50% of the time you will see odds calculated from probabilities either way you get the same results note out there in the wild world of statistics you will often see this formula simplified to this where P is the probability of winning BAM [Music] now that we know what the odds are let's talk about the log of the odds let's go back to the original example in this example we calculated the odds of winning as one to four or 0.25 if my team was worse the odds of winning could be one to eight or 0.125 and if my team was terrible the odds of winning could be one to sixteen or 0.063 and lastly if my team was the worst the odds of winning could be one to 32 or 0.03 one we can see that the worse my team is the odds of winning get closer and closer to zero in other words if the odds are against my team winning then they will be between zero and one now if my team was good then the odds might be four to three or one point three in favor of my team winning and if my team was better the odds might be eight to three or two point seven in favor of winning and if my team was really good the odds might be thirty two to three or ten point seven in favor of winning we can see that the better my team is the odds of winning started one and just go up and up in other words if the odds are for my team winning then they will be between 1 and infinity another way to look at this is with a number line the odds of my team losing go from zero to one and the odds of my team winning go from one to infinity and beyond the asymmetry makes it difficult to compare the odds for or against my team winning for example if the odds are against one to six then the odds are 1/6 which equals zero point 1 7 but if the odds are in favor 6 to 1 then the odds are 6 divided by 1 which equals 6 the magnitude of these odds looks way smaller than these odds taking the log of the odds solves this problem by making everything symmetrical for example if the odds are against one to six then the log of the odds or the log of one divided by six which equals the log of 0.1 seven which equals negative one point seven nine and if the odds are in favor six to one then the log of the odds are the log of 6 divided by 1 which equals the log of 6 which equals one point seven nine using the log function the distance from the origin or zero is the same for one to six or six to one odds double bail okay now that we know the main idea about the log of the odds let's get into some details earlier we saw that odds can be calculated from counts and we saw that the same odds could be calculated from probabilities and that means we can calculate the log of the odds with counts or probabilities either way we'll get the same value note the log of the ratio of the probabilities is called the logit function and it forms the basis for logistic regression I mention it because if you do logistic regression you'll see it a whole lot it's no big deal ok I get it odds are just the ratio of something happening to something not happening and the log of the odds is just the duh log of the odds what's the big deal to show you what the big deal is all about if I pick pairs of random numbers that add up to 100 for example and use them to calculate the log of the odds and draw a histogram the histogram is in the shape of a normal distribution this makes the log of the odds useful for solving certain statistics problems specifically ones where we are trying to determine probabilities about win or lose or yes or no or true or false type situations triple bam in summary the odds are just the ratio of something happening I Eve my team winning to something not happening ie my team not winning and the log of the odds is just the log of the odds it's no big deal the log of the odds makes things symmetrical easier to interpret and easier for fancy statistics one last thing before we go even though the odds is a ratio it's different from an odds ratio but don't panic we'll talk about the odds ratio in the next stat quest hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you'd like to support stat quest well liked the video and consider buying one or two of my original songs alright until next time quest on
fSytzGwwBVw,2018-04-24T18:37:54.000000,Machine Learning Fundamentals: Cross Validation,"StatQuest Check it out talking about Machine-learning. Yeah StatQuest Check it out Talking about cross-validation StatQuest Hello, I'm Josh stormer and welcome to StatQuest today we're going to talk about cross validation and it's gonna be clearly explained Okay, let's start with some data We want to use the variables chest pain good blood circulation Etc To predict if someone has heart disease Then when a new patient shows up we can measure these variables and Predict if they have heart disease or not However, first we have to decide which machine learning method would be best we could use logistic regression or K nearest neighbors Or support vector machines and Many more machine learning methods. How do we decide which one to use? Cross-validation allows us to compare different machine learning methods and get a sense of how well they will work in practice Imagine that this blue column represented all of the data that we have collected about people with and without heart disease We need to do two things with this data One we need to estimate the parameters for the machine learning methods in In other words to use logistic regression we have to use some of the data to estimate the shape of this curve in machine learning lingo Estimating parameters is called training the algorithm The second thing we need to do with this data is evaluate how well the machine learning methods work in? Other words we need to find out if this curve will do a good job categorizing new data in In machine learning lingo Evaluating a method is called testing the algorithm Thus using machine learning lingo we need the data to one train the machine learning methods and to test the machine learning methods a A terrible approach would be to use all the data to estimate the parameters ie to train the algorithm Because then we wouldn't have any data left to test the method Reusing the same data for both training and Testing is a bad idea because we need to know how the method will work on data. It wasn't trained on a Slightly better idea would be to use the first seventy-five percent of the data for training and the last 25% of the data for testing   We could then compare methods by seeing how well each one categorized the test data But how do we know that using the first? Seventy-five percent of the data for training in the last 25% of the data for testing is the best way to divide up the data What if we use the first 25% of the data for testing Or what about one of these middle blocks? Rather than worry too much about which block would be best for testing cross-validation uses them all one at a time and summarizes the results at the end     For example cross-validation would start by using the first three blocks to train the method and then use the last block to test the method and Then it keeps track of how well the method did with the test data then it uses this combination of blocks to train the method and this block is used for testing and Then it keeps track of how well the method did with the test data, etc Etc, etc   in the end every block of data is used for testing and we can compare methods by seeing how well they performed in This case since the support vector machine did the best job classifying the test data sets. We'll use it BAM!!! Note: in this example, we divided the data into 4 blocks. This is called four-fold cross validation However, the number of blocks is arbitrary In an extreme case we could call each individual patient (or sample) a block This is called ""Leave One Out Cross Validation"" Each sample is tested individually That said in practice it is very common to divide the data into ten blocks. This is called 10-fold cross-validation Double BAM!!! One last note before we're done Say like we wanted to use a method that involved a tuning parameter a parameter that isn't estimated but is just sort of guessed For example Ridge regression has a tuning parameter Then we could use 10-fold cross validation to help find the best value for that tuning parameter Tiny Bam! Hooray we've made it to the end of another exciting StatQuest if you like this StatQuest and want to see more please subscribe And if you want to support StatQuest well Please click the like button down below and consider buying one of my original songs Alright until next time quest on"
nkWGmaYRues,2018-04-16T20:12:28.000000,StatQuest: A gentle introduction to ChIP-Seq,come inclusive let me whisper something in you about gypsy yeah it's that quest yeah hello I'm Josh starrin welcome to stat quest today we have a gentle introduction to chip seek note this stack quest is based on the gentle introduction to RNA seek so watch that first unless you're already totally down with RNA seek oK we've got a bunch of cells inside each cell there's a nucleus and inside each nucleus there are a bunch of chromosomes let's focus on the chromosomes specifically let's focus on these chromosomes chromosomes are made out of chromatin chromatin is made out of DNA plus histones a type of protein plus other proteins that we'll talk about some other day they're not that important for understanding chips seek DNA wraps around the histones to package DNA in the packaging can regulate gene transcription depending on how they are modified histones can activate or repress genes lots more can be said about chromatin and how it's packaged but that's for another day today we just need to know that chromatin is essentially DNA wrapped around histones to simplify things let's just use this big blue line to represent chromatin from here on out but remembering that it's made of DNA and histone proteins and let these big brown arrows represent genes and let this green circle represent a histone that allows transcription and let this red stop sign represent a histone that represses transcription in a cell all kinds of proteins can bind to DNA this mustard-colored thing might promote gene transcription of this gene and this pinkish colored thing might repress this gene and who knows what this green thing is doing we can use chips seek to find out chips seek stands for chromatin immunoprecipitation combined with high-throughput sequencing it identifies the locations in the genome bound by proteins that's the most important thing in this stat quest so I'll say it again in bold chip seek identifies the locations in the genome bound by proteins for example say like we wanted to identify all the regions in the genome bound by the green thing the first thing we do is use formaldehyde or something like it to glue all the proteins bound to the DNA together with the DNA this means that all of the DNA bound proteins including the ones were not interested in are glued to the DNA the next thing we do is cut the DNA up into small approximately 300 base pair fragments then we isolate the protein were interested in in this case we're interested in the green thing using an antibody the black star represents an antibody that is attached to a bead then we isolate the proteins bound by the antibody and wash everything else away then we reverse the formaldehyde glue by warming up everything then we isolate the DNA by washing away the proteins including the histones now that we see how to isolate DNA that is bound by a particular protein let's take a step back so far the example has focused on just these chromosomes but the process glue proteins to DNA cut up DNA bindings of interest with antibodies isolate antibodies unglue and wash away proteins applies to all the chromosomes in the cell and it is usually applied to a pool of six million cells give or take a few so we end up with a lot of DNA fragments from a lot of cells then just like with RNA seek we prepare a sequencing library by adding sequencing adapters to both ends of the DNA fragments then just like RNA seek we PCR amplify the library check the library concentration sequence filter out garbage reads and then align the high quality reads to a genome that is to say if this is the genome the first read might come from here a location on chromosome 2 the second read might come from here a location on chromosome 1 the third read might come from here another position on chromosome 1 etc etc etc ultimately we get a long list of genomic coordinates for all the reads usually between 50 and 100 million reads and we can use those reads to create a genome browser track these are genes and chromosomes sessions in the mouse genome mmm tend to be exact this is the track that we created for our chip seek reads a lot of reads mapped to this region and relatively few reads mapped to these other regions this track was made from a control experiment the control track was made by taking some of the input chromatin from the original chip seek experiment and without using an antibody to enrich for any particular protein on gluing all the proteins and washing them off then sequencing aligning and converting into a track in summary the control track uses some of the same input chromatin for the chip seek experiment but doesn't try to enrich for any particular protein binding we use the control track to verify that a high concentration of reeds in the chip seek track is due to a protein binding there and not because a lot of reeds map to a repetitive region statistically significant Peaks are usually represented on genome tracks by rectangular bars we could then compare Peaks for the same protein in different cell types like long verses kidney or if we didn't know the specific DNA sequence that this green thing bound to we could guess that it is a motif found in all of the Peaks here's a motif found within the peaks that indicate where the green thing bound the large letters are more frequently associated with the green thing than the little letters we can also try to determine the functional role of the green thing by looking at where it binds relative to the genes here we see that the green thing binds near the start of a gene so it might regulate that gene in some way anyway those are some of the things you can do with chip seek in summary chip seek identifies the locations in the genome bound by proteins BAM hooray we made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stack quest please click the like button and consider buying one of my original songs okay until next time quest on
_ft2yqArm-I,2018-04-11T12:30:00.000000,"StatQuest: 10,000 Subscriber Milestone","hello I'm Josh stormer and welcome to stat quest today we're going to talk about the 10,000 subscriber milestone today my channel picked up its 10,000 subscriber and I just want to say thank you we've come a long way we've talked about principle component analysis linear discriminant analysis RNA sequencing maximum likelihood quantiles regression and lots more stuff check out the growth in the past year this channel is really going somewhere except on Christmas and New Year's but even people are watching it more often on weekends crazy and the best is yet to come my to-do list has over a hundred items on it including neural networks lasso and rate regression boosting algorithms factor analysis ISO maps and tons more stuff so BAM and hooray we've made it to the end of another exciting stat quest tune in next time and the time after that and the time after that we've got a ton of cool stuff we're going to be covering and I'm really excited about it quest are"
oRvgq966yZg,2018-04-09T14:13:54.000000,StatQuest: PCA - Practical Tips,[Music] I've got a few PCA tips just for you hello I'm Josh starmer and welcome to St Quest today we're going to be talking about PCA and I'm going to give you a few practical tips specifically we're going to talk about one scaling your data two centering your data and three how how many principal components you should expect to get note this is a follow-up to my video principal component analysis PCA step by step so make sure you've seen that video first or at least already know how PCA Works practical tip number one make sure the variables are on the same scale and if not scale them here we have math and reading scores for a bunch of students let's plot the data math scores are from 0 to 100 and they are spread out between 0 and 100 in the graph in contrast reading scores are only from 0 to 10 and they are all crammed between 0 and 10 on the graph if we centered the data and did PCA on it we'd get this recipe for pc1 to make pc1 Mak 0.99 Parts math with 0.1 part reading it suggests that math is 10 times better than reading for capturing variation in aptitude but this is only because the math scores are on a scale 10 times larger than the scale for reading scores if we divided the math scores by 10 and replotted and then centered the data and did PCA on it we'd get this recipe for pc1 to make pc1 mix 0.77 Parts math with 0.77 parts reading this suggests that reading and math are equally good at capturing variation in aptitude the moral of this story is that you need to make sure the scales for each variable in this case math and reading scores are roughly equivalent otherwise you will be bias towards one of them the standard practice is to divide each variable by its standard deviation thus if a variable has a wide range it will have a large standard deviation and dividing by it will scale the values a lot if a variable has a narrow range it will have a small standard deviation and scaling will be minimal practical tip number two make sure your data is centered the very first step centering the data is an important one but not every PCA program does this by default if you do PCA using SVD without centering it will still try to fit a line to the data that goes through the origin and your PCS will not be what you expect so double check that the PCA program you are using centers the data or Center it yourself practical tip number three how many principal components can you expect to find in the first example we had math and reading scores from a bunch of students we then plotted the data on a two-dimensional graph we then centered the data and then we found the best fitting line that goes through the origin this is pc1 the second PC PC2 was the line perpend I to pc1 then we just moved on without asking if there were any more principal [Music] components so now let's ask the question is there a third principal component to find a third PC we'd need to find a line perpendicular to both pc1 and PC2 and in two dimensions that's not possible so the answer is no to see why it's not possible to draw draw a line perpendicular to both pc1 and PC2 we could add a third line and rotate it until it is perpendicular to pc1 and PC2 this position is perpendicular to PC2 but not pc1 so we keep rotating this position is perpendicular to pc1 but not PC2 thus when we measure two things math and reading in this case per sample I.E per student at most we can have two principal components now imagine that math and reading scores are 100% correlated then we centered the data then we found the best fitting line principal component one technically speaking we could find a line perpendicular to pc1 but if we projected the data onto this line it would all just go to the origin this means that the igen value for the new line the sum of squares of the distances between the points projected onto the line and the origin would be zero this means that pc1 accounts for 100% of the variation and the new line accounts for 0% thus for all practical purposes pc1 is the only one that matters likewise if we only had two students and then we centered the data and then we found the best fitting line pc1 then just like before we could find a line that is perpendicular to pc1 but the igen value would be zero so just like before there's really only one principal component worth talking about another perhaps simpler way to think about this is that two points Define a line so these two points can only Define a single line in order to define a plane something with two axes we need at least three points now imagine we had two students and three test scores math reading and gymnastics we can Center the data and just like before since we only have two points and two points Define a line we only have one PC now imagine we had three students and we remember that three points Define a plane and a plane has only two axes then we would predict that there would only be two principal components so we can Center the data find the line through the or origin that fits best pc1 then find the line perpendicular to pc1 that fits best and that's it since a third line would have an igen value equal to zero there's no PC3 in summary technically there is a PC for each variable in the data set however if there are fewer samples than variables then the number of samples puts an upper bound on the number of PCS with igen values greater than zero hooray we've made it to the end of another exciting stack Quest if you like this stat Quest and want to see more please subscribe and if you'd like to support stat Quest well consider buying one or two of my original songs okay until next time Quest on
FgakZw6K1QQ,2018-04-02T20:06:10.000000,"StatQuest: Principal Component Analysis (PCA), Step-by-Step","StatQuest breaks it down into bite-sized pieces, hooray! Hello, I'm Josh Starmer and welcome to StatQuest. In this StatQuest we're going to go through Principal Component Analysis (PCA) one step at a time using Singular Value Decomposition (SVD). You'll learn about what PCA does, how it does it, and how to use it to get deeper insight into your data. Let's start with a simple data set. We've measured the transcription of two genes, Gene 1 and Gene 2,in6different mice. Note: If you're not into mice and genes, think of the mice as individual samples and the genes as variables that we measure for each sample. For example, the samples could be students in high school and the variables could be test scores in math and reading, or the samples could be businesses and the variables could be market capitalization and the number of employees. Okay, now we're back to mice and genes, because I'm a geneticist and I work in a genetics department. If we only measure one gene we can plot the data on a number line. Mice 1, 2, and 3 have relatively high values and mice 4, 5, and 6 have relatively low values. Even though it's a simple graph, it shows us that mice 1, 2, and 3 are more similar to each other than they are to mice 4, 5, and 6. If we measured 2 genes, then we can plot the data on a two-dimensional X-Y graph. Gene 1 is the x-axis and spans one of the two dimensions in this graph. Gene 2 is the y-axis and spans the other dimension. We can see that mice 1, 2, and 3 cluster on the right side and mice 4, 5, and 6 cluster on the lower left hand side. If we measured three genes, we would add another axis to the graph and make it look 3D, i.e.3-dimensional. The smaller dots have larger values for Gene 3 and are further away. The larger dots have smaller values for Gene 3 and are closer. If we measured 4 genes, however, we can no longer plot the data. 4  genes require 4 dimensions.  So we're going to talk about how PCA can take 4 or more gene measurements, and thus 4 or more dimensions of data, and make a 2-dimensional PCA plot. This plot will show us that similar mice cluster together. We'll also talk about how PCA can tell us which gene, or variable, is the most valuable for clustering the data. For example PCA might tell us that Gene 3 is responsible for separating samples along the x-axis. Lastly, we'll talk about how PCA can tell us how accurate the 2D graph is. To understand what PCA does and how it works, let's go back to the data set that only had 2 genes. We'll start by plotting the data. Then we'll calculate the average measurement for Gene 1, and the average measurement for Gene 2. With the average values, we can calculate the center of the data. From this point on, we'll focus on what happens in the graph -  we no longer need the original data. Now, we'll shift the data so that the center is on top of the origin in the graph. Note: Shifting the data did not change how the data points are positioned relative to each other. This point is still the highest one, and this is still the rightmost point, etc. Now that the data are centered on the origin, we can try to fit a line to it. To do this, we start by drawing a random line that goes through the origin. Then we rotate the line until it fits the data as well as it can, given that it has to go through the origin. Ultimately, this line fits best. But I'm getting ahead of myself, first we need to talk about how PCA decides if a fit is good or not. So, let's go back to the original random line that goes through the origin. To quantify how good this line fits the data, PCA projects the data onto it and then it can either measure the distances from the data to the line and try to find the line that minimizes those distances, or it can try to find the line that maximizes the distances from the projected points to the origin. If those options don't seem equivalent to you, we can build intuition by looking at how these distances shrink when the line fits better, while these distances get larger when the line fits better. Now, to understand what is going on in a mathematical way, let's just consider one data point. This point is fixed and so is its distance from the origin. In other words, the distance from the point to the origin doesn't change when the red dotted line rotates. When we project the point onto the line, we get a right angle between the black dotted line and the red dotted line.That means that if we label the sides like this: A, B, and C, then we can use the Pythagorean theorem to show how B and C are inversely related. Since A, and thus A squared, doesn't change if B gets bigger then C must get smaller. Likewise, if C gets bigger then B must get smaller. Thus, PCA can either minimize the distance to the line, or maximize the distance from the projected point to the origin. The reason I'm making such a fuss about this is that, intuitively, it makes sense to minimize B and the distance from the point to the line, but it's actually easier to calculate C, the distance from the projected point to the origin, so PCA finds the best fitting line by maximizing the sum of the squared distances from the projected points to the origin. So, for this line, PCA projects the data onto it and then measures the distance from this point to the origin, let's call it D1. Note: I'm going to keep track of the distances we measure up here. And then PCA measures the distance from this point to the origin, we'll call that D2. Then it measures D3, D4, D5, and D6. Here are all six distances that we measured. The next thing we do is square all of them. The distances are squared so that negative values don't cancel out positive values. Then we sum up all these squared distances, and that equals the sum of the squared distances. For short we'll call this SS distances, for sum of squared distances. Now we rotate the line, project the data onto the line, and then sum up the squared distances from the projected points to the origin. And we repeat until we end up with the line with the largest sum of squared distances between the projected points and the origin. Ultimately, we end up with this line. It has the largest sum of squared distances. This line is called Principal Component 1, or PC1 for short. PC1 has a slope of 0.25. In other words, for every 4 units that we go out along the Gene 1 axis, we go up 1 unit along the Gene 2 axis. That means that the data are mostly spread out along the Gene 1 axis, and only a little bit spread out along the Gene 2 axis. One way to think about PC1 is in terms of a cocktail recipe. To make PC1 mix four parts Gene 1 with one part Gene 2. Pour over ice and serve! The ratio of Gene 1 to Gene 2 tells you that Gene 1 is more important when it comes to describing how the data are spread out. Oh no, terminology alert! Mathematicians call this cocktail recipe a linear combination of Genes 1 and 2. I mention this because when someone says PC1 is a linear combination of variables, this is what they're talking about.  It's no big deal. The recipe for PC1, going over 4 and up 1 gets us to this point. We can solve for the length of the red line using the Pythagorean theorem, the old A squared equals B squared plus C squared. Plugging in the numbers gives us A equals 4.12. So the length of the red line is 4.12. When you do PCA with SVD, the recipe for PC1 is scaled so that this length equals1. All we have to do to scale the triangle so that the red line is1unit long is to divide each side by 4.12. For those of you keeping score, here's the math worked out that shows that all we need to do is divide all3sides by 4.12. Here are the scaled values. The new values change our recipe, but the ratio is the same. We still use four times as much Gene 1 as Gene 2. So now we are back to looking at the data, the best fitting line, and the unit vector that we just calculated. Oh no, another terminology alert! This 1 unit long vector, consisting of 0.97 parts Gene 1 and 0.242 parts Gene 2, is called the Singular Vector, or the Eigenvector for PC1, and the proportions of each gene are called loading scores. Also while I'm at it, PCA calls the average of the sums of the squared distances for the best fit line the Eigenvalue for PC1. And the square root of the sums of the squared distances is called the Singular Value for PC1. Bam! That's a lot of terminology. Now that we've got PC1 all figured out let's work on PC2. Because this is only a two-dimensional graph, PC2 is simply the line through the origin that is perpendicular to PC1 without any further optimization that has to be done. And this means that the recipe for PC2 is -1 parts Gene 1 to 4 parts Gene 2. If we scale everything so that we get a unit vector, the recipe is -0.242 parts Gene 1 and 0.97 parts Gene 2. This is the singular vector for PC2 or the eigenvector for PC2. These are the loading scores for PC2, they tell us that, in terms of how the values are projected onto PC2, Gene 2 is 4 times as important as Gene 1. Lastly the eigenvalue for PC2 is the average of the sum of the squares of the distances between the projected points and the origin. Hooray! We've worked out PC1 and PC2! To draw the final PCA plot, we simply rotate everything so that PC1 is horizontal. Then we use the projected points to find where the samples go in the PCA plot. For example, these projected points correspond to sample 6, so sample 6 goes here. Sample 2 goes here. And Sample 1 goes here. Etc. Double bam! That's how PCA is done using singular value decomposition. Okay, one last thing before we dive into a slightly more complicated example. Remember the eigenvalues? We got those by projecting the data onto the principal components, measuring the distances to the origin, then squaring and adding them together. Well, if you're familiar with the equation for variation, you will notice that eigenvalues are just measures of variation. For the sake of this example imagine that the variation for PC1 equals 15 and the variation for PC2 equals 3. That means that the total variation around both PCS is 15 plus 3 equals 18. And that means PC1 accounts for 15 divided by 18 equals 0.83 or 83% of the total variation around the PCs. PC2 accounts for 3 divided by 18 equals 17% of the total variation around the PCs. Oh no another terminology alert! A scree plot is a graphical representation of the percentages of variation that each PC accounts for. We'll talk more about scree plots later. Bam. Okay, now let's quickly go through a slightly more complicated example. PCA with 3 variables, in this case that means 3 genes, is pretty much the same as 2 variables. You center the data. You then find the best fitting line that goes through the origin. Just like before, the best fitting line is PC1. But the recipe for PC1 now has 3 ingredients. In this case Gene 3 is the most important ingredient for PC1. You then find PC2, the next best fitting line given that it goes through the origin and is perpendicular to PC1. Here's the recipe for PC2. In this case Gene 1 is the most important ingredient for PC2. Lastly, we find PC3, the best fitting line that goes through the origin and is perpendicular PC1 and PC2. If we had more genes, we just keep on finding more and more principal components by adding perpendicular lines and rotating them. In theory, there is 1 per gene or variable, but in practice the number of PCs is either the number of variables or the number of samples, whichever is smaller. If this is confusing, don't sweat it. It's not super important and I'm going to make a separate video on this topic in the next week. Once you have all the principal components figured out you can use the eigenvalues, i.e. the sums of squares of the distances, to determine the proportion of variation that each PC accounts for. In this case, PC1 accounts for 79% of the variation, PC2 accounts for 15% of the variation and PC3 accounts for 6% of the variation. Here's the scree plot. PC1 and PC2 account for the vast majority of the variation. That means that a 2D graph, using just PC1 and PC2, would be a good approximation of this 3D graph, since it would account for 94% of the variation in the data. To convert the 3D graph into a two-dimensional PCA graph, we just strip away everything but the data and PC1 and PC2, then project the samples onto PC1 and PC2. Then we rotate so that PC1 is horizontal and PC2 is vertical. This just makes it easier to look at. Since these projected points correspond to sample 4, this is where sample 4 goes in our new PCA plot. etc. etc. etc. Double bam! To review, we started with an awkward 3D graph that was kind of hard to read, then we calculated the principal components, then, with the eigenvalues for PC1 and PC2, we determined that a 2D graph would still be very informative. Lastly, we used PC1 and PC2 to draw a two-dimensional graph with the data. If we measured 4 genes per mouse, we would not be able to draw a 4-dimensional graph of the data, but that doesn't stop us from doing the PCA math, which doesn't care if we can draw a picture of it or not, and looking at the scree plot. In this case, PC1 and PC2 account for 90% of the variation, so we can just use those to draw a 2-dimensional PCA graph. So we project the samples onto the first 2 PCs. These 2 projected points correspond to sample 2, so sample 2 goes here. Bam! Note, if the scree plot looked like this, where PC3 and PC4 account for a substantial amount of variation, then just using the first two PCs would not create a very accurate representation of the data. However, even a noisy PCA plot like this can be used to identify clusters of data. These samples are still more similar to each other than they are to the other samples. Little bam. Hooray! We've made it to the end of another exciting StatQuest. If you liked this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, please consider buying one or two of my original songs, the link to my bandcamp page is in the lower right corner and in the description below. All right until next time quest on!"
pYxNSUDSFH4,2018-03-09T19:38:43.000000,"In Statistics, Probability is not Likelihood.",stet quest makes me feel so happy so very very very very very very very happy stat quest hello I'm Josh Dahmer and welcome to stat quest today we're gonna be talking about the difference between probability and likelihood these are two closely related concepts that are very easy to get confused even I mix them up from time to time so enough of this jibber-jabber let's get down to it for me the easiest way to understand the difference between probability and likelihood is to just see it in pictures so let's start by looking at probability with respect to a normal distribution keeping in mind that this concept applies to all continuous distributions in this case let's imagine that this is a distribution of Mouse weights it has a mean of 32 grams and a standard deviation of 2.5 on the low end we have 24 grams and on the high end we have 40 grams the probability that we will weigh a randomly selected Mouse between 32 and 34 grams is the area under the curve between 32 and 34 grams in this case the area under the curve equals zero point 2 9 meaning there's a 29% chance a randomly selected Mouse will weigh between 32 and 34 grams mathematically we say this with the following notation the probability of weighing a mouse between 32 and 34 grams given the mean of the distribution is 32 and the standard deviation is 2.5 and all this equals zero point two nine this is the part of the equation we change if we are interested in different Mouse weights for example if we wanted to know the probability of a mouse weighing more than 34 grams we would change the bit on the left side to reflect this the right side which defines the shape and location of the distribution stays the same so when we talk about probabilities we are talking about a distribution that's described by the right side of this equation and the area under the curve that is described on the left side using the same distribution we can change the left side to get a new probability BAM now that we have probability worked out let's talk about likelihood to talk about likelihood you assume that you have already weighed your mouse or mice if you have weighed more than one so here's our mouse it weighs 34 grams the likelihood of weighing a 34 gram Mouse is this point on the curve and that value is 0.12 mathematically we say this with the following notation the likelihood of a distribution with mean equals 32 and the standard deviation equals 2.5 given we weighed a 34 gram Mouse and all that equals 0.12 if we shifted the distribution over so that the mean was 34 grams the new likelihood would be zero point two one so with likelihoods the measurements on the right side are fixed and we modify the shape and location of the distribution with the left side double bam in summary probabilities are the areas under a fixed distribution and mathematically we have the probability of data given a distribution likelihoods are the y-axis values for fixed data points with distributions that can be moved mathematically this is written as the likelihood of a distribution given data if you want to see the actual equations check out the stat quest that derives the maximum likelihood estimator for the exponential distribution hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more of them please subscribe and if you want to support stat quest well consider buying one or two of my original songs the link to my Bandcamp page is down below in the comments all right until next time quest arm
yIYKR4sgzI8,2018-03-05T15:17:39.000000,StatQuest: Logistic Regression,"If you can fit a line you can fit a squiggle if you can, make me laugh you can, make me giggle stat quest Hello, i'm josh stormer and welcome to stat quest today we're going to talk about logistic regression This is a technique that can be used for traditional statistics as, well as machine learning so let's get right to it Before we dive into logistic regression let's take. A, step back and review, linear regression in Another stat quest, we talked, about linear regression We had some data Weight and size then, we fit a line to it and With that line, we could do a lot of things First we could calculate r-squared and determine if weight and size are correlated large values imply a large effect, and Second calculate a p-value to determine if the r-squared value is statistically significant and Third, we could use the line, to predict, size given weight if a, new, mouse has this weight Then this, is the size that, we predict, from the weight although We didn't mention it at the time using data to predict something falls under the category of machine learning So plain old linear regression is a form of machine learning We also talked a little bit about multiple regression Now, we are trying to predict, size, using weight and blood volume Alternatively we could, say that, we are trying to model size using weight and blood volume Multiple regression, did the same things that normal regression did we calculated r-squared and we calculated the p-value and We could predict, size, using weight and blood volume and This, makes multiple regression a slightly fancier machine learning method We also talked, about how, we can use discrete measurements like genotype to predict size if you're Not familiar with the term genotype don't freak out it's. No, big deal just know that it refers to different types of mice lastly, we could compare models So on the left side we've got normal regression, using weight to predict size and We can, compare those predictions to the ones, we get from multiple regression, where we're using weight and blood volume to predict size Comparing the simple model to the complicated one tells us if we need to measure weight and blood volume to accurately predict Size or if we can get, away, with just weight Now that we remember all the cool, things, we can, do with linear regression Let's talk, about logistic regression Logistic regression is similar to linear regression except Logistic regression predicts whether something, is true or false instead of predicting something continuous, like, sighs these mice are obese and These mice are not Also instead of fitting a line to the data logistic regression fits an s-shaped logistic function The curve goes from zero to one? And that, means that the curve tells you the probability that a mouse is obese based on its weight If we weighed a very heavy mouse? There is a high probability that the new, mouse is obese? If we weighed an intermediate mouse Then there is only a 50% chance of the mouse is obese? Lastly, there's only a small probability that a light mouse is obese Although, logistic regression, tells the probability that a mouse is obese or not it's usually used for classification For example if the probability of mouse is obese is greater than 50% Then we'll classify it as obese Otherwise we'll classify it as not obese Just like with linear regression, we can, make simple models in this case, we can have obesity predicted, by weight or? more complicated models in this case obesity is predicted by weight and genotype in This, case, obesity is predicted. By weight and genotype and age and Lastly, obesity is predicted by weight genotype, age and Astrological sign in other words just like linear regression logistic Regression can work with continuous data, like weight and age and discrete data like genotype and astrological sign We can, also test to see if each variable is useful for predicting obesity however Unlike normal regression, we can't easily compare the complicated model to the simple model and we'll talk more about, why in a bit Instead we just test to see if a variables affect on the prediction is significantly different from zero If not it, means that the variable is not helping the prediction We use, wald's tests to figure this out we'll talk, about that in another stat quest in This, case, the astrological sign is totes useless That statistical jargon for not helping That, means we can, save time and space in our study. By leaving it out Logistic regressions ability to provide probabilities and classify, new samples using continuous and discrete measurements Makes it a popular machine learning method One big difference between linear regression and logistic regression is how the line is fit to the data With linear regression, we fit the line, using least squares In other words, we find the line that minimizes the sum of the squares of these residuals We also use the residuals to calculate r. Squared and to compare simple models to complicated models Logistic regression doesn't have the same concept of a residual so it can't use least squares and it can't calculate r squared instead it uses something called maximum likelihood There's a whole stack quest on maximum likelihood so see that for details but in a nutshell You, pick a probability scaled. By weight of observing an obese mouse just like this curve and You, use that to calculate the likelihood of observing a, non obese mouse that weighs this much and then you calculate the likelihood of observing, this mouse and you, do that for all of the mice and Lastly, you multiply all of those likelihoods together that's the likelihood of the data given this line then you shift the line and calculate a new, likelihood of the data and then ship the line and calculate the likelihood, again, and again Finally the curve with the maximum value for the likelihood is selected bam in summary logistic regression can be used to classify samples and it can, use different types of data like, size and/or genotype to do that classification and it can, also be used to assess what variables are useful for classifying samples ie Astrological sign is totes useless Hooray, we've made it to the end of another exciting stat quest do you, like this StackQuest, and want to see more please subscribe if you, have suggestions for future stat quests, well put them in the comments below, until next time quest on"
6EXPYzbfLCE,2018-02-26T16:44:24.000000,StatQuest: Random Forests in R,"you don't need a ukulele to do statistics but it makes it more fun hello I'm Josh stormer and welcome to stat quest today we're going to talk about how to build use and evaluate random forests in our this stat quest builds on two stat quests that I've already created that demonstrate the theory behind random forests so if you're not familiar with it check them out just so you know you can download all the code that I describe in this tutorial using the link in the description below the first thing we do is load in ggplot2 so we can draw fancy graphs and when I do that our prints out a little message it's no big deal then we load Cal plot which just improves some of ggplot2 s default settings it overwrites the GG save function and that's fine with me so no worries here the last library we need to load is random forests duh so we can make random forests it also prints out some stuff in red but it's no big deal we can move on from here for this example we're going to get a real data set from the UCI machine learning repository specifically we want the heart disease data set so we make a variable called URL and set it to the location of the data we want for our random forest and this is how we read the data set into our from the URL the head function shows us the first six rows of data unfortunately none of the columns are labeled wha-wha so we named the columns after the names that were listed on the UCI website the UCI website actually lists a whole lot of information about this data so it's worth checking out if you haven't done that already hooray now when we look at the first six rows with the head function things look a lot better however the stur function which describes the structure of the data tells us that some of the columns are messed up sex is supposed to be a factor where zero represents female and one represents male CP aka chest pain is also supposed to be a factor where levels 1 through 3 represent different types of pain and 4 represents no chest pain see a and Thal are correctly called factors but one of the levels is question mark when we need it to be an n/a so we've got some cleaning up to do the first thing we do is change the question marks to n A's then just to make the data easier on the eyes we convert the zeros in sex to F for female and the ones to M for male lastly we convert the column into a factor then we convert a bunch of other columns into factors since that's what they're supposed to be see the UCI website or the sample code on the stat quest blog for more details since the CA column originally had a question mark in it rather than in a are thinks it's a column of strings we correct that assumption by telling our it's a column of integers and then we convert it to a factor then we do the exact same thing for Thal the last thing we need to do to the data is make HD aka heart disease a factor that is easy on the eyes here I'm using a fancy trick with if-else to convert the zeros to healthy and the ones to unhealthy we could have done a similar trick for sex but I wanted to show you both ways to convert numbers to words once we're done fixing up the data we can check that we've made the appropriate changes with the stir function sex is now a factor with levels F and M and everything else looks good too hooray we're done with the boring part now we can have some fun since we are going to be randomly sampling things let's set the seed for the random number generator so that we can reproduce our results now we impute values for the NA s in the dataset with our F impute the first argument to our F impute is HD tilde dot and that means we want the HD aka heart disease column to be predicted by the data in all of the other columns here's where we specify which data set to use in this case there's only one data set and it's called data here's where we specify how many random forests our F impute should build to estimate the missing values in theory four to six iterations is enough just for fun I set this parameter editor equal to 20 but it didn't improve the estimates lastly we save the results the data set with imputed values instead of n A's as data dot imputed after each iteration our F impute prints out the out of bag OOB error rate this should get smaller if the estimates are improving since it doesn't we can conclude that our estimates are as good as they're going to get with this method here's where we actually build a proper random forest using the random forest function just like when we imputed values for the NA s we want to predict HD aka heart disease using all of the other columns in the data set however this time we specify data imputed as the data set we also want random forests to return the proximity matrix we'll use this to cluster the samples at the end of the stat quest lastly we save the random forest and associated data like the proximity matrix as model to get a summary of the random forest and how well it performed we can just type model on the command prompt and then hit enter here's what gets printed to the screen the first thing is the original call to random forest next we see that the random forest was built to classify samples if we had used the random forest to predict weight or height it would say regression and if we had omitted the thing the random forest was supposed to predict entirely it would say unsupervised then it tells us how many trees are in the random forest the default value is 500 later we will check to see if 500 trees is enough for optimal classification then it tells us how many variables or columns of data were considered at each internal node classification trees have a default setting of the square root of the number of variables regression trees have a default setting of the number of variables divided by 3 since we don't know if 3 is the best value we'll fiddle with this parameter later on here's the out-of-bag oob error estimate this means that 83.5% of the oob samples were correctly classified by the random forest lastly we have a confusion matrix there were 141 healthy patients that were correctly labeled healthy hooray there were 27 unhealthy patients that were incorrectly classified as healthy boo there were 23 healthy patients that were incorrectly classified unhealthy ooh lastly there were 112 unhealthy patients that were correctly classified unhealthy hooray to see if 500 is enough for optimal classification we can plot the error rates here we create a data frame that formats the error rate information so that ggplot2 will be happy this is kind of complicated so let me walk you through it for the most part this is all based on a matrix within model called dot rate this is what the earth dot rate matrix looks like there's one column for the out-of-bag error rate one column for the healthy error rate ie how frequently healthy patients are misclassified and one column for the unhealthy error rate ie how frequently unhealthy patients are misclassified each row reflects the error rates at different stages of creating the random forest the first row contains the error rates after making the first tree the second row contains the error rates after making the first two trees the last row contains the error rates after making all 500 trees so what we're doing here is making a data frame that looks like this there's one column for the number of trees there's one column for the type of error and one column for the actual error value and here's the call to ggplot BAM the blue line shows the error rate when classifying unhealthy patients the green line shows the overall out-of-bag error rate the red line shows the error rate when classifying healthy patients in general we see the error rates decrease when our random forest has more trees if we added more trees with the error rate go down further to test this hypothesis we make a random forest with 1,000 trees the out-of-bag error rate is the same as before and the confusion matrix shows that we didn't do a better job classifying patients and we can plot the error rates just like before double bam and we see that the error rates stabilize right after 500 trees so adding more trees didn't help but we would not have known this unless we used more trees now we need to make sure we are considering the optimal number of variables at each internal node in the tree we start by making an empty vector that can hold ten values and then we create a loop that tests different numbers of variables at each step each time we go through the loop I increases by one it starts at one and ends after ten in this line we are building a random forest using AI to determine the number of variables to try at each step specifically we are setting m try equals I and I equals values between 1 and 10 this is where we store the out-of-bag error rate after we build each random forest that uses a different value for M try this is a bit of complex code here's what's going on tamp top model contains a matrix called air dot rate just like model did before and we want to access the value in the last row and in the first column ie the out-of-bag error rate when all 1000 trees have been made now we can print out the out-of-bag error rates for different values for m try the third value corresponding to m try equals 3 which is the default in this case has the lowest out-of-bag error rate so the default value was optimal but we wouldn't have known that unless we tried other values lastly we want to use the random forests to draw an MDS plot with samples this will show us how they are related to each other if you don't know what an MDS plot is don't freak out just check out the stat quest on it we start by using the dist function to make a distance matrix from 1 minus the proximity matrix then we run CMD scale on the distance matrix CMD scale stands for classical multi-dimensional scaling then we calculate the percentage of variation in the distance matrix that the x and y axes account for again see the other stat quest for details then we format the data for ggplot and then we draw the graph with ggplot triple bam unhealthy samples are on the left side healthy samples are on the right side I wonder if patient 253 was misdiagnosed and actually has heart disease note the x-axis accounts for 47% of the variation in the distance matrix the y-axis only accounts for 14% of the variation in the distance matrix that means that the big differences are along the x-axis lastly if we got a new patient and didn't know if they had heart disease and they clustered down here we'd be pretty confident that they had heart disease [Music] hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more of them please subscribe and if you have any suggestions for future stat quests well put them in the comments below until next time quest on"
J4Wdy0Wc_xQ,2018-02-05T15:55:40.000000,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating","Wandering around a random forest. I won't get lost because of stat quest Hello, I'm Josh Dharma and welcome to stat quest today We're gonna be starting part one of a series on random forests, and we're going to talk about building and evaluating random forests Note random forests are built from decision trees. So if you don't already know about those check out my stat quest and beef up Decision trees are easy to build easy to use and easy to interpret But in practice they are not that awesome to quote from the elements of statistical learning Aka the Bible of machine learning Trees have one aspect that prevents them from being the ideal tool for predictive learning Namely in accuracy. In other words, they work great with the data used to create them But they are not flexible when it comes to classifying new samples The good news is that random forests combine the simplicity of decision trees with flexibility Resulting in a vast improvement in accuracy So let's make a random forest step 1 create a bootstrap data set imagine that these 4 samples are the entire data set that we are going to build a tree from I Know it's crazy small, but just pretend for now To create a bootstrap data set that is the same size as the original. We just randomly select samples from the original data set The important detail is that we're allowed to pick the same sample more than once This is the first sample that we randomly select So it's the first sample in our bootstrap data set This is the second randomly selected sample from the original data set So it's the second sample in our bootstrap data set Here's the third randomly selected sample So here it is in the bootstrap data set Lastly here's the fourth randomly selected sample note. It's the same as the third and Here it is BAM we've created a bootstrap data set Step2 for creating a random forest is to create a decision tree using the bootstrap dataset But only use a random subset of variables or columns at each step in This example, we will only consider two variables or columns at each step Note, we'll talk more about how to determine the optimal number of variables to consider later Thus instead of considering all four variables to figure out how to split the root node We randomly select two in This case we randomly selected good blood circulation and blocked arteries as candidates for the root node Just for the sake of the example assume that good blood circulation. Did the best job separating the samples? Since we used a good blood circulation, I'm going to gray it out so that we focus on the remaining variables Now we need to figure out how to split samples at this node just like for the route we randomly select two variables as candidates instead of all three remaining columns and We just build the tree as usual, but only considering a random subset of variables at each step double bound we built a tree one using a bootstrap data set and Two only considering a random subset of variables at each step Here's the tree we just made Now go back to step one and repeat Make a new bootstrap data set and build a tree considering a subset of variables at each step Ideally you do this hundreds of times, but we only have space to show six, but you get the idea Using a bootstrap sample and considering only a subset of variables at each step results in a wide variety of trees The variety is what makes random forests more effective than individual decision trees Sweet now that we've created a random forest. How do we use it? Well first we get a new patient We've got all the measurements and now we want to know if they have heart disease or not So we take the data and run it down the first tree that we made Booboo, dooba, dooba, dooba dooba, dooba. Do the first tree says yes, the patient has heart disease and We keep track of that here now we run the data down the second tree that we made the second tree also says yes and We keep track of that here. And then we repeat for all the trees we made After running the data down all of the trees in the random forest. We see which option received more votes in This case yes received the most votes so we will conclude that this patient has heart disease BAM Oh No terminology alert Bootstrapping the data plus using the aggregate to make a decision is called bagging Okay, now we've seen how to create and use a random forest How do we know if it's any good Remember when we created the bootstrapped data set We allow duplicates in trees in the bootstrapped data set as A result. This entry was not included in the bootstrap data set Typically about one third of the original data does not end up in the bootstrap data set Here's the entry that didn't end up in the bootstrapped dataset If the original dataset were larger, we'd have more than just one entry over here This is called the out-of-bag data set If it were up to me I would have named it thee out of boot data set since it's the entries that didn't make it into the bootstrap dataset Unfortunately, it's not up to me Since the out-of-bag data was not used to create this tree We can run it through and see if it correctly classifies the sample as no heart disease In this case the tree correctly labels the out of bag sample. No Then we run this out of bag sample through all of the other trees that were built without it This tree incorrectly labeled the out of bag sample. Yes These trees correctly labeled the out of bag sample know Since the label with the most votes wins is the label that we assign this out of bag sample in This case the out of bag sample is correctly labeled by the random forest We then do the same thing for all of the other out of bag samples for all of the trees This out of bag sample was also correctly labeled This out of bag sample was incorrectly labeled Etc etc, etc Ultimately we can measure how accurate our random forest is by the proportion of out-of-bag samples that were correctly classified by the random forest The proportion of out-of-bag samples that were incorrectly classified is the out of bag error Okay, we now know how to one build a random forest to use a random forest and three estimate the accuracy of a random forest However now that we know how to do this we can talk a little more about how to do this Remember when we built our first tree and we only use two variables columns of data to make a decision at each step Now we can compare the out-of-bag error for a random forest built using only two variables per step to a random forest built using three variables per step and We test a bunch of different settings and choose the most accurate random forest In other words one we build a random forest and then two we estimate the accuracy of a random forest then we change the number of variables used per step and We do this a bunch of times and then choose the one that is the most accurate Typically we start by using the square of the number of variables and then try a few settings above and below that value Triple bail Hooray We've made it to the end of another exciting static quest tune in next week And we'll talk about how to deal with missing data and how to cluster the samples. All right, and tell them quest are armed"
wpNl-JwwplA,2018-01-29T15:06:20.000000,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",when you've got too much data don't freak [Music] out when you've got missing data don't freak out you've got stat Quest hello I'm Josh starmer and welcome to stack Quest today we're going to be talking about decision trees part two feature selection and missing data this is just a short and sweet stack quest to touch on a few topic ICS we didn't get to in the original stat Quest on decision trees in the first stat Quest on decision trees we started with a table of data and built a decision tree that gave us a sense of How likely a patient might have heart disease if they have other symptoms we first asked if a patient had good blood circulation if so we then asked if they had blocked arteries and if so we then asked if they had chest pain if so there's a good chance that they have heart disease since 17 people with similar answers did and only three people with similar answers did not if they don't have chest pain there's a good chance that they do not have heart disease however remember that if someone had good circulation and did not have blocked arteries we did not ask about chest pain because there was less impurity in our result s if we didn't in other words we calculated the impurity after separating the patients using chest pain then we calculated impurity without separating and since the impurity was lower when we didn't separate we made it a leaf node now imagine if chest pain never gave us a reduction in impurity score if this were the case we would never use chest pain to separate the patients and chest pain would not be part of our tree now even though we have data for chest pain it is not part of our tree anymore all that's left are good circulation and blocked arteries this is a type of automatic feature selection however we could also have created a threshold such that the impurity reduction has to be large enough to make a big difference as a result we end up with simpler trees that are not overfit oh no some jargon just snuck up on us overfit means our tree does well with the original data the data we used to make the tree but doesn't do well with any other data set decision trees have the downside of often being overfit requiring each split to make a large reduction in impurity helps a tree from being overfit so in a nutshell that's what feature selection is all about now let's talk about missing data in the first video on decision trees we calculated impurity for blocked arteries and we skipped this patient since we didn't know if they had blocked arteries or not but it doesn't have to be that way we could pick the most common option if overall yes occurred more times than no we could put yes here alternatively we could find another column that has the highest correlation with blocked arteries and use that as a guide in this case chest pain and blocked arteries are often very similar the first patient has no in both categories the second patient has yes in both categories the third patient has no in both categories and so for the fourth patient since chest pain is yes we'll make blocked arteries yes as well now imagine we had weight data instead of blocked artery data we could replace this missing value with the mean or the median alternatively we could find another column that has the highest correlation with weight in this case height is highly correlated with weight and then we can do a linear regression on the two columns and use the least squares line to predict the value for weight so you can see that if we're missing some data there are a lot of ways to guess at what it might be bam hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more like it please subscribe and if you have any ideas for future stack quests well put them in the comments below until next time queston
Lsue2gEM9D0,2018-01-08T21:06:15.000000,StatQuest: PCA in Python,that quest is off some he comes out [Music] hello I'm Josh stormer and welcome to stat quest today we're gonna be talking about how to do PCA and Python it's gonna be clearly explained however before I get started I need to have a big shout out for Chris yoga zimsky he wrote the code that I'm gonna be talking about and he's the reason why the stat quest exists to begin with here's what we're going to talk about first we'll talk about how to make up some data that we can apply PCA to then we'll talk about how to use the PC a function from scikit-learn to do PCA we'll also talk about how to determine how much variation each principal component accounts for we'll talk about how to draw a fancy PCA graph using matplotlib and lastly we'll talk about how to examine the loading scores to determine what variables have the largest effect on the graph the first thing we do is import the pandas package pandas short for panel data makes it easy to manipulate data in python then we import the numpy package numpy will allow us to generate random numbers and do other mathy things the random package will be useful for generating an example dataset if you're working with real data you won't need this package here we are importing the PC a function from scikit-learn note just like our there are a few Python PCA functions to choose from the one in scikit-learn is the most commonly used that I know of the pre-processing package from psych had learned gives us functions for scaling the data before performing PCA lastly we'll import matplotlib so we can draw some fancy graphs Python being a general-purpose programming language doesn't have built-in support for tables of data random number generation or graphing like R so we import all the stuff we need it's no big deal now that we've imported all the packages and functions we need let's generate a sample data set the first thing we do is generate an array of a hundred gene names since this is just an example data set our gene names are super boring gene one gene two etc now we create arrays of sample names we have five wild-type or WT samples and five knockout or ko samples if you don't know what wild-type or knockout samples are don't worry about it just know that there's two different types of samples note the range one comma six function generates values one two three four and five in other words it generates all integer values from 1 up to the value less than six now we create a panda's data frame to store the made-up data the stars unpack the WT and ko arrays so that the column names are a single array that looks like this without the stars we would create an array of two arrays and that wouldn't create twelve columns like we want the gene names are used for the index which means that they are the equivalent of row names this is where we finally create the random data for each gene in the index ie gene one gene two dot dot all the way to gene 100 we create five values for the WT samples and five values for the KO samples the made up data comes from two Poisson distributions one for the WT samples and one for the ko samples note RNA sequencing aficionados will recognize that the Poisson distribution isn't the right one to use for RNA seek data no big deal this is just a made-up dataset so don't worry about it for each gene we select a new mean for the Poisson distribution the means can vary between 10 and 1000 the head method for our data frame returns the first five rows of data this is useful for verifying that the data look the way we expect they should the shape attribute returns the dimensions of our data matrix in this case we get a hundred comma 10 100 genes by 10 total samples now that we've created a sample data set let's do PCA actually before we do PCA we have to Center and scale the data after centering the average value for each gene will be zero and after scaling the standard deviation for the values for each gene will be 1 notice that we are passing in the transpose of our data the scale function expects the samples to be in rows instead of columns note we use samples as columns in this example because that is often how genomic data is stored if you have other data you can store it however is easiest for you there's no requirement that the samples be rows or columns just be aware that if it is columns you'll need to transpose it before analysis one other note this is just one way to use scikit-learn to Center and scale the data so that the means for each gene are 0 and the standard deviations for each gene are 1 alternatively we could have use standard scalar fit underscore transform the second method is more commonly used for machine learning and that's what scikit-learn was designed to do that's why it's available one last note about scaling with scikit-learn versus using scale or per comp in our to do the scaling in scikit-learn variation is calculated as the measurements minus the mean squared divided by the number of measurements in contrast in our using scale or per comp variation is calculated as the measurements minus the mean squared divided by the number of measurements minus one this method results in larger but unbiased estimates of the variation the good news is that these differences do not affect the PCA analysis the loading scores and the amount of variation per principal component will be the same either way the bad news is that these differences will have a minor effect on the final graph this is because the coordinates on the final graph come from multiplying the loading scores by the scaled values now we create a PCA object rather than just have a function that does PCA and returns results scikit-learn uses objects that can be trained using one data set and applied to another data set since we're only using PCA to explore one data set and not using PCA in a machine learning setting the additional steps are a little tedious but they set us up for the machine learning topics that I'll be covering very soon then we call the fit method on the scale data this is where we do all the PCA math ie we calculate the loading scores and the variation each principal component accounts for lastly this is where we generate coordinates for a PCA graph based on the loading scores in the scaled data now we're ready to draw a graph we'll start with a scree plot to see how many principle components should go into the final plot the first thing we do is calculate the percentage of variation that each principal component accounts for now we create labels for the scree plot these are pc-1 pc2 etc one label per principal component now we use matplotlib to create a bar plot and here's what the scree plot looks like with all the fancy annotation that we added using the y label X label and title methods almost all of the variation is along the first principal component so a 2d graph using pc1 & pc2 should do a good job representing the original data to draw a PCA plot well first put the new coordinates created by PCA transform of the scaled data into a nice matrix where the rows have sample labels and the columns have PC labels these commands draw chatter plot with the title and nice access labels and this loop adds sample names to the graph lastly we display the graph there it is wow so awesome BAM the WT samples clustered on the left side suggesting that they are correlated with each other the KO samples clustered on the right side suggesting that they are correlated with each other and the separation of the two clusters along the x-axis suggests that the wild-type samples are very different from the knockout samples lastly let's look at the loading scores for PC one to determine which genes had the largest influence on separating the two clusters along the x axis we'll start by creating a panda's series object with the loading scores in principal component one note the principal components are zero index so PC one equals zero now we sort the loading scores based on their magnitude aka we take the absolute value here we are just getting the names of the top ten indexes which are the gene names lastly we print out the top ten gene names and their corresponding loading scores and this is what we get these values are super similar so a lot of genes played a role in separating the samples rather than just one or two double bam hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you have any suggestions for future stat quests well put them in the comments below until next time quest on
pGAUHhLYp5Q,2017-12-18T22:56:21.000000,StatQuest: MDS and PCoA in R,stack west on a Monday makes for a fun day stack west on any day makes that day fun hello I'm Josh Dharma and welcome to stack west today we're going to talk about doing multi-dimensional scaling mb/s and principle coordinate analysis PCO a in r if you don't already know MDS or classical or metric MDS is the exact same thing as PCO a one last thing before we move on the code that I use in the stack quest is available on the stack quest website and the link to that code is in the description below first we load in ggplot2 since we'll need it later to draw fancy looking graphs now we generate some fake data this is exactly the same as the data we used in the PCA in our stat quest so I'm going to breeze through this pretty quickly if you need more details check out that stat quest the data will consist of a matrix with ten columns corresponding to ten samples and 100 rows corresponding to measurements from 100 genes the first five columns will be WT or wild-type samples and the last five columns will be ko or knockout samples the genes will have really creative names like gene one gene two this is where we generate the fake data and the head function shows us the first six rows or genes for all ten samples now just for comparison will do PCA on the data set I'm going to breeze through these steps since we went over them already in the PCA in our stat quest now we create a PCA plot using GG plot note we covered this command in the PCA and our stat quest so check that out if this looks totally crazy BAM the wild-type samples are on the left side of the graph and the knockout samples are on the right side the x-axis PC one for the first principal component accounts for 91% of the variation in the data and the y-axis for PC - or principal component - only accounts for 2.7 percent of the variation in the data this means that most of the differences were between the wild type and the knockout samples now let's create an MDS or PC Oh a plot to compare to this one step one create a distance matrix we do this with the dist function just like with PCA we transpose the matrix so the samples are rows we also Center and scale the measurements for each gene which are now the columns lastly we tell the disk function that we want to create the matrix using euclidean distance metric note the dist function has six different distance metrics to choose from step to perform multi-dimensional scaling on the distance matrix using the CMD scale function CMD scale stands for classical multi-dimensional scaling we tell CMD scale that we want it to return the eigen values we use these to calculate how much variation in the distance matrix each access in the final MDS plot accounts for we can also get CMD scale to return the doubly centered ie both rows and columns are centered version of the matrix this is useful if you want to demonstrate how to do MDS using the eigen function instead of the CMD scale function originally I thought I was going to demonstrate how to use the eigen function to do multi-dimensional scaling but in the end I really wanted to keep this practical and you're gonna do MDS you're gonna use the CMD scale function step 3 calculate the amount of variation each access in the MDS plot accounts for using the eigenvalues step four format the data for ggplot lastly call ggplot to make a fancy graph just like in the pca graph the wild-type samples are on the left side of the graph and the knockout samples are on the right side and just like in the PCA graph the X access accounts for 91% of the variation in the data and the y axis only accounts for 2.7 percent of the variation in the data actually the pca graph and the MDS graph don't just look similar they are exactly the same this is because we use the Euclidean metric to calculate the distance matrix now let's see what happens when we use a different metric to calculate the distance matrix let's use the average of the absolute value of the log fold change just for all you gene expression folks this is what edge R does when you call the plot MDS function the first thing we do is calculate the log to values of the measurements for each gene since the average of absolute values of the log fold change isn't one of the distance metrics built into the dist function we'll create our own distance matrix by hand in this step we're just creating an empty matrix this is where we fill the matrix with the average of the absolute values of the log fold changes and here's what that matrix looks like because the full matrix would be symmetrical we only have to calculate the values for the lower triangle now we perform multi-dimensional scaling on our new distance matrix here I'm just converting our homemade matrix into a true distance matrix so that cm descale knows what it's working with in other words a true distance matrix only needs the bottom triangle to be computed and not the whole thing everything else is the same just like before we calculate the amount of variation each access in the MDS plot accounts for using the eigenvalues and again just like before we format the data for ggplot lastly we create the graph using ggplot double bound the two different MDS plots one using the Euclidean distance and the other using the average of the absolute value of the log fold change are similar but not the same in the new graph the x-axis accounts for more of the variation 99.2% versus 91% hooray we've made it to the end of another exciting stat quest if you like this tack quest and want to see more of them please subscribe and if you have any ideas for additional stat quests well put them in the comments below alright until next time quest on
HMOI_lkzW08,2017-12-04T20:18:06.000000,StatQuest: PCA main ideas in only 5 minutes!!!,that quest is the best if you don't think so then we have different opinions hello I'm Josh stommer and welcome to stat quest today we're gonna be talking about the main ideas behind principle component analysis and we're going to cover those concepts in five minutes if you want more details than you get here be sure to check out my other PCA video let's say we had some normal cells if you're not a biologist imagine that these could be people or cars or cities or etc they could be anything even though they look the same we suspect that there are differences these might be one type of cell or one type of person or car or city etc these might be another type of cell and lastly these might be a third type of cell unfortunately we can't observe differences from the outside so we sequence the messenger RNA in each cell to identify which genes are active this tells us what the cell is doing if they were people we could measure their weight blood pressure reading level etc okay here's the data each column shows how much each gene is transcribed in each cell for now let's imagine there are only two cells if we just have two cells then we can plot the measurements for each gene this gene gene one is highly transcribed in cell one and lowly transcribed in cell two and this gene gene 9 is lowly transcribed in cell 1 and highly transcribed in cell 2 in general cell one and cell to have an inverse correlation this means that they are probably two different types of cells since they are using different genes now let's imagine there are three cells we've already seen how we can plot the first two cells to see how closely they are related now we can also compare cell one to sell three cell one and cell three are positively correlated suggesting they are doing similar things lastly we can also compare cell two to cell three the negative correlation suggests that cell two is doing something different from cell 3 alternatively we could try to plot all three cells at once on a three dimensional graph cell one could be the vertical axis cell two could be the horizontal axis and sell three could be depth we could then rotate this graph around to see how the cells are related to each other but what do we do when we have four or more cells draw tons and tons of to sell plots and try to make sense of them all or draw some crazy graph that has an axis for each cell and makes our brain explode no both of those options are just plain silly instead we draw a principal component analysis or PCA plot a PCA plot converts the correlations or lack thereof among the cells into a 2d graph cells that are highly correlated cluster together this cluster of cells are highly correlated with each other so are these and so are these to make the clusters easier to see we can color-code them once we've identified the clusters in the PCA plot we can go back to the original cells and see that they represent three different types of cells doing three different types of things with their genes BAM here's one last main idea about how to interpret PCA plots the axes are ranked in order of importance differences among the first principal component access PC one are more important than differences along the second principal component access PC two if the plot looked like this where the distance between these two clusters is about the same as the distance between these two clusters then these two clusters are more different from each other than these two clusters before we go you should know that PCA is just one way to make sense of this type of data there are lots of other methods that are variations on this theme of dimension reduction these methods include heat maps tea Snee plots and multiple dimension scaling plots the good news is that I've got stat quests for all of these so you can check those out if you want to learn more note if the concept of dimension reduction is freaking you out check out the original stat quest on PCA I take it nice and slow so it's clearly explained hooray we've made it to the end of another exciting stat quest if you like the stat quest and want to see more of them please subscribe and if you have any ideas for additional stat quests well put them in the comments below until next time quest on
0Jp4gsfOLMs,2017-11-27T14:35:50.000000,StatQuest: PCA in R,stat Quest is moving around watch it go hello I'm Josh stormer and welcome to stat quest stat quest is brought to you by the friendly folks in a genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about how to do PCA and are it's going to be clearly explained note in the description below this video I provided a link to the code that I use so if you want to do it yourself all you got to do is follow that link copy and paste and you're good to go here's what we're going to be talking about how to use the per comp function to do PCA how to draw a PCA plot using base graphics and ggplot2 how to determine how much variation each principal component accounts for and lastly how to examine the loading scores to determine what variables have the largest effects on the graph first let's generate a fake data set that we can use in the demonstration note if the details in this section don't make a whole lot of sense because we're talking about genes and read counts don't worry too much the important thing is that we have some data and you might have your own data to work with we will make a matrix of data with ten samples where we measured a hundred genes in each sample this is where we name the samples the first five samples will be WT or wild-type samples the WT samples are normal everyday samples the last five Center poles will be ko or knockout samples these are samples that are missing a gene because we knocked it out this is where we named the genes usually you'd have things like sox9 and UT x but since this is a fake data set we have gene one gene two to gene 100 this is where we give the fake genes fake read counts sticklers for details will recognize that I'm using the Poisson distribution instead of the negative binomial distribution like I said these are fake reads for fake genes so don't worry about it too much and here are the first six rows in our data matrix note the samples are columns and the genes are rows now that we have our data we call per comp to do the PCA on our data the goal is to draw a graph that shows how the samples are related or not related to each other note by default per comp expects the samples to be rows in the genes to be columns since the samples and our data matrix are columns and the genes variables are rows we have to transpose the matrix using the T function if we don't transpose the matrix we will ultimately get a graph that shows how the genes are related to each other and this isn't what we want in this case per comp returns three things X s dev and rotation I'll talk about all three things as we use them we'll start with X X contains the principal components P C's for drawing a graph here we are using the first two columns and X to draw a 2d plot that uses the first two principal components remember since there are 10 samples there are 10 principal components the first principal component accounts for the most variation in the original data the gene expression across all 10 samples the second principal component accounts for the second most variation and so on to plot a 2d PCA graph we usually use the first two principal components however sometimes we use principal component two in principal component three here's our graph using the base graphics function plot pc1 is on the x-axis because pc 1 is the first column in x pc 2 is on the y-axis it's the second column in x five of the samples are on one side of the graph and the other five samples are on the other side of the graph to get a sense of how meaningful these clusters are let's see how much variation in the original data principal component one accounts for to do this we use the square of s dev which stands for standard deviation to calculate how much variation in the original data each principal component accounts for since the percentage of variation that each principal component accounts for is way more interesting than the actual value we calculate the percentages and plotting the percentages is easy with the bar plot function principal component 1 accounts for almost all of the variation in the data this means that there is a big difference between these two clusters we can use ggplot2 to make a fancy PCA plot that looks nice and also provides us with tons of information first format the data the way ggplot2 likes it we make a data frame where one column has the sample IDs two columns are for the x and y coordinates for each sample here's what the data frame looks like we have one row per sample each row has a sample ID and XY coordinates for that sample here's the call to ggplot I'll go through this one line at a time but first let's look at the graph that draws the x axis tells us what percentage of the variation in the original data that pc1 accounts for the y axis tells us what percentage of the variation in the original data that pc2 accounts for now the samples are labels so we know which ones are on the left and the right in the first part of our ggplot function call we pass in the pca data data frame until ggplot which columns contain the x and y coordinates and which column has the sample labels then we use geum text to tell ggplot to plot the labels rather than dots or some other shape then we use X lab and while Abe to add X and y axis labels here I'm using the paste function to combine the percentage of variation with some text to make the labels look nice calling themed BW makes the graphs background white this is optional but I prefer the way this looks lastly we add a title to the graph using GG title BAM [Music] lastly let's look at how to use loading scores to determine which genes have the largest effect on where samples are plotted in the PCA plot the per comp function calls the loading scores rotation there are loading scores for each principal component here I'm just going to look at the loading scores for principal component one since it accounts for 92 percent of the variation in the data genes or variables that push the samples to the left side of the graph we'll have large negative values in genes or variables that push the samples to the right will have large positive values since we're interested in both sets of genes or variables we use the absolute value function to sort based on the numbers magnitude rather than from high to low now we sort the magnitudes of the loading scores from high to low now we get the names for the top 10 genes with the largest loading score magnitudes lastly we can see which of these genes have positive loading scores these push the chaos intervals to the right side of the graph and then we see which genes have negative loading scores these push the wild-type samples to the left side of the graph hooray we've made it to the end of another exciting static quest if you liked this stack quest and want to see more of them please subscribe and if you have any suggestions for future stack quests well put them in the comments below until next time quest on
ecjN6Xpv6SE,2017-11-20T10:51:06.000000,"Quantile Normalization, Clearly Explained!!!",burn burn burn burn burn burn stat quest hello I'm Josh stormer and welcome to stat quest stat quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about quantile normalization and it's going to be clearly explained imagine we have some data from a microarray experiment if you're not familiar with microarrays here's the scoop microarrays measure how active genes are in a sample they do that by measuring the intensity of different colors of light if you have a better light bulb for one experiment every measurement might be brighter than every measurement from another experiment thus we need to normalize the data to account for this technical difference between experiments which has nothing to do with biology this is just an example of the type of data you might want to quantile normalize however there are lots of other types of data so if you are doing microarray experiments well it still might be useful to pay attention and see how this is done because it might apply to some other data set that you're working with here's our data in this graph each color represents a different gene these are different from the colors that are scanned in the actual microarray experiment at this point those colors have already been converted into intensity values and that's what we're looking at here each gene has its own color and the value on the y-axis represents the intensity that that gene had on a microarray by the way the word microarray is really hard to articulate unless you're super focused on your pronunciation so I hope I don't mess it up too many times in this stat quest okay back to the good stuff these are the mean values for each sample each sample has a different mean value suggesting that we need to compensate for different overall intensities of light quantile normalization corrects for this technical artifact so let's do it so that we can keep track of how this method works we'll keep the raw data on the left side and the quantile normalized data on the right side you start by focusing on the most highly expressed gene in each sample then calculate the mean value now you extend the mean value into the new plot the quantile normalized value for the genes with the highest expression is their mean value now focus on the next most highly expressed gene in each sample then calculate the mean value extend the mean value into the new plot the quantile normalized value for the genes with the next highest expression is their mean value then do the same thing for the third most Express gene in each sample and so on and so on finally you do the same thing for the least Express gene in each sample hooray now we've quantile normalized the data after quantile normalization the values for each sample are the same but the original gene orders are preserved so in sample one the red gene has the lowest expression value in the raw data set and also the quantile normalized data set and in sample two the green gene has the lowest expression values in both the raw data and in the normalized data lastly in sample number three the red gene has the lowest expression values in both the raw data and the normalized data BAM they call it quantile normalization because the normalized data sets have identical quartiles double by now that was easy wasn't it quantile normalization sounds so fancy and complicated but that's really all there is to it hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more of them please subscribe and if you have any ideas for stat quests you'd like to see me do well put them in the comments below alright until next time quest on
okjYjClSjOg,2017-11-13T13:09:12.000000,"Quantile-Quantile Plots (QQ plots), Clearly Explained!!!",draw breath draw a line tell me if you think it's fun stat quest hello I'm Josh stormer and welcome to stat quest today we're gonna be talking about quantile quantile plots these are called QQ plots sometimes and they're gonna be clearly explained this stat quest assumes you already know what a quantile is if you don't no big deal just check out the stat quest on quantiles and percentiles and then you can watch this one in or make a lot more sense we'll start with this data set where we measured expression in 15 jeans total one question we might have is is this data normally distributed a QQ plot will help answer that question step 1 give each point its own quantile step 2 get yourself a normal curve any normal curve will do step 3 add the same number of quantiles to the curve as you created for the data that is to say there are 15 lines dividing the data into equal sized groups and there are 15 lines dividing the normal curve into equal sized groups for the normal curve equal sized groups means that there is an equal probability of observing a value within each group this means that groups on the edge must be wider to account for the lower probability of observing a value out there and groups in the middle are narrower since there is a higher probability of observing a value in that region step 4 now plot your QQ graph a QQ graph has two axes one for the quantiles that we generated from our data set and one for the quantiles we generated from the normal distribution we'll start with the smallest quantile in our data set this quantile is at 0.6 and our data set and so we'll draw a dotted line on the QQ plot to indicate that location then we draw a vertical line representing the position of the smallest quantile in the normal distribution this quantile is at negative 1.5 so that's where our vertical dotted line goes now all we do is put a dot where those two lines intersect now we move on to the second quantile in the data set that we collected this one is at one point one so we have a horizontal dotted line at one point one in our quantile quantile graph then we draw a vertical line at the location of the second quantile in the normal distribution this quantile is at negative 1.2 so we draw a vertical line at negative 1.2 then we draw a dot where the two lines intersect the third quantile in the data set that we collected is at 1.9 so we draw a horizontal line at 1.9 in the 3rd quantile in our normal distribution is at negative zero point 8 9 so we draw a vertical line at that position and then we draw a dot where the two lines intersect and then we just do the same thing over and over again for each quantile until we have all the quantiles represented in our quantile quantile graph hooray now we've got dots showing where the quantiles from our data set intersected with the quantiles from the normal distribution on our graph now we see how well the dots fit a straight line if the data were normally distributed most of the points would be on the line this would mean that both the data set and the normal distribution have comparable quantiles in this case the fit is not awesome so we should compare the data that we collected to another distribution what if we compare our data to a uniform distribution the process is the same step 1 give each point its own quantile step 2 get yourself a uniform distribution any old uniform distribution will do step 3 add the same number of quantiles to the distribution as you created for your data step 4 now plot your QQ graph just like before we've got the data quantiles on the y axis and the distribution quantiles on the x axis now we draw a straight line that goes through the points in the graph and look the points are much closer to the line in the QQ plot indicating that the uniform distribution is a better fit so if you're ever wondering which distribution matches your data well you could use QQ plots to answer that question ok one last thing before we're done what if we want to compare the original data set to another one this second one is much smaller the new data set only has four quartiles quartiles are just quantiles when you only have four of them so we determine four quartiles for the original data set and compare those and just like we did when we compared the data set that we gathered to the normal distribution and the uniform distribution we plot a QQ plot by drawing horizontal and vertical lines representing the quantiles from the two data sets that we collected and where the two lines intersect we draw a dot and we just repeat this process for each pair of quantiles until we have all the quantiles represented on the graph hooray now we've got all the quantiles plotted on our QQ graph now at a straight line to determine how similar the distributions are with just a few quantiles in our plot it's hard to have a definitive answer about whether or not the two data sets have similar distributions but you get the idea if we collected more data we'd have more quantiles and would know more hurray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more of them please subscribe and if you have any suggestions for future stat quests put them in the comments below alright until next time quest on
IFKQLDmRK0Y,2017-11-06T17:42:22.000000,"Quantiles and Percentiles, Clearly Explained!!!",sted quest is special yes it is stat quest hello i'm josh star and welcome to stat quest stat quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about quantiles and percentiles and they're gonna be clearly explained I thought putting this stat quest together was going to be super easy but doing a little research sent me down a crazy rabbit hole pretty much every webpage I looked at had a slightly different explanation part of the problem is that there's a difference between how the word quantile is defined and how quantiles are used in practice the other problem is that there's a ton of different ways to calculate quantiles the quantile function in R alone has nine different methods let's start by talking about the strict definition of quantile and then we'll move on and talk about how the word is used in practice if you can recognize the definition you can skip it and move on to something more useful we'll start by measuring expression for a gene then we measure expression for another gene we measured expression from 15 genes total this is the median value 50% of the genes have higher expression and 50% of the genes have lower expression technically speaking the median is a quantile because it splits the data into groups that contain the same number of data points in this example there are seven data points below the line and seven data points above the line sometimes this quantile the medium is labeled 0.5 since it splits the data in half and sometimes it's labeled 50% since 50% of the data is above it and 50% is below it the median value is 4.5 thus the 50 percent or 0.5 quantile value is 4.5 to summarize this first point the median is a quantile because it splits the data into equal sized groups this is called the 0.5 quantile or the 50% quantile regardless of which notation is correct you're likely to see both of them in the wild so let's just roll with it and not get to been out of shape over the nomenclature now we've added two more lines together with the first line they divide the data into four equally sized groups these new lines are quantiles because they divide the data into equally sized groups this one is called the 0.25 or 25 percent quantile because 1/4 or 25% of the points are less than it the 25% or 0.25 quantile is 2.5 this one is called the 0.75 or 75 percent quantile because three-quarters or 75% of the points are less than it the 75 percent or 0.75 quantile is 7.3 in general quantiles are just the lines that divide data into equally sized groups at least that's the technical definition and technically speaking percentiles are just quantiles that divide the data into 100 equally sized groups however in practice the terminology is much more flexible even though this data set isn't large enough to be divided into 100 groups we still call the median or the fiftieth quantile the fiftieth percentile and this is called the 75th percentile and this is called the twenty-fifth percentile often the terms quantile and percentile are used when we divide each data point into its own group since no values are less than this one at the bottom it's called the zero percent quantile or the zeroth percentile this data point is the one divided by fifteen equals seven percent quantile or the seventh percentile I use one divided by fifteen because there's one data point of the fifteen that are less than it this data point is the three divided by fifteen or twenty percent quantile or the twentieth percentile etc etc etc calculating quantiles and percentiles is just a matter of finding out how many values are less than the value you're interested in one last thing before we move on so far I've shown you one way to calculate the quantiles and percentiles however there are many more R's quantile function provides nine different ways to calculate quantiles each one resulting in slightly different results what this means is that if your data set is small don't put too much stock in the quantiles since they can change a lot from method to method and sample to sample however when your data set is large then all of the methods give fairly similar results now that we know what quantiles and percentiles are we can talk about quantile quantile plots and quantile normalization which I'll cover in separate stat quests and they'll come out next week and the week after that I'm going to do a whole series on quantiles because I think they're pretty interesting and you see them a lot hooray we've made it to the end of another exciting stat quest be sure to subscribe so you get updates when the next stat quests on quantiles come out and if you have any suggestions for future stat quests well put them in the comments below alright until next time quest on
hokALdIst8k,2017-10-30T16:56:57.000000,"Multiple Regression in R, Step-by-Step!!!",stat quest is totes cray-cray stat quest hello I'm Josh stormer and welcome to stat quest stat quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to compare simple and multiple regression and our and just so you know the R code used in this video is available on the stack quest website stack quest or G this stat quest picks up from where the stat quest on multiple regression left off here's the raw data that we're going to use in our analysis I've created a data frame with measurements for size weight and tail for simple regression we will focus on how well weight predicts size step 1 always plot your data we specified weight for the x-axis and we specified size for the y-axis plotting your data as a first step is super important because it allows us to evaluate whether doing a linear regression to begin with is a good idea can we see a relationship in the data between size and weight in this case we can and that means doing a regression makes sense step 2 use the LM function where L M stands for linear model to fit a line to the data in R this is how you specify the following equation we specify size is predicted by weight by using the tilde character between size and weight and by default our adds the terms for the y-intercept and the slope our then uses least-squares to find the values for the y-intercept and the slope that minimize the squared residuals from the line once we've run the linear models function and save the output in a variable called simple dot regression we can get a summary of that regression using the summary function calling the summary function on simple dot regression gives us a huge pile of stuff the most important stuff is down here specifically the r-squared and the adjusted R squared multiple r-squared is just another way to say R squared by the way also for simple regression the multiple r-squared value or just plain old r-squared is one we're interested in the adjusted R squared only applies when we have more complicated models we'll use it later when we do multiple regression there's also the p-value down here together the R squared which equals zero point six one three and the p-value which equals zero point zero one to say that weight does a pretty good job predicting size the last thing we want to do for our simple regression is add a line that shows the least squares fit on the graph we do this using the a beeline function now let's do some multiple regression for multiple regression we will use weight and tail to predict size step one always plot your data since we didn't specify the x and y axes our plots all the data columns size weight and tail against each other this is super useful because it generates all the plots we need to decide whether doing a multiple regression with this data makes sense or not this graph plots size on the y axis and weight on the x axis this is what we used before in the simple regression down here we have the same exact data however this time sizes on the x axis and weight is on the y axis these two graphs are similar only the axes have been flipped the graph in the upper right hand corner has size on the y axis and tail on the x axis and in the lower left hand corner we have another graph that's very similar just the axes have been flipped this graph has weight on the y-axis and tail on the x-axis and just like for the other graphs these two graphs are similar just the axes have been flipped we can see that both weight and tail are correlated with size this is good it means that both weight and tail are reasonable predictors for size we can also see that weight and tail are correlated this means they provide similar information and that we might not need both in our model we might only need weight or tail step to use the linear model function to fit a plane to the data in R this is how you specify the following equation using the tilde and the plus symbols we specify that size is predicted by weight and tail and by default R as the terms for the y-intercept and slope 1 and slope 2 once we've run the linear models function we can print out a summary of the results using the summary function again summary gives us a big pile of stuff the r-squared adjusted r-squared and the p-value look good hooray note since we're doing multiple regression we're now more interested in the adjusted r-squared value with multiple regression this section is more interesting this line compares the multiple regression to the simple regression it compares this model which uses both weight and tail to predict size to this simpler model where we're just using tail to predict size this is the p-value it means that using weight and tail isn't significantly better than using tail alone to predict size now let's look at this line it compares the fancy multiple regression where we use weight and tail to predict size to a simple regression where we just use weight to predict size this is the p-value it means that using weight and tail is significantly better than using weight alone to predict size in summary using weight and tail to predict size is good but if we wanted to save time we could spare ourselves the agony of weighing mice and just use their tail lengths to predict size hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you have any suggestions for future stat quests we'll just put them in the comments below until next time quest on
zITIFTsivN8,2017-10-30T16:52:41.000000,"Multiple Regression, Clearly Explained!!!","StatQuest, StatQuest, StatQuest, StatQuest!  Yeah! StatQuest! Hello, I'm Josh Stommer and welcome to Stat Quest. StatQuest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill. Today we're gonna be talking about multiple regression, and it's gonna be clearly explained. This StatQuest builds on the one for linear regression. So, if you haven't already seen that one yet, check it out. Alright, now let's get to it! People who don't understand linear regression tend to make a big deal out of the ""differences"" between simple and multiple regression. It's not a big deal and a StatQuest on simple linear regression already covered most of the concepts we're going to cover here. You might recall from the StatQuest on the linear regression that simple regression is just fitting a line to data. We're interested in the r-squared and the p-value to evaluate how well that line fits the data. In that same stat quest. I also showed you how to fit a plane to data. Well, that's what multiple regression is. You fit a plane or some higher-dimensional object to your data. A term like higher-dimensional object sounds really fancy and complicated, but it's not. All it means is that we're adding additional data to the model. In the previous example all that meant was that instead of just modeling body length by mouse weight, we modeled body length using mouse weight and tail length. If we added additional factors like the amount of food eaten or the amount of time spent running on a wheel, well those would be considered additional dimensions, but they're really just additional pieces of data that we can add to our fancy equation. So, from the StatQuest on linear regression you may remember the first thing we did was calculate R squared. Well, the good news is calculating r squared is the exact same for both simple regression and multiple regression. There's absolutely no difference. Here's the equation for R squared, and we plug in the values for the sums of squares around the fit and then we plug in the sums of squares around the mean value for the body length. Regardless of how much additional data we add to our fancy equation, if we're using it to predict body lengths, then we use the sums of squares around the body length. One caveat is for multiple regression you adjust r-squared to compensate for the additional parameters in the equation. We covered this in the StatQuest for linear regression, so it's no big deal. Now we want to calculate a p-value for our r-squared. Calculating F And the p-value is pretty much the same. You plug in the sums of squares around the fit and then you plug in the sums of squares around the mean. For simple regression P fit equals 2 because we have two parameters in the equation that least-squares has to estimate. And for this specific example the multiple regression version of P fit equals 3 because least-squares had to estimate three different parameters. If we added additional data to the model for example the amount of time a mouse spins running on a wheel then we have to change P fit to equal the number of parameters in our new equation. And for both simple regression and multiple regression, P mean equals 1 because we only have to estimate the mean value of the body length. So far we have compared this simple regression to the mean and this multiple regression to the mean, but we can compare them to each other. And this is where multiple regression really starts to shine. This will tell us if it's worth the time and trouble to collect the tail length data because we will compare a fit without it, the simple regression, to a fit with it, the multiple regression. Calculating the F value is the exact same as before only this time we replace the mean stuff with the simple regression stuff. So instead of plugging in the sums of squares around the mean, we plug in the sums of squares around the simple regression. Ane instead of plugging in P Mean we plug in P Simple, which equals the number of parameters in the simple regression. That's 2. And then we plug in the sums of squares for the multiple regression and we plug in the number of parameters in our multiple regression equation. BAM! If the difference in r squared values between the simple and multiple regression is big and the p value is small then adjusting tail length to the model is worth the trouble Hooray, we've made it to the end of another exciting StatQuest! Now for this StatQuest I've made another one that shows you how to do multiple regression in R. It shows all the little details and sort of what's important and what's not important about the output that R gives you. So, check that one out and don't forget to subscribe! Ok, until next time, Quest on!"
67zCIqdeXpo,2017-10-23T19:41:10.000000,"Sample Size and Effective Sample Size, Clearly Explained!!!",whoa whoa whoa damn quistis movin whoa whoa whoa whoa dad Quest is on the move hello I'm Josh stormer and welcome to stat quest stat quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about sample size and effective sample size they're gonna be clearly explained this stat quest builds on what we learned in the stat quest on technical and biological replicates so if you're not already familiar with those concepts you better check that stat quest out ASAP let's start with a simple example let's imagine we are interested in testing a hypothesis about gene expression in blue dudes if you're a mouse geneticist you can think of blue dudes as a specific strain of mouse if you're interested in trees you can think of blue dudes as a specific type of tree we start with a blue dude and then we take a blood sample and then we use that blood sample to measure gene expression then we do the same thing with another blue dude and then lastly we do the same thing with a third blue dude we've got measurements from three separate blue dudes since we're interested in reporting gene expression in blue dudes n equals three wherein is shorthand for sample size on the other hand if we're interested in reporting gene expression in blue ladies then N equals zero duh if we measure gene expression twice in each blue dude and we still wanted to report a result that applied to dudes the sample size would still be three these pairs of measurements are technical replicates and tell us about the accuracy of our method for measuring gene expression they don't tell us about differences between blue dudes if we were interested in describing the accuracy of our method for measuring gene expression and were not interested in describing dudes then we would count the technical replicates in this case N equals four technical replicates only count when we want to describe a method and not people or mice or something else so far so good everything is pretty straightforward now let's look at something a little more complicated let's imagine we wanted to test a hypothesis that applied to dudes in general not just blue dudes in this case it is important that in addition to blue dudes we measure orange dudes and green dudes and here in equals 3 still no big deal but what if the blue dude had a twin and we added that due to the study and we measured his gene expression what's the sample size now is it 3 or 4 it's actually somewhere in between while these twins are clearly two different and separate people their genomes are the same and thus their gene expression will be highly correlated compared to the other dudes in the study if we know or can calculate the correlation between the twins we can calculate their effective sample size for example suppose we calculated the correlation between the twins to be 0.7 the equation for the effective sample size is the number of samples divided by one plus the number of samples minus one times the correlation so we plug in the numbers for the number of samples and the correlation and then we do the math we end up with 1 point 1 8 when the correlation is high between the twins instead of being counted as two people they are counted as one point 1 8 people now let's see what happens when the correlation is low this time it's equal to 0.1 again we can use the equation for the effective sample size to figure out how many people these two people actually are first plug in the numbers then do the math in this case we got 1 point 8 2 just a little bit less than 2 when the correlation is low between the twins they almost count as much as two people in practice calculating the effective sample size can be more complicated but you get the general idea samples that are highly correlated don't count as individual samples in summary if we want to report about the accuracy of a method the sample size is just the number of technical replicates in this case N equals 3 if we want to report about blue dudes or a specific strain of mouse or tree or whatever then the sample size is the number of blue dudes in this case N equals 3 if we want to report about all kinds of dudes or all kinds of mice we need to take correlations into account when calculating the sample size if the correlation between blue dudes is 0.7 the effective sample size is three point one eight if the correlation between blue dudes is zero point one the effective sample size is three point eight two hooray we made it to the end of another exciting stack quest if you like this stack quest and want to see more of them please subscribe and if you have any suggestions for new stack quests put them in the comments below I'd love to hear them until next time quest on
Exk0OoRG0PQ,2017-10-09T15:20:45.000000,The Difference Between Technical and Biological Replicates,Steff quest is not far away Stan Quest is here to stay Stan quest hello I'm Josh stormer and welcome to stack quest stack quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about technical biological and other replicates and they're going to be clearly explained let's start off with some dude and let's take a blood sample from that dude and using that sample let's measure gene expression and let's measure it again and again and again these are technical replicates because every single experiment is performed on the exact same sample technical replicates give us two things one they give us an accurate measurement of this dudes gene expression if we wanted to tell a story about just this dude and we didn't want to generalize a result to a broader population technical replicates would be the way to go technical replicates also tell us how accurately were measuring gene expression if the numbers were way different after each technical replicate we would know not to trust any single measurement if we wanted to publish a paper about how awesome our new method is we'd use technical replicates here's another example again we start with some dude however now we take three samples at the same time and we measure the gene expression in each individual sample these are still considered technical replicates because they only tell us about an individual however these tell more of a story about the individual than the method this is because there can be variation in the samples that the method is not responsible for now let's look at taking samples from three dudes we've got some dude some other dude and some other dude once we get the samples we measure gene expression in each one these are biological replicates because each measurement comes from a different sample that comes from a different dude biological replicates tell us about the gene expression in a group of dudes or animals or plants or cell lines etc this particular experiment tells us about gene expression in dudes it doesn't tell us about gene expression in ladies if I only wanted my result to tell a story about dudes because I'm interested in gene expression from the y-chromosome then this would be fine but if I wanted to have a result that generalized to dudes and ladies I would need to measure gene expression in ladies too the same thing can be said about different ethnicities if I only measure gene expression in blue dudes then my result would only tell a story about blue dudes and not orange or green dudes or dudes in general you can mix biological and technical replicates but the wisdom of doing this depends on the type of experiment sometimes you get more bang for your buck if you add more biological replicates and ignore technical replicates for example when doing RNA seek it's better to do biological replicates rather than technical replicates I have a stat quest that explains why this is so check it out fundamentally it depends on how much is already known about the type of experiment you want to perform in summary technical replicates are just repetition of the same experiment on the same person that means we can take one sample from one person and do a bunch of replicates on that one sample or we could take a bunch of samples from that one person and do experiments on each sample in both cases there technical replicates biological replicates use different biological sources of samples ie different people different plants or different cell lines decide which story you want to tell to determine what types of replicates you need to perform do you want to talk about an individual or a method use technical replicates do you want to talk about a group use biological replicates hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you have any suggestions for future stat quests well put them in the comments below until next time quest on
qBigTkBLU6g,2017-09-25T13:38:28.000000,"StatQuest: Histograms, Clearly Explained","My cat does stats  when she sleeps   I like to do stats how bout you when I'm awake StatQuest Hello and welcome to StatQuest!!! StatQuest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill Today we're going to be talking front histograms, and they're going to be clearly explained Imagine we went out and measured someone and they were this tall. And then we measured someone else And then we measured a whole bunch of people We've measured so many people that the dots overlap; some dots are completely hidden. We could try to make it easier to see the hidden measurements by stacking any that are exactly the same But measurements that are the exact same are rare and a lot of the hidden measurements are still hidden. So, instead of stacking measurements that are the exact same we divide the range of values into bins And stack the measurements that fall in the same bin This, my friends, is a histogram Bam The taller the stack within a bin the more measurements we made that fall into that bin. Duh We can use the histogram to predict the probability of getting future measurements. I would be willing to bet that the next measurement we make is somewhere in this range. Measurements out here are rarer and less likely to happen in the future. If you want to use a distribution to approximate your data or future measurements, histograms are a good way to justify your decision. By the way, if you don't know what a distribution is, there's a StatQuest for that. In this case we might use a normal distribution to approximate the data and future measurements. If the data look like this we might use an exponential distribution to approximate this data and future measurements Note: Figuring out how wide to make the bins is tricky If the bins are too narrow, then they are not much help In this case the bins are so narrow that pretty much every measurement gets its own bin This doesn't give us much more insight than what we had before so it's not very useful And if the bins are too wide they are not much help In this case the bins are so wide that the measurements are split 50/50 All this tells us this how many measurements are above the average and how many are below this is more insight than before, but we can do better Sometimes you have to try a bunch of different bin widths before you get a clear picture In other words don't rely on the default setting of whatever program you're using to draw the histogram You've got to try a bunch of different settings before you're sure that you've got the best histogram you can draw Hooray, we've made it to the end of another exciting StatQuest!!! If you like this StatQuest and want to see more like it Please subscribe It's really easy, and if you have any suggestions for future StatQuests Just let me know in the comments below. Until next time... Quest On!!!"
NEaUSP4YerM,2017-09-18T14:55:30.000000,"StatQuest: t-SNE, Clearly Explained",I'm drawing a graph doesn't it look cool but I didn't know how until I watched a quest hello and welcome to stack quest stack quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about T Snee or tis knee to be honest I don't actually know how it's pronounced but it's gonna be clearly explained I know that bet also this stack quest is by request a couple of people put it in the comments below and I got a couple of emails from other people so I'm doing it because you guys want it here goes if you're watching this stack quest chances are you've seen an example of a tea sneak graph before what tea Snee does is it takes a high dimensional data set and reduces it to a low dimensional graph that retains a lot of the original information if you're not familiar with those terms of taking a high dimensional data set and reducing it to a low dimensional graph you might want to watch the stat quest for pca because i explain what that means in that video here's a basic 2d scatter plot let's do a walk through of how tea stain would transform this graph into a flat one dimensional plot on a number line I'm going to use this super simple example to explain the concepts behind tea Snee so that when you see it applied to a much larger data set a much more complex data set you'll still know how that graph was drawn note if we just projected the data onto one of the axes we just get a big mess that doesn't preserve the original clustering if we project it on to the y axis instead of two distinct clusters we just see a mishmash and the same thing happens if we just project the data onto the x axis what T Snee does is find a way to project data into a low dimensional space in this case the one-dimensional number line so that the clustering in the high dimensional space in this case the 2-dimensional scatterplot is preserved so let's step through the basic ideas of how tasty does this we'll start with the original scatter plot then we'll put the points on the number line in a random order from here on out T sneem moves these points a little bit at a time until it has clustered them let's figure out where to move this first point should it move a little to the left or a little to the right because it is part of this cluster in the two-dimensional scatter plot it wants to move closer to these points but at the same time these points are far away in the scatter plot so they push back these points attract while these points repel in this case the attraction is strongest so the point moves a little to the right BAM now let's move this point a little bit these points attract because they are close to each other in the two-dimensional scatter plot and this point repels a little bit because it is far from the point in the two-dimensional scatter plot so it moves a little closer to the other orange points double bam at each step a point on the line is attracted to points it is near in the scatterplot and repelled by points it is far from triple bail now that we've seen what tea Snee tries to do let's dive into the nitty-gritty details of how it does what it does step 1 determine the similarity of all the points in the scatter plot for this example let's focus on determining the similarities between this point and all of the other points first measure the distance between two points then plot that distance on a normal curve that is centered on the point of interest lastly draw a line from the point to the curve the length of that line is the unscaled similarity I made that terminology up but it'll make sense in just a bit so hold on now we calculate the unscaled similarity for this pair of points now we calculate the unscaled similarity for this pair of points and now we calculate the unscaled similarity for this pair of points etc etc etc using a normal distribution means that distant points have very low similarity values and close points have high similarity values ultimately we measure the distances between all of the points and the point of interest then plot them on a normal curve and then measure the distances from the points to the curve to get the unscaled similarity scores with respect to the point of interest the next step is to scale the unscaled similarities so that they add up to 1 um why do the similarity scores need to add up to 1 it has to do with something I didn't tell you earlier and to illustrate the concept I need to add a cluster that is half as dense as the others the width of the normal curve depends on the density of data near the point of interest less dense regions have wider curves so if these points have 1/2 the density as these points and this curve is half as wide as this curve then scaling the similarity scores will make them the same for both clusters here's an example where I've worked out the math this curve has a standard deviation equal to 1 these are the unscaled similarity values this curve has a standard deviation equal to 2 these points are twice as far from the middle the unscaled similarity values are half of the other ones to scale the similarity scores so that they sum to one you take a score and you divided by the sum of all the scores that equals the scaled score here's how the math works out when the distribution has a standard deviation equals to one we get zero point eight two and zero point one eight as the scaled similarity scores and here's the math for when everything is spread out twice as much we get zero point eight two and zero point one eight the similarity scores on top are equal to the similarity scores below they are the same that implies that the scaled similarity scores for this relatively tight cluster are the same for this relatively loose cluster the reality is a little more complicated but only slightly T Snee has a perplexity parameter equal to the expected density around each point and that comes into play but these clusters are still more similar than you might expect now back to the original scatter plot we've calculated similarity scores for this point now we do it for this point and we do it for all the points one last thing and the scatter plot will be all set with similarity scores because the width of the distribution is based on the density of the surrounding data points the similarity score for this node might not be the same as the similarity to this node so T Snee just averages the two similarity scores from the two directions no big deal ultimately you end up with a matrix of similarity scores each row and column represents the similarity scores calculated from that point of interest red equals high similarity and white equals low similarity I've drawn the similarity from a point of interest to itself as dark red however it doesn't really make sense to say that a point is similar to itself because that doesn't help the clustering so T's knee actually defines that similarity as zero hooray were done calculating similarity scores for the scatter plot now we randomly project the data onto the number line and calculate similarity scores for the points on the number line just like before that means picking a point measuring a distance and lastly drawing a line from the point to a curve however this time we're using a t-distribution a t-distribution is a lot like a normal distribution except the tea isn't as tall in the middle and the tails are taller on the ends the t-distribution is the tea in tea stay we'll talk about why the t-distribution is used in a little bit so using a t-distribution we calculate unscaled similarity scores for all the points and then scale them like before like before we end up with a matrix of similarity scores but this matrix is a mess compared to the original matrix the goal of moving this point is we want to make this row look like this row t Snee moves the points a little bit at a time edit each step it chooses a direction that makes the matrix on the left more like the matrix on the right it uses small steps because it's a little bit like a chess game and can't be solved all at once instead it goes one move at a time BAM now to finally tell you why the t-distribution is used without it the clusters would all clump up in the middle and be harder to see triple bam and now we know how teeny works i've used a really simple example here but the concepts are the exact same for more complicated data sets hooray we've made it to the end of another exciting stack quest if you like this stack quest and want to see more like it please subscribe and if you have any ideas for future stack quests just put them in the comments below until next time quest on
tlf6wYJrwKY,2017-08-31T18:28:40.000000,StatQuest: A gentle introduction to RNA-seq,"even when you're feeling bad even when you've got the flu even when you're down and said you can watch stat quest stat quest hello and welcome to stat quest stat quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're gonna do an introduction to RNA seek we'll start out with a bunch of normal neural cells and a bunch of mutated neuro cells these guys are abnormal the mutated cells behave differently than the normal cells we want to know what genetic mechanism is causing the difference this means we want to look at differences in gene expression so let's figure out a way to do this each cell has a bunch of chromosomes and each chromosome has a bunch of genes and some of the genes are active these wavy lines represent messenger RNA transcripts but this gene is not active high-throughput sequencing tells us which genes are active and how much they are transcribed this is super important so I'm gonna say it again high throughput sequencing tells us which genes are active and how much they are transcribed we can use RNAi seek to measure gene expression in normal cells and then use it to measure gene expression in mutated cells then we can compare the two cell types and figure out what's different in the mutated cells for gene one there's no difference between normal and mutated cells for gene two we see a big difference between normal and mutated cells and for gene three we see a subtle difference between normal and mutated cells there are three main steps for RNA seek one prepare a sequencing library to sequence and three data analysis let's start with preparing an RNA seek library note I'm using the Illumina protocol and sequencer as my example because they are commonly used but keep in mind there are other protocols and sequencers that do it differently step one isolate the RNA step to break the RNA into small fragments we do this because RNA transcripts can be thousands of bases long but the sequencing machine can only sequence short 200 to 300 base pair fragments step 3 convert the RNA fragments into double-stranded DNA double stranded DNA is more stable than RNA and can be easily amplified and modified this leads us to the next step step 4 add sequencing adapters the adapters do two things one they allow the sequencing machine to recognize the fragments 2 they allow you to sequence different samples at the same time since different samples can use different adapters this helps you save time and money notice that this step doesn't work a hundred percent of the time there are a couple of DNA fragments that didn't get sequencing adapters in step 5 we PCR amplify the library only the fragments with sequencing adapters are amplified they are enriched step 6 is quality control you verify the library concentration and you also verified the library fragment lengths to make sure they're not too long or too short hooray now we sequence the library let's see how this is done imagine this is a fragment of DNA we want a sequence it's vertical because that's how it is inside the sequencer actually there about 400 million fragments laid out vertically in a grid I'm just showing you four fragments so your brain doesn't explode this grid is called a flow cell the machine has fluorescent probes that are color coded according to the type of nucleotide they can bind to the probes are attached to the first base in each sequence once the probes have attached the Machine takes a picture of the flow cell from above that looks like this the picture tells the Machine that the first base in the bottom left-hand corner is an A this base is a G and these two bases are C then the machine washes the color off the probes then the probes are bound to the next base in each fragment the machine takes a picture from above and now it knows that this base is C this base is G and these two bases are T then the machine washes the color off the probes and the process repeats until the machine has determined each sequence of nucleotides this is how it works with four DNA fragments with 400 million DNA fragments the matrix is much denser this matrix still isn't 400 million DNA fragments but it illustrates one type of problem that can occur sometimes a probe will not shine as bright as it should and the machine isn't super confident that it is calling the correct color quality scores that are part of the output reflect how confident the machine is that it correctly called a base in this case the faded dot would get a low quality score another reason you might get a low quality score is when there are lots of probes the same color in the same region this is called low diversity and the overabundance of a single color can make it hard to identify the individual sequences the colors will blur together low diversity is especially a problem when the first few nucleotides are sequenced because that is when the Machine determines where the DNA fragments are located on the grid now that we've seen how the machine works let's take a look at the raw data each sequencing read consists of four lines of data the first line which always start with an add symbol is a unique ID for the sequence that follows the second line contains the bases called for the sequenced fragment the third line is always a plus character I have no idea why I asked the internet and I don't think it knows either the fourth line contains quality scores for each base in the sequenced fragment a typical sequence run with 400 million reads will generate a file containing 1.6 billion lines of data now that we understand the raw data and how its generated we need to filter out garbage reads align the high quality reads to a genome and count the number of reads purged so let's talk about how to filter out garbage reads garbage reads are one reads with low quality bass calls and two reads that are clearly artifacts of the chemistry we've already talked about low quality bass calls so now let's talk about artifacts from the chemistry a typical read is a DNA fragment plus adapter sequences but sometimes the adapters just bind to each other and the read is just adapter sequence this is a garbage read now we need to align the reeds to a genome we start with a genome and the genome sequence we then split it into small fragments for reasons that will be explained in a little bit we then create an index of all the fragments and their locations within the genome now we have a sequenced read just like with the genome we split the read into fragments we then match the read fragments to the genome fragments the genome fragments that matched the read fragments will determine a location chromosome and position in the genome why are we breaking the sequences up into small fragments this allows us to align reads even if they are not exact matches to the reference genome imagine this space wasn't in the reference genome because for example my genome is slightly different from yours then this fragment won't match anything in the index but the other fragments will and we will still be able to figure out where the read came from next we have to count the reads per gene once we know the chromosome and position for a read we can see if it falls within the coordinates of a gene or some other interesting feature here are two genes and their coordinates within the genome we have this information for all 20,000 genes in the genome after you count the reads per gene you end up with a matrix of numbers like this the first column contains gene names the human genome has about 20,000 genes so this matrix has about 20,000 rows we're just looking at the first few the remaining columns contained counts for each sample you sequenced they are usually between six and eight hundred plus samples bulk RNA sequencing where a sample is the average of a pool of cells usually six million cells might have three normal samples and three disease state samples or a total of six bulk RNA sequencing is the more common method used these days it was the original method single-cell RNA seek treats each cell like an individual sample so it can generate a lot of samples each row gives counts per sample for a specific gene if this were a single cell RNA seek experiment we would have 20,000 rows genes by 800 plus columns samples giving us at least 16 million values to keep track of that's a huge matrix and it's only going to get bigger since sequencing gets cheaper and people are doing more and more samples the last thing we do before analysis is normalize the data this is because each sample will have a different number of reads assigned to it due to the fact that one sample might have more low-quality reads or another sample might have a slightly higher concentration on the flow cell here's an example sample one has a total of 635 reads assigned to it sample two as 1270 reads assigned to it twice as many reads as sample number one this does not mean that the genes and sample number two were all transcribed twice as much as in sample number one instead it means that sample number two had fewer low-quality reads and might have landed on more spots on the flow cell than sample number one however the read counts make it look like the genes in sample number two were transcribed twice as much as in sample number one so we need to adjust the read counts per gene to reflect the differences in how many reads were assigned to each sample the simplest method is to just divide the read counts per gene by the total mapped to each sample however there are many more sophisticated ways to do this for more details check out the videos in my high-throughput sequencing playlist we started out with a bunch of normal neural cells and a bunch of mutated neural cells then we extracted the messenger RNA then we sequenced aligned counted the reads per gene in each sample and normalized now it's time to analyze the data step 1 in any analysis is always the same plot the data remember the data is a huge matrix if there are only two genes then plotting the data would be easy first we'd replace the gene names with x and y and then just plot the samples on an XY graph sample number one would go at x equals 30 and y equals 24 sample number two would go at x equals 5 and y equals 10 and sample number 3 would go at x equals 13 and y equals 18 but we have 20,000 genes so we would need a graph with 20,000 axes to plot the raw data so we use pca principal component analysis or something like it to plot this data pca reduces the number of axes you need to display the important aspects of the data this is a PCA plot from a real RNA seek experiment done on neural cells the WT samples are normal the ko samples are samples that were mutated by their researchers the KO samples make a nice little cluster in the corner the WT samples are all on the left side but spread out on the y-axis the way these graphs are drawn the most important differences are on the x-axis differences along the y-axis are not as important this means that the biggest differences are between the WT samples and the kayo symbols however when we do further analyses we may wish to exclude WT to excessive self promotion if you want to learn more about how PCA does what it does check out stat quest principal component analysis PCA clearly explained this is a single-cell RNA seek PCA plot from neural cells the colors were added based on what we know about the cells the green cells were stationary and the orange cells moved around the petri dish most of the orange cells are separated from the green cells however there are a few orange cells that seem more like the green cells if we want to determine what is different between these cells and these cells we might exclude these cells from the analysis in summary plotting the data one tells us if we can expect to find interesting differences and two tells us if we should exclude some samples from any downstream analysis now we're ready for step two in our analysis identified differentially expressed genes between the normal and mutant samples this is typically done using our a programming language with either edge R or Dec - and the results are generally displayed using this sort of graph a red dot is a gene that is different between normal and mutant samples black dots are genes that are the same the x-axis tells you how much each gene is transcribed CP M stands for counts per million and is a type of normalization James on the left side are lowly transcribed and James on the right side are highly transcribed the y-axis tells you how big the relative differences between normal and muted log FC equals the log of the fold change between the two sample types we've identified interesting genes now what if you know what you're looking for you can see if the experiment validated your hypothesis if you don't know what you're looking for you can see if certain pathways are enriched in either the normal or mutant gene sets and then what check out the other videos on stat quest you'll find complete tutorials on all kinds of stuff related to RNA seek PCA heatmaps p-values false discovery rates etc all kinds of stuff hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more of them please subscribe and if you have any suggestions for a subject you'd like to see a stat quest on post it in the comments below alright tune in next time for another exciting stat quest"
01205ys-GtI,2017-08-31T18:07:09.000000,StatQuest: How to make a Mean Pizza Crust!!!,sometimes you've got to do what you got to do step quest hello and welcome to stat quest stat quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to talk about how to make a mean pizza crust Here I am in my kitchen and the first thing I need to do is measure out 500 grams of bread flour BAM and nailed it then I measure an eighth of a teaspoon of instant yeast and then I measure out 500 grams of water close enough then I just mix it all together that's all there is to do for the first step now we cover it and let it rest for 8 to 12 hours after waiting 8 to 12 hours I add another 500 grams of bread flour now I add 20 grams of salt lastly I add 200 grams of water now I mix the dough together I start by stirring it around but then I switched to a pinching motion where I use my thumb and index finger to pinch through the wet dough to get the dry flour all the way into the middle and I just pinch and stir and sort of grab I'm gonna move the dough around to try to get all of the flour and the salt and the new water corporated into the existing wet dough once I've combined all the new flour water and salt I've got a lot of sticky dough on my hand the way to get it off is to just run a little water on your hand you don't have to wash it off just get a little water in your hand and then go back to kneading just kind of reaching under you know folding the dough over and a lot of the dough that was stuck to your hand will start coming off and what I do is I do this a couple of times I'll get more water in my hand and I'll just need a little bit more and over time my hand will gradually get pretty clean now that I've got the dough off my hand I cover it up and let it rest for 15 minutes here's the dough after letting it rest for 15 minutes now I get a lot of water in my hands and I reach under the dough scoop under pull up and fold over then rotate 90 degrees and I do this four times so I've folded the dough over on itself four times put it in a nice little ball and then let it rest for another 15 minutes you need to do this every fifteen to thirty minutes for the next one to two hours the goal is to let the dough get kind of stretchy and also don't let the yeast start activating you'll know when you're done when you can start feeling little bubbles of air in the dough after one or two hours of folding every 15 to 30 minutes it's time to prepare the dough into little balls that can be made into pizza later on I start by pouring a little bit of oil on the dough and then I pour a little bit of oil and some Tupperware containers there's enough dough for six pizza crusts but what I like to do is make three pizzas and a loaf of bread now that I've prepared the containers I rub the oil on the dough and the divide the dough by half by pinching it right down the middle since I'm going to use half of this dough for a loaf of bread I fold it into a ball and then I put it in this wicker basket that I've put rice flour and bran flakes in this just makes it look fancy it's no big deal now I go back to the remaining dough and split it into three pieces then I shape each piece into a ball I then use that ball to rub the oil in the Tupperware around so everything's nice and coated with oil and then I just repeat for the next two pieces of dough once I've finished shaping each piece of dough I put the lids on Tupperware containers and I put the loaf of bread in a plastic bag it's one of those produce bags you get in the grocery store then I put it in the refrigerator and I let it rest for at least eight hours but you can actually let it rest for 24 or even a couple of days it's no big deal it also freezes well once I'm ready to make pizza I put a pizza stone as close to the broiler element as I can and I preheat it for 10 minutes on high now we're ready to make pizza I have a pizza peel and I spread a little cornmeal on top of it I want it to be nice and even now I get a piece of dough out of one of the toughest and I press it in to some flour just to get flour on one side the bottom side now I stretch the dough out into a nice round circle that I can use for a pizza once I have a nice shape for my pizza I spread a little bit of sauce on top then I add a little bit of cheese and then I add whatever toppings I have around the house once the pizza is ready to go I slide it on top of the pizza stone and turn the broiler down then I let it cook for five minutes and then turn the broiler back up and I watch it like a hawk for the next minute sometimes just 30 seconds it'll burn quick so keep your eye on it once the pizza's done I just scoop it out with a big pan that's all there is to it voila if you want to bake bread preheat the oven to 475 degrees Fahrenheit with a cast-iron pot in there when it's preheated take the lid off and then flip the bread in and then cut slits in the top of the loaf now put the lid back on the pot and bake it for about 25 minutes after 25 minutes take the lid off the top of the pot and bake for another 10 minutes when the breads done baking pull it out of the oven and use a spatula to pop it out onto a cooling rack this bread tastes great fresh but you know what it tastes even better if you freeze it and then reheat it I have no idea why but it just tastes better that way hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more like it feel free to subscribe tune in next time for another exciting stat quest
NF5_btOaCig,2017-08-07T15:26:19.000000,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",stat Quest stat Quest stat Quest yeah hello and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're doing part two of our series on General linear models last time we talked about how to do linear regression this time we're going to talk about how to use those exact same techniques to do tea tests and a Nova we'll do this using something called a design Matrix which is a cool concept that will expand upon in future stat quests on General linear models let's start with a super quick review of linear regression last time we measured mouse weight and mouse size and we wanted to learn two things from it we wanted to learn how useful mouse weight was for predicting Mouse size R squar told us this and we wanted to know if the relationship was due to Chance the P value told us this now let's see if we can apply those Concepts to a T Test in this specific example we're going to be comparing gene expression between control mice and mutant mice mutant mice are just normal mice that have a specific Gene that's been knocked out and is no longer functioning correctly the goal of a T Test is to compare means and see if they are significantly different from each other if the same method can calculate P values for a linear regression and a t test then we can easily calculate P values for more complicated situations so now I'm going to walk you through the steps for using the techniques from linear regression to do a t test on the left side of the screen our remind you how each step applies to linear regression on the right side of the screen I'll show you how those steps apply to T tests step one ignore the xais and find the overall mean to emphasize that we want to focus on the Y AIS I've removed the labels on the X AIS here are the overall means for the linear regression and the T Test the next step is to calculate the sum of squared residuals around the mean this is SS mean these are the residuals the distance from the data points to the lines in this case the lines are the overall means bam calculating the sum of squared residuals around the mean was easy step three fit a line to the data Note this is when we start caring about the X AIS again on the left side we have the least squares fit to the data however how do we do a least squares fit to a T Test let's start by just fitting a line to the Control Data we start by finding a least squares fit to the Control Data it turns out that the mean is the least squares fit the mean intercepts the Y AIS at 2.2 this is the equation for horizontal line that intercepts the Y AIS at 2.2 thus this is the line that we fit to the Control Data now let's fit a line to the mutant data the least squares fit is the mean of the mutant data the mean intercepts the y axis at 3.6 this is the equation for a horizontal line that intercepts the Y AIS at 3.6 thus this is the line that we fit to the mutant data data we have fit two lines to the data originally when we did the regression we fit a single line to the data however there is a way to combine these two lines into a single equation this will make the steps for computing F the exact same for the regression and the test which in turn means a computer can do it automatically this is key because we don't want to do this by hand hand ever this is going to look a little weird but just bear with me keep in mind that the goal is to have a flexible way for a computer to solve this and every other least squares based problem without having to create a whole new method each time this is the equation which combines both lines for this point we have 1 time the mean of the Control Data 0er times the mean of the mutant data plus the residual yes this is strange especially multiplying the mutant mean by zero but bear with me if we multiplied things out the equation for this point would be y = 2.2 plus the residual and that sort of makes sense but just bear with me this is the equation for the next point the only difference is the residual this one is smaller this is the equation for the next Point again the only difference is the residual this is the equation for the next point and again the only difference is the residual this is the equation for the first point in the mutant data set now we are multiplying the control mean by zero and multiplying the mutant mean by one these are the equations for the remaining points now let's focus on the ones and zeros they function like on and off switches for the two means a one turns the mean on and a zero turns the mean off when we isolate the ones and zeros they form a matrix called a design Matrix the design Matrix can be combined with an abstract version of the equation to represent a fit to the data column one turns the control mean on or off column two turns the mutant mean on or off in practice the role of each column is assumed and the equation is written out like this y equals the mean of the Control Data plus the mean of the mutant data now that we have the fit for the control and mutant data down to a single equation plus design Matrix we can move on to calculating F and the P value so step four calculate the sum of squares of the residuals around the fitted lines with the linear regression that means the sum of these squared residuals the sum of squares around the fit for the T Test is the sum of these squared residuals to viw what we've done so far we've calculated the sum of squared residuals around the mean and then we calculated the sum of squared residuals around the fitted line now we can just plug these things in to our equation for f f will lead to a P value for the linear regression p mean refers to the number of parameters in the equation for the mean Mouse size that's one parameter in the T Test p mean refers to the number of parameters in the equation for the mean of the gene expression that's also just one parameter for the linear regression P fit refers to the number of parameters in the equation for the fitted line in this case that's two the parameters are the intercept and the slope for the T Test P fit refers to the number of parameters in the line that we fit to the T Test data in this case P fit equals 2 because we had to estimate two parameters one for the mean of the control data and one for the mean of the mutant data now we can calculate a P value for the T Test bam let's review what we've done so far here's the original data gene expression for control mice and mutant mice the first thing we did is we calculated the sum of squares of the resist residuals around the overall mean then we calculated the sum of squares of the residuals around the fit in order to do this with a single equation we had to create a design Matrix once we've calculated the sums of squares all we have to do is plug the values into the equation for f and then we'll get our P value now let's do an anova an NOA tests if all five categories are the same here we have control and mutant mice just like before but we also have control and mutant mice on a funky diet and we also have heterozygote mice the first thing we do is calculate the sum of squares around the mean we do this just like before we calculate an overall mean value for all of the categories and then Square the residuals and sum them up no big deal the equation for the overall mean is just y equals mean expression that equation only has a single parameter the overall mean so p mean equals 1 now we calculate the sum of squares around the fitted lines the equation for the fitted lines has five parameters one for each mean therefore P fit equals five here's what the design Matrix looks like one column per category now now that we've calculated the sum of squares around the mean and the sum of squares around the fit along with p mean and P fit we can plug those values in and calculate F triple bam if we can calculate F then we've got ourselves a P value one last important detail before we're done the design matrices that I've shown you are not the standard design matrices used for doing T tests and a NOA this is what we used for the T Test in this stat Quest but this is a more common design Matrix for the same thing both design matrices will get the job done it's just the one on the right is more commonly used we'll talk about this one and other more elaborate designs in the next stat Quest hooray we've made it to the end of another exciting stat Quest if you like this and would like to see more stat quests like it feel free to subscribe and if you if you have any suggestions for future stat quests put them in the comments below tune in next time for another exciting stack Quest
XepXtl9YKwc,2017-07-31T17:26:45.000000,"Maximum Likelihood, clearly explained!!!",stack Quest check it it's bad to the bone stack quiz check it out it's bad to the bone hello and welcome to stack Quest stack Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about maximum likelihood let's say we weighed a bunch of mice the goal of Maximum likelihood is to find the optimal way to fit a distribution to the data there are lots of different types of distributions for different types of data here's a normal distribution here's what an exponential distribution looks like and here's what a gamma distribution looks like and there are many more the reason you want to fit a distribution to your data is it can be easier to work with and it is also more General it applies to every experiment of the same type in this case we think the weights might be normally distributed that means we think it came from this type of distribution normally distributed means a number of things first we expect most of the measurements for for example mouse weights to be close to the mean or average and we see lo and behold in our data set most of the mice weigh close to the average we also expect the measurements to be relatively symmetrical around the mean although the measurements are not perfectly symmetrical around the mean they are not crazy skewed to one side either this is pretty good normal distributions come in all kinds of shapes and sizes they can be skinny medium or large boned once we settle on the shape we have to figure out where to Center the thing is one location better than another before we get too technical let's just pick any old normal distribution and see how well it fits the data this distribution says most of the values you measure should be near my average the distribution average is the black dotted line in this case that's different from the average of the actual measurements unfortunately most of the values we measured are far from the distribution's average according to a normal distribution with a mean value over here the probability or likelihood of observing all these weights is low what if we shifted the normal distribution over so that it mean was the same as the average weight according to a normal distribution with a mean value here the probability or likelihood of observing these weights is relatively high if we kept Shifting the normal distribution over then the probability or likelihood of observing these measurements would go down again we can plot the likelihood of observing the data over over the location of the center of the distribution we start on the left side and we calculate the likelihood of observing the data and then we shift the distribution to the right and recalculate we just do this all the way down the data once we've tried all the possible locations we could center the normal distribution on we want the location that maximizes the likelihood of observing the weights we measured this location for the mean maximizes the likelihood of observing the weights we measured thus it is the maximum likelihood estimate for the mean in this case we're specifically talking about the mean of the distribution not the mean of the data however with the normal distribution those two things are the same great now we have figured out the maximum likelihood estimate for the mean now we have to figure out the max maximum likelihood estimate for the standard deviation again we can plot the likelihood of observing the data over different values for the standard deviation now we found the standard deviation that maximizes the likelihood of observing the weights we measured this is the normal distribution that has been fit to the data by using the maximum likelihood estimations for the mean and the standard deviation now when someone says that they have the maximum likelihood estimates for the mean or the standard deviation or for something else you know that they found the value for the mean or the standard deviation or for whatever that maximizes the likelihood that you observed the things that you observed terminology Alert in everyday conversation probability and likelihood mean the same thing however in stats land likelihood specifically refers to this situation we've covered here where you are trying to find the optimal value for the mean or standard deviation for a distribution given a bunch of observed measurements this is how we fit a distribution to data hooray we've made it to the end of another exciting stack Quest if you like this stack Quest and want to see more like it please subscribe it's super easy easy just click the little button below and if you have any suggestions for other stat quests that I could do put them in the comments all right until next time Quest on
nk2CQITm_eo,2017-07-24T19:06:27.000000,"Linear Regression, Clearly Explained!!!","Say that on a boat headed towards that quest Join me on this boat. Let's go to stab quest it's super cool Hello, and welcome to static quest Static Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill Today, we're going to be talking about linear regression Aka General Linear models part one there's a lot of parts to linear models But it's a really cool and powerful concept so let's get right down to it. I Promise you I have lots and lots of slides that talk about all the nitty-Gritty details behind linear regression But first let's talk about the main ideas behind it The first thing you do in linear regression is use least squares to fit a line to the data The second thing you do is calculate R squared lastly calculate a p-value for R squared There are lots of other little things that come up along the way but these are the three most important concepts behind linear regression in the stat Quest fitting align to Data we talked about fitting align to Data duh But let's do a quick review. I'm Going to introduce some new terminology in this part of the video so it's worth watching even if you've already seen the earlier stat quest That said if you need more details check that stat quest out For this review we're going to be talking about a data set where we took a bunch of mice And we measured their size, and we measured their weight our Goal is to use mouse weight as a way to predict mouse size first draw a line through the data Second measure the distance from the line to the Data square each distance and then add them up Terminology alert the distance from the line to the Data point is called a residual third rotate the line a little bit With the new line measure the residuals square them and then sum of the squares Now rotate the line a little bit more sum up the squared residuals ETc, Etc, ETc. We rotate and then sum of the squared residuals Rotate that sum of the squared residuals just keep doing that After a bunch of rotations you can plot the sum of squared residuals and corresponding rotation so in this graph We have the sum of squared residuals on the y-Axis and the different rotations on the x-Axis Lastly you find the rotation that has the least sum of squares More details about how this is actually done in practice are provided in a stat quest on fitting a line to data So we see that this rotation is the one with the least squares, so it will be the one to fit to the data This is our least squares rotation Super imposed on the original Data Bam Now we know why the method for fitting a line is called least squares Now we have fit along to the data. This is awesome Here's the equation for the line least Squares estimated two parameters A yxs intercept and a slope Since the slope is not zero it means that knowing a mouse's weight will help us make a guess About that mouse's size How good is that guess? Calculating r-squared is the first step in determining. How good that guess will be? the scat Quest R-Squared explained talks about you got it R-Squared Let's do a quick review. I'm also going to Introduce some additional terminology So it's worth watching this part of the video even if you've seen the original stat quest on r-Squared first Calculate the average mouse size okay, I've just Shifted all the Data points to the y access to emphasize that at this point We are only interested in mouse size Here I've drawn a black line to show the average mouse size bam Now some the squared residuals Just like in least squares We measure the distance from the mean to the data point and square it and then add those squares together Terminology alert we'll call this ss. Mean for sum of squares around the mean Note the sum of squares around the mean equals the data minus the mean squared The variation around the mean equals the data minus the mean squared Divided by n n Is the sample size in this case N equals 9? The shorthand notation is the variation around the mean equals the sum of squares around the mean? Divided by N. The sample size Another way to think about variance is as the average sum of squares per mouse Now go back to the original plot and sum of the squared residuals around our least squares fit We'll call this ss. Fit for the sum of squares around the least squares fit The sum of Squares around the least squares fit is the sum of the distances between the data and the line squared Just like with the mean the variance around the fit is the distance between the line and the data squared Divided by N. The sample size The shorthand is the variation around the fitted line Equals the sum of Squares around the fitted line divided by n. The sample size Again, we can think of the variation around the fit as the average of the sum of squares around the fit for each mouse in general the Variance of something equals the sum of squares Divided by the number of those things in other words. It's an average of sum of Squares I Mention this because it's going to come in handy in a little bit. So keep it in the back of your mind Okay, let's step back a little bit. This is the raw variation in mouse size and This is the variation around the least squares line there's less variation around the line that we fit by least squares that is to say the residuals are smaller as a result we say that some of the variation in mouse size is explained by taking mouse weight into account in Other words Heavier mice are bigger Lighter mice are smaller R-Squared tells us how much of the variation in Mouse size can be explained by taking Mouse weight into account? This is the formula for R. Squared. It's the variation around the mean Minus the variation around the fit divided by the variation around the Let's look at an example in this example the variation around the mean equals Eleven Point one and the variation around the fit Equals four point four so we plug those numbers into the equation The result is that R squared equals zero point six? Which is the same thing as saying 60%? This means there is a 60% reduction in the variance when we take the mouse weight into account Alternatively we can say that mouse weight explains 60 percent of the variation in mouse size We can also use the sum of squares to make the same calculation This is because when we're talking about variation everything's divided by n. The sample size Since everything scaled by N We can pull that term out and just use the raw sum of squares In this case the sum of squares around the mean equals 100 and the sum of squares around the fit equals 40 plugging those numbers into the equation Gives us the same value we had before R squared equals zero point six which equals 60 percent? 60 percent of the sums of squares of the mouse size can be explained by mouse weight Here's another example We're also going to go back to using variation in the calculation since that's more common in This case knowing mouse weight means you can make a perfect prediction of mouse size The variation around the mean is the same as it was before Eleven point one, but now the variation around the fitted line equals zero because there are no residuals Plugging the numbers n. Gives us an r-Squared equal to one which equals one hundred percent in This case mouse weight explains 100 percent of the variation in mouse size Okay, one last example In this case knowing mouse weight doesn't help us predict mouse size If someone tells us they have a heavy mouse well that mouse could either be small or large with equal probability Similarly if someone said they had a light mouse well again We wouldn't know if it was a big mouse or a small mouse because each of those options is equally likely Just like the other two examples the variation around the mean is equal 11.1 However in this case the variation around the fit is also equal 11.1 so we plug those numbers in and we get R squared equals zero which equals zero percent in This case mouse wait doesn't explain any of the variation around the mean when calculating the sum of squares around the mean we collapse the points onto the Y-Axis just to emphasize the fact that we were ignoring mouse weight but we could just as easily draw a line y equals the mean mouse size and Calculate the sum of squares around the mean around that in This example we applied R squared to a simple equation for a line y equals zero point one plus zero point seven eight times x This gave us an r-Squared of sixty percent Meaning 60 percent of the variation in mouse size could be explained by mouse weight But the concept applies to any equation no matter how complicated First you measure square and sum the distance from the data to the mean In measure Square and sum the distance from the data to the complicated equation Once you've got those two sums of squares. Just plug them in and you've got R squared Let's look at a slightly more complicated example Imagine we wanted to know if mouse weight and tail length did a good job predicting the length of the mouse's body? So we measure a bunch of mice To plot this data. We need a three-dimensional graph We want to know how well weight and tail length predict body length The first mouse we measured had weight equals 2.1 Tail length equals 1.3 and body length equals 2.5 So that's how we plot this data on this 3D graph Here's all the data in the graph The larger Circles are points that are closer to us and represent mice that have shorter tails The smaller Circles are points that are further from us and represent mice with longer tails Now we do a least-squares fit Since we have the extra term in the equation Representing an extra dimension we fit a plane instead of a line Here's the equation for the plane the Y-value represents body length least Squares estimates three different parameters The first is the y-intercept that's when both tail length and mouse weight are equal to zero The second parameter zero point seven is for the mouse weight the last term zero point five is for the tail length if We know a mouse's weight and tail length. We can use the equation to guess the body length for example given the weight and tail length for this mouse the equation predicts this body length Just like before we can measure the residuals square them and then add them up to calculate R squared now if the tail length where the z axis is Useless and doesn't make the sum of squares fit any smaller then least squares We'll ignore it by making that parameter equal to zero in this case Plugging the tail length into the equation would have no effect on predicting the mouse size this means Equations with more parameters will never make the sum of squares around the fit worse than equations with fewer parameters in other words this equation mouse size equals 0.3 plus mouse weight plus flip of a coin plus favorite color plus Astrological sign plus extra stuff will never perform worse than this equation mouse size equals 0.3 plus mouse weight this is because Li squares will cause any term that makes sum of squares around the fit worse to be multiplied by 0 and in a sense no longer exists Now due to random chance There is a small probability that the small mice in the data set might get heads more frequently than large mice If this happened, then we'd get a smaller sum of squares fit and a better r-squared Wha-wha here's the frowny face of sad times? The more silly parameters we add to the equation the more Opportunities we have for random events to reduce sum of squares fit and result in a better r-squared Thus people report an adjusted r-Squared value that in essence Scales R-Squared by the number of Parameters R Squared is awesome, but it's missing something What if all we had were two measurements? We'd calculate the sum of squares around the mean in this case. That would be 10 Then we calculate the sum of squares around the fit which equals 0 The sum of Squares around the fit equals 0 because you can always draw a straight line to connect any two points What this means is when we calculate r squared by plugging the numbers end we're going to get 100% 100% is a great number we've explained all the variation But any two random points will give us the exact same thing. It doesn't actually mean anything We need a way to determine if the r-Squared value is statistically significant. We need a p-value Before we calculate the p-value Let's review the main Concepts behind R-Squared one last time The general equation for R Squared is the variance around the mean minus the variance around the fit divided by the variance around the mean in Our example this means the variation in the mouthsize - the variation after taking weight into account divided by the variation in mouth size in Other words R. Squared equals the variation in mouth size explained by weight divided by the variation mouth size without taking weight into account in This particular example R. Squared equals zero point six Meaning we saw 60 percent reduction in variation once we took mouse weight into account Now that we have a thorough understanding of the ideas behind r squared let's talk about the main ideas behind Calculating a p-value for it The P-value for R. Squared comes from something called f f is equal to the variation in mouth size explained by weight divided by the variation mouth size not explained by weight the numerators for r-Squared and for f are the same That is to say it's the reduction in Variance when we take the weight into account The denominator is a little different These dotted lines the residuals Represent the variation that remains after fitting the line. This is the variation that is not explained by weight So together we have the variation in mouth size explained by weight divided by the variation in mouth size not explained by weight Now let's look at the underlying mathematics just as a reminder. Here's the equation for R squared This is the general equation that will tell us if R squared is significant The meat of these two equations are very similar and rely on the same sums of squares Like we said before the numerators are the same in our mouth size and weight example the numerator is the variation of mouth size explained by weight and The sum of Squares around the fit is just the residuals squared and summed up around the fitted line So that's the variation that the fit does not explain These numbers over here are the degrees of freedom? They turn the sums of squares into variances. I'm Going to dedicate a whole stack west to degrees of freedom but for now Let's see if we can get an intuitive feel for what they're doing here Let's start with these P fit is the number of parameters in the fit line Here's the equation for the fit line in a general format. We just have the y-intercept plus the slope times x The Y-intercept and the slope are two separate parameters That means p fit equals two P mean is the number of parameters in the mean line? in general that equation is y equals the y-intercept that's what gives us a horizontal line that cuts through the Data in This case the y-intercept is the mean value This equation just has one parameter Thus P mean Equals 1 both equations have a parameter for the y-intercept However, the fit line has one extra parameter the slope in our example This slope is the relationship between weight and size in This example p. Fit minus p mean equals 2 minus 1 Which equals 1 The fit has one extra parameter mouse weight Thus the numerator is the variance explained by the extra parameter in Our example that the variance in mouse size explained by mouse weight if We had used mouse weight and tail length to explain variation in size Then we would end up with an equation that had three parameters and p fit would equal 3 Thus P fit minus P mean would equal 3 minus 1 Which equals 2 Now the fit has two extra parameters mouse weight and tail length With the fancier equation for the fit the numerator is the variance in mouse size explained by mouse weight and tail length Now let's talk about the denominator for our equation for f The denominator is the variation in mouse size not explained by the fit that Is to say it's the sum of squares of the residuals that remain after we fit our new line to the data Y divided sum of squares fit by N. Minus p fit instead of just n Intuitively the more parameters you have in your equation the more data You need to estimate them for example. You only need two points to estimate a line but you need three points to estimate a plane if The fit is good then the variation explained by the extra parameters in the fit will be a large number and the Variation not explained by the extra parameters in the fifth will be a small number that makes f a really large number Now that question we've all been dying to know the answer to how do we turn this number into a p-value conceptually generate a set of random Data calculate the mean and the sum of squares around the mean Calculate the fit in the sum of squares around the fit Now plug all those values into our equation for f and that will give us a number in this case that number is 2 Now plot that number in a histogram Now generate another set of random Data calculate the mean and the sum of squares around the mean Then calculate the fit and the sum of squares around the fit Plug those values into our equation for f and in this case we get f equals 3 so we then plug that value into our histogram and Then we repeat with yet another set of random Data in this case We got f equals 1 that's plotted on our histogram, and we just keep generating more and more random data sets Calculating the sums of squares plugging them into our equation for f and plotting the results on our histogram now Imagine we did that hundreds if not millions of times When we're all done with our random data sets we return to our original data set We then plug the numbers into our equation for f in this case. We got f equals 6 the P value is the number of More Extreme values divided by all of the values So in this case we have the value at f equals 6 and the value at f equals 7 divided by all the other Randomizations that we created originally if this concept is confusing to you I have a stat quest that explains p values so check that one out Bam You can approximate the histogram with a line in Practice rather than generating tons of random Data sets people use the line to calculate the P value Here's an example of one standard f distribution that people use to calculate p values the degrees of freedom Determine the shape the Red Line represents another Standard F distribution That people use to calculate P values in This case the sample size used to draw the red line is smaller than the sample size used to draw the blue line Notice that when n minus p fit equals 10 the distribution tapers off faster This means that the p value will be smaller when there are more samples relative to the number of parameters in the fit equation triple Bam Hooray we finally got our p value now. Let's review the main ideas Given some data that you think are related linear regression Quantifies the relationship in the data This is R squared this needs to be large It also Determines how reliable that relationship is This is the p value that we calculated with f this needs to be small you Need both to have an interesting result Hooray We've made it to the end of another exciting stat quest wow this was a long one I hope you had a good time If you like this and want to see more stat quest like it wants you subscribe to my channel it's real easy just click the red button and If you have any ideas of that quest that you'd like me to create just put them in the comments below That's all there is to it alright. Tune in next time for another really exciting stat quest"
yQhTtdq_y9M,2017-07-17T15:31:28.000000,What is a (mathematical) model?,it's our boss that quest gray hello and welcome to stat quest stat quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about what is a model the word model is used in a lot of contexts when I was a kid a model was a toy that I glued together when I was a little older a model with someone who wore fancy clothes now that I'm an adult the term model has more to do with math and statistics for example I might model mouth size with Mouse wait what does this mean and why would I want to do this in this context model refers to a relationship the model is a way to explore the relationship between weight and size in this case the relationship is pretty obvious the heavier the mouse the bigger the mouse and the lighter the mouse the smaller the mouse a model can also be an equation here we have the equation for the line that we have fit to the data the equation is a mathematical model the model or equation can tell us about mice we haven't measured yet someone might want to know how large a mouse might be if it weighs 4 units so we plug that value into our equation and we get Mouse size equals 3.3 the model predicts that a mouse that weighs 4 units will be 3.3 units big the model or equation is an approximation of the real data here the dotted lines show the distance from the model to the actual data points a lot of Statistics is dedicated to determining if a MA makes a good or bad approximation of the data right now I'm working on a bunch of new stat quests to cover these subjects specifically this includes linear regression general linear models t-tests ANOVA's and F tests and all kinds of really exciting things that I can't wait to cover sometimes a model isn't a straight line in this case the model helps us understand the relationship between a drug and hair growth for aging men we see that after a point increasing the dosage doesn't help grow any more hair models can be simple or complex here we are using two genes x and z to model mouth size in summary we use models to explore relationships for example I might be interested in the relationship between gene X and a mouth size we then use statistics to determine how useful and how reliable our model is hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more of them please subscribe and if you have any static quest ideas of your own that you'd like me to cover just write them down in the comments below alright tune in next time for another really exciting static quest
fHLhBnmwUM0,2017-07-11T00:25:59.000000,Boxplots are Awesome!!!,stat Quest is so cool when you watch it you're no fool that's what makes it cool hello and welcome to stack Quest stack Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about box plots box plots are bad to the bone check them out sometimes box plots are called box and whisker plots here's the box and here are the whiskers the line in the middle of the box is the median value 50% of the data is above the median 50% of the data is below the median medians are useful because they're not as swayed by outliers as the typical mean or average is George Carlin host of the children's television show shining Time Station once said think of how stupid the average person is and realize that half of them are stupider than that I think Mr Carlin was referring to the median not the average but that's okay it's still funny within the Box itself we have 25% of the data above the median and 25% of the data below the median so 50% of the data is within the Box outliers are plotted as dots beyond the whiskers totes Bad to the Bone it's common to Overlay the original data onto the box plots now it's easy to see that this guy didn't get many measurements and this guy got tons of measurements this means we'll be more confident in the statistics calculated from this guy super totes Bad to the Bone bask in the awesomeness of box plots compared to the traditional bar plots both graphs show the exact same data hooray we've made it to the end of another exciting stat Quest tune in next time for another fun adventure if you'd like what you've seen and want to see more of it please subscribe and if you have something specific that you'd like to learn more about just put it in the comments below
HVXime0nQeI,2017-06-26T18:54:55.000000,"StatQuest: K-nearest neighbors, Clearly Explained",[Music] St Quest St Quest stack Quest hello and welcome to stack Quest stack Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about the K nearest neighbors algorithm which is a super simple way to classify data in a nutshell if you already had a lot of data that Define these cell types we could use it to decide which type of cell this guy is let's see it in action step one start with a data set with known categories in this case we have different cell types from an intestinal tumor we then cluster that data in this case we used PCA step two add a new cell with unknown category to the plot we don't know this cell's category because it was taken from another tumor where the cells were not properly sorted and so what we want to do is we want to classify this new cell we want to figure out what cell it's most similar to and then we're going to call it that type of cell step three we classify the new cell by looking at the nearest nearest annotated cells I.E the nearest neighbors if the K in K nearest neighbors is equal to one then we will only use the nearest neighbor to define the category in this case the category is green because the nearest neighbor is already known to be the green cell type if k equals 11 we would use the 11 nearest Neighbors in this case the category is still green because the 11 cells that are closest to the unknown cell are already green now the new cell is somewhere more interesting it's about halfway between the green and the red cells if k equals 11 and the new cells between two or more categories we simply pick the category that gets the most votes in this case seven nearest neighbors are red three nearest neighbors are orange one nearest neighbor is green since red got the most votes the final assignment is red this same principle applies to heat Maps this heat map was drawn with the same data and clustered using hierarchical clustering if our new cell ended up in the middle of the light blue cluster and if k equals 1 we just look at the nearest cell and that cell is light blue so we classify the unknown cell as a light blue cell if k equals 5 we'd look at the five nearest cells which are also light blue so we'd still classify the unknown cell as light blue if the new cell ended up closer to the edge of the light blue cells and k equals 11 then we take a vote seven nearest neighbors are light blue and four are light green so we'd still go with light blue if the new cell is right between two categories well if K is odd then we can avoid a lot of ties if we still get a tied vote we can flip a coin or decide not to assign the cell to a category before we go let's talk about a little machine learning SL data mining terminology the data used for the initial clustering the data where we know the categories in advance is called training data bam a few thoughts on picking a value for K there is no physical or biological way to determine the best value for K so you may have to try out a few values before settling on one do this by pretending part of the training data is unknown and then what you do is you categorize that unknown data using the K nearest neighbor algorithm and you assess how good the new categories match what you know already low values for K like k equal 1 or k equals 2 can be noisy and subject to the effects of outliers large values for K smooth over things but you don't want K to be so large that a c category with only a few samples in it will always be outvoted by other categories hooray we've made it to the end of another exciting stack Quest if you like this stack Quest go ahead and subscribe to my channel and you'll see more like it and if you have any ideas of things you'd like me to do a stack Quest on feel free to put those ideas in the comments okay guess that's it tune in next time for another exciting stag Quest
7xHsRkOdVwo,2017-06-20T12:33:24.000000,StatQuest: Hierarchical Clustering,[Music] going on a quest on a stat Quest stat Quest hello and welcome to stat Quest today we're going to be talking about hierarchical clustering hierarchical clustering is often associated with heat Maps if you're not already familiar with what heat maps are just know that the columns typically represent different samples and that the rows typically represent measurements from different genes red typically signifies High expression of a gene and blue or purple means lower expression for a gene hierarchical clustering orders the rows and or the columns based on similarity this makes it easy to see correlation in the data for example these samples express the same genes and these genes behave the same on the left we have a heat map without hierarchical clustering and on the right we have a heat map with hierarchical clustering so you can see that the clustering makes a big difference on how the data is presented heat Maps often come with dendrograms so we'll talk about those too let's get started we'll start with a simple example here we've got a simple heat map that has three samples and four genes for this example we are just going to Cluster or reorder the rows or the genes conceptually the first step is to figure out which Gene is most similar to Gene number one genes number one and two are different we can tell because the colors are very different Gene one is highly expressed in Sample number one so it has a red color Gene 2 however is not highly expressed on Sample number one so it has a blue color in Sample number three Gene one is lowly expressed so it's blue and Gene 2 is highly expressed so it's red genes 1 and three are similar so that means in Sample one both Gene 1 and three are red they're highly expressed and in Sample three they're both blue meaning they're lowly expressed genes one and four are also similar however Gene number one is most similar to Gene number three so the second step is to figure out what Gene is most similar to Gene number two so we do all the comparisons and we see that Gene number two is most similar to Gene number four and then we do the same thing for Gene number three and then Gene number four in Step number three we look at the different combinations and figure out which two genes are the most similar once we've done that we merge them into a cluster in this case genes number one and three are more similar than any other combination of genes so genes 1 and three are now cluster number one step four go back to step one but now treat the new cluster like it's a single Gene so in step one we figure out which Gene is most similar to Cluster number one cluster number one is most similar to Gene number four and we figure out which Gene is most similar to Gene number two in this case Gene number two is most similar to Gene number four but notice that we compared Gene number two to Cluster number one and then we do the same thing for Gene number four of the different combinations figure out which two genes are the most similar now merge them into a cluster in this case genes 2 and four are the most similar combination so we've merged them into a cluster now we go back to Step One however since all we have left are two clusters we merge them bam we're all done hierarchical clustering is usually accompanied by a dendrogram it indicates both the similarity and the order that the Clusters were formed cluster number one was formed first and is is most similar it has the shortest Branch cluster number two was second and is the second most similar it has the second shortest Branch cluster number three which contains all of the genes was formed last it has the longest Branch now let's go over a few nitpicky details remember the first step figure out which Gene is most similar to Gene number one well we have to Define what most similar means the method for determining similarity is arbitrarily chosen however the ukian distance between genes is used a lot let's look at an example we'll use a very simple heat map that just has two samples and two genes now we're displaying the values that underly the the colors that we have in the heat map the ukian distance between genes 1 and two is just the square root of the difference in Sample number one squared plus the difference in Sample number two squared here we'll just plug in the values for sample number one we have 1.6 minus 0.5 now let's plug in the values to calculate the difference in Sample number two we have 0.5 minus -1.9 doing the subtraction gives us the square < TK of 2.12 + 2.4 2ar we can think of these values within the parentheses as sides on a triangle so on the x axis we have the distance between Gene 1 and Gene 2 in Sample number one and on the Y AIS we have the distance between Gene 1 1 and two in Sample number two the hypotenuse is the distance between genes 1 and two the Pythagorean theorem says that the hypotenuse equals theare < TK of x^2 + y^2 in this case that means the Square t of 2.12 + 2.4 SAR and that gives us 3.2 the distance between Gene number one and Gene number two when we have more samples we just extend the equation it's no big deal the ukian distance is just one method there are lots more including the Manhattan distance the Manhattan distance is just the absolute value of the differences so instead of squaring the differences and then taking the square root all we do is take the absolute value of the differences we can think of the Manhattan distance in geometric terms by imagining that each difference is a line segment if we take all those line segments and put them together head to tail head to tail and then add that total length of all those line segments together that's the Manhattan distance yes it makes a difference here's a heat map Drawn using the ukian distance and here's the same information drawn as a heat map but now we're using the Manhattan distance the heat maps are very similar but there are also a few differences the choice and distance metric is arbitrary W there is no biological or physical reason to choose one and not the other pick the one that gives you more insight into your data now do you remember how we merged genes 1 and three into cluster number one and compared it to other genes well there are different ways to compare clusters too one simple idea is to use the average of the measurements from each sample but there are lots more and these have effect on clustering as well so let's talk about the different ways to compare clusters for the sake of visualizing how the different methods work imagine our data was spread out on an XY plane now imagine that we have already formed these two clusters and we just want to figure out which cluster this last Point belongs to we can compare that point to the average of each cluster this is called the centroid the closest point in each cluster this is called single linkage or we can compare it to the furthest point in each cluster this is called complete linkage and there are other methods as well here's a heat map that compares the furthest points in the clusters by the way if you use R this is the default setting for the hclust function this heat map compares the average points in the Clusters and this last Heat Map compares the closest points in the Clusters these heat maps are all very similar but there are also differences in the way the data is presented in some summary clusters are formed based on some notion of similarity you have to decide what that is however most programs have reasonable defaults once you have a subcluster you have to decide how it should be compared to other rows columns or subclusters Etc and most programs have good default settings for this as well and the height of the branches in the Dinger gram shows you what is most simple similar hooray we've made it to the end of another exciting stat Quest if you liked this presentation please subscribe to my channel and you'll get more like it also if you'd like me to do something specific feel free to mention it in the comments below
Vf7oJ6z2LCc,2017-06-05T17:17:53.000000,"Lowess and Loess, Clearly Explained!!!",stat Quest is cool stack Quest rules stack Quest stack Quest hello and welcome to stack Quest stack Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to talk about fitting a curve to data AKA lowest smoothing AKA lowest smoothing I'm not really certain how to pronounce those two words or if the pronunciation is different so if you know please leave a comment below last time we talked about fitting a line to data we used leas squares to find the line with the minimal squar distance from the data points if this is news to you check out the stat Quest fitting aligned to data today we're going to talk about fitting a curve to data let's go over the main ideas first the first main idea is to use a type of sliding window to divide the data into smaller blobs the second main idea is at each data point use a type of least squares to fit a line now let's dive into the nitty-gritty details the good news is that if you understand least squares this is going to be a snap let's start with a simple example we'll fit a curve to this data using a window size five we'll start by making a window size five for the first point we'll call this point the focal point of the window here are the five points in the first window this point is closest to the focal point it's one unit away on the X AIS this is the second closest point it's two units away on the x-axis this point is the third closest and this is the fourth closest we'll do awaited least squares fit on all five points the closer the point is to the focal point the more weight or influence it has on determining the fit of the line the focal point has the most weight of all the closest point has the second most weight the furthest point has the least weight normal unweighted lease squares without the weights would fit the data like this with weights however the last Point has less influence and doesn't pull the slope up as much much now that we have fit aligned to the data we'll use it to define the first point in the fitted curve hooray we've got the first point for our fitted curve actually don't get too excited it's just a first draft of where the first point should go more on that later for now let's work on the second point now the second point is the focal point here's the window that contains the four closest points but wait isn't this window the same as before yep these two points are the closest to the focal point they are both one unit away on the x axis this is the next closest point it's 2 units away and this is the furthest point three units away so we take the four points that are closest to the focal point even if that means the window stays in the exact same place as before now we do weighted Le squares where the focal point gets the most weight these two points get the next most weight and this point gets the least weight here's our fitted line and we use the line to define the second point on the new curve so far we have two points on our new curve actually they're both just rough drafts but but let's press on to the third point now the third point is the focal point again the window is the same because the same points are closest but the distances are different and so are the weights for the least squares fit and here's our new point now the fourth point is the focal point here's the window finally it's shifted over one these two points are both one unit away these two points are both two units away as you can see the distance along the Y AIS isn't factored into how we pick points for the window at this stage we're only interested in distances along the x axis here's the weighted lease squares fit and here's the new point for the curve these are the new points so far now this is the focal point and here's our least squares fit and the new point now this is the focal point and here's our least squares fit and the new point now this is the vocal Point okay you're getting the idea moving along here are all the new points that we created using weighted Le squares and a sliding window remember when I said these points were preliminary these points were all pulled up by this guy this guy is an outlier to reduce its influence on the new curve we create an additional weight for the weighted Le squares based on how far the original point is from the new Point old and new points that are close together along the Y AIS get high weights this point would get a lower weight because the old and new points are far from each other along the Y AIS this point would get a really low weight with the new weights you do the whole thing over again however this time we have two sets of Weights we have the original weights based on distance along the xaxis and the weights based on distance from the new points these are the new new points after adjusting for the distance from the original points the curve is smoother after adjusting all the new points based on their distance from the original we get a nice smooth progression the process of using the distance between the old and new points to adjust the weights can be repeated several times until you get a nice smooth curve hooray Here's the final curve fit to the data now that we've got the curve let's talk about some additional considerations we could fit lines to the data in each window or we could fit parabas the choice is yours here let's look at a more complicated data set to see the effect of using lines versus parabas here's the curve using weighted Le squares to fit a line at each step here's the curve fitting parabas at each step as you can see the parabola fits the data just a little bit better before you do your fit make sure you look at the original data to decide which you think would be better the lowest function in R will only fit a line and the low s function in R can fit a line or a parabola however note that the default for low S is to fit a parabola so even though these two functions implement the same algorithm they have different default settings the low s function in R also allows you to draw confidence intervals around the curve it's also worth talking about the window size you can change the size of the window to contain more or fewer points the choice is yours here's the fit where the window contains a fifth of the overall points note instead of specifying the exact number of points you usually specify the proportion of the total here's the line using 1/3 of the total points per window this red line was fit using the default setting for low s which uses 75% of the points in each window oh let's also talk about the weight functions the formulas for determining the weights were chosen because they seem to work well but there's no physical or biological justification for them here's a graph of the weights for distance along the X AIS here's the graph for weights for distance between the old and new points along the Y AIS overlapping the standard weight functions shows their subtle differences there's no real reason we can't use an alternative weight function like this so keep that in mind hooray we've made it to the end of another exciting stat Quest tune in next time for a fun adventure into the land of statistics
PaFPbb66DxQ,2017-05-22T20:37:28.000000,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),"When we go on a quest and that quest is really awesome. It's that StatQuest Yeah, yeah, yeah Hello, and welcome to StatQuest. StatQuest is brought to you by the friendly folks in the genetics Department at the University of North Carolina at Chapel Hill. Today, we're going to talk about fitting a line to Data. aka Least Squares aka Linear regression. Now let's get to it. Okay, you worked really hard. You did the experiment and now you got some data. Here it is plotted on an XY graph. We usually like to add a line to our data so we can see what the trend is. But is this the best line we should use? Or does this new line fit the data even better? Or what about this line is it better or worse than the other options? A horizontal line that cuts through the average y value of our data is probably the worst fit of all. However, it gives us a good starting point for talking about how to find the optimal line to fit our data. So now let's focus on this horizontal line. It cuts through the average Y value which is 3.5. Let's just call this point B. Because different data sets will have different average values on the Y axis. That is to say the Y value for this line is B, and for this particular data set B equals 3.5. We can measure how well this line fits the data by seeing how close it is to the data points. We'll start with the point in the lower left-hand corner of the graph with Coordinates X-One Y-one. We can now draw a line from this point up to the line that cuts across the average Y value for this data set. The distance between the line and the first data point equals B minus Y1. The distance between the line and the second data point is B minus Y2? So far the total distance between the data points and the line is the sum of the two distances and we can calculate the distance between the line and the third point that equals B minus Y3. Now we've added the third distance to our total sum. The distance for the fourth point is B minus Y4. Note Y4 is greater than B. Because it's above the horizontal line, so this value will be negative. That's no good, since it will subtract from the total and make the overall fit appear better than it really is. The fifth data point is even higher relative to the horizontal line this distance is going to be very negative. Back in the day when they were first working this out they probably tried taking the absolute value of everything and then discovered that it made the math pretty tricky. So they ended up squaring each term. Squaring ensures that each term is positive. Here's the equation that shows the total distance the data points have from the horizontal line. In this specific example, 24.62 is our measure of how well this line fits the data. It's called the sum of squared residuals because the residuals are the differences between the real data and the line and we are summing the square of these values. Now let's see how good the fit is if we rotate the line a little bit. In this case, the sum of squared residuals equals 18.72. This is better than before. Does this fit improve if we rotate a little more? Yes, the sum of squared residuals now equals 14.05. That value keeps going down the more we rotate the line. What if we rotate the line a whole lot? Well as you can see the fit gets worse, in this case the sum of squared residuals is 31.71. so there's a sweet spot in between horizontal and two vertical. To find that sweet spot let's start with the generic line equation. This is Y equals AX or A times X plus B. A is the slope of the line and B is the Y-intercept of the line. That's the location on the Y axis that the line crosses when X equals 0. We want to find the optimal values for A and B so that we minimize the sum of squared residuals. In more general math terms the sum of squared residuals is this complicated mathematical equation. But it's actually not that complicated, this first part is the value of the line at X1 and this second part is the observed value at X1. So really all we're doing in this part of the equation is calculating the distance between the line and the observed value. So this is no big deal. Since we want the line that will give us the smallest sum of squares this method for finding the best values for A and B is called least squares. If we plotted the sum of squared residuals versus each rotation we get something like this, where on the Y axis we have the sum of squared residuals and on the X axis we've got each different rotation of the line. We see that the sum of squared residuals goes down when we start rotating the line, but that it's possible to rotate the line too far in the sum of squared residual starts going back up again. How do we find the optimal rotation for the line? Well, we take the derivative of this function. The derivative tells us the slope of the function at every point. The slope at the point on the far left side is pretty steep. As we move to the right we see that the slope isn't as steep. The slope at the best point where we have the least squares is zero after that the slope starts getting steep again. Let's go back to that middle point where we have the least squares value and the slope is zero. Remember the different rotations are just different values for A the slope and B the intercept. We can use a 3D graph to show how different values for the slope and intercept result in different sums of squares. In this graph the intercept is the Z axis so it's going back sort of deep into your computer screen and if we select one value for the intercept. For example, assume we set the intercept value to be 3. Then we could change values for the slope and see how an intercept of 3 plus different values for the slope would affect the sum of squared residuals. Anyways, we do that for bunches of different intercepts and slopes. Taking the derivatives of both the slope and the intercepts tells us where the optimal values are for the best fit. Note: no one ever solves this problem by hand, this is done on a computer. So for most people It's not essential to know how to take these derivatives. However, it's essential to understand the concepts. Big important concept number one, we want to minimize the square of the distance between the observed values and the line. Big important concept number two, we do this by taking the derivative and finding where it is equal to zero. The final line minimizes the sums of squares. It gives the least squares between it and the real data. In this case, the line is defined by the following equation Y = 0.77 * X + 0.66. Hooray, we've made it to the end of another StatQuest. Tune in next time for another exciting adventure in statistics land."
Gi0JdrxRq5s,2017-05-16T13:15:19.000000,"StatQuest: edgeR and DESeq2, part 2 - Independent Filtering","[Music] stat Quest stats are going crazy stat Quest stats are going boners baby stat Quest hello and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're doing part two of our in-depth analysis of how edgr and de seek to work specifically we're going to talk about filtering out genes with low read counts AKA independent filtering AKA mitigating the multiple testing problem every time we do a statistical test there's a chance that the conclusion is wrong if this this is news to you check out the stat Quest on P values in a nutshell when we say p values less than 0.05 are significant then we are also saying 5% of the time we will report a false positive this isn't a big deal when we test one or two genes for differential expression because 5% of two tests is small and it is unlikely we will report a false positive however it's a big deal when we check every Gene in the genome about 20,000 to see which are misregulated in cancer cells 5% of 20,000 equals 1,000 false positives the good news is that FDR and the benjamini hochberg method compensate for this problem if this is news to you check out the stat Quest on FD but there's another problem to see what it is let's look at an example example let's imagine we have two separate distributions on the Y AIS we have the probability of measuring different weights and on the x axis we have different weights in grams the red curve represents the weights of mouse strain X the blue curve represents the weights of mouse strain y if we weigh one strain X Mouse there's a high probability that the value will be near the middle of the red distribution likewise if we weigh two more strain X mice it is likely that those values will also be near the middle similarly if we weighed three strain y mice there's a good chance those values will be near the middle of the blue distribution a t test on these weights would result in a P value less than 0.05 and we would correctly conclude that the measurements came from from different distributions however every now and then we'll get overlapping values in this case the P value will be large this is a false negative now get a computer to take 1,000 samples from these distributions and do 1,000 T tests here's a histogram of the 1,000 P values that I got when I tried this at home the T tests compared samples from two different but overlapping distributions on the far left side of the histogram we see 949 true positives P values that are less than 0.05 however we also predicted that sometimes the values would overlap and we get p values larger than 05 this happened 51 times so far every single test we've done was supposed to result in a true positive and a P value less than 0.05 this is because every single test use samples from two different distributions let's make it more realistic by adding tests where there is no difference we'll start with the 1,00 p values we already calculated using different distributions then we'll add 1,000 additional P values from samples taken from the same distribution these P values should should be greater than 0.05 but every now and then say 5% of the time we'll get something like this where the values are separated from each other and the P value will be less than 0.05 now we have a histogram of 2,000 P values 1,000 P values from the first bunch of tests where we had two separate distributions and 1,000 P values from the second set of tests where we took all the data from the same distance distribution the 993 P values less than 0.05 are now a combination of 949 true positives from the first set of P values and 44 false positives from the second set of P values since only 4% of the P values less than 0.05 are false positives we don't need to use FDR but this this is only because we made up the data if it were real we wouldn't know the percentage so we'd use FDR so let's use FDR and see what happens after adjusting the P values for multiple testing 846 FDR adjusted P values remained less than 0.05 827 of the true positive survived that's 89% of the original 949 and also 19 of the false positives survived 2% of the 846 with fdrp values less than 0.05 now let's make it a little bit more like RNA seek and increase the number of P values to 6,000 we have the th P values from our first bunch of tests where we took the samples from different distributions and we'll add to that 5,000 additional P values where the samples came from the same distribution these additional 5,000 tests should give large P values most of the time here's a histogram of those 6,000 P values there are now 1,215 P values less than 0.05 as before 949 P values are true positives but now 266 are false positives 2 2% of the P values less than 0.05 are false positives that means it's time for FDR after adjusting for multiple testing only 256 FDR adjusted P values remained less than 0.05 250 of the true positives survived that's only 26% of the original 949 six of the false positives surv survived that's 2% of the 256 with FDR values less than 0.05 FDR is doing a great job limiting the number of false positives in the significant results but it's not awesome at keeping the true positives now let's increase the total number of P values to 11,000 1,000 of those P values will be from the original tests which were taken from two separate districts distributions and 10,000 of those P values will be taken from the same distribution and here's a histogram of those 11,000 P values there are now 1,430 P values less than 0.05 as before 949 P values are true positives but now 481 or 34% are false positives after adjusting for multiple testing only 56 FDR adjusted P values remained less than 0.05 54 of the true positives survived only 6% of the original 949 two of the false positive survived 4% of the 56 with FDR values less than 0.05 each time we increase the number of bogus tests we reduce the number of true positive P values less than 0.05 that survived the FDR adjustment here's a graph of the data that we just generated the green line represents true positives that passed FDR less than 0.05 and the Orange Line represents false positives that passed FDR less than 0.05 this graph shows that even though FDR can control the percentage of false positives that we report regardless of the number of tests it is in our interest to filter out bogus tests it also suggests that there is room to improve the benjamini hochberg method so get to it we need something better both EDR and dec2 have methods to filter bogus tests the general idea is that genes with super low read counts are not informative thus they can be removed from the data set in other words even if those genes are biologically interesting it's hard to get accurate read counts if there are only one or two transcripts in one sample type and three or four in the other sample type before doing anything EDR recommends removing all genes except those with more than one CPM in two or more samples CPM stands for counts per million it compensates for differences in read depth between libraries let's quickly see how to calculate CPM here we have a data set from a kidney this data set has just over 5 million total reads here's the formula for calculating CPM you take the reads for Gene X and sample Y and you divide that by the total reads in Sample y divided by 1 million in this case we'd plug in the total number of reads for our kidney sample into the bottom part of the equation this gives us 5.32 in the denominator now we put each Gene's read counts into the numerator starting with the first Gene we put 65 in the numerator this gives us a CPM value of 12.3 for the first Gene now we plug in five for the second Gene and this gives us 0.9 CPM for Gene number two the reason why we scale the total read counts by 1 million is to keep the denominator from being too large otherwise the resulting CPM values would be these tiny tiny tiny numbers with tons of decimal places and that's no fun to look at now that we have CPM values for all genes and all samples let's remove all genes except those with more than one CPM in two or more samples we'll start by examining Gene one since it has two samples with more than one CPM in them we'll keep Gene one now let's look at Gene number two again since two samples have more than one CPM in them we'll keep Gene number two now let's look at Gene number three since only one sample has more than one CPM we'll filter this Gene out even though that one sample has a lot of reads mapping to it now let's look at Gene number four since the two samples with reads mapping to them have less than one CPM in them we'll filter this one out now let's look at Gene number five this Gene has reads in all four samples but none are greater than one CPM so we'll filter it out looking at Gene six this Gene has greater than one CPM and two samples so we'll keep it even though those two samples are from different tissue types Ed R's method is simple but you should be aware of of how sequencing depth can influence it for example if you have 5 million reads mapping to a sample the CPM scaling factor is 5 million / 1 million which equals 5 if you have five reads mapping to a gene it will have 5 / 5 equal 1 CPM if you have 80 million reads then the scaling factor is 80 in this case one CPN m equals 80 reads this might be too high on the other hand sometimes you need a larger CPM cut off for example if you have 50,000 reads mapping to a sample which isn't horrible for some single cell RNA sequencing experiments the CPM scaling factor is 0.05 if you have one read mapping to a gene it will turn into 1 / 0.05 20 CPM even if this Gene is transcribed at a biologically relevant level and you only got one read because of the low overall counts it is still in a very noisy range since CPM can change from data set to data set how can we determine what a good cut off is go with our gut look at real data let's look at real data we're signed scientist after all to do that I got a data set from a coworker that has an average of 22 million reads per sample four wild type samples and four knockout samples I then ran Edge r on it without filtering a single Gene this generated raw P values lastly I filtered out genes using different CPM cut offs and then adjusted the P values here's what the data look like on the y- AIS we have the number of genes with FDR less than 0.05 and on the x axis we have the minimum CPM threshold zero means that there is no filtering at all the recommended threshold is one because there are lots of reads the suggested cut off is too strict it's actually worse than no filtering at all using a lower threshold identifies about two 200 more significant genes the moral of the story with EDR is be careful try different CPM cut offs after you've calculated the P values just so you know I didn't come up with the idea to calculate P values before trying different cut offs for the minimum CPM that's how it's done in Deek 2 there are a few differences though so let's talk about them difference number one EDR looks at individual samples and makes sure that at least two have more cpms than the cut off EDR looks at this Gene and says yep those two samples are above the cut off so I'll keep the gene in contrast de seek 2 looks at the average normalized reads across all samples de seek 2 looks at this Gene and says yep the average of all four samples is is above the cut off so I'll keep the gene at this point you're probably thinking cool I get de 2's approach but what about genes without lier measurements liver 2 in this example might be an outlier measurement this Gene would not pass Ed R's criteria since only one sample has a large CPM but what about de 2os the average CPM for this Gene is 5,000 / 4 which equals 1250 CPM this would probably pass the filter that said Deek 2 has an outlier detection method that we'll talk about in another stat Quest but it only kicks in when there are more than two samples per category here are both methods applied to the same set of genes and P values they both Peak in the same region and that's the most important thing both methods result in a similar cut off so now let's look at another difference another difference changes the x axis de 2 plots the number of significant genes relative to quantiles instead of the minimum CPM cut off zero on the X AIS means that 0% of the genes are below the cut off 0.2 on the xaxis means that 20% of the genes are below the cut off and 0.4 on the x-axis means that 40% of the genes are below the cut off quantiles are useful because as we saw cpms depend on the sequencing depth but quantiles are always quantiles no matter what it doesn't matter if the library has 8 million or 80 million reads 10% of the genes will always be less than the 0.1 quantile but you don't have to choose you can actually use both now let's talk about the third difference between EDR and de 2 de 2 fits a curve to the points this Smooths out occasional bumps generated by random noise and makes the results more reliable de seek 2 then locate the maximum location on the fitted curve lastly the threshold is the maximum location on the curve minus the standard deviation between the fitted curve and the raw values in other words the first quantile that gets you within noise range of the peak becomes the CPM cut off if none of the raw values goes above the threshold then no filtering is done okay now we know how EDR and de 2 filter genes EDR keeps genes that have more than the minimum CPM in two or more samples de se 2 keeps genes with the average CPM greater than the minimum CPM it then plots the significant genes versus quantiles it then fits a curve to those points and the threshold is the maximum on that curve minus the noise now let you know you can mix and match advice if you use EDR figure out the CPM cut off after you calculate the P values it's easy to apply de 2's method to find the optimal CPM to Edge RS Gene selection criteria I've gone ahead and written template code and put it on my website so check it out if you think you might want to do that however if if you do just make sure you site both Publications if you use Deek 2 be careful of outliers when there are only two samples per category hooray we've made it to the end of another exciting stat Quest tune in for another exciting Adventure in the land of Statistics until then Quest on"
XLCWeSVzHUU,2017-05-08T13:47:35.000000,"Sampling from a Distribution, Clearly Explained!!!",[Music] stack Quest hello and welcome to stack Quest stat Quest is brought to you by the friendly folks in the genetics department in the University of North Carolina at Chapel Hill today we're going to be talking about sampling a distribution or getting samples from a distribution this is something that we do all the time in stat Quest so I wanted to make a video that we could reference rather than covering the same material over and over and over again so let's get down to it here we have a histogram of height measurements each Red Dot represents a different person that we measured the tallest part of the histogram shows the region where measurements are most likely in this case most of the people we measured were between 5T 7 in and 6 ft tall the low parts of the histogram show where measurements are less likely in this case we didn't measure many people that were shorter than 4 1/2 ft or taller than 6 and 1/2 ft we can approximate the histogram with a smooth curve you guys already know all this from the stat Quest on statistical distributions what we want to know today is what it means to take a sample from a distribution all that means is that we use a computer to pick a random number based on the probabilities described by the histogram or the curve for example if we wanted to take one sample from this distribution there's a good chance the computer will pick a value near the Middle where the histogram and curve are tallest however every now and then the computer will return a value from the edges where the histogram and curve are the shortest why would you want to take a sample from a distribution we do this to explore statistics the computer can generate lots of samples and we can plug them into statistical tests to see what happens since we know what the original distribution is we can compare our expectations of what will happen to reality for example I could take two samples where n equals 3 from a single distribution and do T tests on the samples in this case n equals the number of measurements we take within each sample since the distribution is the same the T Test should give me a large P value doing lots of tests will give me a sense of how frequently the T Test successfully gives me a large P value if I had two separate distributions a t test is supposed to give me a small P value if I took lots of samples I could do lots of T tests and see how frequently the T Test worked and gave me a small P value this would tell me if I needed to increase my sample size or not taking samples from a distribution or multiple distributions I.E getting a computer to generate a bunch of random numbers that reflect the probabilities of a distribution lets us determine what a statistical test is capable of doing without doing much real work hooray we've made it to the end tune in next time for another exciting stack Quest
bsZGt-caXO4,2017-04-24T21:17:32.000000,StatQuest:  One or Two Tailed P-Values,"Steff quest stat quest stat quest stat quest hello and welcome to stat quest stat quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about one versus two-tailed tests people frequently ask me which one they should use so I'm gonna settle the matter once and for all right here with this stat quest imagine you've got a new cancer treatment you hope that people do better with your new treatment than the standard treatment you do a small clinical trial on six patients and here's your data the red dots represent people that took your new treatment and the black dots represent people that took the standard treatment the values range from better to worse the data suggests that people who use your new treatment do better than people on the standard treatment however there is a little bit of ambiguity in the results so you run the stats a one-tailed or one-sided test gives you a p-value of 0.03 awesome 0.03 is smaller than that pesky 0.05 cutoff that we usually use to determine significance a two-tailed or two-sided test gives you a p-value of 0.06 DAG not so awesome which p-value should you use the one-tailed p-value tests the hypothesis that your treatment is better than the standard treatment great that's what we wanted right it's certainly tempting but let's not jump to conclusions before learning about the two-tailed p-value the two-tailed p-value tests whether the new treatment is better worse or not significantly different the one-tailed p-value is smaller because it doesn't distinguish between worse and not significantly different since we'd want to know if our new treatment was worse than the standard treatment we should use the two-tailed p-value but wait doesn't the data being skewed towards the new method being better suggests we don't need to test if it is worse no good statistical practice means we need to decide what tests and what p-value we want to use before we do the experiment otherwise were pee hacking this increases the probability that we will report bogus results let's see why this is I started with a standard normal distribution the x-axis represents measurements from small to large the y-axis represents the probability that I'll get certain measurements most of the time I should get measurements in the middle but every now and then I'll get a really small measurement or a really big one then I took a sample from this distribution that means a computer picked three numbers that had a high likelihood of being from the center of the distribution but every now and then one of them might be really small or really big I then took another sample from the exact same distribution in most cases a two-tailed t-test on these two samples should give me a p-value greater than 0.05 this is because most of the time the samples will overlap but every now and then the samples will not overlap and the t-test will give me a p-value less than 0.05 this is called a false positive it happens 5% of the time I did 10,000 two-tailed t-test on data like this percent of 10,000 equals 500 so I was expecting 500 false positives here's a histogram of the p-values the blue line shows that each bin contains about 500 tests these are the false positives the tests with p-values less than 0.05 we pretty much got what we expected there were close to 500 false positives then I changed things to mimic switching to a one tailed test when things looked good if sample number one had two or more values that were less than all of the values in sample number two then I used a one-tailed t-test since these two values are less than all of the values in sample number two I used a one-tailed t-test on this data set here's a histogram of the new p-values the blue line shows the expected number of p-values per bin there are now close to 800 false positives the chance of reporting a false positive went from 5 percent to 8 percent even though we're using 0.05 as the threshold for significance thus you can't wait till you see the data to decide you want to use a one-tailed p-value so let's take a step back to before we did the experiment what do we want to learn from it with a cancer treatment it's obvious we must learn if it improves things or makes them worse but really it's the same for all data that has the option for a 1 or two-tailed p-value we always want to know both sides of the story not just one so when you have a choice always go with a two-tailed p-value note not all statistical tests have a choice in that case don't worry about it hooray we've made it to the end we now know that when we have a choice we should always select a two-tailed test tune in next time for another exciting stat quest"
Wdt6jdi-NQo,2017-04-03T16:19:36.000000,"StatQuest: edgeR, part 1, Library Normalization",that quest is super cool that class won't make you truth that quest stand quest guaranteed not to make you drool hello and welcome to stack quest stack quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're starting part 1 of our exploration of how edge our works we're going to talk about library normalization just like Dec to our does not use rpkm TPM or any of the other standard normalization techniques this is because it needs to adjust for two things sequencing depth that's what rpkm and all those other methods deal with it also needs to take library composition into account this means different samples can contain different active genes and that can change things I cover this concept in depth in the stat quest on Dec 2 part 1 library normalization so check that out if you have any more questions without further ado let's talk about how edge are normalizes libraries step 1 remove all untranscribed genes that is to say remove all genes with 0 read counts in all samples here's an example we've got 3 samples and 5 genes the last gene gene 5 has 0 read counts in all 3 samples and so we will remove gene 5 from our list of genes step two pick one sample to be the reference sample this is the sample that we will use to normalize all of the other samples against imagine these blue dots are samples or libraries a jar chooses one sample to be the reference sample and uses it to normalize all of the remaining samples here's a question for you what's a good reference sample alternatively what's a bad reference sample here's an example of an extremely bad reference sample sample three would be a terrible reference sample scaling would be based on a single potentially very noisy measurement to avoid choosing extreme samples edge our attempts to identify the most average sample let's see how it does this in order to pick a reference sample the first thing it does is it scales each sample by its total read counts so here if we imagine each sample has four genes we just sum the read counts for each sample and we divide the original read counts for each gene in each sample by each samples total and here's what the scaled read counts look like the second part of picking a reference sample is for each sample determine the value such that 75% of the scaled data or equal to or smaller than it in sample number one three of the four values 75% are less than or equal to zero point two six in sample number two three of the four values 75% are less than or equal to 0.36 lastly in sample number three three of the four values or 75% are less than or equal to zero point one three now we've calculated the 75th quantiles for each sample the third part of picking a reference sample is to average the 75th quantiles in this case the average is 0.25 the reference sample is the one whose 75th quantile is closest to the average in this case sample number 1 will be the reference sample that's because its 75th quantile 0.26 is closest to the average 0.25 now that we've picked a reference sample we now need to select the jeans for calculating the scaling factors this is done separately for each sample relative to the reference sample here we see the reference sample we selected in step number two and now we will select a set of genes to create a scaling factor for sample number two and we will select another set of genes to create a scaling factor for sample number three this is one of the ramifications of edge ARS approach different samples use different genes to derive their scaling factors let's see how we select genes for sample number two we'll start by looking at the different types of genes to choose from he's an XY plot that will demonstrate the different types of genes we have to choose from on the left side we see a gene primarily transcribed in the reference sample on the right side we see a gene primarily transcribed in sample number two way up high we see a gene with a ton of reads in both samples and way down low we see a gene with hardly any reads in both samples genes in the middle don't have much bias and have a moderate number of reads mapped to them in both samples edge our selects the genes in the middle with more effort put into excluding biased genes let's see how it does this we'll start with the scaled read counts these are the read counts for each gene divided by the samples total the next few steps only makes sense if there are a lot of genes too we need to put on the screen so that's what the dot-dot-dot and gene n are all about they represent lots more genes edge our filters out biased genes by looking at log fold differences remember with logs if the reference is way high relative to sample number two you'll get a value way up here and if sample number two is way high relative to the reference we'll get a value way down here ultimately we'll pick a threshold like plus or minus six and remove all genes with more extreme biases for more information about logs check out the stat quest on logs we'll start by calculating the log to ratio for gene number one plugging in the scaled read counts we get the log two of zero divided by zero point nine the log 2 of 0 is defined as negative infinity by are the programming language that edge R was written in so we put negative infinity as the value for gene 1 in our table of log 2 ratios now let's calculate the log 2 ratio for gene number 2 plugging in the scaled read counts gives us the log base 2 of 0.8 which equals negative 0.32 and we just do that for every single gene in our list of genes now we remove genes with infinite values ie genes without any reads mapping to them in one or both samples in this example we'll remove gene number 1 now that we have a table of log ratios to identify biased genes let's make a table to identify genes that are highly and lowly transcribed in both samples to identify genes that are high and low in both samples first calculate the geometric mean for each gene remember the geometric mean is cool since it is not easily influenced by outliers more details on the geometric mean are in the stat quest for logs anyway here's how we calculate the geometric mean for gene number one we take the average of the log two of the scaled read counts for each sample plugging in the numbers gives us negative infinity that's because there's zero read counts in the reference for gene number one and we put that value in our table of the mean of the logs technically we are not calculating the geometric mean since we are not converting the average back into normal number land but the effect is the same outliers are less influential on the value that we are keeping track of here's how we calculate the mean of the logs for genome or two and then we calculate the mean of the logs for all of the other genes in the list now we remove all genes with infinite values ie genes without any reads mapping to one or more samples in this case that means we will be removing gene number one now we have two tables one to identify biased genes that's the log of the ratio between the two samples and one to identify genes that are highly and lowly transcribed in both samples and that's the mean of the logs now sort both tables from low to high filter out the top 30% and the bottom 30% of the biased genes and filter out the top 5% and the bottom 5% of the highly and lowly transcribed genes genes that are still in both lists are used to calculate the scaling factor unfortunately the only genes in this example that are in both lists are the dot-dot-dot genes well you get the idea hooray we figured out which genes to use to scale sample number two now we need to figure out which genes to use for sample number three I'll leave this as an exercise for the reader now that we figured out which genes we're going to use to create a scaling factor we can move on a step four calculate the weighted average of the remaining log two ratios for your information edge our calls this the weighted trimmed mean of the log to ratios because we trimmed off the most extreme genes by excluding extreme genes we avoid the effect of outliers sort of like using the geometric mean okay so we're back to talking about sample number two and imagine genes a through Z are the dot-dot-dot genes these are the genes that made it through the filters in step 3 once you have selected which genes will be used to calculate the scaling factor just calculate the weighted average of their log2 ratios genes with more reads map to them get more weight this is because log ratios have more variance with low read counts let's look at an illustration here we have a table of read counts for sample number one and sample number two we've got six genes and on the right side we show the log to ratio between the two samples the first three genes have relatively high read counts however the difference between gene number one and gene number three is for the bottom three genes have relatively low read counts however the difference between gene four and gene six is also four now in the right hand column we see the log two ratios and when there are lots of reads small differences in recount do not make big log fold differences when there are only a few reads small differences and read counts can make big log fold differences so we're back to calculating the weighted average of the log two ratios genes with more reads mapped to them get more weight because they're less noisy and then we do the same thing for sample number three we calculate the weighted average of the log two ratios step 5 convert the weighted average of the log two ratios to normal numbers that is to say we raise to two the weighted average of the sample two log two ratios that gives us a scaling factor however this is not quite the scaling factor that edge our reports we also calculate a scaling factor for sample number three and we continue to do that for all the samples in our data set here are actual scaling factors that I calculated from an rna-seq experiment we see that WT 2 was used as the reference sample the other samples were scaled to it 1 step 2 step 6 Center the scaling factors around 1 here our raw scaling factors the values are roughly centered around 0.95 and here are the final centered scaling factors they've been shifted over so that they are centered on one the sintering is done by dividing each raw scaling factor by their geometric mean although sintering does not change the results mark robinson the guy who wrote this method says it gives the scaling factor some nice mathematical properties so I guess it's a sort of artistic signature on a mathematical formula that's it now you know how scaling factors are calculated in a jar tune in next time for another exciting stat quest
UFB993xufUU,2017-03-27T16:06:30.000000,"StatQuest: DESeq2, part 1, Library Normalization","stack Quest stack Quest oo I love you stack Quest hello and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to talk about de seek 2 which is a program people use to identify differential gene expression de seek 2 is a big and complicated program so we're going to break it down into Parts this is part one Library normalization remember rpkm fpkm and TPM those nice methods for adjusting for differences and overall read counts among libraries de 2 doesn't use those methods and neither does EDR by the way why not there are two main problems in library normalization so let's talk about them problem number one adjusting for differences in library sizes for the sake of keeping the example simple let's assume there are only six genes in the genome here we have sample number one which has a total of 635 re reads mapped to it and here we see how the reads are distributed among the six genes sample number two has 1,270 reads mapping to it and we see how those reads are distributed among the genes in the genome sample number one has half as many reads as sample number two the read counts for each gene in Sample number two are twice the read counts in Sample number one this difference is not due to biology but to sequencing depth rpkm fpkm TPM and CPM all deal with this no big deal however there is another problem problem number two adjusting for differences in library composition RNA seek and other high throughput sequencing is often used to compare one tissue type to another for example liver versus spleen it could be that there are a lot of liver specific genes transcribed in liver but not in the spleen this is an example of a difference in library composition you can also imagine seeing differences in library composition in the same tissue type if you knock out a transcription factor or something that regulates Gene transcription let's look at a specific example in this example both libraries are the same size both have 635 reads now assume expression of all genes is the same with one exception assume that only sample number one transcribes the gene called a2m to make matters worse sample number one transcribes a2m at a very high level here we see that 563 reads of a total of 635 reads mapped to this Gene in Sample number one this means that the 563 reads used up by a2m in Sample number one will be distributed to other genes in Sample number two here we see the read counts for all of the genes in Sample number one and Sample number two the read counts for everything but a2m are crazy high in Sample number two however the only differentially expressed Gene is a2m because sample number two does not transcribe a2m all of the other genes get the read counts that would have gone to it and this makes those read counts larger the folks that wrote Deek 2 and EDR were aware that their tools would be used with all kinds of data sets so they wanted their normalization to to handle one differences in library sizes and two differences in library composition we'll start with a small data set to illustrate how DEC 2 scales the different samples the goal is to calculate a scaling factor for each sample the scaling Factor has to take read depth and Library composition into account so the first thing that de 2 does is it takes the log of all the values de seek 2 uses the log base e so these numbers are what we would need to raise E2 in order to get the original value so if the original read count was 10 we'd have to raise e to 2.3 to get that value de seek 2 could have used log base 2 or log base 10 but log based e is the default and R which is the programming language that was used to create de2 anyways I think because it's the default that's why they chose it notice that the log of0 equals negative Infinity this is just because R defines log of zero to be negative Infinity if you'd like to learn more about logs check out the stat Quest on logs the next thing that de seek 2 does is it averages each row anytime you add a number to infinity or negative Infinity you end up with infinity or negative Infinity which is why the average for Gene one is negative Infinity one cool thing about the average of log values is that the average is not easily swayed by outliers to see this let's calculate the average read count for Gene 3 we see that the read counts for Gene 3 in Sample number three are really high that makes it an outlier if we just average the raw read counts for Gene 3 we get 96 now convert the average log value for gene3 into a normal number remember that logs are exponents and in this case they are exponents of e so we have to raise e by 4.3 to get a normal number e raised by 4.3 equal 73.7 the average calculated with the logs is smaller and thus not swayed as much by the outlier and for all you stat questers out there that can remember the names of things averages calculated with logs are called geometric averages hooray we've made it all the way through step two only five more steps to go step three is is an easy step filter out genes with infinity as their average so in this case we're going to filter out Gene number one in general this step filters out genes with zero read counts in one or more samples if you are comparing liver and spleen this will remove all of the genes only transcribed in liver or spleen in theory this helps Focus the scaling factors on the housekeeping Gene genes transcribed at similar levels regardless of tissue type step four subtract the average log value from the log of the counts here we have the log of the counts for each gene in each sample and here we have the average of the log values and all we have to do is subtract that average from each sample so in this case for Gene number two in example number one we subtract 1.7 from 0.7 that gives us -1 for Gene number three in Sample number one we subtract 4.3 from 3.5 that gives us 0.8 and we just do the same thing for all the other genes remember when we subtract the log of one value from the log of another value that's the same thing as the log of dividing those two values or the ratio of those two values so we're really checking out the ratio of the reads in each sample to the average across all samples this will allow us to identify genes within each sample that are expressed at levels significantly higher than the average or close to the average or significantly less than the average okay now let's move on to step five calculate the median of the ratios for each sample so here we have the log of the ratios of the reads for each gene divided by the average for each gene and all we have to do is calculate the median value for each sample note using the median is another way to avoid extreme genes from swaying the value too much in One Direction genes with huge differences in expression have no more influence on the median than genes with minor differences since genes with huge differences will most likely be rare the effect is to give more influence to moderate differences and housekeeping genes okay now we're ready for step six convert the medians to normal numbers to get the final scaling factors for each sample again these are log values so they are exponents in this case exponents for E to calculate the scaling factor for each sample we raise e to the median value for each sample awesome we have scaling factors for the three samples now all we do is divide the original read counts by them that leads us to step seven divide the original read counts by the scaling factors here's our table that lists the original read counts note Gene one is part of the original read counts we didn't use Gene one when we calculated the scaling factors because it has a zero for read counts in Sample number one however we still need to scale the read counts for the other samples and here we have a table of scaled read counts I've rounded to the nearest read just to make this table easy to look at we can see that the read counts for sample number one were scaled up and that the read counts for sample number three were scaled down here's a summary of De 2's Library size scaling Factor logs eliminate all genes that are only transcribed in one sample type liver versus spleen they also help smooth over outlier read counts via the geometric mean the median further downplays genes that se soak up a lot of reads putting more emphasis on moderately expressed genes the idea behind using logs and the median is to hopefully Focus the scaling Factor on just the housekeeping genes the genes that are transcribed at the same levels in all of the samples you're looking at hooray we've made it to the end we now know how de 2 normalizes the read counts in each Library tune in next time when we talk about how Edge R normalizes the read counts until then Quest on"
A82brFpdr9g,2017-03-20T15:03:59.000000,"Standard Deviation vs Standard Error, Clearly Explained!!!",hello and welcome to another stat quickie this time we're going to talk about the standard deviation versus the standard error these terms are often confused so let's clear them up once and for all I think the easiest way to understand the differences between the standard deviation and the standard error is to look at an example for the sake of this example imagine we weighed five mice this is the average or mean of the values we measured this is the standard deviation on both sides of the mean it quantifies how much the data are spread out now imagine we did the exact same experiment weighed five mice five separate times using different mice each time this would result in five means or averages one for each set of measurements we would also have five separate standard deviations around the means one for each set of measurements they quantify how much the measurements are spread around their means here's what it would look like if we plotted all five means on the same number line This is the mean of the means and this is the standard deviation on both sides of the mean of the means it's a little bit of a tongue twister there the standard deviation of the means is called the standard error and that's all there is to it it's just that simple now let's summarize the differences between the standard deviation and the standard error the standard deviation quantifies the variation within a set of measurements the standard error quantifies the variation in the means from multiple sets of measurements the confusing thing is that the standard error can be estimated from a single set of measurements even though it describes the means from multiple sets thus even if you only have a single set of measurements you are often given the option to plot the standard error in almost all cases you should plot the standard deviation since graphs are usually intended to describe the data that you measured for more information on standard errors check out the stack Quest on standard errors also the stack Quest on pval pitfalls and power calculations I've provided links to those in the description below all right we made it to the end hooray tune in next week for another stat quickie
nnBJeb_I-q8,2017-03-06T19:51:38.000000,StatQuickie: Which t test to use,hello and welcome to a stat quickie on t-tests I work in a large genetics lab and people often ask me questions about T tests the first question they ask me is what type of T tests they should use there are two main categories for T tests paired and unpaired pair T tests are useful when you have before and after measurements taken from the same test subject for example if you have a drug for blood pressure say like you take a group of people and you measure their blood pressure and then you give them this drug and then after they've taken the drug you test their breath you test their blood pressure again so for each person in the study each individual you have a pair of measurements you have a before measurement and then you have an after measurement and so when you have data like that that's paired used a paired t-test when you don't have paired data for example say like you have a one group of people and you measure their height and we call that group a and then we have another group of people and we measure their height and we croc when we call that group B then we have unpaired data and so that's when you use an unpaired t-test and there are two subcategories of unpaired T tests one assumes that the variance within each group so the variation around the height measurements in Group A is the exact same as the variation around the height measurements in Group B so they call that a well actually I don't know what they call that they call that a t-test that assumes equal variation or equal variance there's another type of t-test that does not assume equal variation so you could have a different measurement of variance in Group A as opposed to group B now I often recommend that people select the test that does not assume that the variation is equal in both groups simply because that test is slightly more conservative and if your data can pass this more conservative t-test then you know your data are rock solid so I think that's my general recommendation for using t-test you go with if you have unpaired data go with the test that does not assume equal variance in both groups okay now the second question people ask me is should they use a one-tailed or one sided T test or a two-tailed or two sided t-test now a two-sided T tests say for example we'll go back to our height measurements so we have Group A and Group B and we've measured the heights in both groups a two-tailed t-test would test to see if group a is higher than group B and it also tests to see if it's significantly smaller than Group B so it tests both sides it tests two conditions so you've got a tail up here and you've got a tail down here um it's it's agnostic to the data it doesn't have some preconceived notion that group a should be taller or group a should be smaller it says I'm going to test both sides and just see what group a is a one tailed t-test is a lot less conservative because it it requires you to say oh I know which direction a is gonna be a is has to be higher than B and generally speaking in academic journals that's not a good way to go generally speaking you want the data to speak for itself and that's why I always recommend people use two-sided t-test because it's slightly more conservative and it lets the data speak for itself and again the conservative thing means you've got rock silence rock solid data so in summary if you've got paired data meaning you've got a single you've got subjects and for each subject you've got two measurements you've got a before and an F Oh Oh then use a pair of t-test but if you don't have paired data so you've got to group two separate groups of people you've measured everything and there's and there's not overlap between who's and which group so if you've got unpaired data use an unpaired t-test and try not to assume equal variance because that'll make the test more conservative and use a two teeth two-tailed t-test because that makes the data speak for itself and again it makes it a little more conservative so if your data can pass that test you know it's rock-solid all right that's all there is to it thanks so much for paying attention to this stack quickie auntie Tess
KEofcJ1tfkI,2017-02-22T16:20:16.000000,StatQuickie: Thresholds for Significance,hello and welcome to the very first stat quickie stat quickies are a little short video it's not a full-on stat quest where it's only gonna take a few minutes where we address a question that someone's asked me in the comments or sent me an email or if I just ran into someone in the hallway and they said hey I got a question so that's what's that quickies are for this very first one we're gonna talk about what a good threshold for significance is people often ask me is point zero five the best threshold for significance should I use it for everything all the time no matter what or are there exceptions or what would how do I deal with this okay so what I'm going to talk about is one the original threshold of point O five was randomly selected well I'm not necessarily randomly but it's relatively arbitrary there's no biological or natural reason at point zero five is the optimal threshold for significance the one that we use mostly in science in publications and whatnot we even used it for a long time and for the most part it doesn't okay job lately there have been some high-profile articles about people doing pee hacking and kind of getting away with murder in terms of statistics but for the most part zero point four oh five has been sort of a good way to hedge the bets five percent of the time you're wrong but 95 percent of the time you're right and that seems to be a cost-effective threshold for most of at least biological science okay so now let's talk about what good thresholds are other than 0.05 okay so if you're got data that you're gonna publish you've done your analysis you're gonna publish it what's a good threshold for that well almost always point oh five is a great threshold for that because that's what the editors are expecting that's what the reviewers are expecting if you can go below that that's awesome okay but there are exceptions to that down before you run off let's listen to all those exceptions because sometimes it needs to be smaller and we're going to get to that in just a second okay so here's another threshold for significant we're going to talk about one is when you're not necessarily gonna publish the data you say like you've downloaded a dataset from the internet you've got a hypothesis that seems reasonable you've tested that hypothesis the p-value is not point over five okay it's a little bit higher okay is that a bad thing well it depends on what you're gonna do if you're gonna publish immediately yes you want it to be lower than pointer heart but if instead you're just using this limited data set that you got off the internet for free and you're just trying to test some basic hypothesis that you're later going to confirm or validate using alternative methods well in that case who cares what the p-value is as long as it's kind of small that's good enough you don't need a massively tiny p-value when you're just exploring some data set that you found you're generating new hypotheses that you're later going to test and validate using all additional data and additional methods okay so if that's the case you know don't don't feel like you have to stick to 0.05 okay now here's another thing you need to think about say like you've done your looks like I just got a text message next time I do this I'm gonna turn off I guess alerts or something like that okay so what's the next thing we need to talk about we need to talk about the effect size okay say like you did an R squared and I've got a whole static West on R squared so check this out because I go into this in detail but say like you did an R squared and your R squared value is isn't very good okay you've got like a you know you know r-squared of 10 percent right that means that 10 percent of the variation in the data is explained by whatever it is you're interested in that's not very much okay so even if you have a tiny tiny tiny tiny p-value okay it doesn't actually matter unless you know in certain circumstances maybe that's good enough but generally speaking you want a good r-squared you want your you want a good correlation you want whatever you're studying to explain the data okay and and so if you have a tiny p-value and a tiny effect size and not much explanation of what's go on then who cares how small your p-value is you know I've seen these things in publications where the p-value is zero point zero zero zero zero zero zero on the r-squared is horrible and I say who cares okay so you want to have a decent effect size okay so up the small p-value is not enough okay one last thing okay extraordinary claims need extraordinary data so say like you've done a great experiment proving that there are extraterrestrials flying around New York City in UFOs okay that's an extraordinary claim okay you've done your thing and you've got a p-value less than 0.05 it's point oh four seven okay it's less than that threshold of significance is that good enough absolutely not extraordinary claims need extraordinary data if you're gonna go out and publish a thing sending extraterrestrials are flying around New York City and UFOs your p-value better be so crazy small it's unbelievable and then people say well that p-value is so small I can't even believe it therefore I have to believe that there are extraterrestrials flying around New York City and UFOs okay that's it so to summarize if you're publishing your p-values you want a small p-value a PETA threshold of 0.05 it's okay unless you're the thing you're studying only explains a little bit and your r-squared is kind of crummy you want a good r-squared okay so it's you know point oh five is fine if you're if you've got a good correlation um if you're just using it doing statistical stuff just to explore stuff that you're going to validate later on who really cares what your threshold is you're just trying to go find something of interest don't worry about 0.05 and the other thing is extraordinary claims require extraordinary data if you're gonna say that there are extraterrestrials flying around New York City and UFOs you better have a darn small p-value all right tune in next time for our next stat quickie
K8LQSvtjcEo,2017-01-10T13:40:32.000000,"False Discovery Rates, FDR, clearly explained","holy freaking smokes what it's time for stat Quest hello and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about false Discovery rates or FDR if you've ever seen or done anything with high throughput sequencing chances are you've heard false Discovery rates FDR before you may have even used them but where do false Discovery rates come from and how do they work before we get down to the nitty-gritty let me blurt out the main idea of this whole stat Quest false Discovery rates are a tool to weed out bad data that looks good now let's get down to the nitty-gritty let's start with an example of measuring gene expression using RNA sequencing here we're going to plot the measurements or the read counts for a gene called Gene X which is an imaginary gene on a graph with the y axis being Gene counts and the xaxis being the samples for this example imagine that we are looking at normal wild type mice later on we'll be comparing them to mice that have been treated with a drug isn't it funny that normal mice are called wild type if someone said I was a wild type I don't think they would also think I was normal anyway here's our measurement for the first mouse that we do this RNA sequencing on and here's the measurement for the second mouse that we do the RNA sequencing on RNA sequencing isn't perfect and different samples are always a little different so each time we measure expression we'll get slightly different values here's the measurement for the third Mouse and here are the measurements for a bunch of mice if we measured all normal mice we'd be able to calculate the average for all normal mice most of the values are going to be close to the mean rarely we'll get a value that is much larger than the mean and rarely we'll get a value that is much smaller than the mean we can summarize the distribution of the measurements using this bell-shaped curve most of the measurements which are close to the mean will come from the middle of this curve the rare measurement that is significantly larger than the average would come from the right side of the bell-shaped curve and the rare measurement that significantly less than the average would come from the left side of the bell-shaped curve now imagine that we do RNA seek on three mice collectively we'll call these three measurements sample number one because these measurements are close to the meain they come from the middle of the distribution now imagine we compare sample number one to another three measurements taken from normal wild type mice we'll call these new measurements sample number two again these three measurements come from the middle of the distribution if we did a statistical test to compare sample number one to sample number two the P value would be large greater than 0.005 because the two samples overlap very rarely we'll get two samples that do not overlap when this happens the P value will be less than 05 this is called a false positive because the small P value suggests that the samples are from two types of mice or two separate distributions and this is false normally false positives are rare unless you're a packer but that's another stat Quest already on YouTube anyways normally false positives are rare 95% of the time the two samples will overlap this will result in a P value greater than 0.05 5% of the time they don't this will result in a false positive with a P value less than 05 but human and mouse cells have at least 10,000 transcribed genes if we took two samples from the same type of mice and compared all 10,000 genes well 5% of 10,000 equals 5 00 false positives that means there will be 500 genes that appear to be interesting even when they are not 500 false positives is a lot can we do something about them the false Discovery rate can control the number of false positives technically the false Discovery rate is not a method to limit false positives but the term is used interchangeably with the method me in particular it is used for the benjamini hogberg method now there's a high probability that I just mispronounced benjamini or hurg and if I did I apologize before we talk about the details of the benjamini hosb method let's review the concepts that it's based on we'll start by generating 10,000 P values from samples taken from the same distribution that is to say we'll start with test number one and we'll use wild type mice and we'll take two samples from them we'll then compare the two samples with a statistical test and calculate the P value in this case the P value is large it's 0.83 this is exactly what we expect because both samples are taken from the same type of mice and then we repeat the procedure for test number two and calculate another P value this time at 0.98 again this is as expected to make a long story short we just repeat this procedure 10,000 times no big deal here I've drawn a histogram of the 10,000 P values generated by testing samples taken from the same Distribution on the X AIS we have possible value for p values on the Y AIS we have the number of P values in each bin 510 P values or 5.1% are less than 005 close to 5% of the P values are between 0.5 and 0.1 actually each bin contains about 5% of the P values about 500 P values per B since the p values are uniformly distributed there's an equal probability that a test P value falls into any one of these bins now let's look at how P values are distributed when they come from two different distributions and by two different distributions I mean two different types of mice or we have wild type versus knockout or control versus drugged we're just comparing two different situations like before we start off with test number one but now we have two different distributions the black distribution is for our control mice the red distribution is for mice that have been treated with a drug in this example the drug increases this Gene's transcription like before we take two samples since the sample are now coming from two separate distributions there's a higher likelihood that the two samples will be separated and not overlap when we do the statistical test in this case we get a P value that equals 0.03 and then we do the exact same thing for test number two notice that both of the P values are less than 05 so they're statistically significant since the samples were taken from two separate distributions this is what we'd expect like before we repeat this process 10,000 times here I've drawn a histogram of the 10,000 P values generated by testing samples taken from two different distributions most of the P values are less than 05 this is what we'd expect the P values greater than 0.05 are false negatives from where the samples overlapped you can reduce the number of false negatives by increasing the sample size to summarize what we know so far when the samples come from the same distribution the P values are uniformly distributed but when the samples come from different distributions the P values are heavily skewed and closer to zero now imag imagine we're doing an experiment where we are testing all of the active genes in neuronal cells one set of neuronal cells is treated with a drug the other is not the drug might affect 1,000 genes the measurements for these genes will come from two different distributions the black sample is from the control cells and the red sample is from the cells treated with the drug since the samples come from different distributions the P values are skewed the remaining 9,000 active genes might not be affected by the drug this means the measurements for most of the genes will come from the same distribution the P values for these genes should be uniformly distributed the histogram of P values we obtain from all 10,000 genes is the sum of the two separate histograms the uniformly distributed P values come from the genes unaffected by the drug the P values on the left side are a mixture from genes affected by the drug and genes unaffected by the drug by I we can see where the P values are uniformly distributed and determine how many tests are in each bin here I've drawn a line indicating that about 450 P values are in each bin in the uniformly distributed part of the histogram we can extend this line and use it as a cut off to identify the true positives since we usually use a cut off of 05 we're going to focus on these P values roughly 450 P values less than 05 are above the dotted line and roughly 450 P values less than 05 are below the dotted line one way to isolate the true positives genes affected by the drug from the false positives would be to only consider the smallest 450 P values this procedure Works fairly well because the p values within the bins are skewed for the genes affected by the drug note this histogram is for p values between 0 and .5 and spread evenly for the genes not affected by the drug bam if you can understand these Concepts then you understand more about false Discovery rates and the benjamini hochberg method than most people who use it all benjamini and hochberg did is convert this procedure that we just did by I into a mathematical formula so now let's talk about the details of the benjamini hochberg method like I just said it's based on the eyeball method we just saw the benjamini hurg method adjusts P values in a way that limits the number of false positives that are reported as significant adjust P values means that it makes them larger for example before the false Discovery rate correction your P value might be 0.04 I.E significant after the FDR correction your P value might be 0.06 no longer significant if your cut off for significance is FDR values less than 0.05 then less than 5% of the significant results will be false positives in other words these are the genes with P values less than 0.05 the black box shows the genes with FDR modified P values less than 0.05 notice that not all of the true positive genes are inside the Box however only 5% of the modified P values in the Box are false positives the remaining 95% are true positives why don't all of the true positive genes have adjusted fdrp values less than 0.05 because not all true positive genes will have super small P values here's the histogram of true positive P values less than 0.05 these genes on the right side of the histogram probably won't remain significant after adjustment surprisingly the math behind the benjamini hochberg method is simple let's take a look let's start with another simple example we'll take 10 pairs of samples taken from the same distribution I.E 10 genes that were not affected by the drug and here are the P values from those 10 tests the first thing we do is we order the P values from smallest to largest notice that one of the P values is a false positive that is to say it's less than 0.05 let's see what the Benjamin hochberg method does to it the second step is to rank the P values and Let's Make spaces for the FDR adjusted P values that we're going to create the largest FDR adjusted P value and the largest P value are the same the next largest adjusted P value is the smaller of two options either the previous adjusted P value which in this case equals 0.91 or the current P value times the total number of P values divided by the P Val rank in this case the current P value is 0.81 the total number of P values is 10 and the P value rank is nine plugging these numbers in we get 0.90 since we select the smaller option we're going to go with 0.90 for the next largest adjusted P value we just repeat step four that is to say we select the smaller of these two options plugging in the numbers gives us the choice of 0.90 or 0.89 since we use the smaller value we go with 0.89 and for the next largest adjusted P value okay you get the idea we just repeat until we've adjusted the remaining P values and here I've plugged in the remaining adjust P values that false positive P value is no longer significant hooray now let's look at a huge example the blue boxes represent the P values from when the samples came from two separate distributions that is to say these P values are for genes that were affected by the drug I've made these P values relatively small to deflect their normal skew the P values in red boxes came from samples taken from the same distribution note we've got some false positives the eyeball method suggests we draw a line at the top of the uniformly distributed P values and extended to separate the false positives from the true positives these are the P values that the eyeball method suggests are true positives now let's see what the benjamini hurg method does here I've shown you the adjusted P values the false positives are now all greater than 05 but these true positives remain less than 05 double bam hooray we made it to the end tune in next time for another exciting stat Quest"
MIgCbrNJyGo,2017-01-02T16:28:35.000000,Psycho Killer,we persist for about three or four months every day about two and a half hours this is the final product [Applause] [Music] [Music] [Music] [Music] just discern [Music] [Music] but your face [Music] [Music] [Music] [Music] yeah [Music] [Music] [Applause] [Music] [Music] [Music] [Applause]
UFhJefdVCjE,2016-10-11T17:46:29.000000,p-hacking and power calculations,"St St stat Quest hello and welcome to stat Quest stat Quest is brought to you by the friendly Folks at the University of North Carolina at Chapel Hill in the genetics department today we're going to be talking about packing and power we're going to describe a p Pitfall and how to prevent it here's the scenario it's late at night and the grant is due tomorrow you finished a batch of experiments you do the stats and the P value is 0.051 ug ug Deluxe you have time to run one more replicate what do you do before you answer that question let's do a little thought experiment here's an old favorite the normal distribution for the sake of the example Le imagine that this curve represents the weights of an inbred laboratory Mouse strain if we weighed mice most of the measurements would be in the middle close to the average only a handful would be far from the average since these weights are from an inbred Mouse strain one sample shouldn't be very different from another sample for example if we weigh three mice and call that sample one and then we weigh three morice and call that sample two and then do a t test on the two samples in theory 95% of the time the test P value should be greater than 0.005 P values less than 05 are called false positives in other words most of the time we expect the two samples to be close to each other and to overlap rarely we might get something like this where the two samples are pretty far apart from each other when this happens the PV value is less than 05 and we conclude that the data were generated from two separate distributions or in this case two different Mouse strains here we see what the test thinks happened it thinks that the samples came from two separate distributions instead of one distribution this is how you get a false positive sometimes I want to focus on the samples that have P values barely greater than 05s we'll talk about false positives some other time to do this I generated 1,000 data sets from a normal distribution and then I performed T tests on each data set here we see the first 20 P values there were 980 more and we see that I got the occasional false positive as as long as this doesn't happen more than 5% of the time this is okay the goal is to have a test that works 95% of the time for most of science the cost of benefit ratio of being more stringent doesn't make sense here's a histogram that I drew of all 1,000 P values and here are the P values that were less than 05 I had 53 of them 53 divided by a th000 is 5.3% hooray this means the T Test performs as expected these P values are the ones that are close to being significant these are the 0.0051 P values the ones that keep you up late at night sweating out thinking maybe I should do one more replicate so just to see what would happen I added one more random value to data sets with P values between 05 and 0.1 and guess what 30% of the new T tests resulted in P values less than 05 in other words when totally bogus data gave me a close P value adding more totally bogus data gave me a significant P value 30% of the time that my friends is a huge false positive rate so the moral of the story is this don't just add samples until you get a good P value this increases your chances of reporting a false positive instead do a Power calculation before you do the experiment of determine how many samples you need to do alternatively if you didn't do it before use your new data to do it now um what's a power calculation it's a way to determine how many samples you need in ADV advance of doing an experiment in order to correctly get a small P value before we get into the nitty-gritty details of what power is and how to do power calculations let's get a general sense of the concepts here we have two normal distributions imagine this one on the left represents weights of mice on a diet and the one on the right represents weights of mice not on a diet it's pretty easy to see the difference between these two groups with relatively small samples from each group for example when n equals 3 there is a high probability that a t test would correctly give us a small P value if the diet wasn't very good the two distributions might have a lot of overlap with the same relatively low sample size Nal 3 there's a lower probability that a test will correctly give us a small P value power equals the probability a test will correctly give you a small P value in the first example I showed you where we had two distributions that were clearly separated from each other we had high power even with a small sample size there was a high probability the T Test would correctly give us a small P value the second example I showed you is an example of having low power it's when the two normal distributions or whatever the distributions you're working with happen to be overlap considerably in this case a small sample size won't likely give us a small P value that says these are two separate distributions four things affect power the first thing is the effect size and we just looked at two examples of effect size in the first example we had a large effect size in the second example we had a small effect size the second thing that can affect power is the variation in the data here's an example of how variation can affect power in this specific example the effect size is relatively small but so is the variation in the distributions because the variation in the distributions is small there is a high probability that our samples will also have low variation this means that there is a relatively high probability that we will correctly get a small P value here's a second example of how variation can affect power the distributions have lots of variation because the variation in the distributions is large there's a high probability that our samples will also have large variation this means that there is a relatively low probability that we will correctly get a small P value when we don't have very much variation in the data we can have high power even when the effect size is relatively small on the other hand lots of variation in the distributions can cause us to have low power okay so now we're back to talking about the four things that affect power and so far we've talked about effect size and we've talked about variation in the data now let's talk about the sample size a large sample size can compensate for a small effect size and high variation to keep things simple let's assume we're using a T Test T tests compare means and the means are estimated from the samples let's take a look look at how sample size can affect these estimates specifically let's take a look at how sample size affects our confidence in these estimates if the sample size was set to one then we would only weigh a single Mouse to estimate the mean here's a sample where we just weighed a single mouse and with that single measurement we estimated the mean if we did the experiment again we would weigh another mouse and have another estimate for the mean most of the time our estimated means would be in this range but sometimes the estimates would be way way off to see how much variation there is in the estimates I plotted a histogram of 1,000 random samples from a normal distribution centered on zero note this histogram looks squished compared to the distribution above because the y axis have different scales because a fair number of estimated means are pretty far from the true mean it's hard to have a lot of confidence in the estimates now let's see what happens when the sample size is set to two this time the mean is estimated by the average of the two samples here's where the second sample would estimate the mean and here's where the third sample would estimate the location of the mean notice that the occasional wonky point doesn't pull the estimated mean too far away from the true mean here's the histogram of 1,00 means measured when the sample size is one and here's a histogram of 1,000 means when the sample size is two we see that more of the estimated means are closer to the true mean there is a higher probability that the estimated mean is close to the true mean the higher the probability we have a good estimate the more confidence we have in it here's a histogram of 1,000 means when the sample size is set to five as we increase the sample size more and more of the estimated means are closer to the true mean again the larger the sample size the higher the probability we have a good estimate when the effect size is massive we don't need to have a lot of confidence in the accuracy of the estimated mean in order to correctly detect a difference even with a small sample size we have a high probability of getting a small P value power equals the probability you will correctly get a small P value when the effect size is small we need to have a lot of confidence in the accuracy of the estimated mean in order to correctly detect a difference here are histograms of means calculated with a sample size set to three from these two distributions that are overlapping considerably since there's so much overlap the T Test will probably give us a large P value now I've set the sample size to eight that is to say I'm estimating the mean from each distribution using eight samples and here we see that there is still a lot of overlap but we have a greater chance of correctly getting a small P value we have more power with Nal 8 than n = 3 and when n equal 101 we can see that our estimated means create two distinct distributions with a huge sample size we have lots of power a high probability we will correctly get a small P value one super important detail the variation in the distributions determines how much power will increase when you increase the sample size because the Distribution on the left doesn't have much variation samples will be relatively close to the true mean from the get-go it will not take a large sample size to get a reliable estimate of the mean in contrast the Distribution on the right has lots of variation and samples can be relatively far from the mean this distribution will require a larger sample size than the other to get a reliable estimate of the mean so far we've talked about three of the four things that affect power the effect size the variation in the data and the sample size the last thing that can affect power is the statistical test that you use some tests are more powerful than others the good news is that while different tests have different Power the T Test which is very commonly used is a surprisingly powerful test now that we know what power is let's talk about how to use it to determine what sample size we'll need for an experiment here's how to do a Power calculation first you need some preliminary data or preliminary guess you might have some old data laying around or you might have some publicly available data that's very similar to what you want to acquire or you might just have a good hunch that your experiment might be similar to another one that you've already perform if you don't have any data or even a guess that's okay there are still things you can do for now let's assume we have some preliminary data we can use use that data to estimate means we can also use it to calculate standard deviations with the means and standard deviations we can estimate what the distributions look like in this case the effect size is small there's quite a bit of overlap so we will need a large sample size to get good power remember the histograms of means calculated from different sample sizes we saw that the variation in those histograms decreased as the sample size increased the variation in the means is called the standard error and it is easily calculated here's the equation for the standard error it's just the standard deviation divided by the square root of the sample size we've got the standard deviation from the preliminary data and here we have the sample size which is what we're interested in finding out because the sample size is in the denominator that confirms what we can see the larger the sample size the smaller the variation power calculations boil down to solving for n in the standard error equation n has to be large enough so that the probability of correctly getting a small P value is high since you're never going to calculate power by hand and I'll I'll mention software that does this in a bit I'm not going to focus on the details of how to solve for n instead it is more important to understand the concepts behind the equation understanding the concepts will also help when you don't have existing data or a good hunch here are the main concepts for power calculations effect size variation in the data and sample size to do a Power calculation we need two of these three variables if we have preliminary data and can estimate the effect size and the variation you can calculate n if you do not have preliminary data you can start by assuming you will do three replicates then you can assume that you are looking for a two-fold difference then you will see how much variation will still give you good power if the variation is unrealistically small increase your sample size or increase the effect size if you doing RNA seek the biological variation has already been worked out for human data and inbred laboratory species so for this particular experiment you can just plug in the known variation then you either plug in an effect size you want to detect and then determine the sample size or you plug in a sample size and determine the effect size like I said before you should never have to do a Power calculation by hand there are nice websites and programs that will calculate in or the effect size or variation for you back to our scenario it's late at night and the grant is due tomorrow you finished a batch of experiments you do the stats and the P value is 0. 051 ug ug Deluxe you have time to run one more replicate what do you do the answer to this question is use your results as preliminary data and do a proper power calculation instead of writing about a P value say that the P value is suggestive and that your power calculation gives you the sample size for the next experiment and that's the end of our exciting stat Quest tune in next time for super duper exciting stat Quest on false Discovery rates and P values and how they're related and what they mean it's going to be cool so check it out"
azXCzI57Yfc,2016-07-10T21:36:51.000000,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,"St qu stats coming at you st qu step's going to find you st qu watch out hello and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetic department at the University of North Carolina at Chapel Hill today we're going to be talking about linear discriminant analysis which let's be honest sounds really fancy and it kind of is but not really I think we can understand it let's see what it does and then we'll work it out that is let's look at some examples of why we might need linear discriminate analysis and then we'll talk about the details of how it works a imagine that we have this cancer drug and that cancer drug works great for some people but for other people it just makes them feel worse we want to figure out who to give the drug to we want to give it to people who it's going to help but we don't want to give it to people that it might harm and since I'm a geneticist and I work in a genetics department the way I answer all my questions is to look at gene expression maybe gene expression can help us decide here's an example using one gene to decide who gets the drug and who doesn't we've got a number line and on the left side we've got fewer transcripts and on the right side we've got more transcripts the dots represent individual people the green dots are people who the drug works for the red dots represent people whom the drug just makes them feel worse we can see that for the most part the drug works for for people with low transcription of Gene X and for the most part the drug does not work for people with high transcription of Gene X in the middle we see that there's overlap and that there's no obvious cut off for who to give the drug to in summary Gene X does an okay job at telling us who should take the drug and who shouldn't can we do better what if we used more than one gene to make a decision here's an example of using two genes to decide who gets the drug and who doesn't on the xais we have Gene X and on the Y AIS we have Gene y now that we have two genes we can draw a line that separates the two categories the green where the drug works and the red where the drug doesn't work and we can see that using two genes does a better job separating the two categories than just using one gene however it's not perfect would using three genes be even better here I've got an example where we're trying to use three genes to decide who gets the drug and who doesn't Gene Z is on the Z axis which represents depth so imagine a line going through your computer screen and into the wall behind it and the big circles or the big samples are the ones that are closer to you and the smaller circles smaller samples or the ones that are further away and those are along the z-axis when we have three dimensions we use a plane to try to separate the two categories now I'll be honest I drew this picture but even for me it's hard to tell if this plane separates the two categories correctly it's hard for us to visualize three dimensions on a flat computer screen we need to be able to rotate the figure and look at it from different angles to really know and that's tedious what if we need four or more genes to separate two categories well the first problem is we can't draw a four-dimensional graph or a 10,000 dimensional graph we just can't draw it that's a bummer we ran into the same problem when we talked about principal component analysis or PCA and if you don't know about principal component analysis be sure to check out the stat Quest on that subject it's got a lot of likes and it's helped a lot of people people understand how it works and what it does PCA if you can remember about it reduces Dimensions by focusing on genes with the most variation this is incredibly useful when you're plotting data with a lot of Dimensions or a lot of genes onto a simple XY plot however in this case we're not super interested in the genes with the most variation instead we're interested in maximizing the separability between between the two groups so that we can make the best decisions linear discriminant analysis LDA is like PCA it reduces Dimensions however it focuses on maximizing the separability among the categories let's repeat that to emphasize the point linear discriminant analysis LDA is like PCA but it focuses on maximizing the separability among the known categories here we're going to start with a super simple example we're just going to try to reduce a two-dimensional graph to a 1D graph that is to say we want to take this two-dimensional graph AKA and XY graph and reduce it to a one-dimensional graph AKA a number line in such a way that maximizes the separability of the two categories what's the best way to reduce the dimensions well to answer that let's start by looking at a bad way and understanding what its flaws are one bad option would be to ignore Gene Y and if we did that we would just project the data down onto the X AIS this is bad because it ignores the useful information that Gene y provides projecting the genes onto the y- axis I.E ignoring the gene X isn't any better LDA provides a better way here we're going to try to reduce this two-dimensional graph to a 1D graph using LDA LDA uses the information from both genes to create a new axis and IT projects the data onto this new axis in a way to maximize the separation of the two categories so the general concept here is that LDA creates a new axis and IT projects the data onto that new AIS in a way that maximizes the separation the two categories now let's look at the nitty-gritty details and figure out how LDA does that how does LDA create the new axis the new axis is created according to two criteria that are considered simultaneously the first criteria is that once the data is projected onto the new axis we want to maximize the distance between the two means here we have a green m character which is a Greek character representing the mean for the green category and a red Mew representing the mean for the red category the second criteria is that we want to minimize the variation which LDA calls scatter and is represented by S squar within each category on the left side we see the scatter around the green dots on the right side we see the scatter around the red dots and this is how we consider those two criteria simultaneously we have a ratio of the difference between the two means squared over the sum of the scatter the numerator is squared because we don't know if the green me is going to be larger than the red mu or the red mu is going to be larger than the green me uh we don't want that number to be negative we want it to be positive so whatever it is um whether it's negative or positive begin with we Square it and it becomes a positive number now ideally the numerator would be very large there'd be a big difference or a big distance between the two means and ideally the denominator would be very small in that the scatter the variation of the data around each mean in each category would be small now I know this isn't a very complicated equation but to make things simpler later on in this discussion let's call the difference between the two means D for distance so we can replace the difference between the two means with d now I want to show you an example of why both the distance between the two means and the scatter are important here's a new data set we still just have two categories green and red in this case there's a little bit of overlap on the Y AIS but lots of spread along the X AIS if we only maximize the distance between the means then we'll get something like this and the result is we'll have a lot of overlap in the middle this isn't great separation however if we optimize the distance between the means and the scatter then we get nice separation here the means are a little closer to each other than they were in the graph on the top but the scatter is much less so if we optimize both criteria at the same time we can get good separation so what if we have more than two genes that is to say what if we have more than two Dimensions the good news is that the process is the same we create a new AIS that maximizes the distance between the means for the two categories while minimizing the scatter so here's an example of trying to do LDA with three genes we've got that three-dimensional graph that I showed you earlier here we've created a new axis and the data are projected onto the new axis this new axis was chosen to maximize the distance between the two means between the two categories that is while minimizing the scatter what if we have three categories in this case two things change but just barely here's a plot that has two genes but now we have three categories the first difference between having three categories as opposed to just two categories like we had before is how we measure the distances among the means instead of just measuring the distance between the two means we first find a point that is Central to all of the data then we measure the distances between a point that is Central in each category and the main Central Point now we want to maximize the distance between each category and the Central Point while minimizing the scatter for each category and here's the equation that we want to optimize and this is the same equation as before but now there are terms for the blue category the second difference is LDA creates two axes to separate the data this is because the three Central points for each category Define a plane remember from high school two points toine a line and three points to find a plane that is to say we create new X and Y axes however these are now optimized to separate the categories when we only use two genes this is no big deal the data started out on an XY plot and plotting them on a new XY plot doesn't change all that much but what if we use data from 10,000 genes that would mean we need 10,000 Dimensions to draw the data suddenly being able to create two axes that maximize the separation of the three categories is super cool it's way better than drawing a 10,000 Dimension figure that we can't even imagine what it would look like here's an example using real data I'm trying to separate three categories and I've got 10,000 genes plotting the raw data would require 10,000 axes we used LDA to reduce the number to Two And although the separation isn't perfect it is still easy to see three separate categories now let's use that same data set to compare LDA to PCA here's the LDA plot that we saw before and now we've applied PCA to the exact same set of genes PCA doesn't separate the categories nearly as well we can see lots of overlap between between the black and the Blue Points however PCA wasn't even trying to separate those categories it was just looking for the genes with the most variation so we've seen the differences between LDA and PCA but now let's talk about some of the similarities the first similarity is that both methods rank the new axes that they create in order of importance pc1 the First new access that PC a creates accounts for the most variation in the data likewise PC2 the second new access does the second best job and this goes on and on for the number of axes that are created from the data ld1 the First new AIS that LDA creates accounts for the most variation between the categories ld2 the second new access does the second best job etc etc etc also both methods let you dig in and see which genes are driving the new axes in PCA this means looking at the loading scores in LDA one thing you can do is look and see which uh which genes or which variables correlate with the new axes so in summary ldas like PCA both try to reduce Dimensions PCA does this by looking at the genes with the most variation in contrast LDA tries to maximize the separation of known categories and that's it tune in next time for another exciting stack Quest"
oMtDyOn2TCc,2016-01-06T15:46:44.000000,Drawing and Interpreting Heatmaps,stack Quest stack Quest stack Quest hello and welcome to stack Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about heat Maps I know you've seen them many times before so let's just get started here's a heat map that I drew for a friend of mine not too long ago the rows are jeans and the columns are RNA seek samples the data displayed in this heat map has been modified in two ways so that we can gain some insights from it the first way is that the relative abundancies have been scaled in this case this was done on a per Gene basis other heat Maps you've seen out there scale all the genes at once anyways this makes it easy to see that sample X has more or less of Gene y than sample Z for example the scaling makes it easy to see that sample one expresses this Gene highlighted in the Black Box more than the others however this specific Gene by Gene scaling means that we can't compare across genes the dark red bar in Sample one for this Gene doesn't mean that sample one transcribes it more than other genes just other samples the other modification that was done to this data is that the row that is to say the genes have been grouped according to similarity this grouping or clustering makes it easy to see genes that are transcribed most in the second sample and least in the fourth sample these genes are transcribed most in the first sample and also least in the fourth sample and lastly these genes are transcribed most in the second sample and least in the third sample the clustering isn't by chance but due to a computer program it tries to put similar things close together we'll talk about this in more detail later on without clustering the data would look like this a mish mash that's harder to interpret without clustering or scaling the data would look like this which is completely uninterpretable notice that one gene is highly transcribed compared to the others it's an outlier okay now that we've seen one heat map let's look at another slightly more complicated heat map like the heat map before this heat map has been scaled and clustered however the scaling is global not per row or Gene but for all rows and genes we can use Global scaling because we don't have an outlier like we did in the last data set the clustering in this heat map is done for both The Columns and the rows not just the rows like we did last time that is to say both the samples and the gene are clustered for example we can see that these columns or samples that we've highlighted in the Black Box cluster together indicating that the transcription patterns in these samples are similar we also see that these rows or genes cluster together this indicates genes that have similar transcription patterns across samples without the clustering we have a total mismash and it's hard to see what's what without clustering or scaling we have a total disaster before we move on to the details of scaling and clustering here's a quick aside what if we had used Global scaling with the first heat map when we do this we see that the outlier skews the scale so much that it's impossible to see the other genes also notice that the clustering changes and the genes have a new order scaling can affect two things how brightly colored the genes are and whether you can compare between them and the clustering and now back to the action now we're going to talk about how to scale the data regardless of whether you do it by Gene or globally the most common method is nameless I hate to coin a new term but let's call it zcore scaling because technically it converts the data to z-scores here's how to do it let's talk about converting to zc scores or zcore scaling in this example we have RNA seek read Counts from six samples the first step is to calculate the mean of the data in this case that's 16.5 the second step is to subtract the mean from each value by subtracting the mean from each value we centered the data around zero samples with relatively High transcription get positive values and samples with relatively low transcription get Negative values the third step is to calculate the standard deviation in this case it's 6.28 the fourth step is to divide each data point by the standard deviation notice that the scale on the axis has changed the data used to be spread from minus 8 to + 8 now it's between -1 2 and 1.2 for you math nuts out there here's the formula for zcore scaling in the numerator we have the mean subtracted from each sample value this is then divided by the standard deviation over on the right we've got the fancy Greek notation for this regardless of the variation in the original data dividing by the standard deviation ensures that it's tightly grouped and you might ask yourself why do we need to ensure the data is tightly grouped we do this because we can only discern so many shades of colors The Wider the range the more subtle the differences in the shades by tightly grouping the data we use fewer Shades and it's easier to see sample one has more transcription than sample two another brief aside what if there is an outlier in this example sample a is our outlier everything else is relatively tightly clustered in this case the standard deviation is going to be much larger than what we had before that is to say the denominator will be much larger and the values near zero will get compressed a lot and it will be hard to separate them with only a few Shades this will make it a lot harder to see what's going on when we did Global scaling on the data set with the outlier we saw what happens one gene is clearly highly expressed but we can't see any differences in the other genes okay now that we understand scaling let's move on to clustering this is the fun part there are two main types of clustering hierarchical and K means we'll focus on hierarchical clustering for now here's an example of how hierarchical clustering Works we're going to start with a simple example that has three samples and four genes for this example we are just going to use clustering to reorder the rows or the genes conceptually here's what we do first we figure out which Gene is most similar to Gene one so we look and we see that genes one and two are different genes 1 and three are similar and genes 1 and four are also similar however Gene one is most similar to Gene three the second step is to figure out which Gene is most similar to Gene number two and then we'll figure out which Gene is most similar to three and then which Gene is most similar to four in this example Gene 2 is most similar to Gene 4 also note there's a little typo in the text where I have the plural genes when I meant the singular Gene this is embarrassing but we're just going to go with it here's the third conception step of the different combinations figure out which two genes are the most similar merge them into a cluster in this example genes 1 and three are more similar than any other combination so genes 1 and three are now cluster number one now the last step in our conceptual list of things to do is step four we go back to step one but now we treat the new cluster that's cluster number one like it's a single Gene that is to say we compare cluster one to find out which Gene it's most similar to in this case it's most similar to Gene number four Gene 2 is most similar to Gene number four and genes 2 and four are the most similar combination lastly we merge genes two and four into a cluster and we're all done hierarchical clustering is usually accompanied by what's called a dendr it indicates both the similarity and the order that the Clusters were formed cluster number one was formed first and the genes within it are most similar to each other cluster number two was formed second and the genes within it are the second most similar cluster number three which contains all of the genes and merges the two clusters was formed last now that we have a conceptual idea of what's going on let's get down to the nitpicky details in the first step we have to figure out which Gene is most similar to Gene one however before we do that we have to Define what most similar means unfortunately the method for determining similarity is arbitrarily chosen however there are some common practices the first and most common one is the ukian distance between genes here's the formula for that it's the square root of the square of the difference in Sample one between Gene one and Gene 2 plus the square of the difference in Sample two of Gene one and Gene 2 lastly we have the difference in Sample three between genes one and genes 2 and if you have more samples the equation just keeps going on and on and on off the page but you get the idea to see the ukian distance in action let's assume there are only two samples and two genes so here we've restricted our data set to two samples and two genes when there are only two samples and two genes the formula boils down to the Pythagorean theorem just to show everything action here are some values that we can use to compute the distance now we've plugged those numbers into our formula on the left side of the equation we see the values for sample one the difference between genes 1 and two on the right side we see the values for sample number two the difference between genes 1 and two here we've drawn it out so that you can see how it's related to the Pythagorean theorem we've put sample one on the xais and and Sample two on the Y AIS the hypotenuse is the distance between genes 1 and two so that's how the ukian method Works however the ukian distance is just one method there are lots more including the Manhattan method can Bara and many others for example the Manhattan distance is just the absolute value of the differences rather than squaring and then taking the square root and and yes it makes a difference bummer here's that first heat map I showed I drew it using the ukian distance here's what it looks like when we use the Manhattan distance instead we see that the large clusters remain intact even though they might be in different orders than they were before however with some of the smaller clustering in the finer resolution we see more differences however the choice to use the ukian distance or the Manhattan distance is arbitrary there's no real biological reason why one metric might work better than another bummer here's some more nitpicky details about how hierarchical clustering Works do you remember how we merged genes 1 and three into cluster number one and then compared that cluster to the other genes well there are different ways to do that too one simple idea is to compare other genes to the average of the measurements from each sample but there are lots more and these different methods affect clustering as well bummer here we're going to look at different ways to compare clusters for the sake of visualizing how different methods work imagine our data was spread out on an XY plane now imagine that we have already formed these two clusters the green dots and the orange dots and we just want to figure figure out which cluster this last Point belongs to we can compare that point to one the average two the closest point three the furthest point and there are many many other ways to do this here are some examples from the second heat map that I showed you here what we're doing is we're just swapping out the ways we're comparing clusters in this first heat map we're comparing the points to the furthest in the cluster so we're comparing a gene to a cluster and we're finding the gene that's most different from that one we're comparing it to this is the default setting in R here's what the clustering looks like when we compare points to the cluster average as you can see the major blocks of cluster genes and samples have been retained even though they've been reordered however there are differences in the detail details here's an example where we compare points to the closest point in the cluster again the major features of the clustering have been retained but are now reordered again and again there are differences in the details in summary to make a heat map you first scale the data either per gene or globally second you cluster the data either by Gene or sample are both Gene and Sample in this stat Quest We focused on hierarchical clustering and we've seen that within that we've got to make some decisions the first is what's the distance metric going to be will it be ukian Manhattan or something else and we also have to decide what the clustering method's going to be will it be centroid or average or look at the furthest point or the closest point there are all these different choices that we have to make the good news is is most of these choices are set to some default value if you're doing this in R just run it and see what you get if it looks good go with it if we don't want to use hierarchical clustering we can use a method called K means when we do this we have to decide how many clusters there should be in advance then the computer figures out which samples go in which cluster by Trying to minimize some Metric of dispers erson I.E it's trying to reduce the amount of variance within each cluster this deserves a separate stat Quest we're not going to talk about it today so that brings us to the end thanks for listening to stat Quest and look forward to more exciting quests in the land of statistics
K2QWmW6LKmg,2015-12-05T17:11:39.000000,Christmas In Rio! (now on iTunes!),it's Christmas time in Rio yeah while the sun shines down on Rio wo wo Pap in the well he suring the wav he's got his speed W and he's got his Shades he shaved his beer last week cuz it's just too warm this is the midsummer heat not a mid winter storm we got people singing no we just finished plugging in our electrical tree because Christmas time in Rio wo wo yeah while the sun shines down on Rio wo wo when he not hanging CH butes and green light PHX employed making presents by hand check out the praise it's back to the Bone you've got your Amigo secretto so you never alone the Miso dualo is crazy it lasted all night let's eat a fish water till we see the Morning Light because Christmas time in Rio wo wo yeah while the sun shines down on Rio wo [Music] wo cuz it's Christmas time in Rio yeah while the sun shines down on Rio w W it's Christmas time in real W wo yeah while the sun shines down real W wo
gKnfP2_Xdpo,2015-08-27T19:47:18.000000,StatQuest: RNA-seq - the problem with technical replicates,hello and welcome to stat quest stat quest is brought to you by the friendly folks in the genetics department at the University of North Carolina in Chapel Hill today we're going to talk about RNA seek technical replicates and answer the question do we need them and the answer is a resounding no not if we have biological replicates you may have heard that answer before but you might not know why that's why we're also going to answer the question why not just to warn you this stat quest gets a little mathy but trust me you can understand all of it it's simple and I walk through it step-by-step so just bear with me it'll all make sense in the end when we do high throughput RNA sequencing there are two main sources of variation that we get in our data the first source of variation is biological variation no two mice will have the exact same number of RNA transcripts even if they are genetically identical this could be said about anything from humans to fruit flies there's also technical variation each time we do an experiment there's so many little factors involved including some that are 100% random that is impossible to get the exact same results twice even from the same organism to understand how these two sources of variation can affect our results we're going to start with an example in which there is only biological variation that is to say for the time being imagine we can do RNA sequencing without any technical variation here we're going to plot read counts for an imaginary gene X for several different mice for this example the exact number of reads per Mouse isn't important just the difference between replicas thus the y-axis isn't labeled with numbers now we've plotted the read count for gene X for our first Mouse now we've plotted the read counts for our second Mouse since there is no technical variation the difference in read counts is only from the biological variation between the two mice here are the read counts for a third Mouse and here the read counts for a bunch of other mice so many that we haven't even numbered them all now if we had an infinite budget and an infinite amount of time we could sequence every single Mouse on the planet and get an average for all mice for gene X in this example we use the Greek letter mu to represent the average for all mice now that we've calculated mu the average for all mice we can quantify the biological variation for each individual Mouse as the difference between its read counts for gene X and mu the average in this example Mouse number one has five more reads than the average read count and Mouse number two has one less read than the average and Mouse number three has four more reads than the average Mouse number four has two more reads than the average Mouse number five has five fewer reads than the average okay this is getting tiring let's imagine we only did five samples we can now formulate the read counts for each Mouse as mu plus the difference from you why we would like to do this will be seen Shirley so just hang on given our new fancy formulation for the read counts for each Mouse we can use them to calculate the average read count for gene X from the five samples that we've collected to calculate the average we do what we learned how to do in fifth grade we add up the five measurements and we divide by five in this case the measurements are fancy formulations for read count now I'm going to show you what happens when we shuffle the numbers around a little bit first I've shifted all the Meuse to the left side of the numerator and all the differences from you to the right side of the numerator we can now split the equation into the sum of two fractions the Meuse divided by five plus the differences divided by five here we've just combined the addition of the Meuse into a simple five times mu when we do this it's obvious that we can cancel out the five in the numerator and in the denominator once we cancel out the fives we see that the average is mu plus an additional fraction because the numerator of this additional fraction contains both positive and negative numbers they're going to cancel each other out that makes this fraction gets smaller and smaller the more samples we have this is one reason why when we estimate the mean with a small sample that estimate gets better and better the larger the sample gets and after a lot of samples all were left with is mu which is exactly what we're interested in finding out now let's consider what happens if we have both biological variation and technical variation to keep things clear we're going to label the biological variation with an orange color here we have orange lines indicating the difference between the read counts and the average we're also going to label the technical variation with a green color for the first mouse the technical variation reduced to the read count by two so we're going to give it a green arrow pointing down the second mouse had technical variation that increased the read count so we'll use an arrow pointing up and here we see the effects of technical variation on all five mice that we sampled just like before we can represent the read counts for each Mouse using a fancy equation like before we've got mu plus the biological variation but now we also have the additional technical variation we can use these fancy formulas to calculate the average now we can break that average up into three fractions one for MU one for the biological variation and one for the technical variation and just like before because the numerators contain both positive and negative numbers that cancel each other out with more samples these variation terms will go to zero at long last we're going to see what happens when we do technical replicates first let's just imagine that we had one mouse that we did five tests on just like before we're going to indicate biological variation with an orange color and just like before technical variation will be indicated with green arrows pointing up or down depending on which way the technical variation influenced the read count now let's calculate our averages as we can see with the biological variation the numerator only contains the number five over and over and over again there isn't a mixture of positive and negative numbers that could possibly cancel each other out in what we're left with is mu plus five the biological variation and a fraction that represents the technical variation because the numerator and the technical variation has positive and negative numbers that can cancel each other out it'll go to zero but the biological variation will stay constant at five now let's compare biological versus technical replicates as you'll recall when we have biological replicates the average read count for gene X equals mu plus two fractions one for the biological variation and one for the technical variation because these fractions contain both positive and negative numbers in the numerator that cancel each other out we know that both of these fractions we'll get smaller and smaller and closer to zero with each additional sample that we add thus the final average read count will tell us something about all mice when we don't have any biological replicates and only technical replicates then only the fraction representing the technical variation will cancel itself out this leaves our estimate for the read counts as mu plus the biological variation which only tells us about the single Mouse that we sampled thus this experiment only tells us about a single Mouse it's unlikely that this result will be replicated in any other lab what if we do biological and technical replicates can we get the best of both worlds unfortunately the answer is no and I'll show you why imagine we have two technical replicates for Mouse number one and three technical replicates for mouse number two just like before when we calculate the average read count from our sample we get the average mu plus the two fractions one for Biological and one for technical variation however look at the numerator for the biological variation if we add additional technical replicates not biological replicates we won't converge on zero if we do add additional biological replicates it will converge on zero but it'll take more replicates than before in this case it'll take three times the replicates since the terms don't cancel each other out as quickly let's examine how much slower the convergence is since we did three technical replicates for Mouse number two with three technical replicates we need 15 samples to get the same term as we did with five biological replicates as you can see on the left side this is what we had when we had five biological replicates it's equivalent to this long fraction on the right side so now let's compare all three different types of experiments experiments with just illogical replicates experiments with a mixture of biological and technical replicates and experiments in which there are only technical replicates the worst possible scenario is to not have any biological replicates and only have technical replicates this is because our result will only tell us about the single mounts that we studied it means that our results will not generalize meaning no one else will be able to replicate what we've done it's still pretty bad when we have a mixture of technical and biological replicates this is because if we don't do enough biological replicates we'll end up with an average read count that is equal to MU plus a constant which is only representative of the sample at hand or if we do enough biological replicates we'll have to add so many extra copies of them this takes more space on the sequencing machine and cost more money the best situation is when we have lots of biological replicates when this happens both variants terms go to 0 the fastest so when we use our sample to calculate the average read counts for gene X we end up with an unbiased estimate of mu the average for all mice this is a value that could be reproduced in any lab all around the world the moral of the story is uses many biological replicates as you can if you use technical replicates you run the risk of biasing your result because the biological variation term won't go to zero nearly as fast as it could have alright folks that's it tune in next time for another exciting stat quest
_UVHneBUBW0,2015-08-13T17:37:04.000000,Principal Component Analysis (PCA) clearly explained (2015),"step Quest step Quest stack Quest hello and welcome to stat Quest stack Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about principal component analysis or PCA for short let's start off with an example of principal component analysis in action here's an example PCA plot that I got from an article that I was just reading it shows clusters of cell types this graph was drawn from single cell RNA sequencing data there were about 10,000 transcribed genes in each cell and each dot in this graph represents a single cell and its transcription profile the general idea is that cells with similar transcription profiles should cluster and we sort of see that in this graph we see that blood cells form one cluster that's different from plur potent cells which is different from neuronal cells and dermal or epidermal cells so the big question is how does transcription from 10,000 genes get compressed into a single dot on a graph the answer is PCA PCA is a method for compressing a lot of data into something that captures the essence of the original data in this stat Quest we're going to learn all about how PCA does this compression also we're going to find out what these access labels refer to before we dive into the nitty-gritty of PCA we're going to cover a little background material we're going to have an introduction to Dimensions just to warn you this is going to seem very very simple but just hang in there you'll be glad we did this it'll keep your head from exploding if you can remember all the way back to first or second grade you'll remember that one dimension equals a number line now imagine we had a pretend RNA seek data set for a single cell here I've labeled the genes just a b and c and the read counts are 10 0 and 14 for those genes we can plot these values on the number on just like we did in first or second grade a with 10 reads gets a DOT at 10 Gene B with zero reads gets a DOT at zero and lastly Gan C with 14 reads gets a DOT at 14 if we plotted all genes we might see something like this a uniform distribution of transcript counts or we might get a non-uniform distribution of transcript counts some genes might not be transcribed very much and they'd be on the left side of our number line and some genes might get transcribed a lot and they'd be on the right side of our number line even though our number line is a very simple graph we can get some useful information out of it now let's fast forward to fifth or sixth grade when we learned about two dimensional graphs now we have two axes instead of just one and now we can plot data from two different cells instead of just one here's a pretend to RNA sequencing data set for two single cells just like before we have the same genes but now we have read counts for two separate cells if you can remember from fifth or sixth grade the way we plot the data for Gene a is we go over to 10 for cell one and we go up to eight for cell two and we put a point there for Gene B we go over zero for cell one so we don't move it all and we go up two for cell two and for Gene C we go over 14 and up 10 if we plotted all of the genes we might see something that looks like this here we see that the expression in the two cells is correlated meaning genes that are highly transcribed in cell one are also highly transcribed in cell two and genes that are lowly transcribed in cell one are also lowly transcribed in cell two or we might see that the expression in the two cells is not correlated meaning if a gene is highly transcribed in cell one that doesn't tell us anything about whether it's highly or lowly transcribed in cell two okay so maybe sometime when we took calculus we started drawing three-dimensional graphs that's just a fancy graph that has depth with three separate axes we can now plot data from three separate cells so now now our pretend RNA sequencing data set has data for three single cells and just like before if we wanted to plot the data for Gene a we would go over to 10 for cell one up to eight for cell two and then back eight for cell three we then draw lines perpendicular to each axis to figure out where they all meet and then we put a dot there I'm not going to do too many examples of this because you get the idea so this is what we know about Dimensions so far if we have one cell's worth of data we only need to have a one-dimensional graph which is just a number line if we have data from two cells then we need a two-dimensional graph which is just an XY graph that we learned about in fifth grade if we have data from three cells then we need a three-dimensional graph that's a fancy graph with depth what happens if we of data from four separate cells you guessed it we need a four-dimensional graph the problem is we can't draw that on paper and if we had data from 200 individual cells we'd need a 200 dimensional graph and there's no way we can draw that so the question is are all of those Dimensions super important or are some more important than others to answer that question we're going to go back to a data set that just has two cells and two dimensions hypothetically speaking what if we had two cell data that look like this here we see that almost all of the variation in the data is from left to right that is to say cell one has some genes that are lowly transcribed and some genes that are highly transcribed but it looks like all of cell 2 genes are all transcribed at the same level if we flatten the data that is removed the up and down variation our graph wouldn't look look much different from what it looked like before and if we flatten the data we could just graph it with a single number line in this case we can take two-dimensional data and display it on a one-dimensional graph without too much loss of information both graphs say the important variation is left to right here's another example of how some dimensions are more important than others TV and movies TV and movies are almost always 2D that is they're shown on flat screens at home or in the movie theater and we don't usually have fancy 3D goggles on when we watch them so they're 2D even though the subjects in the movie are 3D this is okay the third dimension usually doesn't add that much to the story This Is Why when we cough up the extra three or four dollars to watch a movie in 3D we're usually disappointed anyways people look like people things look like things even when they have no depth and are flattened on a screen basically a movie camera takes 3D information and flattens it to 2D without too much loss of information to summarize what we know so far we know that each cell that we sequence adds another dimension and we also know that some dimensions are more important than others so what does all this have to do with PCA well PCA takes a data set with a lot of Dimensions I.E of cells and flattens it to just two or three dimensions so we can look at it it tries to find a meaningful way to flatten the data by focusing on the things that are different between the cells we're going to talk a lot more about this later for any biologists out there this is sort of like flattening a zstack of microscope images to make a single two-dimensional image for publication so let's start with an example again we'll just start with two cells here's the the data like before the genes are imaginary so I've just listed them from a to I and here's a 2d plot from the data from two cells generally speaking the dots are spread out along a diagonal line another way to think about this is that the maximum variation in the data is between the two end points of this line and generally speaking the dots are also spread out a little above and a little below the first line that we drew another way to think about this is that the second largest amount of variation is at the end points of this new line that we just drew if we rotate the whole graph the two lines that we drew make new X and Y axes this makes the left right above and below variation easier to see we don't have to tilt our head anymore and like we saw before the data varies a lot to the left and the right and and the data varies a little up and down note all of the points can be drawn in terms of left and right and up and down just like any other 2D graph that is to say we don't need another line to describe diagonal variation we've already captured the two directions we can have variation with these two lines these two new or rotated axes that describe the variation in the data are principal components principal component one or pc1 the first principal component is the access that spans the most variation in the data PC2 or principal component number two is the access that spans the second most variation so these are the general ideas we've covered so far for each gene we plotted a point based on how many reads were from each cell principal component one captures the direction where most of the variation is principal component two captures the direction of the second most variation what if we had three cells just like before principal component one would span the direction of the most variation and principal component two would span the direction of the second most variation however since we have another direction we can have variation we need another principal component component that's principal component number three it spans the direction of the third most variation what if we had four cells principal component one would span the direction of the most variation principal component two would span the direction of the second most variation principal component three would span the direction of the third most variation and you guessed it principal component four would span the direction of the fourth most variation there's a principal component for each Dimension or each cell in the data if we had 200 cells we would have 200 principal components principal component 200 would span the direction of the 200th most variation hooray now that we know what pc1 and PC2 are we know what the X and Y axes are in this figure pc1 is the direction of the most variation in gene expression and PC2 is the second most variation in gene expression but I bet just right now you're asking yourself this question this is a plot of cells not genes how do we plot cells so far all we've talked about is how to plot genes to answer your question we're going to go back to the original scatter plot for two cells for now let's focus on principal component one the length and direction of pc1 is mostly determined by the circle genes the genes on the end points or the extreme genes now we're just going to move the graph over to the left side of the screen so we can put other interesting things on the right side if we wanted to we could score genes based on how much they influenced principal component number one and here's a list of qualitative scores that we might give each gene genes close to the ends of the line like a and F would have high scores because they highly influence pc1 the genes in the middle like B and C would have low scores we could also use quantitative scores for each gene so genes with little influence on principal component one would get values close to zero and genes with more influence would get numbers further from zero genes on opposite ends of the line we get similarly large numbers but with different signs so a might get a positive number like positive 10 and F because it's all the way at the other end of the line might get a negative number like -14 similarly we could also rank genes and how they influence principal component number two now we have two tables of genes and the influence they have on the principal components one is for principal component one and the other table is for principal component number number two now that we have these two tables for the first two principal components we can use them to plot cells and not just genes we do that by combining the read counts for all genes in a cell to get a single value here's how to do that first we return to the original read counts for each cell we can then calculate a score for cell one by taking the read count for Gene a and multiplying it by Gene A's influence on the principal component and adding that to the read count for Gene B multiplied by the influence of Gene B and doing that for all genes here's a concrete example for cell one gene a we have 10 read counts and the influence Gene a has is 10 so the first part of this summation is 10 * 10 the second part of the summation is the read count for Gene B which is zero multip IED by the influence Gene B has which is 05 we just continue to multiply and sum and multiply and sum until we've done it for each gene in the cell for this example we might end up with a number like 12 that would be our value for pc1 to calculate a value for principal component 2 we do the same thing as before except instead of using the weights or the influences on principal component one we use the weights or influences on principal component number two so in this case Gene a has 10 reads and we multiply it by three because that's the influence Gene a has on principal component number two we add to that the read counts for Gene B multiplied by the influence that Gene B has on principal component number two in this case that's 0 * 10 and we just do that for every single Gene again and we end up with a score for principal component number two and in this case that might equal six so we've done the math for cell one we've got values for principal component number one and a value for principal component number two now all we have to do is plot it on a graph and if we create a graph where the X AIS is principal component one and the Y AIS is principal component number two we can do what we did in fifth grade we just go over 12 and up six and put our DOT right there now we have to calculate scores for cell number two and if we did the math by multiplying the read counts for each gene by the influence that each gene has on the principal component we might end up with numbers like two for principal component one and eight for principal component number two again we just plot it like we did in fifth grade if we sequenced a third cell and it's transcription was similar to cell one it would get scores similar to cell ones and as a result when we plotted it on the graph cell number three would be closer to cell number one than it would be to cell number two Hooray at long last we know how they plotted all of the cells on this graph these are the general ideas we covered so far genes with the largest variation between cells will have the most influence on the principal components that is to say genes highly expressed in some cells and not expressed in others will have a lot of variation and influence on the principal components the first principal component captures the most variation in the data the second principal component captures the second most variation in the data you can use the original data and the first two principle components to get XY values to plot on a figure cells with similar transcription patterns will cluster together and just like they say on TV but wait there's more we can use the graph to identify key genes do you see how cells are spread out left and right above and below if we wanted to find out which genes had a big influence in putting dermal cells on the left side of the graph and neuros cells on the right side we could look at the influence scores on principal component number one and if we wanted to find out which genes help distinguish blood cells from neural and dermal cells we could look at the influence scores in principal component number two but wait there's even more yes there's a couple Diagnostics you can do if you're drawing your own PCA plot these are ways you can tell if your PCA is actually worth anything one diagnostic plot is called a scree plot where you plot how much variation each principle component can account for what you want to see in this diagnostic plot is that most of the variation is accounted for by the first two principal components lastly here's a terminology alert the ways I've been describing things has been fairly intuitive but there's actually a lot of technical jargon for principal component analysis the numbers that describe the weights for the importance for each gene to principal component one I've just been calling influence or weights but in PCA terminology those weights are called loadings an array of loadings is called an igen vector and that's all there is to PCA so tune in next time for another exciting stat Quest"
O33LgXNk87g,2015-07-01T01:37:00.000000,How to puree garlic,all right to puree a clove of garlic uh first thing you do is you smash it so it's all flat and then you chop it a little bit and then what I like to do is sprinkle a large pinch of salt on top and then just spread it out with the blade a little little more chopping but it's mostly spreading out there you go that is pureed garlic
2AQKmw14mHM,2015-02-03T14:48:20.000000,"R-squared, Clearly Explained!!!","Stat! Quest! Stat! Quest! Stat! Quest! StatQuest! StatQuest is brought to you by the friendly people in the genetics department at the University of North Carolina at Chapel Hill. Hello and welcome to StatQuest in this video we're going to talk about R-squared. R-squared is a metric of correlation that is easy to compute and intuitive to interpret. Most of us are already familiar with correlation and the standard metric of it plain old 'r'. Correlation values that are close to 1 or negative 1 are good and tell you that two quantitative variables for example weight and size are strongly related. Correlation values close to zero are lame. Some of you may be asking why should we care about R-squared; we already have regular 'r'. Some of you might just be asking what is R-squared. R-squared is very similar to its hipper cousin 'r', but interpretation is easier. For example it's not obvious that when 'r'  equals 0.7 that's twice as good a correlation as when 'r' equals 0.5. However R-squared equals 0.7 is what it looks like it's 1.4 times as good as R-squared equals 0.5. The other thing that I like about R-squared is that it's easy and intuitive to calculate. Let's start with an example. Here were plotting Mouse weight on the y-axis with high weights towards the top and low weights towards the bottom and mouse identification numbers on the x-axis with ID numbers 1 through 7. We can calculate the mean or average of the mouse weights and plot it as a line that spans the graph. We can calculate the variation of the data around this mean as the sum of the squared differences of the weight for each mouse 'i', where 'i' as an individual mouse represented by a red dot, and the mean. The difference between each data point is squared so that the points below the mean don't cancel out the points above the mean. Now, what if instead of ordering our mice by their identification number we ordered them by their size? Instead of using identification number on the x-axis we have mouse size, with the smallest size on the left side and the largest size on the right side. All we have done is reorder the data on the x-axis, the mean and variation are the exact same as before. Here we show the mean again as a black bar that spans the graph in the exact same location as it was before. Also the distances between the dots and the line have not changed, just the order of the dots. Here's a question for you: Given that we know an individual Mouse's size, is the mean, or average weight, the best way to predict that individual Mouse's weight? Well the answer is 'no'. We can do way better. All we have to do is fit a line to the data. Now we can predict weight with our line. You tell me you have a large Mouse, I can look at my line and make a good guess about the weight. Here's another question: Does the blue line that we just drew fit the data better than the mean? If so how much better? By eye, it looks like the blue line fits the data better than the mean. How do we quantify that difference? R-squared. In the bottom of the graph I've drawn the equation for R-squared. We're going to walk through it one step at a time. The first part of the equation is just the variation around the mean. We already calculated that. It's just the sum of the squared differences of the actual data values from the mean. The second part of the equation is the variation around our new blue line. This is calculated in a very similar way. Here, we just want the sum of the squared differences between the actual data points and our new blue line. The numerator, which is the difference between the variation around the mean and the variation around the blue line, is then divided by the variation around the mean. This makes R-squared range from zero to one, because the variation around the line will never be greater than the variation around the mean and it will never be less than zero. This division also makes our squared a percentage and we'll talk more about that in just a second. Now, we'll walk through an example where we calculate things one step at a time. First, we'll start with the variation around the mean. In this case that equals 32. The variation around the blue line is only six, which is what we suspected since it appears to fit the data much better. Once we've calculated the variation around the mean and the variation around our blue line we can plug these values in to our formula for R-squared. After plugging in our values, we get R-squared equals 32 minus 6 over 32. After subtracting 6 from 32, we get 26. Doing the division, 26 divided by 32, gives us 0.81, or 81%. This means that there is 81% less variation around the line than the mean. In other words, the size-weight relationship accounts for 81% of the total variation. This means that most of the variation in the data is explained by the size-weight relationship. Here's another example. In this example we're comparing two possibly uncorrelated variables. On the y-axis we have mouse weight again, but on the x-axis we now have ""time spent sniffing a rock"". Like before, we calculate the variation around the mean and just like before we got 32. However, this time when we calculated the variation around the blue line, we got a much larger value, 30. Now we just plug those values into our formula for R-squared. By doing the math we see that R-squared equals 0.06 or 6%. Thus there is only 6% less variation around the line than the mean. In other words, the sniff-weight relationship accounts for only 6% of the total variation. This means that hardly any of the variation in the data is explained by the sniff-weight relationship. Now, when someone says ""The statistically significant R-squared was 0.9"", you can think to yourself ""Very good! The relationship between the two variables explains 90% of the variation in the data."" And when someone else says, ""The statistically significant R-squared was 0.01."" You can think to yourself, ""Dag! Who cares if that relationship is significant? It only accounts for 1% of the variation in the data. Something else must explain the remaining 99%."" What about plain old 'r'? How is it related to R-squared? R-squared is just the square of 'r' Now, when someone says ""The statistically significant 'r' was 0.9"", and we're talking about just plain old 'r', you can think to yourself ""0.9 times 0.9 equals 0.81. Very good! The relationship between the two variables explains 81% of the variation in the data."" And when someone else says ""The statistically significant 'r'"", that's plain old 'r', ""was 0.5."" You can think to yourself ""0.5 times 0.5 equals 0.25. The relationship accounts for 25% of the variation in the data. That's good if there are a million other things accounting for the remaining 75% and bad if there's only one thing."" I like R-squared more than just plain old 'r' because it's easier to interpret. Here's an example: How much better is 'r' equals 0.7 then 'r' equals 0.5? Well if we convert those numbers to R-squared, we see that when R-squared equals 0.7 squared it actually equals 0.5, which means 50% of the original variation is explained by the relationship. When R-squared equals 0.5 squared which equals 0.25, we see that only 25% of the original variation is explained by the relationship. With R-squared it's easy to see that the first correlation is twice as good as the second. Explaining 50% of the original variation is twice as good as only explaining 25% of the original variation. That said our squared does not indicate the direction of the correlation because squared numbers are never negative. If the direction of the correlation isn't obvious, you can say ""the two variables were positively or negatively correlated with R squared equals dot dot dot"", whatever that value may be. These are the two main ideas for R-squared. R-squared is the percentage of variation explained by the relationship between two variables, and also, if someone gives you a value for plain old are just square it in your head, you'll understand what's going on a whole lot better. We've reached the end of our StatQuest. Tune in next time for an exciting adventure into the land of statistics."
Sn-keVh_ERw,2014-09-28T16:12:29.000000,onion-dice,this is how I dice an onion uh I cut along its length I cut it in half first and then I cut along its length like this I don't go all the way to the stem or all the way to the end and then I just cut kind of perpendicular to my original Cuts along the length and you'll notice that the way the onion is layered just sort of automatically dices it for you so we've got a nice D ice without having to do very much work
yMYcH8-OfJk,2014-08-24T21:40:26.000000,Cutting Butter,just grate the butter and when it starts to get a little sticky in my hand i'll put it back in but i can also stir stir the grated butter into the flour to keep it from clumping it'll clump a little bit but it's not that big a deal but uh let's get a little more flour on the butter
